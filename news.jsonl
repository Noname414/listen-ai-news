{"id": "2505.10561v1", "url": "http://arxiv.org/abs/2505.10561v1", "title": "T2A-Feedback: Improving Basic Capabilities of Text-to-Audio Generation via Fine-grained AI Feedback", "summary": "Text-to-audio (T2A) generation has achieved remarkable progress in generating\na variety of audio outputs from language prompts. However, current\nstate-of-the-art T2A models still struggle to satisfy human preferences for\nprompt-following and acoustic quality when generating complex multi-event\naudio. To improve the performance of the model in these high-level\napplications, we propose to enhance the basic capabilities of the model with AI\nfeedback learning. First, we introduce fine-grained AI audio scoring pipelines\nto: 1) verify whether each event in the text prompt is present in the audio\n(Event Occurrence Score), 2) detect deviations in event sequences from the\nlanguage description (Event Sequence Score), and 3) assess the overall acoustic\nand harmonic quality of the generated audio (Acoustic&Harmonic Quality). We\nevaluate these three automatic scoring pipelines and find that they correlate\nsignificantly better with human preferences than other evaluation metrics. This\nhighlights their value as both feedback signals and evaluation metrics.\nUtilizing our robust scoring pipelines, we construct a large audio preference\ndataset, T2A-FeedBack, which contains 41k prompts and 249k audios, each\naccompanied by detailed scores. Moreover, we introduce T2A-EpicBench, a\nbenchmark that focuses on long captions, multi-events, and story-telling\nscenarios, aiming to evaluate the advanced capabilities of T2A models. Finally,\nwe demonstrate how T2A-FeedBack can enhance current state-of-the-art audio\nmodel. With simple preference tuning, the audio generation model exhibits\nsignificant improvements in both simple (AudioCaps test set) and complex\n(T2A-EpicBench) scenarios.", "authors": ["Zehan Wang", "Ke Lei", "Chen Zhu", "Jiawei Huang", "Sashuai Zhou", "Luping Liu", "Xize Cheng", "Shengpeng Ji", "Zhenhui Ye", "Tao Jin", "Zhou Zhao"], "published_date": "2025-05-15", "title_zh": "T2A-Feedback：透過細粒度AI回饋提升文字轉語音生成之基礎能力", "summary_zh": "現今的文字轉語音模型在生成複雜多事件音訊時，難以完全符合人類對提示遵循度和音訊品質的期望。本研究提出利用AI回饋學習來提升模型基礎能力。首先，建立了精細的AI音訊評分流程，包含事件發生評分、事件序列評分，以及音訊和諧品質評分。這些評分流程能更準確地反映人類偏好。接著，利用這些評分流程，構建了包含41k個提示和249k個音訊的大型音訊偏好數據集T2A-FeedBack，並推出專注於長描述、多事件和故事敘述場景的評測基準T2A-EpicBench。實驗證明，透過簡單的偏好調整，T2A-FeedBack能有效提升現有音訊生成模型在簡單和複雜場景下的表現。", "audio": "audios/2505.10561v1.mp3", "timestamp": "2025-05-18T23:05:41.112280"}
{"id": "2505.10556v1", "url": "http://arxiv.org/abs/2505.10556v1", "title": "An AI-driven framework for the prediction of personalised health response to air pollution", "summary": "Air pollution poses a significant threat to public health, causing or\nexacerbating many respiratory and cardiovascular diseases. In addition, climate\nchange is bringing about more extreme weather events such as wildfires and\nheatwaves, which can increase levels of pollution and worsen the effects of\npollution exposure. Recent advances in personal sensing have transformed the\ncollection of behavioural and physiological data, leading to the potential for\nnew improvements in healthcare. We wish to capitalise on this data, alongside\nnew capabilities in AI for making time series predictions, in order to monitor\nand predict health outcomes for an individual. Thus, we present a novel\nworkflow for predicting personalised health responses to pollution by\nintegrating physiological data from wearable fitness devices with real-time\nenvironmental exposures. The data is collected from various sources in a secure\nand ethical manner, and is used to train an AI model to predict individual\nhealth responses to pollution exposure within a cloud-based, modular framework.\nWe demonstrate that the AI model -- an Adversarial Autoencoder neural network\nin this case -- accurately reconstructs time-dependent health signals and\ncaptures nonlinear responses to pollution. Transfer learning is applied using\ndata from a personal smartwatch, which increases the generalisation abilities\nof the AI model and illustrates the adaptability of the approach to real-world,\nuser-generated data.", "authors": ["Nazanin Zounemat Kermani", "Sadjad Naderi", "Claire H. Dilliway", "Claire E. Heaney", "Shrreya Behll", "Boyang Chen", "Hisham Abubakar-Waziri", "Alexandra E. Porter", "Marc Chadeau-Hyam", "Fangxin Fang", "Ian M. Adcock", "Kian Fan Chung", "Christopher C. Pain"], "published_date": "2025-05-15", "title_zh": "一個AI驅動的框架，用於預測個人化健康對空氣污染的反應", "summary_zh": "本研究提出一個新的AI框架，結合穿戴裝置收集的生理數據和即時環境暴露數據，來預測個人對空氣污染的健康反應。透過雲端架構訓練AI模型，精準重建時間序列健康訊號，並捕捉非線性污染反應。研究使用個人智慧手錶的數據進行遷移學習，提升模型泛化能力，展現此方法在真實世界使用者數據中的適應性。", "audio": "audios/2505.10556v1.mp3", "timestamp": "2025-05-18T23:05:46.038004"}
{"id": "2505.10527v1", "url": "http://arxiv.org/abs/2505.10527v1", "title": "WorldPM: Scaling Human Preference Modeling", "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.", "authors": ["Binghai Wang", "Runji Lin", "Keming Lu", "Le Yu", "Zhenru Zhang", "Fei Huang", "Chujie Zheng", "Kai Dang", "Yang Fan", "Xingzhang Ren", "An Yang", "Binyuan Hui", "Dayiheng Liu", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang", "Bowen Yu", "Jingren Zhou", "Junyang Lin"], "published_date": "2025-05-15", "title_zh": "WorldPM：擴展人類偏好建模", "summary_zh": "受到語言模型擴展定律的啟發，我們發現類似的定律也存在於偏好建模中。我們提出 WorldPM (World Preference Modeling) 以強調這種擴展潛力，它統一了人類偏好的表達。我們收集了來自公共論壇的偏好數據，涵蓋不同的用戶群體，並使用1500萬規模的數據和從15億到720億參數的模型進行了廣泛的訓練。結果顯示，對抗性指標（識別欺騙性特徵的能力）隨著訓練數據和模型大小的增加而持續提升；客觀性指標（有明確答案的客觀知識）在大語言模型中展現出湧現行為，突顯了 WorldPM 的可擴展性；主觀性指標則未顯示出擴展趨勢。實驗驗證了 WorldPM 作為偏好微調基礎的有效性。在七個基準測試的 20 個子任務中，WorldPM 顯著提高了跨不同規模人類偏好數據集（7K、100K 和 800K 樣本）的泛化性能，在許多關鍵子任務上的性能提升超過 5%。將 WorldPM 整合到我們的內部 RLHF 流程中，我們觀察到內部和公共評估集的顯著改進，內部評估的增益顯著提高了 4% 到 8%。", "audio": "audios/2505.10527v1.mp3", "timestamp": "2025-05-18T23:05:53.910110"}
{"id": "2505.10525v1", "url": "http://arxiv.org/abs/2505.10525v1", "title": "Sobolev and quasiconformal distortion of intermediate dimension with applications to conformal dimension", "summary": "We study the distortion of intermediate dimension under supercritical Sobolev\nmappings and also under quasiconformal or quasisymmetric homeomorphisms. In\nparticular, we extend to the setting of intermediate dimensions both the\nGehring--V\\\"ais\\\"al\\\"a theorem on dilatation-dependent quasiconformal\ndistortion of dimension and Kovalev's theorem on the nonexistence of metric\nspaces with conformal dimension strictly between zero and one. Applications\ninclude new contributions to the quasiconformal classification of Euclidean\nsets and a new sufficient condition for the vanishing of conformal box-counting\ndimension. We illustrate our conclusions with specific consequences for\nBedford--McMullen carpets, samples of Mandelbrot percolation, and product sets\ncontaining a polynomially convergent sequence factor.", "authors": ["Jonathan M. Fraser", "Jeremy T. Tyson"], "published_date": "2025-05-15", "title_zh": "中間維度的Sobolev及擬共形扭曲，及其於共形維度的應用", "summary_zh": "本研究探討超臨界Sobolev映射及擬共形/擬對稱同胚變換下，中間維度的扭曲現象。我們將Gehring-V\"ais\"al\"a關於膨脹係數與擬共形維度扭曲的定理，以及Kovalev關於不存在共形維度介於0與1之間的度量空間的定理，推廣到中間維度的情境。研究成果應用於歐幾里得集合的擬共形分類，並提出新的充分條件判斷共形盒計數維度是否消失。我們以Bedford-McMullen地毯、Mandelbrot滲流樣本，以及包含多項式收斂序列因子的乘積集合為例，闡述了我們的結論。", "audio": "audios/2505.10525v1.mp3", "timestamp": "2025-05-18T23:05:58.394013"}
{"id": "2505.10496v1", "url": "http://arxiv.org/abs/2505.10496v1", "title": "CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs", "summary": "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/", "authors": ["Raman Dutt", "Pedro Sanchez", "Yongchen Yao", "Steven McDonagh", "Sotirios A. Tsaftaris", "Timothy Hospedales"], "published_date": "2025-05-15", "title_zh": "CheXGenBench：合成胸腔X光片逼真度、隱私與效用之統一評估基準", "summary_zh": "CheXGenBench是一個綜合性的評估框架，用來衡量合成胸腔X光片生成模型的品質。它同時考量逼真度、潛在隱私風險以及在臨床上的實用性。 研究團隊發現現有的評估方法存在缺陷，導致結果不一致且缺乏參考價值。 因此，他們設計了一個標準化的評估基準，包含超過20個量化指標，並使用11種主流的文字轉圖像模型進行測試。研究結果揭示了現有評估方法的不足。 此外，研究團隊也釋出一個高品質的合成胸腔X光片資料集，SynthCheX-75K，包含7萬5千張圖像，以支持醫學AI領域的進一步研究。", "audio": "audios/2505.10496v1.mp3", "timestamp": "2025-05-18T23:06:03.275100"}
{"id": "2505.10490v1", "url": "http://arxiv.org/abs/2505.10490v1", "title": "Campus AI vs Commercial AI: A Late-Breaking Study on How LLM As-A-Service Customizations Shape Trust and Usage Patterns", "summary": "As the use of Large Language Models (LLMs) by students, lecturers and\nresearchers becomes more prevalent, universities - like other organizations -\nare pressed to develop coherent AI strategies. LLMs as-a-Service (LLMaaS) offer\naccessible pre-trained models, customizable to specific (business) needs. While\nmost studies prioritize data, model, or infrastructure adaptations (e.g., model\nfine-tuning), we focus on user-salient customizations, like interface changes\nand corporate branding, which we argue influence users' trust and usage\npatterns. This study serves as a functional prequel to a large-scale field\nstudy in which we examine how students and employees at a German university\nperceive and use their institution's customized LLMaaS compared to ChatGPT. The\ngoals of this prequel are to stimulate discussions on psychological effects of\nLLMaaS customizations and refine our research approach through feedback. Our\nforthcoming findings will deepen the understanding of trust dynamics in LLMs,\nproviding practical guidance for organizations considering LLMaaS deployment.", "authors": ["Leon Hannig", "Annika Bush", "Meltem Aksoy", "Steffen Becker", "Greta Ontrup"], "published_date": "2025-05-15", "title_zh": "校園AI vs. 商業AI：一項關於LLM即服務客製化如何形塑信任與使用模式的最新研究", "summary_zh": "隨著大學生、講師和研究人員廣泛使用大型語言模型（LLM），各大學正面臨制定完善AI策略的需求。本研究探討使用者可見的LLM客製化，例如介面修改和品牌形象，如何影響使用者對大學客製LLM的信任感和使用模式，並與ChatGPT進行比較。這項前期研究旨在引發關於LLM客製化心理效應的討論，並為後續的大規模實地研究提供方向，最終目標是深入了解LLM中的信任動態，為考慮部署LLMaaS的組織提供實用建議。", "audio": "audios/2505.10490v1.mp3", "timestamp": "2025-05-18T23:14:13.567508"}
{"id": "2505.10472v1", "url": "http://arxiv.org/abs/2505.10472v1", "title": "Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI", "summary": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools.", "authors": ["Agnik Saha", "Victoria Churchill", "Anny D. Rodriguez", "Ugur Kursuncu", "Muhammed Y. Idris"], "published_date": "2025-05-15", "title_zh": "用於癌症溝通的大型語言模型：評估生成式人工智慧中的語言品質、安全性和可及性", "summary_zh": "關於乳癌和子宮頸癌的有效溝通仍然是個健康挑戰。本研究評估了大型語言模型（LLMs）生成準確、安全且易於理解的癌症資訊的能力。研究發現，通用LLMs在語言品質和感染力方面表現較好，而醫療LLMs則在溝通可及性方面更勝一籌。然而，醫療LLMs也更容易產生潛在危害、毒性和偏見。總體而言，研究強調了在設計模型時需要有針對性地改進，特別是在降低危害和偏見方面，從而創建更安全、有效且可信賴的AI健康資訊工具。", "audio": "audios/2505.10472v1.mp3", "timestamp": "2025-05-18T23:14:30.270685"}
{"id": "2505.10468v1", "url": "http://arxiv.org/abs/2505.10468v1", "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge", "summary": "This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "published_date": "2025-05-15", "title_zh": "AI代理程式 vs. 具代理能力AI：概念分類、應用與挑戰", "summary_zh": "本研究區分了AI代理程式（AI Agents）和具代理能力AI（Agentic AI）兩個概念，並建立了結構化的分類體系。AI代理程式著重於利用大型語言模型（LLMs）和大型圖像模型（LIMs）進行狹窄、特定任務的自動化。而具代理能力AI則是一種範式轉移，強調多代理程式協作、動態任務分解、持久記憶和協調自主。研究比較了兩者的架構演進、運作機制、互動方式和自主程度，並探討了各自在客戶支援、研究自動化等領域的應用。同時，也分析了幻覺、脆弱性、突發行為和協調失敗等挑戰，並提出了針對性的解決方案。本研究旨在為開發穩健、可擴展且可解釋的AI代理程式和具代理能力AI系統提供清晰的指引。", "audio": "audios/2505.10468v1.mp3", "timestamp": "2025-05-18T15:29:49.604850"}
{"id": "2505.10454v1", "url": "http://arxiv.org/abs/2505.10454v1", "title": "Emotion-sensitive Explanation Model", "summary": "Explainable AI (XAI) research has traditionally focused on rational users,\naiming to improve understanding and reduce cognitive biases. However, emotional\nfactors play a critical role in how explanations are perceived and processed.\nPrior work shows that prior and task-generated emotions can negatively impact\nthe understanding of explanation. Building on these insights, we propose a\nthree-stage model for emotion-sensitive explanation grounding: (1) emotional or\nepistemic arousal, (2) understanding, and (3) agreement. This model provides a\nconceptual basis for developing XAI systems that dynamically adapt explanation\nstrategies to users emotional states, ultimately supporting more effective and\nuser-centered decision-making.", "authors": ["Christian Schütze", "Birte Richter", "Britta Wrede"], "published_date": "2025-05-15", "title_zh": "情緒敏感的解釋模型", "summary_zh": "傳統的可解釋AI(XAI)研究主要關注理性用戶，旨在提升理解和減少認知偏差。然而，情緒在解釋的感知和處理中扮演關鍵角色。本文提出一個三階段的情緒敏感解釋模型，包含情緒激發、理解和認同。此模型為開發能根據用戶情緒狀態動態調整解釋策略的XAI系統提供概念基礎，最終支持更有效且以用戶為中心的決策。", "audio": "audios/2505.10454v1.mp3", "timestamp": "2025-05-18T15:43:10.566009"}
{"id": "2505.10453v1", "url": "http://arxiv.org/abs/2505.10453v1", "title": "Vision language models have difficulty recognizing virtual objects", "summary": "Vision language models (VLMs) are AI systems paired with both language and\nvision encoders to process multimodal input. They are capable of performing\ncomplex semantic tasks such as automatic captioning, but it remains an open\nquestion about how well they comprehend the visuospatial properties of scenes\ndepicted in the images they process. We argue that descriptions of virtual\nobjects -- objects that are not visually represented in an image -- can help\ntest scene comprehension in these AI systems. For example, an image that\ndepicts a person standing under a tree can be paired with the following prompt:\nimagine that a kite is stuck in the tree. VLMs that comprehend the scene should\nupdate their representations and reason sensibly about the spatial relations\nbetween all three objects. We describe systematic evaluations of\nstate-of-the-art VLMs and show that their ability to process virtual objects is\ninadequate.", "authors": ["Tyler Tran", "Sangeet Khemlani", "J. G. Trafton"], "published_date": "2025-05-15", "title_zh": "視覺語言模型難以識別虛擬物件", "summary_zh": "視覺語言模型（VLMs）結合了語言和視覺編碼器，可以處理多模態輸入，例如自動生成圖片描述。然而，它們對圖像中場景的視覺空間理解程度仍然是個問題。研究發現，當提示VLMs想像圖像中不存在的虛擬物件（例如：一棵樹下站著一個人，提示想像樹上卡著風箏），它們難以合理更新場景表徵並推理物件之間的空間關係。這表明目前的VLMs在處理虛擬物件方面的能力不足。", "audio": "audios/2505.10453v1.mp3", "timestamp": "2025-05-18T15:52:48.305855"}
{"id": "2505.10427v1", "url": "http://arxiv.org/abs/2505.10427v1", "title": "Influence of prior and task generated emotions on XAI explanation retention and understanding", "summary": "The explanation of AI results and how they are received by users is an\nincreasingly active research field. However, there is a surprising lack of\nknowledge about how social factors such as emotions affect the process of\nexplanation by a decision support system (DSS). While previous research has\nshown effects of emotions on DSS supported decision-making, it remains unknown\nin how far emotions affect cognitive processing during an explanation. In this\nstudy, we, therefore, investigated the influence of prior emotions and\ntask-related arousal on the retention and understanding of explained feature\nrelevance. To investigate the influence of prior emotions, we induced happiness\nand fear prior to the decision support interaction. Before emotion induction,\nuser characteristics to assess their risk type were collected via a\nquestionnaire. To identify emotional reactions to the explanations of the\nrelevance of different features, we observed heart rate variability (HRV),\nfacial expressions, and self-reported emotions of the explainee while observing\nand listening to the explanation and assessed their retention of the features\nas well as their influence on the outcome of the decision task. Results\nindicate that (1) task-unrelated prior emotions do not affected the ratantion\nbut may affect the understanding of the relevance of certain features in the\nsense of an emotion-induced confirmation bias, (2) certain features related to\npersonal attitudes yielded arousal in individual participants, (3) this arousal\naffected the understanding of these variables.", "authors": ["Birte Richter", "Christian Schütze", "Anna Aksonova", "Britta Wrede"], "published_date": "2025-05-15", "title_zh": "先前情緒與任務產生情緒對XAI解釋保留與理解的影響", "summary_zh": "理解AI決策的解釋越來越重要。本研究探討情緒如何影響使用者對AI解釋的理解與記憶。我們透過誘導快樂與恐懼情緒，以及監測心率、面部表情等生理反應，觀察情緒如何影響使用者理解AI解釋中的特徵重要性。研究發現，先前情緒可能影響使用者對特定特徵的理解，產生情緒誘導的確認偏誤；某些與個人態度相關的特徵會引起使用者的情緒反應，進而影響他們對這些特徵的理解。", "audio": "audios/2505.10427v1.mp3", "timestamp": "2025-05-18T16:06:13.256765"}
{"id": "2505.10426v1", "url": "http://arxiv.org/abs/2505.10426v1", "title": "Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility", "summary": "The legal compliance and safety of different Human-in-the-loop (HITL) setups\nfor AI can vary greatly. This manuscript aims to identify new ways of choosing\nbetween such setups, and shows that there is an unavoidable trade-off between\nthe attribution of legal responsibility and the technical explainability of AI.\nWe begin by using the notion of oracle machines from computability theory to\nformalise different HITL setups, distinguishing between trivial human\nmonitoring, single endpoint human action, and highly involved interaction\nbetween the human(s) and the AI. These correspond to total functions, many-one\nreductions, and Turing reductions respectively. A taxonomy categorising HITL\nfailure modes is then presented, highlighting the limitations on what any HITL\nsetup can actually achieve. Our approach then identifies oversights from UK and\nEU legal frameworks, which focus on certain HITL setups which may not always\nachieve the desired ethical, legal, and sociotechnical outcomes. We suggest\nareas where the law should recognise the effectiveness of different HITL setups\nand assign responsibility in these contexts, avoiding unnecessary and\nunproductive human \"scapegoating\". Overall, we show how HITL setups involve\nmany technical design decisions, and can be prone to failures which are often\nout of the humans' control. This opens up a new analytic perspective on the\nchallenges arising in the creation of HITL setups, helping inform AI developers\nand lawmakers on designing HITL to better achieve their desired outcomes.", "authors": ["Maurice Chiodo", "Dennis Müller", "Paul Siewert", "Jean-Luc Wetherall", "Zoya Yasmine", "John Burden"], "published_date": "2025-05-15", "title_zh": "人機迴路正式化：計算歸約、失效模式與法律-道德責任", "summary_zh": "本研究探討不同人工智慧人機迴路(HITL)架構在法律合規和安全上的差異，指出法律責任歸屬與AI技術可解釋性之間存在不可避免的權衡。我們利用計算理論的預言機概念，形式化不同HITL架構，並建立HITL失效模式分類法。研究揭示了現有法律框架的不足，並建議如何根據不同HITL架構的有效性來分配責任，避免不必要的“替罪羊”效應。 總而言之，本研究揭示了HITL架構設計的複雜性以及潛在的失效風險，為AI開發者和立法者設計更有效的HITL架構提供了新的分析視角。", "audio": "audios/2505.10426v1.mp3", "timestamp": "2025-05-18T16:20:24.598334"}
{"id": "2505.10405v1", "url": "http://arxiv.org/abs/2505.10405v1", "title": "Visual Fidelity Index for Generative Semantic Communications with Critical Information Embedding", "summary": "Generative semantic communication (Gen-SemCom) with large artificial\nintelligence (AI) model promises a transformative paradigm for 6G networks,\nwhich reduces communication costs by transmitting low-dimensional prompts\nrather than raw data. However, purely prompt-driven generation loses\nfine-grained visual details. Additionally, there is a lack of systematic\nmetrics to evaluate the performance of Gen-SemCom systems. To address these\nissues, we develop a hybrid Gen-SemCom system with a critical information\nembedding (CIE) framework, where both text prompts and semantically critical\nfeatures are extracted for transmissions. First, a novel approach of semantic\nfiltering is proposed to select and transmit the semantically critical features\nof images relevant to semantic label. By integrating the text prompt and\ncritical features, the receiver reconstructs high-fidelity images using a\ndiffusion-based generative model. Next, we propose the generative visual\ninformation fidelity (GVIF) metric to evaluate the visual quality of the\ngenerated image. By characterizing the statistical models of image features,\nthe GVIF metric quantifies the mutual information between the distorted\nfeatures and their original counterparts. By maximizing the GVIF metric, we\ndesign a channel-adaptive Gen-SemCom system that adaptively control the volume\nof features and compression rate according to the channel state. Experimental\nresults validate the GVIF metric's sensitivity to visual fidelity, correlating\nwith both the PSNR and critical information volume. In addition, the optimized\nsystem achieves superior performance over benchmarking schemes in terms of\nhigher PSNR and lower FID scores.", "authors": ["Jianhao Huang", "Qunsong Zeng", "Kaibin Huang"], "published_date": "2025-05-15", "title_zh": "具有關鍵資訊嵌入的生成式語義通訊視覺保真度指標", "summary_zh": "研究提出一種混合生成式語義通訊系統，透過嵌入關鍵資訊框架，同時傳輸文字提示和語義上重要的圖像特徵，以解決純粹提示驅動的生成導致細節丟失的問題。為評估系統性能，提出生成式視覺資訊保真度（GVIF）指標，以量化失真特徵與原始特徵之間的互信息。實驗結果驗證了GVIF指標對視覺保真度的敏感性，並設計了一種基於GVIF最大化的通道自適應系統，能夠根據通道狀態調整特徵數量和壓縮率，進而提升重建圖像的品質。", "audio": "audios/2505.10405v1.mp3", "timestamp": "2025-05-18T17:14:39.044337"}
{"id": "2505.10377v1", "url": "http://arxiv.org/abs/2505.10377v1", "title": "The Art of Two-Round Voting", "summary": "We study the voting problem with two alternatives where voters' preferences\ndepend on a not-directly-observable state variable. While equilibria in the\none-round voting mechanisms lead to a good decision, they are usually hard to\ncompute and follow. We consider the two-round voting mechanism where the first\nround serves as a polling stage and the winning alternative only depends on the\noutcome of the second round. We show that the two-round voting mechanism is a\npowerful tool for making collective decisions. Firstly, every (approximated)\nequilibrium in the two-round voting mechanisms (asymptotically) leads to the\ndecision preferred by the majority as if the state of the world were revealed\nto the voters. Moreover, there exist natural equilibria in the two-round game\nfollowing intuitive behaviors such as informative voting, sincere voting\n[Austen-Smith and Banks, 1996], and the surprisingly popular strategy [Prelec\net al., 2017]. This sharply contrasts with the one-round voting mechanisms in\nthe previous literature, where no simple equilibrium is known. Finally, we show\nthat every equilibrium in the standard one-round majority vote mechanism gives\nan equilibrium in the two-round mechanisms that is not more complicated than\nthe one-round equilibrium. Therefore, the two-round voting mechanism provides a\nnatural equilibrium in every instance, including those where one-round voting\nfails to have a natural solution, and it can reach an informed majority\ndecision whenever one-round voting can. Our experiments on generative AI voters\nalso imply that two-round voting leads to the correct outcome more often than\none-round voting under some circumstances.", "authors": ["Qishen Han", "Grant Schoenebeck", "Biaoshuai Tao", "Lirong Xia"], "published_date": "2025-05-15", "title_zh": "兩輪投票的藝術", "summary_zh": "研究選民偏好取決於不可直接觀察狀態變數的雙選項投票問題。單輪投票機制雖能做出好的決策，但其均衡通常難以計算和遵循。論文探討兩輪投票機制，首輪作為民調，勝負僅取決於第二輪結果。研究表明兩輪投票機制是強大的集體決策工具。首先，兩輪投票機制的每個（近似）均衡（漸近地）導向多數人偏好的決策，如同世界狀態已被揭露給選民。此外，存在自然的兩輪賽局均衡，遵循直觀行為，例如資訊性投票、真誠投票和令人驚訝的流行策略。這與先前文獻中單輪投票機制形成鮮明對比，因為單輪投票機制沒有已知的簡單均衡。最後，研究表明標準單輪多數投票機制中的每個均衡都給出兩輪機制中的均衡，且不比單輪均衡更複雜。因此，兩輪投票機制在每個實例中都提供了一種自然的均衡，包括那些單輪投票未能產生自然解決方案的實例，並且只要單輪投票可以，它就可以達成知情的多数人决策。我們在生成式AI選民上的實驗也表明，在某些情況下，兩輪投票比單輪投票更有可能導致正確的結果。", "audio": "audios/2505.10377v1.mp3", "timestamp": "2025-05-18T18:23:30.321298"}
{"id": "2505.10375v1", "url": "http://arxiv.org/abs/2505.10375v1", "title": "Are Sparse Autoencoders Useful for Java Function Bug Detection?", "summary": "Software vulnerabilities such as buffer overflows and SQL injections are a\nmajor source of security breaches. Traditional methods for vulnerability\ndetection remain essential but are limited by high false positive rates,\nscalability issues, and reliance on manual effort. These constraints have\ndriven interest in AI-based approaches to automated vulnerability detection and\nsecure code generation. While Large Language Models (LLMs) have opened new\navenues for classification tasks, their complexity and opacity pose challenges\nfor interpretability and deployment. Sparse Autoencoder offer a promising\nsolution to this problem. We explore whether SAEs can serve as a lightweight,\ninterpretable alternative for bug detection in Java functions. We evaluate the\neffectiveness of SAEs when applied to representations from GPT-2 Small and\nGemma 2B, examining their capacity to highlight buggy behaviour without\nfine-tuning the underlying LLMs. We found that SAE-derived features enable bug\ndetection with an F1 score of up to 89%, consistently outperforming fine-tuned\ntransformer encoder baselines. Our work provides the first empirical evidence\nthat SAEs can be used to detect software bugs directly from the internal\nrepresentations of pretrained LLMs, without any fine-tuning or task-specific\nsupervision.", "authors": ["Rui Melo", "Claudia Mamede", "Andre Catarino", "Rui Abreu", "Henrique Lopes Cardoso"], "published_date": "2025-05-15", "title_zh": "稀疏自編碼器對Java函式錯誤偵測有用嗎？", "summary_zh": "軟體漏洞，如緩衝區溢位和SQL注入，是資安漏洞的主要來源。傳統的漏洞偵測方法雖然重要，但誤報率高，擴展性差，且依賴人工。這促使人們對基於AI的自動漏洞偵測和安全程式碼生成產生興趣。大型語言模型（LLM）雖然為分類任務開闢了新途徑，但其複雜性和不透明性對可解釋性和部署構成了挑戰。稀疏自編碼器（SAE）為此問題提供了一個有希望的解決方案。本文探討了SAE是否可以作為Java函式中錯誤偵測的輕量級、可解釋的替代方案。我們評估了將SAE應用於GPT-2 Small和Gemma 2B的表示時的有效性，檢驗了它們在不微調底層LLM的情況下突出顯示錯誤行為的能力。我們發現，SAE衍生的特徵能夠以高達89%的F1分數進行錯誤偵測，始終優於微調後的Transformer編碼器基準線。我們的研究提供了第一個經驗證據，證明SAE可以用於直接從預訓練LLM的內部表示中檢測軟體錯誤，而無需任何微調或特定於任務的監督。", "audio": "audios/2505.10375v1.mp3", "timestamp": "2025-05-18T19:13:34.703921"}
{"id": "2505.10360v1", "url": "http://arxiv.org/abs/2505.10360v1", "title": "FactsR: A Safer Method for Producing High Quality Healthcare Documentation", "summary": "There are now a multitude of AI-scribing solutions for healthcare promising\nthe utilization of large language models for ambient documentation. However,\nthese AI scribes still rely on one-shot, or few-shot prompts for generating\nnotes after the consultation has ended, employing little to no reasoning. This\nrisks long notes with an increase in hallucinations, misrepresentation of the\nintent of the clinician, and reliance on the proofreading of the clinician to\ncatch errors. A dangerous combination for patient safety if vigilance is\ncompromised by workload and fatigue. In this paper, we introduce a method for\nextracting salient clinical information in real-time alongside the healthcare\nconsultation, denoted Facts, and use that information recursively to generate\nthe final note. The FactsR method results in more accurate and concise notes by\nplacing the clinician-in-the-loop of note generation, while opening up new use\ncases within real-time decision support.", "authors": ["Victor Petrén Bach Hansen", "Lasse Krogsbøll", "Jonas Lyngsø", "Mathias Baltzersen", "Andreas Motzfeldt", "Kevin Pelgrims", "Lars Maaløe"], "published_date": "2025-05-15", "title_zh": "FactsR：一種更安全的生成高品質醫療文檔的方法", "summary_zh": "現今有許多AI醫療抄寫方案，聲稱利用大型語言模型進行環境文檔記錄。但這些方案仍依賴於少量樣本提示生成筆記，幾乎沒有推理能力，容易產生過長、充滿幻覺、誤解臨床醫生意圖的筆記，需要臨床醫生校對。若工作量大且疲勞，會危及患者安全。本研究提出FactsR方法，在醫療諮詢期間即時提取關鍵臨床資訊（Facts），並遞迴地利用這些資訊生成最終筆記。FactsR透過讓臨床醫生參與筆記生成，產生更精準簡潔的筆記，並開創了即時決策支援的新應用。", "audio": "audios/2505.10360v1.mp3", "timestamp": "2025-05-18T20:19:19.221424"}
{"id": "2505.10338v1", "url": "http://arxiv.org/abs/2505.10338v1", "title": "Telecom-to-Visible Quantum Frequency Converter on a Silicon Nitride Chip", "summary": "Quantum frequency conversion serves a key role in the realization of hybrid\nquantum networks by interfacing between wavelength-incompatible platforms. Here\nwe present the first quantum frequency converter connecting visible and telecom\ndomains on a silicon nitride (SiN) chip, using Bragg-scattering four-wave\nmixing to upconvert heralded single photons from 1260 to 698 nm, which covers a\n192 THz span. We examine the noise sources in SiN and devise approaches to\nsuppress noise photons at the source and target frequencies to enable\nmeasurements at the single-photon level. We demonstrate an on-chip conversion\nefficiency of 5% in photon flux and describe design modifications that can be\nimplemented to significantly improve it. Our results pave the way for the\nimplementation of CMOS-compatible devices in quantum networks.", "authors": ["Sidarth Raghunathan", "Richard Oliver", "Yun Zhao", "Karl McNulty", "Chaitali Joshi", "Michal Lipson", "Alexander L. Gaeta"], "published_date": "2025-05-15", "title_zh": "矽晶氮化矽晶片上用於電信頻段到可見光頻段的量子頻率轉換器", "summary_zh": "量子頻率轉換是連接不同波長量子平台的關鍵技術。這篇論文展示了首個矽晶氮化矽晶片上的量子頻率轉換器，能將電信頻段（1260奈米）的單光子上轉換到可見光頻段（698奈米），跨越192太赫茲的頻寬。研究人員探討了矽晶氮化矽晶片上的雜訊來源，並設計了抑制雜訊光子的方法，實現了單光子層級的測量。晶片上的轉換效率達到了5%，並且論文中也提出了可以顯著提高轉換效率的設計修改方案。這項成果為在量子網路中實現與CMOS相容的元件鋪平了道路。", "audio": "audios/2505.10338v1.mp3", "timestamp": "2025-05-18T21:15:26.692705"}
{"id": "2505.10325v1", "url": "http://arxiv.org/abs/2505.10325v1", "title": "A Representation Learning Approach to Feature Drift Detection in Wireless Networks", "summary": "AI is foreseen to be a centerpiece in next generation wireless networks\nenabling enabling ubiquitous communication as well as new services. However, in\nreal deployment, feature distribution changes may degrade the performance of AI\nmodels and lead to undesired behaviors. To counter for undetected model\ndegradation, we propose ALERT; a method that can detect feature distribution\nchanges and trigger model re-training that works well on two wireless network\nuse cases: wireless fingerprinting and link anomaly detection. ALERT includes\nthree components: representation learning, statistical testing and utility\nassessment. We rely on MLP for designing the representation learning component,\non Kolmogorov-Smirnov and Population Stability Index tests for designing the\nstatistical testing and a new function for utility assessment. We show the\nsuperiority of the proposed method against ten standard drift detection methods\navailable in the literature on two wireless network use cases.", "authors": ["Athanasios Tziouvaras", "Blaz Bertalanic", "George Floros", "Kostas Kolomvatsos", "Panagiotis Sarigiannidis", "Carolina Fortuna"], "published_date": "2025-05-15", "title_zh": "無線網路中基於表徵學習的特徵漂移偵測方法", "summary_zh": "下一代無線網路預計將廣泛應用人工智慧。然而，實際部署中，特徵分佈的改變可能降低人工智慧模型效能，導致不良行為。為了解決這個問題，我們提出 ALERT 方法，它可以偵測特徵分佈的改變，並觸發模型重新訓練。ALERT 包含表徵學習、統計檢定和效用評估三個部分。我們在無線指紋辨識和鏈路異常偵測兩個無線網路應用案例中，驗證了 ALERT 優於現有的十種漂移偵測方法。", "audio": "audios/2505.10325v1.mp3", "timestamp": "2025-05-18T22:16:22.632157"}
{"id": "2505.10320v1", "url": "http://arxiv.org/abs/2505.10320v1", "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning", "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.", "authors": ["Chenxi Whitehouse", "Tianlu Wang", "Ping Yu", "Xian Li", "Jason Weston", "Ilia Kulikov", "Swarnadeep Saha"], "published_date": "2025-05-15", "title_zh": "J1：透過強化學習激勵LLM作為評審的思考能力", "summary_zh": "AI發展受限於評估品質，而強大的LLM評審模型是關鍵解決方案。更強的思考鏈推理能提升判斷力，促使我們尋找訓練這些模型思考的最佳方法。本研究提出J1，一種利用強化學習訓練LLM評審模型的方法。我們的方法將可驗證和不可驗證的提示轉換為可驗證獎勵的判斷任務，激勵思考並減少判斷偏差。我們的模型在8B或70B模型大小下，表現優於所有現有的同規模模型，包括從DeepSeek-R1提煉的模型。J1也優於o1-mini，甚至在某些基準測試中優於R1，儘管訓練的模型較小。我們分析比較了Pairwise-J1 vs Pointwise-J1模型、離線 vs 線上訓練、獎勵策略、初始提示，以及思考長度和內容的變化。我們發現，我們的模型通過學習概述評估標準、與自我生成的參考答案進行比較，以及重新評估模型回應的正確性，來做出更好的判斷。", "audio": "audios/2505.10320v1.mp3", "timestamp": "2025-05-18T23:16:55.410198"}
{"id": "2505.10315v1", "url": "http://arxiv.org/abs/2505.10315v1", "title": "Private Transformer Inference in MLaaS: A Survey", "summary": "Transformer models have revolutionized AI, powering applications like content\ngeneration and sentiment analysis. However, their deployment in Machine\nLearning as a Service (MLaaS) raises significant privacy concerns, primarily\ndue to the centralized processing of sensitive user data. Private Transformer\nInference (PTI) offers a solution by utilizing cryptographic techniques such as\nsecure multi-party computation and homomorphic encryption, enabling inference\nwhile preserving both user data and model privacy. This paper reviews recent\nPTI advancements, highlighting state-of-the-art solutions and challenges. We\nalso introduce a structured taxonomy and evaluation framework for PTI, focusing\non balancing resource efficiency with privacy and bridging the gap between\nhigh-performance inference and data privacy.", "authors": ["Yang Li", "Xinyu Zhou", "Yitong Wang", "Liangxin Qian", "Jun Zhao"], "published_date": "2025-05-15", "title_zh": "MLaaS中的私有Transformer推論：綜述", "summary_zh": "Transformer模型在AI領域取得重大突破，但其在機器學習即服務（MLaaS）中的部署引發隱私問題，因為使用者敏感資料集中處理。私有Transformer推論（PTI）透過安全多方計算和同態加密等技術，在保護使用者資料和模型隱私的同時進行推論。本論文回顧了最新的PTI研究進展，重點介紹了最新的解決方案和挑戰，並提出了針對PTI的結構化分類和評估框架，旨在平衡資源效率與隱私保護，並彌合高性能推論與資料隱私之間的差距。", "audio": "audios/2505.10315v1.mp3", "timestamp": "2025-05-19T01:38:18.066985"}
{"id": "2505.11483v1", "url": "http://arxiv.org/abs/2505.11483v1", "title": "msf-CNN: Patch-based Multi-Stage Fusion with Convolutional Neural Networks for TinyML", "summary": "AI spans from large language models to tiny models running on\nmicrocontrollers (MCUs). Extremely memory-efficient model architectures are\ndecisive to fit within an MCU's tiny memory budget e.g., 128kB of RAM. However,\ninference latency must remain small to fit real-time constraints. An approach\nto tackle this is patch-based fusion, which aims to optimize data flows across\nneural network layers. In this paper, we introduce msf-CNN, a novel technique\nthat efficiently finds optimal fusion settings for convolutional neural\nnetworks (CNNs) by walking through the fusion solution space represented as a\ndirected acyclic graph. Compared to previous work on CNN fusion for MCUs,\nmsf-CNN identifies a wider set of solutions. We published an implementation of\nmsf-CNN running on various microcontrollers (ARM Cortex-M, RISC-V, ESP32). We\nshow that msf-CNN can achieve inference using 50% less RAM compared to the\nprior art (MCUNetV2 and StreamNet). We thus demonstrate how msf-CNN offers\nadditional flexibility for system designers.", "authors": ["Zhaolan Huang", "Emmanuel Baccelli"], "published_date": "2025-05-16", "title_zh": "msf-CNN：基於卷積神經網路的塊狀多階段融合TinyML", "summary_zh": "針對微控制器(MCU)上運行的TinyML，本研究提出msf-CNN，一種尋找卷積神經網路(CNN)最佳融合設定的新技術。透過在有向無環圖表示的融合方案空間中搜尋，msf-CNN能找到更廣泛的解決方案。實驗證明，相比於現有技術MCUNetV2和StreamNet，msf-CNN能減少50%的RAM使用量，為系統設計者提供更大的彈性。", "audio": "audios/2505.11483v1.mp3", "timestamp": "2025-05-19T03:17:07.151829"}
{"id": "2505.11481v1", "url": "http://arxiv.org/abs/2505.11481v1", "title": "MOSAAIC: Managing Optimization towards Shared Autonomy, Authority, and Initiative in Co-creation", "summary": "Striking the appropriate balance between humans and co-creative AI is an open\nresearch question in computational creativity. Co-creativity, a form of hybrid\nintelligence where both humans and AI take action proactively, is a process\nthat leads to shared creative artifacts and ideas. Achieving a balanced dynamic\nin co-creativity requires characterizing control and identifying strategies to\ndistribute control between humans and AI. We define control as the power to\ndetermine, initiate, and direct the process of co-creation. Informed by a\nsystematic literature review of 172 full-length papers, we introduce MOSAAIC\n(Managing Optimization towards Shared Autonomy, Authority, and Initiative in\nCo-creation), a novel framework for characterizing and balancing control in\nco-creation. MOSAAIC identifies three key dimensions of control: autonomy,\ninitiative, and authority. We supplement our framework with control\noptimization strategies in co-creation. To demonstrate MOSAAIC's applicability,\nwe analyze the distribution of control in six existing co-creative AI case\nstudies and present the implications of using this framework.", "authors": ["Alayt Issak", "Jeba Rezwana", "Casper Harteveld"], "published_date": "2025-05-16", "title_zh": "MOSAAIC：管理共享自主性、權威性與主動性，以優化共同創作", "summary_zh": "這篇論文探討人與AI協作創作時，如何取得控制權的平衡。研究提出了一個新的框架MOSAAIC，從自主性、主動性和權威性三個面向，分析和管理創作過程中的控制權分配。透過分析大量文獻和案例，論文展示了MOSAAIC框架的應用，幫助我們了解如何在人與AI之間更有效地分配控制權，促進更好的共同創作。", "audio": "audios/2505.11481v1.mp3", "timestamp": "2025-05-19T04:26:17.442066"}
{"id": "2505.13448v1", "url": "http://arxiv.org/abs/2505.13448v1", "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "summary": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE.", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "published_date": "2025-05-19", "category": "AI", "title_zh": "CIE：使用連續訊號控制語言模型文本生成", "summary_zh": "為了提升使用者體驗，如何讓語言模型更符合使用者意圖越來越重要。本研究提出一種方法，透過連續訊號（例如介於「短」到「長」之間的向量）來控制語言模型生成的文本屬性，例如文本長度。實驗證明，相較於使用自然語言提示或離散訊號的方法，此方法能更可靠地控制生成文本的長度。相關程式碼與資料集已開源。", "audio": "audios/2505.13448v1.mp3", "timestamp": "2025-05-20T03:11:17.454935"}
{"id": "2505.13434v1", "url": "http://arxiv.org/abs/2505.13434v1", "title": "SMOTExT: SMOTE meets Large Language Models", "summary": "Data scarcity and class imbalance are persistent challenges in training\nrobust NLP models, especially in specialized domains or low-resource settings.\nWe propose a novel technique, SMOTExT, that adapts the idea of Synthetic\nMinority Over-sampling (SMOTE) to textual data. Our method generates new\nsynthetic examples by interpolating between BERT-based embeddings of two\nexisting examples and then decoding the resulting latent point into text with\nxRAG architecture. By leveraging xRAG's cross-modal retrieval-generation\nframework, we can effectively turn interpolated vectors into coherent text.\nWhile this is preliminary work supported by qualitative outputs only, the\nmethod shows strong potential for knowledge distillation and data augmentation\nin few-shot settings. Notably, our approach also shows promise for\nprivacy-preserving machine learning: in early experiments, training models\nsolely on generated data achieved comparable performance to models trained on\nthe original dataset. This suggests a viable path toward safe and effective\nlearning under data protection constraints.", "authors": ["Mateusz Bystroński", "Mikołaj Hołysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "published_date": "2025-05-19", "category": "Foundation Model", "title_zh": "SMOTExT：SMOTE 遇上大型語言模型", "summary_zh": "SMOTExT是一種新的文字資料增強技術，它將SMOTE（合成少數類過採樣技術）的概念應用於自然語言處理。此方法透過插值BERT嵌入向量，然後使用xRAG架構將插值後的向量解碼為文本，生成新的合成樣本。初步實驗顯示，SMOTExT在小樣本學習中具有知識蒸餾和數據增強的潛力，甚至能在隱私保護的機器學習中，僅用生成的數據訓練出與原始數據訓練的模型相近的效能。總而言之，SMOTExT為解決資料稀缺和類別不平衡問題提供了一種有前景的方案。", "audio": "audios/2505.13434v1.mp3", "timestamp": "2025-05-20T03:11:21.857638"}
{"id": "2505.13447v1", "url": "http://arxiv.org/abs/2505.13447v1", "title": "Mean Flows for One-step Generative Modeling", "summary": "We propose a principled and effective framework for one-step generative\nmodeling. We introduce the notion of average velocity to characterize flow\nfields, in contrast to instantaneous velocity modeled by Flow Matching methods.\nA well-defined identity between average and instantaneous velocities is derived\nand used to guide neural network training. Our method, termed the MeanFlow\nmodel, is self-contained and requires no pre-training, distillation, or\ncurriculum learning. MeanFlow demonstrates strong empirical performance: it\nachieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet\n256x256 trained from scratch, significantly outperforming previous\nstate-of-the-art one-step diffusion/flow models. Our study substantially\nnarrows the gap between one-step diffusion/flow models and their multi-step\npredecessors, and we hope it will motivate future research to revisit the\nfoundations of these powerful models.", "authors": ["Zhengyang Geng", "Mingyang Deng", "Xingjian Bai", "J. Zico Kolter", "Kaiming He"], "published_date": "2025-05-19", "category": "Diffusion Model", "title_zh": "用於單步生成建模的平均流", "summary_zh": "本論文提出了一個穩健且有效的單步生成模型框架。不同於Flow Matching方法中建模瞬時速度，本文引入了「平均速度」的概念來描述流場。透過推導平均速度和瞬時速度之間明確的關係，引導神經網絡訓練。此方法命名為MeanFlow模型，無需預訓練、知識提煉或課程學習。實驗結果顯示，MeanFlow在ImageNet 256x256上從零開始訓練，僅需單次函數評估（1-NFE）便達到3.43的FID，顯著超越先前最先進的單步擴散/流模型，大幅縮小了單步模型與多步模型之間的差距。期望這項研究能激勵未來對於這些強大模型基礎的深入探討。", "audio": "audios/2505.13447v1.mp3", "timestamp": "2025-05-20T03:11:26.812839"}
{"id": "2505.13445v1", "url": "http://arxiv.org/abs/2505.13445v1", "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "published_date": "2025-05-19", "category": "AI", "title_zh": "信任，但要驗證：一種使用可驗證獎勵的強化學習自驗證方法", "summary_zh": "大型語言模型在複雜推理方面展現了巨大潛力，其中使用可驗證獎勵的強化學習（RLVR）是一種關鍵的增強策略。然而，一個常見的問題是“表面自反思”，即模型無法有效驗證自己的輸出。我們引入了RISE（利用自驗證強化推理），這是一種新的線上強化學習框架，旨在解決這個問題。RISE明確地並且同時訓練大型語言模型，在單一整合的強化學習過程中，提高其解決問題和自我驗證的能力。核心機制是利用來自結果驗證器的可驗證獎勵，為解決方案生成和自我驗證任務提供即時回饋。在每次迭代中，模型生成解決方案，然後批判性地檢視自身生成的解決方案，這兩個軌跡都有助於策略更新。在各種數學推理基準上的廣泛實驗表明，RISE始終如一地提高了模型解決問題的準確性，同時培養了強大的自我驗證能力。我們的分析強調了線上驗證的優勢以及增加驗證計算的好處。此外，RISE模型在推理過程中表現出更頻繁和準確的自我驗證行為。這些優勢鞏固了RISE作為開發更穩健和具有自我意識的推理器的靈活且有效的途徑。", "audio": "audios/2505.13445v1.mp3", "timestamp": "2025-05-20T04:22:04.619238"}
{"id": "2505.13419v1", "url": "http://arxiv.org/abs/2505.13419v1", "title": "FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning", "summary": "Facial Emotion Analysis (FEA) plays a crucial role in visual affective\ncomputing, aiming to infer a person's emotional state based on facial data.\nScientifically, facial expressions (FEs) result from the coordinated movement\nof facial muscles, which can be decomposed into specific action units (AUs)\nthat provide detailed emotional insights. However, traditional methods often\nstruggle with limited interpretability, constrained generalization and\nreasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have\nshown exceptional performance in various visual tasks, while they still face\nsignificant challenges in FEA due to the lack of specialized datasets and their\ninability to capture the intricate relationships between FEs and AUs. To\naddress these issues, we introduce a novel FEA Instruction Dataset that\nprovides accurate and aligned FE and AU descriptions and establishes causal\nreasoning relationships between them, followed by constructing a new benchmark,\nFEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to\ncapture more detailed facial information, enhancing its capability in FEA\ntasks. Our model demonstrates strong performance on FEABench and impressive\ngeneralization capability through zero-shot evaluation on various datasets,\nincluding RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and\neffectiveness in FEA tasks. The dataset and code will be available at\nhttps://github.com/953206211/FEALLM.", "authors": ["Zhuozhao Hu", "Kaishen Yuan", "Xin Liu", "Zitong Yu", "Yuan Zong", "Jingang Shi", "Huanjing Yue", "Jingyu Yang"], "published_date": "2025-05-19", "category": "Foundation Model", "title_zh": "FEALLM：透過情緒協同與推理，推進多模態大型語言模型在臉部表情分析方面的能力", "summary_zh": "FEALLM 提出了一個新的臉部表情分析方法，運用多模態大型語言模型。它建立了一個包含精準的臉部表情和動作單元描述的資料集，並設計了一個新的模型架構，強化模型捕捉細緻臉部資訊的能力。實驗結果顯示，FEALLM 在臉部表情分析任務上表現出色，並展現了良好的泛化能力。", "audio": "audios/2505.13419v1.mp3", "timestamp": "2025-05-20T04:22:10.035222"}
{"id": "2505.13273v1", "url": "http://arxiv.org/abs/2505.13273v1", "title": "Seeing the Unseen: How EMoE Unveils Bias in Text-to-Image Diffusion Models", "summary": "Estimating uncertainty in text-to-image diffusion models is challenging\nbecause of their large parameter counts (often exceeding 100 million) and\noperation in complex, high-dimensional spaces with virtually infinite input\npossibilities. In this paper, we propose Epistemic Mixture of Experts (EMoE), a\nnovel framework for efficiently estimating epistemic uncertainty in diffusion\nmodels. EMoE leverages pre-trained networks without requiring additional\ntraining, enabling direct uncertainty estimation from a prompt. We leverage a\nlatent space within the diffusion process that captures epistemic uncertainty\nbetter than existing methods. Experimental results on the COCO dataset\ndemonstrate EMoE's effectiveness, showing a strong correlation between\nuncertainty and image quality. Additionally, EMoE identifies under-sampled\nlanguages and regions with higher uncertainty, revealing hidden biases in the\ntraining set. This capability demonstrates the relevance of EMoE as a tool for\naddressing fairness and accountability in AI-generated content.", "authors": ["Lucas Berry", "Axel Brando", "Wei-Di Chang", "Juan Camilo Gamboa Higuera", "David Meger"], "published_date": "2025-05-19", "category": "Diffusion Model", "title_zh": "看見未見之處：EMoE 如何揭示文本到圖像擴散模型中的偏見", "summary_zh": "現今文本到圖像的擴散模型參數龐大，難以估算其不確定性。本研究提出「知識混合專家模型」(EMoE)，能有效率地估計擴散模型中的知識不確定性。EMoE利用預訓練網路，無需額外訓練，即可直接從提示詞中估算不確定性，並藉由擴散過程中的潛在空間來捕捉知識不確定性，效果優於現有方法。實驗證明EMoE與圖像品質高度相關，並且能識別訓練集中代表性不足的語言和區域，進而揭示模型中隱藏的偏見。這顯示EMoE能作為AI生成內容中，處理公平性和問責制問題的有效工具。", "audio": "audios/2505.13273v1.mp3", "timestamp": "2025-05-20T04:22:15.863491"}
{"id": "2505.13439v1", "url": "http://arxiv.org/abs/2505.13439v1", "title": "VTBench: Evaluating Visual Tokenizers for Autoregressive Image Generation", "summary": "Autoregressive (AR) models have recently shown strong performance in image\ngeneration, where a critical component is the visual tokenizer (VT) that maps\ncontinuous pixel inputs to discrete token sequences. The quality of the VT\nlargely defines the upper bound of AR model performance. However, current\ndiscrete VTs fall significantly behind continuous variational autoencoders\n(VAEs), leading to degraded image reconstructions and poor preservation of\ndetails and text. Existing benchmarks focus on end-to-end generation quality,\nwithout isolating VT performance. To address this gap, we introduce VTBench, a\ncomprehensive benchmark that systematically evaluates VTs across three core\ntasks: Image Reconstruction, Detail Preservation, and Text Preservation, and\ncovers a diverse range of evaluation scenarios. We systematically assess\nstate-of-the-art VTs using a set of metrics to evaluate the quality of\nreconstructed images. Our findings reveal that continuous VAEs produce superior\nvisual representations compared to discrete VTs, particularly in retaining\nspatial structure and semantic detail. In contrast, the degraded\nrepresentations produced by discrete VTs often lead to distorted\nreconstructions, loss of fine-grained textures, and failures in preserving text\nand object integrity. Furthermore, we conduct experiments on GPT-4o image\ngeneration and discuss its potential AR nature, offering new insights into the\nrole of visual tokenization. We release our benchmark and codebase publicly to\nsupport further research and call on the community to develop strong,\ngeneral-purpose open-source VTs.", "authors": ["Huawei Lin", "Tong Geng", "Zhaozhuo Xu", "Weijie Zhao"], "published_date": "2025-05-19", "category": "AI", "title_zh": "VTBench：評估自迴歸圖像生成中的視覺 Tokenizer", "summary_zh": "自迴歸模型在圖像生成方面表現出色，其中視覺 Tokenizer (VT) 至關重要，它將連續像素輸入映射到離散的 Token 序列。VT 的品質直接影響著自迴歸模型的效能上限。然而，目前的離散 VT 相較於連續變分自編碼器 (VAE) 仍有明顯差距，導致圖像重建品質下降，細節和文字的保留效果不佳。現有評估標準著重於端到端生成品質，忽略了對 VT 效能的獨立評估。為了解決這個問題，我們推出了 VTBench，一個全面的評估標準，它系統性地評估 VT 在三個核心任務上的表現：圖像重建、細節保留和文字保留，並涵蓋多樣的評估場景。我們使用一系列指標系統性地評估了最先進的 VT，以評估重建圖像的品質。我們的研究結果表明，連續 VAE 產生了優於離散 VT 的視覺表徵，尤其是在保留空間結構和語義細節方面。相比之下，離散 VT 產生的劣質表徵通常會導致失真的重建、細粒度紋理的丟失以及在保留文本和物件完整性方面的失敗。此外，我們還對 GPT-4o 圖像生成進行了實驗，並討論了其潛在的自迴歸性質，為視覺 Tokenization 的作用提供了新的見解。我們公開發布了我們的評估標準和程式碼庫，以支持進一步的研究，並呼籲社群開發強大且通用的開源 VT。", "audio": "audios/2505.13439v1.mp3", "timestamp": "2025-05-20T05:18:43.416925"}
{"id": "2505.13418v1", "url": "http://arxiv.org/abs/2505.13418v1", "title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness", "summary": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter.", "authors": ["Lotem Peled-Cohen", "Maya Zadok", "Nitay Calderon", "Hila Gonen", "Roi Reichart"], "published_date": "2025-05-19", "category": "Foundation Model", "title_zh": "用不同的角度看失智症：針對人類與大型語言模型感知的可解釋性建模，以利早期察覺", "summary_zh": "認知功能衰退往往在診斷前數年就體現在語言中。非專業人士，像是病患的親近家人，常常是第一個察覺到變化的。隨著大型語言模型日益融入日常生活，甚至可能由它們發現異狀。這篇論文探討非專業人士如何透過語言感知失智症，並比較其與大型語言模型的判斷。研究發現，人類對失智症的感知不一致，且仰賴狹隘甚至具誤導性的線索。相比之下，大型語言模型利用更豐富、更細膩的特徵，更貼近臨床模式。但兩者都容易出現假陰性，經常忽略失智症病例。透過可解釋性框架，我們希望能幫助非專業人士更好地識別重要的語言徵兆。", "audio": "audios/2505.13418v1.mp3", "timestamp": "2025-05-20T05:18:49.560907"}
{"id": "2505.13244v1", "url": "http://arxiv.org/abs/2505.13244v1", "title": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models", "summary": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection.", "authors": ["Jieying Xue", "Phuong Minh Nguyen", "Minh Le Nguyen", "Xin Liu"], "published_date": "2025-05-19", "category": "Diffusion Model", "title_zh": "JNLP於SemEval-2025 Task 11：使用生成模型進行跨語言多標籤情感偵測", "summary_zh": "隨著全球數位化，跨語言情感偵測日益重要。本研究針對SemEval-2025 Task 11，利用預訓練多語言模型，探討多標籤情感偵測和情感強度這兩個子任務。我們採用微調的BERT分類模型和指令調整的生成LLM兩種架構，並提出兩種方法處理多標籤分類。實驗結果表明，我們的模型在多語言情感辨識方面具有強大的泛化能力，在Track A中，我們的模型在10種語言中取得前四的成績，並在印地語中排名第一。在Track B中，我們的方法在7種語言中也取得了前五名的成績，證明了其簡潔性和有效性。", "audio": "audios/2505.13244v1.mp3", "timestamp": "2025-05-20T05:18:54.395293"}
{"id": "2505.13438v1", "url": "http://arxiv.org/abs/2505.13438v1", "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "published_date": "2025-05-19", "category": "AI", "title_zh": "透過預算相對策略優化來最佳化隨時推理", "summary_zh": "為了提升大型語言模型的推理能力，增加測試時的計算量非常重要。現有方法通常使用強化學習來最大化推理結束時的可驗證獎勵。然而，這些方法只優化固定預算下的最終性能，效率不高。本研究提出一個名為 AnytimeReasoner 的新框架，旨在最佳化隨時推理性能，提升token效率，並在不同token預算限制下提供更靈活的推理。透過將完整推理過程截斷到來自先驗分佈的採樣token預算內，模型必須為每次截斷的思考總結出最佳答案進行驗證。這引入了可驗證的密集獎勵，促進強化學習中更有效的信用分配。此外，我們還提出了一種新的方差縮減技術，即預算相對策略優化（BRPO），以增強學習過程的穩健性和效率。在數學推理任務中的實驗結果表明，我們的模型在各種先驗分佈下，始終優於GRPO，提高了訓練和token效率。", "audio": "audios/2505.13438v1.mp3", "timestamp": "2025-05-20T06:27:24.396559"}
{"id": "2505.13416v1", "url": "http://arxiv.org/abs/2505.13416v1", "title": "Gluon: Making Muon & Scion Great Again! (Bridging Theory and Practice of LMO-based Optimizers for LLMs)", "summary": "Recent developments in deep learning optimization have brought about\nradically new algorithms based on the Linear Minimization Oracle (LMO)\nframework, such as $\\sf Muon$ and $\\sf Scion$. After over a decade of $\\sf\nAdam$'s dominance, these LMO-based methods are emerging as viable replacements,\noffering several practical advantages such as improved memory efficiency,\nbetter hyperparameter transferability, and most importantly, superior empirical\nperformance on large-scale tasks, including LLM training. However, a\nsignificant gap remains between their practical use and our current theoretical\nunderstanding: prior analyses (1) overlook the layer-wise LMO application of\nthese optimizers in practice, and (2) rely on an unrealistic smoothness\nassumption, leading to impractically small stepsizes. To address both, we\npropose a new LMO-based method called $\\sf Gluon$, capturing prior\ntheoretically analyzed methods as special cases, and introduce a new refined\ngeneralized smoothness model that captures the layer-wise geometry of neural\nnetworks, matches the layer-wise practical implementation of $\\sf Muon$ and\n$\\sf Scion$, and leads to convergence guarantees with strong practical\npredictive power. Unlike prior results, our theoretical stepsizes closely match\nthe fine-tuned values reported by Pethick et al. (2025). Our experiments with\nNanoGPT and CNN confirm that our assumption holds along the optimization\ntrajectory, ultimately closing the gap between theory and practice.", "authors": ["Artem Riabinin", "Egor Shulgin", "Kaja Gruntkowska", "Peter Richtárik"], "published_date": "2025-05-19", "category": "Foundation Model", "title_zh": "Gluon：讓Muon和Scion再次偉大！（彌合基於LMO的LLM優化器之理論與實踐差距）", "summary_zh": "近年來，基於線性最小化預言機（LMO）框架的Muon和Scion等優化算法嶄露頭角，有望取代Adam。它們在記憶體效率、超參數遷移和大規模任務（包括LLM訓練）的性能上表現更佳。然而，理論與實踐間存在差距，過去的研究未能考慮這些優化器在實踐中逐層應用LMO的特性，以及過於理想化的平滑性假設導致步長過小。為了解決這些問題，我們提出了一種新的LMO方法Gluon，它涵蓋了先前理論分析過的方法，並引入了一種新的廣義平滑性模型，可以捕捉神經網路的逐層幾何結構，與Muon和Scion的逐層實作相符，並能提供具有實際預測能力的收斂保證。實驗結果表明，我們的理論步長與實際調整值非常接近，最終彌合了理論與實踐之間的差距。", "audio": "audios/2505.13416v1.mp3", "timestamp": "2025-05-20T06:27:30.916151"}
{"id": "2505.13213v1", "url": "http://arxiv.org/abs/2505.13213v1", "title": "Diffusion Models with Double Guidance: Generate with aggregated datasets", "summary": "Creating large-scale datasets for training high-performance generative models\nis often prohibitively expensive, especially when associated attributes or\nannotations must be provided. As a result, merging existing datasets has become\na common strategy. However, the sets of attributes across datasets are often\ninconsistent, and their naive concatenation typically leads to block-wise\nmissing conditions. This presents a significant challenge for conditional\ngenerative modeling when the multiple attributes are used jointly as\nconditions, thereby limiting the model's controllability and applicability. To\naddress this issue, we propose a novel generative approach, Diffusion Model\nwith Double Guidance, which enables precise conditional generation even when no\ntraining samples contain all conditions simultaneously. Our method maintains\nrigorous control over multiple conditions without requiring joint annotations.\nWe demonstrate its effectiveness in molecular and image generation tasks, where\nit outperforms existing baselines both in alignment with target conditional\ndistributions and in controllability under missing condition settings.", "authors": ["Yanfeng Yang", "Kenji Fukumizu"], "published_date": "2025-05-19", "category": "Diffusion Model", "title_zh": "雙重引導的擴散模型：使用聚合數據集進行生成", "summary_zh": "為了訓練高效能的生成模型，建立大規模數據集往往成本高昂。因此，合併現有數據集成為常見策略，但各數據集的屬性往往不一致，直接合併容易導致條件缺失。針對這個問題，我們提出一種名為「雙重引導的擴散模型」的新方法，即使沒有同時包含所有條件的訓練樣本，也能實現精確的條件生成，在不需要聯合標註的情況下，嚴格控制多個條件。實驗證明，在分子和圖像生成任務中，我們的模型在目標條件分佈對齊以及缺失條件下的可控性方面，都優於現有基線。", "audio": "audios/2505.13213v1.mp3", "timestamp": "2025-05-20T06:27:36.081132"}
{"query": "AI", "id": "2505.13400v1", "url": "http://arxiv.org/abs/2505.13400v1", "title": "Robin: A multi-agent system for automating scientific discovery", "summary": "Scientific discovery is driven by the iterative process of background\nresearch, hypothesis generation, experimentation, and data analysis. Despite\nrecent advancements in applying artificial intelligence to scientific\ndiscovery, no system has yet automated all of these stages in a single\nworkflow. Here, we introduce Robin, the first multi-agent system capable of\nfully automating the key intellectual steps of the scientific process. By\nintegrating literature search agents with data analysis agents, Robin can\ngenerate hypotheses, propose experiments, interpret experimental results, and\ngenerate updated hypotheses, achieving a semi-autonomous approach to scientific\ndiscovery. By applying this system, we were able to identify a novel treatment\nfor dry age-related macular degeneration (dAMD), the major cause of blindness\nin the developed world. Robin proposed enhancing retinal pigment epithelium\nphagocytosis as a therapeutic strategy, and identified and validated a\npromising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho\nkinase (ROCK) inhibitor that has never previously been proposed for treating\ndAMD. To elucidate the mechanism of ripasudil-induced upregulation of\nphagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment,\nwhich revealed upregulation of ABCA1, a critical lipid efflux pump and possible\nnovel target. All hypotheses, experimental plans, data analyses, and data\nfigures in the main text of this report were produced by Robin. As the first AI\nsystem to autonomously discover and validate a novel therapeutic candidate\nwithin an iterative lab-in-the-loop framework, Robin establishes a new paradigm\nfor AI-driven scientific discovery.", "authors": ["Ali Essam Ghareeb", "Benjamin Chang", "Ludovico Mitchener", "Angela Yiu", "Caralyn J. Szostkiewicz", "Jon M. Laurent", "Muhammed T. Razzak", "Andrew D. White", "Michaela M. Hinks", "Samuel G. Rodriques"], "published_date": "2025-05-19", "title_zh": "羅賓：一個用於自動化科學發現的多代理人系統", "summary_zh": "科學發現通常需要反覆進行文獻研究、假說生成、實驗以及數據分析。本文介紹了名為「羅賓」的多代理人系統，它整合了文獻搜尋和數據分析代理，首次能夠完全自動化科學發現過程中的關鍵步驟。羅賓能生成假說、提出實驗方案、解讀實驗結果並更新假說，實現半自主的科學發現。利用羅賓，我們發現了一種治療乾性老年黃斑部病變（dAMD）的新方法，並驗證了潛在候選藥物ripasudil。羅賓還分析了後續實驗，揭示了ABCA1的表達上調，這可能是個新的治療靶點。重要的是，本文中的所有假說、實驗計畫、數據分析和圖表均由羅賓生成。作為首個在迭代實驗迴路中自主發現並驗證治療候選藥物的人工智慧系統，羅賓為人工智慧驅動的科學發現建立了一個新典範。", "audio": "audios/2505.13400v1.mp3", "timestamp": "2025-05-20T09:53:34.781452"}
{"query": "Foundation Model", "id": "2505.13291v1", "url": "http://arxiv.org/abs/2505.13291v1", "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents", "summary": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating\nArtificial Intelligence (AI) agents on time series machine learning engineering\nchallenges. Existing benchmarks lack scalability, focus narrowly on model\nbuilding in well-defined settings, and evaluate only a limited set of research\nartifacts (e.g., CSV submission files). To make AI agent benchmarking more\nrelevant to the practice of machine learning engineering, our framework scales\nalong two critical dimensions. First, recognizing that effective ML engineering\nrequires a range of diverse skills, TimeSeriesGym incorporates challenges from\ndiverse sources spanning multiple domains and tasks. We design challenges to\nevaluate both isolated capabilities (including data handling, understanding\nresearch repositories, and code translation) and their combinations, and rather\nthan addressing each challenge independently, we develop tools that support\ndesigning multiple challenges at scale. Second, we implement evaluation\nmechanisms for multiple research artifacts, including submission files, code,\nand models, using both precise numeric measures and more flexible LLM-based\nevaluation approaches. This dual strategy balances objective assessment with\ncontextual judgment. Although our initial focus is on time series applications,\nour framework can be readily extended to other data modalities, broadly\nenhancing the comprehensiveness and practical utility of agentic AI evaluation.\nWe open-source our benchmarking framework to facilitate future research on the\nML engineering capabilities of AI agents.", "authors": ["Yifu Cai", "Xinyu Li", "Mononito Goswami", "Michał Wiliński", "Gus Welter", "Artur Dubrawski"], "published_date": "2025-05-19", "title_zh": "TimeSeriesGym：一個可擴展的機器學習工程（時間序列）代理基準測試", "summary_zh": "TimeSeriesGym 是一個可擴展的基準測試框架，用於評估人工智慧 (AI) 代理在時間序列機器學習工程挑戰中的表現。現有的基準測試缺乏可擴展性，且過於狹隘地關注良好定義環境下的模型構建，並僅評估有限的研究成果。TimeSeriesGym 透過兩個關鍵面向提升可擴展性：首先，它整合了來自多個領域和任務的多樣化挑戰，評估資料處理、理解研究庫和程式碼翻譯等不同技能的組合。其次，它評估多種研究成果，包括提交檔案、程式碼和模型，同時採用精確的數值度量和基於大型語言模型 (LLM) 的彈性評估方法。這個框架開放原始碼，旨在促進對 AI 代理機器學習工程能力的研究。", "audio": "audios/2505.13291v1.mp3", "timestamp": "2025-05-20T09:53:40.535627"}
{"query": "Diffusion Model", "id": "2505.13389v1", "url": "http://arxiv.org/abs/2505.13389v1", "title": "Faster Video Diffusion with Trainable Sparse Attention", "summary": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at \\emph{both}\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight \\emph{critical tokens}; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models.", "authors": ["Peiyuan Zhang", "Haofeng Huang", "Yongqi Chen", "Will Lin", "Zhengzhong Liu", "Ion Stoica", "Eric P. Xing", "Hao Zhang"], "published_date": "2025-05-19", "title_zh": "透過可訓練的稀疏注意力加速影片擴散", "summary_zh": "影片擴散轉換器（DiT）的擴展受限於其二次方的3D注意力，即便大部分注意力集中在一小部分位置。我們提出了VSA，一種可訓練、硬體高效的稀疏注意力，在訓練和推論階段都取代了完整注意力。VSA使用輕量級粗略階段將tokens池化成tiles，並識別高權重的「關鍵tokens」；精細階段僅在這些tiles內部計算token級別的注意力，並採用區塊計算布局以確保硬體效率。這產生了一個可端到端訓練的單一可微核心，無需事後分析，並維持了FlashAttention3 MFU的85%。我們進行了大量的消融研究和縮放律實驗，對參數量從60M到1.4B的DiT進行預訓練。VSA達到了一個帕雷托點，在擴散損失沒有下降的情況下，將訓練FLOPS降低了2.53倍。改造開源的Wan-2.1模型後，注意力時間加速了6倍，並在品質相當的情況下，將端到端生成時間從31秒降低到18秒。這些結果表明，可訓練的稀疏注意力是完整注意力的一個實用替代方案，也是進一步擴展影片擴散模型的關鍵推動因素。", "audio": "audios/2505.13389v1.mp3", "timestamp": "2025-05-20T09:53:47.983791"}
{"query": "AI", "id": "2505.13381v1", "url": "http://arxiv.org/abs/2505.13381v1", "title": "How Adding Metacognitive Requirements in Support of AI Feedback in Practice Exams Transforms Student Learning Behaviors", "summary": "Providing personalized, detailed feedback at scale in large undergraduate\nSTEM courses remains a persistent challenge. We present an empirically\nevaluated practice exam system that integrates AI generated feedback with\ntargeted textbook references, deployed in a large introductory biology course.\nOur system encourages metacognitive behavior by asking students to explain\ntheir answers and declare their confidence. It uses OpenAI's GPT-4o to generate\npersonalized feedback based on this information, while directing them to\nrelevant textbook sections. Through interaction logs from consenting\nparticipants across three midterms (541, 342, and 413 students respectively),\ntotaling 28,313 question-student interactions across 146 learning objectives,\nalong with 279 surveys and 23 interviews, we examined the system's impact on\nlearning outcomes and engagement. Across all midterms, feedback types showed no\nstatistically significant performance differences, though some trends suggested\npotential benefits. The most substantial impact came from the required\nconfidence ratings and explanations, which students reported transferring to\ntheir actual exam strategies. About 40 percent of students engaged with\ntextbook references when prompted by feedback -- far higher than traditional\nreading rates. Survey data revealed high satisfaction (mean rating 4.1 of 5),\nwith 82.1 percent reporting increased confidence on practiced midterm topics,\nand 73.4 percent indicating they could recall and apply specific concepts. Our\nfindings suggest that embedding structured reflection requirements may be more\nimpactful than sophisticated feedback mechanisms.", "authors": ["Mak Ahmad", "Prerna Ravi", "David Karger", "Marc Facciotti"], "published_date": "2025-05-19", "title_zh": "如何在練習測驗中加入元認知需求以支援人工智慧回饋，進而改變學生學習行為", "summary_zh": "在大型大學STEM課程中提供大規模且個人化的回饋是個挑戰。這項研究設計了一個練習測驗系統，結合人工智慧生成的回饋與相關教科書參考，並在生物入門課中實施。該系統鼓勵學生進行元認知，要求他們解釋答案並聲明信心程度。系統使用GPT-4o生成個人化回饋，並引導學生查閱教科書。研究發現，雖然不同回饋類型的成績差異不大，但要求學生評估信心程度並解釋答案，對他們的學習策略有顯著影響，許多學生也表示將這些策略應用於正式考試中。當被回饋提示時，約有40%的學生會參考教科書，遠高於傳統閱讀率。調查顯示學生滿意度高，並表示對練習過的考題更有信心，也更能回憶和應用特定概念。研究表明，比起複雜的回饋機制，嵌入結構化的反思需求可能更具影響力。", "audio": "audios/2505.13381v1.mp3", "timestamp": "2025-05-20T10:20:40.252613"}
{"query": "Foundation Model", "id": "2505.13255v1", "url": "http://arxiv.org/abs/2505.13255v1", "title": "Policy Contrastive Decoding for Robotic Foundation Models", "summary": "Robotic foundation models, or generalist robot policies, hold immense\npotential to enable flexible, general-purpose and dexterous robotic systems.\nDespite their advancements, our empirical experiments reveal that existing\nrobot policies are prone to learning spurious correlations from pre-training\ntrajectories, adversely affecting their generalization capabilities beyond the\ntraining data. To tackle this, we propose a novel Policy Contrastive Decoding\n(PCD) approach, which redirects the robot policy's focus toward object-relevant\nvisual clues by contrasting action probability distributions derived from\noriginal and object-masked visual inputs. As a training-free method, our PCD\ncan be used as a plugin to improve different types of robot policies without\nneeding to finetune or access model weights. We conduct extensive experiments\non top of three open-source robot policies, including the autoregressive policy\nOpenVLA and the diffusion-based policies Octo and $\\pi_0$. The obtained results\nin both simulation and real-world environments prove PCD's flexibility and\neffectiveness, e.g., PCD enhances the state-of-the-art policy $\\pi_0$ by 8% in\nthe simulation environment and by 108% in the real-world environment. Code and\ndemos are publicly available at: https://Koorye.github.io/proj/PCD.", "authors": ["Shihan Wu", "Ji Zhang", "Xu Luo", "Junlin Xie", "Jingkuan Song", "Heng Tao Shen", "Lianli Gao"], "published_date": "2025-05-19", "title_zh": "用於機器人基礎模型的策略對比解碼", "summary_zh": "機器人基礎模型，也就是通用型機器人策略，有潜力打造更靈活的機器人系統。然而，研究發現現有的機器人策略容易從預訓練資料中學到虛假關聯性，導致泛化能力下降。為了解決這個問題，我們提出一種名為「策略對比解碼 (PCD)」的新方法，它通過对比原始視覺輸入和遮蔽物體的視覺輸入所得到的動作概率分佈，引導機器人策略關注與物體相關的視覺線索。PCD無需訓練，可以像插件一樣使用，提升各種機器人策略的性能，而無需微調或訪問模型權重。大量實驗表明PCD具有靈活性和有效性，例如，在模擬環境和真實環境中，它分別將最先進的策略 π₀ 的性能提升了 8% 和 108%。程式碼和演示可在指定連結取得。", "audio": "audios/2505.13255v1.mp3", "timestamp": "2025-05-20T10:20:47.331287"}
{"query": "Diffusion Model", "id": "2505.13377v1", "url": "http://arxiv.org/abs/2505.13377v1", "title": "Restoration Score Distillation: From Corrupted Diffusion Pretraining to One-Step High-Quality Generation", "summary": "Learning generative models from corrupted data is a fundamental yet\npersistently challenging task across scientific disciplines, particularly when\naccess to clean data is limited or expensive. Denoising Score Distillation\n(DSD) \\cite{chen2025denoising} recently introduced a novel and surprisingly\neffective strategy that leverages score distillation to train high-fidelity\ngenerative models directly from noisy observations. Building upon this\nfoundation, we propose \\textit{Restoration Score Distillation} (RSD), a\nprincipled generalization of DSD that accommodates a broader range of\ncorruption types, such as blurred, incomplete, or low-resolution images. RSD\noperates by first pretraining a teacher diffusion model solely on corrupted\ndata and subsequently distilling it into a single-step generator that produces\nhigh-quality reconstructions. Empirically, RSD consistently surpasses its\nteacher model across diverse restoration tasks on both natural and scientific\ndatasets. Moreover, beyond standard diffusion objectives, the RSD framework is\ncompatible with several corruption-aware training techniques such as Ambient\nTweedie, Ambient Diffusion, and its Fourier-space variant, enabling flexible\nintegration with recent advances in diffusion modeling. Theoretically, we\ndemonstrate that in a linear regime, RSD recovers the eigenspace of the clean\ndata covariance matrix from linear measurements, thereby serving as an implicit\nregularizer. This interpretation recasts score distillation not only as a\nsampling acceleration technique but as a principled approach to enhancing\ngenerative performance in severely degraded data regimes.", "authors": ["Yasi Zhang", "Tianyu Chen", "Zhendong Wang", "Ying Nian Wu", "Mingyuan Zhou", "Oscar Leong"], "published_date": "2025-05-19", "title_zh": "復原分數蒸餾：從已損毀的擴散預訓練到一步式高品質生成", "summary_zh": "從損毀數據學習生成模型是個重要的挑戰。復原分數蒸餾(RSD)是一種新的方法，它先用損毀數據訓練一個擴散模型作為老師，然後將其知識提煉成一個一步式生成器，直接重建出高品質的影像。RSD可以處理各種損毀類型，例如模糊、不完整或低解析度的圖像，並且在各種還原任務上都超越了老師模型。理論分析表明，RSD可以從線性測量中恢復乾淨數據的協方差矩陣的特徵空間，因此它不僅是一種加速取樣的技術，更是一種在數據嚴重降級的情況下提升生成性能的有效方法。", "audio": "audios/2505.13377v1.mp3", "timestamp": "2025-05-20T10:20:54.898993"}
{"query": "AI", "id": "2505.13355v1", "url": "http://arxiv.org/abs/2505.13355v1", "title": "Multi-Armed Bandits Meet Large Language Models", "summary": "Bandit algorithms and Large Language Models (LLMs) have emerged as powerful\ntools in artificial intelligence, each addressing distinct yet complementary\nchallenges in decision-making and natural language processing. This survey\nexplores the synergistic potential between these two fields, highlighting how\nbandit algorithms can enhance the performance of LLMs and how LLMs, in turn,\ncan provide novel insights for improving bandit-based decision-making. We first\nexamine the role of bandit algorithms in optimizing LLM fine-tuning, prompt\nengineering, and adaptive response generation, focusing on their ability to\nbalance exploration and exploitation in large-scale learning tasks.\nSubsequently, we explore how LLMs can augment bandit algorithms through\nadvanced contextual understanding, dynamic adaptation, and improved policy\nselection using natural language reasoning. By providing a comprehensive review\nof existing research and identifying key challenges and opportunities, this\nsurvey aims to bridge the gap between bandit algorithms and LLMs, paving the\nway for innovative applications and interdisciplinary research in AI.", "authors": ["Djallel Bouneffouf", "Raphael Feraud"], "published_date": "2025-05-19", "title_zh": "多臂老虎機遇上大型語言模型", "summary_zh": "這篇論文探討了多臂老虎機演算法與大型語言模型（LLM）之間的協同效應。多臂老虎機演算法可以優化 LLM 的微調、提示工程和自適應回應生成，而 LLM 則能利用其強大的上下文理解能力、動態適應能力和自然語言推理能力來改進多臂老虎機演算法的策略選擇。這篇綜述旨在促進這兩個領域的交叉研究，為人工智慧的創新應用鋪平道路。", "audio": "audios/2505.13355v1.mp3", "timestamp": "2025-05-20T11:15:50.184028"}
{"query": "Foundation Model", "id": "2505.13227v1", "url": "http://arxiv.org/abs/2505.13227v1", "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "published_date": "2025-05-19", "title_zh": "透過使用者介面分解與合成來擴展電腦使用接地的能力", "summary_zh": "圖形使用者介面 (GUI) 接地，也就是將自然語言指令映射到 GUI 上的特定操作，仍然是電腦使用代理程式開發中的一個關鍵瓶頸。現有基準測試過於簡化接地任務，將其視為簡短的指稱表達式，未能捕捉到需要軟體常識、版面理解和細粒度操作能力的真實世界互動的複雜性。為了應對這些局限性，我們引入了 OSWorld-G，這是一個全面的基準測試，包含 564 個跨多種任務類型（包括文本匹配、元素識別、版面理解和精確操作）的精細註釋樣本。此外，我們透過多視角解耦任務，合成並發布了最大的電腦使用接地資料集 Jedi，其中包含 400 萬個示例。我們在 Jedi 上訓練的多尺度模型透過優於 ScreenSpot-v2、ScreenSpot-Pro 和我們的 OSWorld-G 上的現有方法，證明了其有效性。此外，我們證明了 Jedi 改進的接地能力直接增強了通用基礎模型在複雜電腦任務上的代理能力，在 OSWorld 上從 5% 提高到 27%。透過詳細的消融研究，我們確定了影響接地效能的關鍵因素，並驗證了針對不同介面元素的專業資料的結合能夠實現對新介面的組合成泛化。所有基準測試、資料、檢查點和程式碼都是開源的，並且可於 https://osworld-grounding.github.io 取得。", "audio": "audios/2505.13227v1.mp3", "timestamp": "2025-05-20T11:16:02.194993"}
{"query": "Diffusion Model", "id": "2505.13375v1", "url": "http://arxiv.org/abs/2505.13375v1", "title": "Minimum-Excess-Work Guidance", "summary": "We propose a regularization framework inspired by thermodynamic work for\nguiding pre-trained probability flow generative models (e.g., continuous\nnormalizing flows or diffusion models) by minimizing excess work, a concept\nrooted in statistical mechanics and with strong conceptual connections to\noptimal transport. Our approach enables efficient guidance in sparse-data\nregimes common to scientific applications, where only limited target samples or\npartial density constraints are available. We introduce two strategies: Path\nGuidance for sampling rare transition states by concentrating probability mass\non user-defined subsets, and Observable Guidance for aligning generated\ndistributions with experimental observables while preserving entropy. We\ndemonstrate the framework's versatility on a coarse-grained protein model,\nguiding it to sample transition configurations between folded/unfolded states\nand correct systematic biases using experimental data. The method bridges\nthermodynamic principles with modern generative architectures, offering a\nprincipled, efficient, and physics-inspired alternative to standard fine-tuning\nin data-scarce domains. Empirical results highlight improved sample efficiency\nand bias reduction, underscoring its applicability to molecular simulations and\nbeyond.", "authors": ["Christopher Kolloff", "Tobias Höppe", "Emmanouil Angelis", "Mathias Jacob Schreiner", "Stefan Bauer", "Andrea Dittadi", "Simon Olsson"], "published_date": "2025-05-19", "title_zh": "最小過剩功引導", "summary_zh": "我們提出一種基於熱力學功的正則化框架，引導預訓練的機率流生成模型（例如連續歸一化流或擴散模型），通過最小化過剩功實現。這種方法在科學應用常見的稀疏數據情況下非常有效，僅需少量目標樣本或部分密度約束。我們介紹了兩種策略：路徑引導，用於採樣罕見的過渡態，以及可觀測量引導，用於將生成的分布與實驗觀測值對齊。在粗粒化蛋白模型上的實驗表明，該框架能夠有效地採樣摺疊/解摺疊狀態之間的過渡構型，並利用實驗數據修正系統偏差。總之，這項工作將熱力學原理與現代生成架構相結合，為數據稀缺領域提供了一種基於物理、高效且有原則的替代方案，優於標準微調方法。", "audio": "audios/2505.13375v1.mp3", "timestamp": "2025-05-20T11:16:08.821722"}
{"query": "AI", "id": "2505.13354v1", "url": "http://arxiv.org/abs/2505.13354v1", "title": "A large-scale analysis of public-facing, community-built chatbots on Character.AI", "summary": "This paper presents the first large-scale analysis of public-facing chatbots\non Character.AI, a rapidly growing social media platform where users create and\ninteract with chatbots. Character.AI is distinctive in that it merges\ngenerative AI with user-generated content, enabling users to build bots-often\nmodeled after fictional or public personas-for others to engage with. It is\nalso popular, with over 20 million monthly active users, and impactful, with\nrecent headlines detailing significant issues with youth engagement on the\nsite. Character.AI is thus of interest to study both substantively and\nconceptually. To this end, we present a descriptive overview of the site using\na dataset of 2.1 million English-language prompts (or ``greetings'') for\nchatbots on the site, created by around 1 million users. Our work explores the\nprevalence of different fandoms on the site, broader tropes that persist across\nfandoms, and how dynamics of power intersect with gender within greetings.\nOverall, our findings illuminate an emerging form of online (para)social\ninteraction that toes a unique and important intersection between generative AI\nand user-generated content.", "authors": ["Owen Lee", "Kenneth Joseph"], "published_date": "2025-05-19", "title_zh": "Character.AI上公開、社群建立的聊天機器人的大規模分析", "summary_zh": "本研究首次大規模分析Character.AI平台上公開的聊天機器人。Character.AI是一個快速成長的社交媒體平台，用戶可以創建並與聊天機器人互動。它結合了生成式AI和使用者產生的內容，讓使用者可以建立模仿虛構或公眾人物的機器人。本研究利用包含210萬條英文提示詞的數據集，描述了Character.AI的概況，並探討了平台上的不同粉絲群體、常見的主題，以及權力動態如何與性別交叉。研究結果揭示了一種新興的線上準社交互動形式，它獨特且重要地結合了生成式AI和使用者產生的內容。", "audio": "audios/2505.13354v1.mp3", "timestamp": "2025-05-20T12:38:41.704889"}
{"query": "Foundation Model", "id": "2505.13192v1", "url": "http://arxiv.org/abs/2505.13192v1", "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics", "summary": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters and\norders of magnitude faster inference times. DynaMix outperforms TS foundation\nmodels in terms of long-term statistics, and often also short-term forecasts,\neven on real-world time series, like traffic or weather data, typically used\nfor training and evaluating TS models, but not at all part of DynaMix' training\ncorpus. We illustrate some of the failure modes of TS models for DSR problems,\nand conclude that models built on DS principles may bear a huge potential also\nfor advancing the TS prediction field.", "authors": ["Christoph Jürgen Hemmer", "Daniel Durstewitz"], "published_date": "2025-05-19", "title_zh": "真實零樣本推論：長期統計量保持的動態系統", "summary_zh": "DynaMix是一種新的動態系統重建模型，它基於ALRNN的專家混合架構進行預訓練。與傳統方法不同，DynaMix無需針對每個新系統進行重新訓練，就能夠零樣本泛化到未知的動態系統。只需提供上下文信號，DynaMix即可忠實地預測新系統的長期演化，其性能優於現有的時間序列基礎模型，且參數更少、推論速度更快。即使面對真實世界的交通或天氣數據，DynaMix也能在長期統計量方面勝過這些模型。這項研究表明，基於動態系統原理構建的模型在時間序列預測領域具有巨大潛力。", "audio": "audios/2505.13192v1.mp3", "timestamp": "2025-05-20T12:38:46.833826"}
{"query": "Diffusion Model", "id": "2505.13358v1", "url": "http://arxiv.org/abs/2505.13358v1", "title": "One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling", "summary": "Diffusion-based generative models have demonstrated exceptional performance,\nyet their iterative sampling procedures remain computationally expensive. A\nprominent strategy to mitigate this cost is distillation, with offline\ndistillation offering particular advantages in terms of efficiency, modularity,\nand flexibility. In this work, we identify two key observations that motivate a\nprincipled distillation framework: (1) while diffusion models have been viewed\nthrough the lens of dynamical systems theory, powerful and underexplored tools\ncan be further leveraged; and (2) diffusion models inherently impose\nstructured, semantically coherent trajectories in latent space. Building on\nthese observations, we introduce the Koopman Distillation Model KDM, a novel\noffline distillation approach grounded in Koopman theory-a classical framework\nfor representing nonlinear dynamics linearly in a transformed space. KDM\nencodes noisy inputs into an embedded space where a learned linear operator\npropagates them forward, followed by a decoder that reconstructs clean samples.\nThis enables single-step generation while preserving semantic fidelity. We\nprovide theoretical justification for our approach: (1) under mild assumptions,\nthe learned diffusion dynamics admit a finite-dimensional Koopman\nrepresentation; and (2) proximity in the Koopman latent space correlates with\nsemantic similarity in the generated outputs, allowing for effective trajectory\nalignment. Empirically, KDM achieves state-of-the-art performance across\nstandard offline distillation benchmarks, improving FID scores by up to 40% in\na single generation step. All implementation details and code for the\nexperimental setups are provided in our GitHub -\nhttps://github.com/azencot-group/KDM, or in our project page -\nhttps://sites.google.com/view/koopman-distillation-model.", "authors": ["Nimrod Berman", "Ilan Naiman", "Moshe Eliasof", "Hedi Zisling", "Omri Azencot"], "published_date": "2025-05-19", "title_zh": "基於Koopman建模的擴散模型一步式離線蒸餾", "summary_zh": "擴散模型在生成任務上表現出色，但迭代採樣過程耗時。本研究提出一種名為 Koopman Distillation Model (KDM) 的創新離線蒸餾方法，利用 Koopman 理論將非線性擴散動態線性地表示在轉換後的空間中。KDM 通過學習線性算子在嵌入空間中傳播噪聲輸入，實現單步生成高品質樣本，在標準離線蒸餾基準測試中，FID 指標提升高達 40%。程式碼和更多資訊可在 GitHub 或專案頁面找到。", "audio": "audios/2505.13358v1.mp3", "timestamp": "2025-05-20T12:38:54.099908"}
{"query": "AI", "id": "2505.13338v1", "url": "http://arxiv.org/abs/2505.13338v1", "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "summary": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities.", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "published_date": "2025-05-19", "title_zh": "用於多模態語音-LLM的情境副語言資料創建：資料濃縮與口語問答生成", "summary_zh": "現有的語音語言模型在情境推理和副語言理解方面能力有限，主要是缺乏涵蓋這兩方面的問答資料集。本文提出一個新穎的框架，從真實世界的語音資料中生成同時整合情境推理和副語言資訊的資料集，包含基於偽副語言標籤的資料濃縮，以及基於大型語言模型的情境副語言問答生成。實驗證明，基於此框架生成的資料集，能有效提升語音語言模型的性能，但也揭示了模型在同理心推理任務上的不足，突顯了創建此類資料集及開發更強大模型的重要性。此框架為首創，有潛力訓練出更強大且具備副語言推理能力的語音語言模型。", "audio": "audios/2505.13338v1.mp3", "timestamp": "2025-05-20T13:31:45.014604"}
{"query": "Foundation Model", "id": "2505.13150v1", "url": "http://arxiv.org/abs/2505.13150v1", "title": "Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics", "summary": "Behavioral Foundation Models (BFMs) proved successful in producing policies\nfor arbitrary tasks in a zero-shot manner, requiring no test-time training or\ntask-specific fine-tuning. Among the most promising BFMs are the ones that\nestimate the successor measure learned in an unsupervised way from\ntask-agnostic offline data. However, these methods fail to react to changes in\nthe dynamics, making them inefficient under partial observability or when the\ntransition function changes. This hinders the applicability of BFMs in a\nreal-world setting, e.g., in robotics, where the dynamics can unexpectedly\nchange at test time. In this work, we demonstrate that Forward-Backward (FB)\nrepresentation, one of the methods from the BFM family, cannot distinguish\nbetween distinct dynamics, leading to an interference among the latent\ndirections, which parametrize different policies. To address this, we propose a\nFB model with a transformer-based belief estimator, which greatly facilitates\nzero-shot adaptation. We also show that partitioning the policy encoding space\ninto dynamics-specific clusters, aligned with the context-embedding directions,\nyields additional gain in performance. These traits allow our method to respond\nto the dynamics observed during training and to generalize to unseen ones.\nEmpirically, in the changing dynamics setting, our approach achieves up to a 2x\nhigher zero-shot returns compared to the baselines for both discrete and\ncontinuous tasks.", "authors": ["Maksim Bobrin", "Ilya Zisman", "Alexander Nikulin", "Vladislav Kurenkov", "Dmitry Dylov"], "published_date": "2025-05-19", "title_zh": "行為基礎模型在未見動力學下的零樣本適應", "summary_zh": "行為基礎模型（BFM）無需訓練或微調就能零樣本執行各種任務。然而，基於後繼度量估計的BFM在動力學改變時表現不佳。本研究指出，一種名為Forward-Backward (FB)表示的BFM無法區分不同的動力學，導致策略混淆。為解決此問題，我們提出一種結合Transformer的信念估計器FB模型，顯著提升了零樣本適應能力。此外，將策略編碼空間劃分為特定動力學的群組，可以進一步提高性能。實驗證明，在動力學變化環境中，我們的模型在離散和連續任務上，零樣本回報比基線方法高達2倍。", "audio": "audios/2505.13150v1.mp3", "timestamp": "2025-05-20T13:31:50.537855"}
{"query": "Diffusion Model", "id": "2505.13280v1", "url": "http://arxiv.org/abs/2505.13280v1", "title": "FlowPure: Continuous Normalizing Flows for Adversarial Purification", "summary": "Despite significant advancements in the area, adversarial robustness remains\na critical challenge in systems employing machine learning models. The removal\nof adversarial perturbations at inference time, known as adversarial\npurification, has emerged as a promising defense strategy. To achieve this,\nstate-of-the-art methods leverage diffusion models that inject Gaussian noise\nduring a forward process to dilute adversarial perturbations, followed by a\ndenoising step to restore clean samples before classification. In this work, we\npropose FlowPure, a novel purification method based on Continuous Normalizing\nFlows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings\nfrom adversarial examples to their clean counterparts. Unlike prior\ndiffusion-based approaches that rely on fixed noise processes, FlowPure can\nleverage specific attack knowledge to improve robustness under known threats,\nwhile also supporting a more general stochastic variant trained on Gaussian\nperturbations for settings where such knowledge is unavailable. Experiments on\nCIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art\npurification-based defenses in preprocessor-blind and white-box scenarios, and\ncan do so while fully preserving benign accuracy in the former. Moreover, our\nresults show that not only is FlowPure a highly effective purifier but it also\nholds a strong potential for adversarial detection, identifying\npreprocessor-blind PGD samples with near-perfect accuracy.", "authors": ["Elias Collaert", "Abel Rodríguez", "Sander Joos", "Lieven Desmet", "Vera Rimmer"], "published_date": "2025-05-19", "title_zh": "FlowPure：使用連續歸一化流進行對抗性淨化", "summary_zh": "機器學習模型的對抗性魯棒性是個重要挑戰。FlowPure 是一種新的淨化方法，它利用連續歸一化流 (CNF) 來學習將對抗性樣本映射到乾淨樣本。與先前依賴固定噪音過程的擴散模型方法不同，FlowPure 能針對特定攻擊知識進行訓練，提升已知威脅下的魯棒性，也能在缺乏相關知識的情況下，使用基於高斯擾動的隨機變體。實驗表明，FlowPure 在 CIFAR-10 和 CIFAR-100 上的表現優於現有的淨化方法，且在預處理器盲測情境下能完全保持良性樣本的準確度。此外，FlowPure 在對抗性檢測方面也表現出色，幾乎能完美識別預處理器盲測的 PGD 樣本。", "audio": "audios/2505.13280v1.mp3", "timestamp": "2025-05-20T13:31:59.957771"}
{"query": "AI", "id": "2505.13329v1", "url": "http://arxiv.org/abs/2505.13329v1", "title": "Recommender Systems for Democracy: Toward Adversarial Robustness in Voting Advice Applications", "summary": "Voting advice applications (VAAs) help millions of voters understand which\npolitical parties or candidates best align with their views. This paper\nexplores the potential risks these applications pose to the democratic process\nwhen targeted by adversarial entities. In particular, we expose 11 manipulation\nstrategies and measure their impact using data from Switzerland's primary VAA,\nSmartvote, collected during the last two national elections. We find that\naltering application parameters, such as the matching method, can shift a\nparty's recommendation frequency by up to 105%. Cherry-picking questionnaire\nitems can increase party recommendation frequency by over 261%, while subtle\nchanges to parties' or candidates' responses can lead to a 248% increase. To\naddress these vulnerabilities, we propose adversarial robustness properties\nVAAs should satisfy, introduce empirical metrics for assessing the resilience\nof various matching methods, and suggest possible avenues for research toward\nmitigating the effect of manipulation. Our framework is key to ensuring secure\nand reliable AI-based VAAs poised to emerge in the near future.", "authors": ["Frédéric Berdoz", "Dustin Brunner", "Yann Vonlanthen", "Roger Wattenhofer"], "published_date": "2025-05-19", "title_zh": "民主推薦系統：邁向投票建議應用程式的對抗性穩健性", "summary_zh": "投票建議應用程式幫助選民了解哪些政黨或候選人最符合他們的觀點。這篇論文探討了這些應用程式在受到對抗性實體攻擊時，對民主進程構成的潛在風險。研究揭露了11種操控策略，發現更改應用程式參數、精心挑選問卷題目或修改政黨/候選人的回答，都可能顯著改變政黨的推薦頻率。為了應對這些漏洞，研究提出了投票建議應用程式應該滿足的對抗性穩健性屬性，引入了評估不同匹配方法韌性的指標，並建議了減輕操控影響的研究方向，旨在確保未來基於AI的投票建議應用程式的安全和可靠性。", "audio": "audios/2505.13329v1.mp3", "timestamp": "2025-05-20T14:18:35.743976"}
{"query": "Foundation Model", "id": "2505.13099v1", "url": "http://arxiv.org/abs/2505.13099v1", "title": "Industry-focused Synthetic Segmentation Pre-training", "summary": "Pre-training on real-image datasets has been widely proven effective for\nimproving instance segmentation. However, industrial applications face two key\nchallenges: (1) legal and ethical restrictions, such as ImageNet's prohibition\nof commercial use, and (2) limited transferability due to the domain gap\nbetween web images and industrial imagery. Even recent vision foundation\nmodels, including the segment anything model (SAM), show notable performance\ndegradation in industrial settings. These challenges raise critical questions:\nCan we build a vision foundation model for industrial applications without\nrelying on real images or manual annotations? And can such models outperform\neven fine-tuned SAM on industrial datasets? To address these questions, we\npropose the Instance Core Segmentation Dataset (InsCore), a synthetic\npre-training dataset based on formula-driven supervised learning (FDSL).\nInsCore generates fully annotated instance segmentation images that reflect key\ncharacteristics of industrial data, including complex occlusions, dense\nhierarchical masks, and diverse non-rigid shapes, distinct from typical web\nimagery. Unlike previous methods, InsCore requires neither real images nor\nhuman annotations. Experiments on five industrial datasets show that models\npre-trained with InsCore outperform those trained on COCO and ImageNet-21k, as\nwell as fine-tuned SAM, achieving an average improvement of 6.2 points in\ninstance segmentation performance. This result is achieved using only 100k\nsynthetic images, more than 100 times fewer than the 11 million images in SAM's\nSA-1B dataset, demonstrating the data efficiency of our approach. These\nfindings position InsCore as a practical and license-free vision foundation\nmodel for industrial applications.", "authors": ["Shinichi Mae", "Ryosuke Yamada", "Hirokatsu Kataoka"], "published_date": "2025-05-19", "title_zh": "針對產業的合成分割預訓練", "summary_zh": "現有的圖像分割預訓練模型常受限於授權問題及與工業圖像的領域差距。為此，我們提出一個名為InsCore的合成預訓練數據集，它基於公式驅動的監督學習，能生成反映工業數據特徵的完整標註分割圖像，例如複雜的遮擋、密集的分層遮罩和多樣化的非剛性形狀。實驗證明，使用InsCore預訓練的模型，在五個工業數據集上的分割表現優於在COCO和ImageNet-21k上訓練的模型，甚至超越微調後的SAM模型，平均提升了6.2個百分點。重點是，InsCore僅使用10萬張合成圖像，效率遠高於SAM的SA-1B數據集，為工業應用提供了一種實用且無授權限制的視覺基礎模型。", "audio": "audios/2505.13099v1.mp3", "timestamp": "2025-05-20T14:18:44.078291"}
{"query": "Diffusion Model", "id": "2505.13152v1", "url": "http://arxiv.org/abs/2505.13152v1", "title": "Higher fidelity perceptual image and video compression with a latent conditioned residual denoising diffusion model", "summary": "Denoising diffusion models achieved impressive results on several image\ngeneration tasks often outperforming GAN based models. Recently, the generative\ncapabilities of diffusion models have been employed for perceptual image\ncompression, such as in CDC. A major drawback of these diffusion-based methods\nis that, while producing impressive perceptual quality images they are dropping\nin fidelity/increasing the distortion to the original uncompressed images when\ncompared with other traditional or learned image compression schemes aiming for\nfidelity. In this paper, we propose a hybrid compression scheme optimized for\nperceptual quality, extending the approach of the CDC model with a decoder\nnetwork in order to reduce the impact on distortion metrics such as PSNR. After\nusing the decoder network to generate an initial image, optimized for\ndistortion, the latent conditioned diffusion model refines the reconstruction\nfor perceptual quality by predicting the residual. On standard benchmarks, we\nachieve up to +2dB PSNR fidelity improvements while maintaining comparable\nLPIPS and FID perceptual scores when compared with CDC. Additionally, the\napproach is easily extensible to video compression, where we achieve similar\nresults.", "authors": ["Jonas Brenig", "Radu Timofte"], "published_date": "2025-05-19", "title_zh": "使用潛在條件殘差去噪擴散模型實現更高保真度的感知圖像與影片壓縮", "summary_zh": "本研究提出一種混合壓縮方法，旨在提升感知圖像與影片壓縮的品質和保真度。利用解碼器網路產生初步重建圖像，優化失真度，然後使用潛在條件擴散模型預測殘差，進一步提升感知品質。實驗結果顯示，在保持感知品質指標（如LPIPS和FID）不變的前提下，PSNR可提升高達2dB，且該方法能輕鬆擴展至影片壓縮，並獲得相似的成果。", "audio": "audios/2505.13152v1.mp3", "timestamp": "2025-05-20T14:18:48.810505"}
{"query": "AI", "id": "2505.13324v1", "url": "http://arxiv.org/abs/2505.13324v1", "title": "From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable AI", "summary": "Counterfactuals play a pivotal role in the two distinct data science fields\nof causal inference (CI) and explainable artificial intelligence (XAI). While\nthe core idea behind counterfactuals remains the same in both fields--the\nexamination of what would have happened under different circumstances--there\nare key differences in how they are used and interpreted. We introduce a formal\ndefinition that encompasses the multi-faceted concept of the counterfactual in\nCI and XAI. We then discuss how counterfactuals are used, evaluated, generated,\nand operationalized in CI vs. XAI, highlighting conceptual and practical\ndifferences. By comparing and contrasting the two, we hope to identify\nopportunities for cross-fertilization across CI and XAI.", "authors": ["Galit Shmueli", "David Martens", "Jaewon Yoo", "Travis Greene"], "published_date": "2025-05-19", "title_zh": "從「如果...會怎樣」到洞見：因果推論與可解釋人工智慧中的反事實分析", "summary_zh": "反事實分析在因果推論和可解釋人工智慧這兩個領域都扮演關鍵角色。雖然核心概念都是探討在不同情況下會發生什麼，但它們的使用和解釋方式存在差異。這篇論文定義了一個涵蓋因果推論和可解釋人工智慧中反事實分析的多面向概念，並比較了它們在應用、評估、生成和實用化方面的不同，旨在促進兩個領域的互相借鑒。", "audio": "audios/2505.13324v1.mp3", "timestamp": "2025-05-20T15:20:23.282037"}
{"query": "Foundation Model", "id": "2505.12890v1", "url": "http://arxiv.org/abs/2505.12890v1", "title": "ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling", "summary": "The real-world complexity of surgeries necessitates surgeons to have deep and\nholistic comprehension to ensure precision, safety, and effective\ninterventions. Computational systems are required to have a similar level of\ncomprehension within the operating room. Prior works, limited to single-task\nefforts like phase recognition or scene graph generation, lack scope and\ngeneralizability. In this work, we introduce ORQA, a novel OR question\nanswering benchmark and foundational multimodal model to advance OR\nintelligence. By unifying all four public OR datasets into a comprehensive\nbenchmark, we enable our approach to concurrently address a diverse range of OR\nchallenges. The proposed multimodal large language model fuses diverse OR\nsignals such as visual, auditory, and structured data, for a holistic modeling\nof the OR. Finally, we propose a novel, progressive knowledge distillation\nparadigm, to generate a family of models optimized for different speed and\nmemory requirements. We show the strong performance of ORQA on our proposed\nbenchmark, and its zero-shot generalization, paving the way for scalable,\nunified OR modeling and significantly advancing multimodal surgical\nintelligence. We will release our code and data upon acceptance.", "authors": ["Ege Özsoy", "Chantal Pellegrini", "David Bani-Harouni", "Kun Yuan", "Matthias Keicher", "Nassir Navab"], "published_date": "2025-05-19", "title_zh": "ORQA：整體手術室建模的基準和基礎模型", "summary_zh": "為了讓電腦系統也能理解手術室的複雜性，如同外科醫生一般，我們推出了ORQA，一個全新的手術室問答基準和多模態基礎模型。ORQA整合了現有公開的手術室數據集，可以同時處理多樣化的手術室挑戰。我們提出的多模態大型語言模型結合了視覺、聽覺和結構化數據等各種手術室訊號，以實現對手術室的整體建模。此外，我們還提出了一種漸進式知識蒸餾方法，可以生成一系列針對不同速度和記憶體需求的模型。實驗結果顯示，ORQA在基準測試中表現出色，並具有零樣本泛化能力，為可擴展、統一的手術室建模奠定了基礎，並顯著推進了多模態手術智慧。", "audio": "audios/2505.12890v1.mp3", "timestamp": "2025-05-20T15:20:37.753880"}
{"query": "Diffusion Model", "id": "2505.13138v1", "url": "http://arxiv.org/abs/2505.13138v1", "title": "Neurosymbolic Diffusion Models", "summary": "Neurosymbolic (NeSy) predictors combine neural perception with symbolic\nreasoning to solve tasks like visual reasoning. However, standard NeSy\npredictors assume conditional independence between the symbols they extract,\nthus limiting their ability to model interactions and uncertainty - often\nleading to overconfident predictions and poor out-of-distribution\ngeneralisation. To overcome the limitations of the independence assumption, we\nintroduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy\npredictors that use discrete diffusion to model dependencies between symbols.\nOur approach reuses the independence assumption from NeSy predictors at each\nstep of the diffusion process, enabling scalable learning while capturing\nsymbol dependencies and uncertainty quantification. Across both synthetic and\nreal-world benchmarks - including high-dimensional visual path planning and\nrule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among\nNeSy predictors and demonstrate strong calibration.", "authors": ["Emile van Krieken", "Pasquale Minervini", "Edoardo Ponti", "Antonio Vergari"], "published_date": "2025-05-19", "title_zh": "神經符號擴散模型", "summary_zh": "傳統神經符號模型假設符號之間彼此獨立，導致無法有效模擬互動和不確定性。為了解決這個問題，我們提出了神經符號擴散模型（NeSyDMs），利用離散擴散過程來模擬符號之間的依賴關係。NeSyDMs在擴散的每一步驟中重用獨立性假設，實現可擴展的學習，同時捕捉符號之間的依賴關係和量化不確定性。在合成和真實世界的基準測試中，包括高維視覺路徑規劃和基於規則的自動駕駛，NeSyDMs在神經符號預測器中實現了最先進的準確性，並展現出強大的校準能力。", "audio": "audios/2505.13138v1.mp3", "timestamp": "2025-05-20T15:20:46.326880"}
{"query": "AI", "id": "2505.13315v1", "url": "http://arxiv.org/abs/2505.13315v1", "title": "KHRONOS: a Kernel-Based Neural Architecture for Rapid, Resource-Efficient Scientific Computation", "summary": "Contemporary models of high dimensional physical systems are constrained by\nthe curse of dimensionality and a reliance on dense data. We introduce KHRONOS\n(Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an\nAI framework for model based, model free and model inversion tasks. KHRONOS\nconstructs continuously differentiable target fields with a hierarchical\ncomposition of per-dimension kernel expansions, which are tensorized into modes\nand then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation\nbenchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L2 square\nerrors of 5e-4 down to 6e-10. This represents a 100 time gain over Kolmogorov\nArnold Networks (which itself reports a 100 times improvement on MLPs/PINNs\nwith 100 times fewer parameters) when controlling for the number of parameters.\nThis also represents a 1e4 times improvement in L2 square error compared to\nstandard linear FEM at comparable DoFs. Inference complexity is dominated by\ninner products, yielding sub-millisecond full-field predictions that scale to\nan arbitrary resolution. For inverse problems, KHRONOS facilitates rapid,\niterative level set recovery in only a few forward evaluations, with\nsub-microsecond per sample latency. KHRONOS scalability, expressivity, and\ninterpretability open new avenues in constrained edge computing, online\ncontrol, computer vision, and beyond.", "authors": ["Reza T. Batley", "Sourav Saha"], "published_date": "2025-05-19", "title_zh": "KHRONOS：一種基於核心的類神經網路架構，用於快速、資源高效的科學計算", "summary_zh": "KHRONOS (核心擴展層級化簡階、神經優化代理模型) 是一個 AI 框架，能處理基於模型、無模型和模型反演的任務。它利用分層式的單維核心擴展構建連續可微的目標場，並透過張量化和疊加來提升效率。在 Poisson 方程的基準測試中，KHRONOS 展現了極高的準確性和速度，在參數數量相當的情況下，相比其他方法有顯著優勢。此外，KHRONOS 還能快速解決反問題，具有良好的延展性和可解釋性，未來有望應用於邊緣計算、線上控制、電腦視覺等領域。", "audio": "audios/2505.13315v1.mp3", "timestamp": "2025-05-20T16:23:27.181018"}
{"query": "Foundation Model", "id": "2505.12738v1", "url": "http://arxiv.org/abs/2505.12738v1", "title": "EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting", "summary": "Advanced epidemic forecasting is critical for enabling precision containment\nstrategies, highlighting its strategic importance for public health security.\nWhile recent advances in Large Language Models (LLMs) have demonstrated\neffectiveness as foundation models for domain-specific tasks, their potential\nfor epidemic forecasting remains largely unexplored. In this paper, we\nintroduce EpiLLM, a novel LLM-based framework tailored for spatio-temporal\nepidemic forecasting. Considering the key factors in real-world epidemic\ntransmission: infection cases and human mobility, we introduce a dual-branch\narchitecture to achieve fine-grained token-level alignment between such complex\nepidemic patterns and language tokens for LLM adaptation. To unleash the\nmulti-step forecasting and generalization potential of LLM architectures, we\npropose an autoregressive modeling paradigm that reformulates the epidemic\nforecasting task into next-token prediction. To further enhance LLM perception\nof epidemics, we introduce spatio-temporal prompt learning techniques, which\nstrengthen forecasting capabilities from a data-driven perspective. Extensive\nexperiments show that EpiLLM significantly outperforms existing baselines on\nreal-world COVID-19 datasets and exhibits scaling behavior characteristic of\nLLMs.", "authors": ["Chenghua Gong", "Rui Sun", "Yuhao Zheng", "Juyuan Zhang", "Tianjun Gu", "Liming Pan", "Linyuan Lv"], "published_date": "2025-05-19", "title_zh": "EpiLLM：釋放大型語言模型在流行病預測中的潛力", "summary_zh": "EpiLLM 是一種基於大型語言模型的新框架，專為時空流行病預測量身定制。它考慮了感染病例和人口流動等關鍵因素，並利用自迴歸建模將預測任務轉化為下一代詞預測。此外，還引入了時空提示學習技術以加強模型對流行病的理解。實驗結果表明，EpiLLM 在 COVID-19 數據集上顯著優於現有方法，並展現了大型語言模型的擴展特性。", "audio": "audios/2505.12738v1.mp3", "timestamp": "2025-05-20T16:23:32.743890"}
{"query": "Diffusion Model", "id": "2505.13131v1", "url": "http://arxiv.org/abs/2505.13131v1", "title": "Constraint-Aware Diffusion Guidance for Robotics: Real-Time Obstacle Avoidance for Autonomous Racing", "summary": "Diffusion models hold great potential in robotics due to their ability to\ncapture complex, high-dimensional data distributions. However, their lack of\nconstraint-awareness limits their deployment in safety-critical applications.\nWe propose Constraint-Aware Diffusion Guidance (CoDiG), a data-efficient and\ngeneral-purpose framework that integrates barrier functions into the denoising\nprocess, guiding diffusion sampling toward constraint-satisfying outputs. CoDiG\nenables constraint satisfaction even with limited training data and generalizes\nacross tasks. We evaluate our framework in the challenging setting of miniature\nautonomous racing, where real-time obstacle avoidance is essential. Real-world\nexperiments show that CoDiG generates safe outputs efficiently under dynamic\nconditions, highlighting its potential for broader robotic applications. A\ndemonstration video is available at https://youtu.be/KNYsTdtdxOU.", "authors": ["Hao Ma", "Sabrina Bodmer", "Andrea Carron", "Melanie Zeilinger", "Michael Muehlebach"], "published_date": "2025-05-19", "title_zh": "機器人約束感知擴散引導：自主競速的即時避障", "summary_zh": "擴散模型在機器人領域潛力巨大，但缺乏約束感知能力。我們提出「約束感知擴散引導 (CoDiG)」，它將障礙函數整合到去噪過程中，引导擴散採樣生成滿足約束的輸出。CoDiG能在訓練數據有限的情況下满足约束，並且具有泛化能力。我們在微型自主競速中驗證了該框架，CoDiG能高效地產生安全輸出，展現其在更廣泛機器人應用中的潛力。", "audio": "audios/2505.13131v1.mp3", "timestamp": "2025-05-20T16:23:38.404568"}
{"query": "AI", "id": "2505.13302v1", "url": "http://arxiv.org/abs/2505.13302v1", "title": "I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models", "summary": "Large language models are increasingly integrated into news recommendation\nsystems, raising concerns about their role in spreading misinformation. In\nhumans, visual content is known to boost credibility and shareability of\ninformation, yet its effect on vision-language models (VLMs) remains unclear.\nWe present the first study examining how images influence VLMs' propensity to\nreshare news content, whether this effect varies across model families, and how\npersona conditioning and content attributes modulate this behavior. To support\nthis analysis, we introduce two methodological contributions: a\njailbreaking-inspired prompting strategy that elicits resharing decisions from\nVLMs while simulating users with antisocial traits and political alignments;\nand a multimodal dataset of fact-checked political news from PolitiFact, paired\nwith corresponding images and ground-truth veracity labels. Experiments across\nmodel families reveal that image presence increases resharing rates by 4.8% for\ntrue news and 15.0% for false news. Persona conditioning further modulates this\neffect: Dark Triad traits amplify resharing of false news, whereas\nRepublican-aligned profiles exhibit reduced veracity sensitivity. Of all the\ntested models, only Claude-3-Haiku demonstrates robustness to visual\nmisinformation. These findings highlight emerging risks in multimodal model\nbehavior and motivate the development of tailored evaluation frameworks and\nmitigation strategies for personalized AI systems. Code and dataset are\navailable at: https://github.com/3lis/misinfo_vlm", "authors": ["Alice Plebe", "Timothy Douglas", "Diana Riazi", "R. Maria del Rio-Chanona"], "published_date": "2025-05-19", "title_zh": "眼見為憑：圖像會增加視覺語言模型中錯誤資訊的傳播", "summary_zh": "大型語言模型越來越多地被整合到新聞推薦系統中，引發了人們對其在傳播錯誤資訊方面所扮演角色的擔憂。研究發現，圖像會顯著增加視覺語言模型轉發新聞的意願，尤其是假新聞，轉發率提高了15%。特定人格特徵，例如「黑暗三性格」，以及政治立場，也會影響模型的轉發行為。只有Claude-3-Haiku模型對視覺錯誤資訊表現出較強的抵抗力。這項研究揭示了多模態模型行為中潛在的風險，並強調需要針對個性化AI系統開發評估框架和緩解策略。", "audio": "audios/2505.13302v1.mp3", "timestamp": "2025-05-20T17:16:15.208753"}
{"query": "Foundation Model", "id": "2505.12684v1", "url": "http://arxiv.org/abs/2505.12684v1", "title": "Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement", "summary": "Recent advances in graph machine learning have shifted to data-centric\nparadigms, driven by two emerging fields: (1) Federated graph learning (FGL)\nenables multi-client collaboration but faces challenges from data and task\nheterogeneity, limiting its practicality; (2) Graph foundation models (GFM)\noffer strong domain generalization but are usually trained on single machines,\nmissing out on cross-silo data and resources.\n  These paradigms are complementary, and their integration brings notable\nbenefits. Motivated by this, we propose FedGFM, a novel decentralized GFM\ntraining paradigm. However, a key challenge is knowledge entanglement, where\nmulti-domain knowledge merges into indistinguishable representations, hindering\ndownstream adaptation.\n  To address this, we present FedGFM+, an enhanced framework with two core\nmodules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based\ndomain-aware initialization strategy. Before pre-training, each client encodes\nits local graph into domain-specific prototypes that serve as semantic anchors.\nSynthetic embeddings around these anchors initialize the global model. We\ntheoretically prove these prototypes are distinguishable across domains,\nproviding a strong inductive bias to disentangle domain-specific knowledge. (2)\nAdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a\nlightweight graph prompt capturing domain semantics during pre-training. During\nfine-tuning, prompts from all clients form a pool from which the GFM selects\nrelevant prompts to augment target graph attributes, improving downstream\nadaptation.\n  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and\ntasks, outperforming 20 baselines from supervised learning, FGL, and federated\nGFM variants.", "authors": ["Yinlin Zhu", "Xunkai Li", "Jishuo Jia", "Miao Hu", "Di Wu", "Meikang Qiu"], "published_date": "2025-05-19", "title_zh": "邁向高效能聯邦圖基礎模型：透過降低知識糾纏", "summary_zh": "現今圖機器學習趨勢轉向以資料為中心，聯邦圖學習(FGL)和圖基礎模型(GFM)是兩個重要領域。FGL雖能促進多方協作，但受限於資料和任務異質性；GFM雖具備強大的領域泛化能力，卻常在單機上訓練，錯失跨機構的資料和資源。因此，我們提出FedGFM，一種去中心化的GFM訓練方法。然而，知識糾纏是主要挑戰，它會讓多領域知識混合成無法區分的表示，阻礙下游適應。為了解決此問題，我們提出FedGFM+，透過AncDAI（錨點式領域感知初始化）和AdaDPP（自適應領域敏感提示池）兩個核心模組來降低知識糾纏。AncDAI在預訓練前，將本地圖編碼成領域特定的原型作為語義錨點，並以此初始化全域模型，提供領域知識解耦的強烈歸納偏置。AdaDPP讓每個客戶端學習捕捉領域語義的輕量級圖提示，在微調時，將所有客戶端的提示形成提示池，GFM從中選擇相關提示來增強目標圖屬性，提升下游適應性。實驗證明，FedGFM+在多個領域和任務的八個基準測試中，優於20個基線模型。", "audio": "audios/2505.12684v1.mp3", "timestamp": "2025-05-20T17:16:25.053705"}
{"query": "Diffusion Model", "id": "2505.13091v1", "url": "http://arxiv.org/abs/2505.13091v1", "title": "Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction", "summary": "Diffusion models have made breakthroughs in 3D generation tasks. Current 3D\ndiffusion models focus on reconstructing target shape from images or a set of\npartial observations. While excelling in global context understanding, they\nstruggle to capture the local details of complex shapes and limited to the\nocclusion and lighting conditions. To overcome these limitations, we utilize\ntactile images to capture the local 3D information and propose a Touch2Shape\nmodel, which leverages a touch-conditioned diffusion model to explore and\nreconstruct the target shape from touch. For shape reconstruction, we have\ndeveloped a touch embedding module to condition the diffusion model in creating\na compact representation and a touch shape fusion module to refine the\nreconstructed shape. For shape exploration, we combine the diffusion model with\nreinforcement learning to train a policy. This involves using the generated\nlatent vector from the diffusion model to guide the touch exploration policy\ntraining through a novel reward design. Experiments validate the reconstruction\nquality thorough both qualitatively and quantitative analysis, and our touch\nexploration policy further boosts reconstruction performance.", "authors": ["Yuanbo Wang", "Zhaoxuan Zhang", "Jiajin Qiu", "Dilong Sun", "Zhengyu Meng", "Xiaopeng Wei", "Xin Yang"], "published_date": "2025-05-19", "title_zh": "Touch2Shape：觸摸條件下的3D擴散模型，用於形狀探索與重建", "summary_zh": "3D擴散模型在形狀生成上表現亮眼，但對複雜形狀的局部細節捕捉能力有限。本論文提出 Touch2Shape 模型，利用觸覺影像捕捉局部3D資訊，並結合觸摸條件的擴散模型來探索和重建目標形狀。模型包含觸摸嵌入模組，產生精簡表示，以及觸摸形狀融合模組，優化重建效果。此外，結合擴散模型與強化學習，訓練觸摸探索策略，進一步提升重建效能。實驗證明此方法能有效重建形狀，並且觸摸探索策略可以改善重建結果。", "audio": "audios/2505.13091v1.mp3", "timestamp": "2025-05-20T17:16:30.788447"}
{"query": "AI", "id": "2505.13292v1", "url": "http://arxiv.org/abs/2505.13292v1", "title": "Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms of AI Systems by Integrating Federated Learning and LLMs", "summary": "In the age of cloud computing, data privacy protection has become a major\nchallenge, especially when sharing sensitive data across cloud environments.\nHowever, how to optimize collaboration across cloud environments remains an\nunresolved problem. In this paper, we combine federated learning with\nlarge-scale language models to optimize the collaborative mechanism of AI\nsystems. Based on the existing federated learning framework, we introduce a\ncross-cloud architecture in which federated learning works by aggregating model\nupdates from decentralized nodes without exposing the original data. At the\nsame time, combined with large-scale language models, its powerful context and\nsemantic understanding capabilities are used to improve model training\nefficiency and decision-making ability. We've further innovated by introducing\na secure communication layer to ensure the privacy and integrity of model\nupdates and training data. The model enables continuous model adaptation and\nfine-tuning across different cloud environments while protecting sensitive\ndata. Experimental results show that the proposed method is significantly\nbetter than the traditional federated learning model in terms of accuracy,\nconvergence speed and data privacy protection.", "authors": ["Huaiying Luo", "Cheng Ji"], "published_date": "2025-05-19", "title_zh": "跨雲端資料隱私保護：整合聯邦學習與大型語言模型優化AI系統的協作機制", "summary_zh": "本研究探討在雲端運算時代，跨雲端共享敏感資料時的資料隱私保護挑戰。我們結合聯邦學習和大型語言模型，優化AI系統的協作機制。透過跨雲端架構，聯邦學習可在不洩露原始資料的情況下匯總模型更新。同時，利用大型語言模型的強大語義理解能力，提升模型訓練效率和決策能力。此外，引入安全通訊層確保模型更新和訓練資料的隱私和完整性。實驗結果顯示，相較於傳統聯邦學習模型，本方法在準確度、收斂速度和資料隱私保護方面有顯著提升。", "audio": "audios/2505.13292v1.mp3", "timestamp": "2025-05-20T18:26:43.658852"}
{"query": "Foundation Model", "id": "2505.12638v1", "url": "http://arxiv.org/abs/2505.12638v1", "title": "ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data", "summary": "The advent of single-cell Assay for Transposase-Accessible Chromatin using\nsequencing (scATAC-seq) offers an innovative perspective for deciphering\nregulatory mechanisms by assembling a vast repository of single-cell chromatin\naccessibility data. While foundation models have achieved significant success\nin single-cell transcriptomics, there is currently no foundation model for\nscATAC-seq that supports zero-shot high-quality cell identification and\ncomprehensive multi-omics analysis simultaneously. Key challenges lie in the\nhigh dimensionality and sparsity of scATAC-seq data, as well as the lack of a\nstandardized schema for representing open chromatin regions (OCRs). Here, we\npresent \\textbf{ChromFound}, a foundation model tailored for scATAC-seq.\nChromFound utilizes a hybrid architecture and genome-aware tokenization to\neffectively capture genome-wide long contexts and regulatory signals from\ndynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues\nand 6 disease conditions, ChromFound demonstrates broad applicability across 6\ndiverse tasks. Notably, it achieves robust zero-shot performance in generating\nuniversal cell representations and exhibits excellent transferability in cell\ntype annotation and cross-omics prediction. By uncovering enhancer-gene links\nundetected by existing computational methods, ChromFound offers a promising\nframework for understanding disease risk variants in the noncoding genome.", "authors": ["Yifeng Jiao", "Yuchen Liu", "Yu Zhang", "Xin Guo", "Yushuai Wu", "Chen Jiang", "Jiyang Li", "Hongwei Zhang", "Limei Han", "Xin Gao", "Yuan Qi", "Yuan Cheng"], "published_date": "2025-05-19", "title_zh": "ChromFound：邁向單細胞染色質可及性數據的通用基礎模型", "summary_zh": "隨著單細胞ATAC-seq技術的發展，我們得以以前所未有的視角解析調控機制。然而，雖然基礎模型在單細胞轉錄組學上取得了巨大成功，但在單細胞染色質可及性數據方面，卻缺乏一個能同時支持零樣本高質量細胞識別和全面多組學分析的基礎模型。為了解決這個問題，我們開發了ChromFound，一個專為單細胞ATAC-seq設計的基礎模型。它通過混合架構和基因組感知的Tokenization技術，有效地捕捉了全基因組的長程上下文和來自動態染色質環境的調控信號。ChromFound預訓練了來自30個組織和6種疾病條件的197萬個細胞，展示了廣泛的適用性，並在多項任務中表現出色，特別是在零樣本細胞表示生成和跨組學預測方面。ChromFound還有望幫助我們理解非編碼基因組中的疾病風險變異。", "audio": "audios/2505.12638v1.mp3", "timestamp": "2025-05-20T18:26:52.420514"}
{"query": "Diffusion Model", "id": "2505.13023v1", "url": "http://arxiv.org/abs/2505.13023v1", "title": "Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions", "summary": "As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models.", "authors": ["Yimao Guo", "Zuomin Qu", "Wei Lu", "Xiangyang Luo"], "published_date": "2025-05-19", "title_zh": "反填補：針對未知條件下基於惡意擴散模型的影像填補器的預防性防禦", "summary_zh": "基於擴散模型的惡意影像篡改日益普遍，針對此問題，我們提出「反填補」這種預防性防禦機制。它透過多層級特徵提取、多尺度語義保留的資料擴增，以及基於選擇的分布偏差優化策略，在未知條件下也能有效地保護影像，抵禦惡意影像填補，並在多項實驗中證明了其效能和魯棒性。", "audio": "audios/2505.13023v1.mp3", "timestamp": "2025-05-20T18:27:07.718187"}
{"query": "AI", "id": "2505.13259v1", "url": "http://arxiv.org/abs/2505.13259v1", "title": "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery", "summary": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.", "authors": ["Tianshi Zheng", "Zheye Deng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Zihao Wang", "Yangqiu Song"], "published_date": "2025-05-19", "title_zh": "從自動化到自主化：大型語言模型在科學發現中的綜述", "summary_zh": "大型語言模型正在徹底改變科學研究。它們不再只是自動化工具，而是逐漸變成具有自主性的智能體，重塑研究流程和人機協作模式。本綜述系統性地探討了這個新興領域，重點關注大型語言模型在科學領域中不斷變化的角色和日益提升的能力。我們從科學方法出發，提出了工具、分析師和科學家三個層級的分類，來描述模型自主性的演進。此外，我們也指出了機器人自動化、自我改進和倫理治理等關鍵挑戰和未來研究方向。總之，本綜述提供了一個概念框架和策略遠見，旨在引導和塑造AI驅動的科學發現的未來，促進快速創新和負責任的發展。", "audio": "audios/2505.13259v1.mp3", "timestamp": "2025-05-20T19:14:36.941246"}
{"query": "Foundation Model", "id": "2505.12583v1", "url": "http://arxiv.org/abs/2505.12583v1", "title": "A Comprehensive Survey on Physical Risk Control in the Era of Foundation Model-enabled Robotics", "summary": "Recent Foundation Model-enabled robotics (FMRs) display greatly improved\ngeneral-purpose skills, enabling more adaptable automation than conventional\nrobotics. Their ability to handle diverse tasks thus creates new opportunities\nto replace human labor. However, unlike general foundation models, FMRs\ninteract with the physical world, where their actions directly affect the\nsafety of humans and surrounding objects, requiring careful deployment and\ncontrol. Based on this proposition, our survey comprehensively summarizes robot\ncontrol approaches to mitigate physical risks by covering all the lifespan of\nFMRs ranging from pre-deployment to post-accident stage. Specifically, we\nbroadly divide the timeline into the following three phases: (1) pre-deployment\nphase, (2) pre-incident phase, and (3) post-incident phase. Throughout this\nsurvey, we find that there is much room to study (i) pre-incident risk\nmitigation strategies, (ii) research that assumes physical interaction with\nhumans, and (iii) essential issues of foundation models themselves. We hope\nthat this survey will be a milestone in providing a high-resolution analysis of\nthe physical risks of FMRs and their control, contributing to the realization\nof a good human-robot relationship.", "authors": ["Takeshi Kojima", "Yaonan Zhu", "Yusuke Iwasawa", "Toshinori Kitamura", "Gang Yan", "Shu Morikuni", "Ryosuke Takanami", "Alfredo Solano", "Tatsuya Matsushima", "Akiko Murakami", "Yutaka Matsuo"], "published_date": "2025-05-19", "title_zh": "基於基礎模型的機器人時代物理風險控制全面綜述", "summary_zh": "近年來，基於基礎模型的機器人展現出更強的通用能力，使得機器人能更靈活地自動化。然而，與一般基礎模型不同，它們會與物理世界互動，其行為直接影響人類和周遭物體的安全，需要仔細部署和控制。本綜述全面總結了機器人控制方法，以減輕物理風險，涵蓋從部署前到事故後的整個生命週期，並將時間線分為部署前、事故前和事故後三個階段。研究發現，事故前的風險緩解策略、假設與人類進行物理互動的研究以及基礎模型本身的基本問題，都還有很大的研究空間。希望本綜述能為分析基於基礎模型的機器人的物理風險及其控制提供高解析度的分析，從而有助於實現良好的人機關係。", "audio": "audios/2505.12583v1.mp3", "timestamp": "2025-05-20T19:14:49.584037"}
{"query": "Diffusion Model", "id": "2505.12935v1", "url": "http://arxiv.org/abs/2505.12935v1", "title": "LatentINDIGO: An INN-Guided Latent Diffusion Algorithm for Image Restoration", "summary": "There is a growing interest in the use of latent diffusion models (LDMs) for\nimage restoration (IR) tasks due to their ability to model effectively the\ndistribution of natural images. While significant progress has been made, there\nare still key challenges that need to be addressed. First, many approaches\ndepend on a predefined degradation operator, making them ill-suited for complex\nor unknown degradations that deviate from standard analytical models. Second,\nmany methods struggle to provide a stable guidance in the latent space and\nfinally most methods convert latent representations back to the pixel domain\nfor guidance at every sampling iteration, which significantly increases\ncomputational and memory overhead. To overcome these limitations, we introduce\na wavelet-inspired invertible neural network (INN) that simulates degradations\nthrough a forward transform and reconstructs lost details via the inverse\ntransform. We further integrate this design into a latent diffusion pipeline\nthrough two proposed approaches: LatentINDIGO-PixelINN, which operates in the\npixel domain, and LatentINDIGO-LatentINN, which stays fully in the latent space\nto reduce complexity. Both approaches alternate between updating intermediate\nlatent variables under the guidance of our INN and refining the INN forward\nmodel to handle unknown degradations. In addition, a regularization step\npreserves the proximity of latent variables to the natural image manifold.\nExperiments demonstrate that our algorithm achieves state-of-the-art\nperformance on synthetic and real-world low-quality images, and can be readily\nadapted to arbitrary output sizes.", "authors": ["Di You", "Daniel Siromani", "Pier Luigi Dragotti"], "published_date": "2025-05-19", "title_zh": "潛在INDIGO：一種用於影像修復的INN引導潛在擴散演算法", "summary_zh": "潛在擴散模型在影像修復領域越來越受歡迎，但現有方法在處理複雜或未知降質、提供穩定潛在空間引導，以及計算效率等方面仍存在挑戰。本文提出一種名為LatentINDIGO的演算法，它使用波小波啟發的可逆神經網路（INN）來模擬降質過程，並通過逆變換重建丟失的細節。該演算法有兩個版本：PixelINN版本在像素域操作，LatentINN版本則完全在潛在空間中操作，以減少複雜度。這兩種方法交替更新潛在變量和精煉INN模型，並通過正則化步驟確保潛在變量接近自然圖像流形。實驗結果表明，該演算法在合成和真實低質量圖像上均取得了最先進的性能，並且可以輕鬆適應任意輸出尺寸。", "audio": "audios/2505.12935v1.mp3", "timestamp": "2025-05-20T19:14:58.408447"}
{"query": "AI", "id": "2505.13246v1", "url": "http://arxiv.org/abs/2505.13246v1", "title": "Agentic Publications: An LLM-Driven Framework for Interactive Scientific Publishing, Supplementing Traditional Papers with AI-Powered Knowledge Systems", "summary": "The exponential growth of scientific literature presents significant\nchallenges for researchers navigating the complex knowledge landscape. We\npropose \"Agentic Publications\", a novel LLM-driven framework complementing\ntraditional publishing by transforming papers into interactive knowledge\nsystems. Our architecture integrates structured data with unstructured content\nthrough retrieval-augmented generation and multi-agent verification. The\nframework offers interfaces for both humans and machines, combining narrative\nexplanations with machine-readable outputs while addressing ethical\nconsiderations through automated validation and transparent governance. Key\nfeatures include continuous knowledge updates, automatic integration of new\nfindings, and customizable detail levels. Our proof-of-concept demonstrates\nmultilingual interaction, API accessibility, and structured knowledge\nrepresentation through vector databases, knowledge graphs, and verification\nagents. This approach enhances scientific communication across disciplines,\nimproving efficiency and collaboration while preserving traditional publishing\npathways, particularly valuable for interdisciplinary fields where knowledge\nintegration remains challenging.", "authors": ["Roberto Pugliese", "George Kourousias", "Francesco Venier", "Grazia Garlatti Costa"], "published_date": "2025-05-19", "title_zh": "具代理能力的出版品：一個由大型語言模型驅動的互動式科學出版框架，透過 AI 驅動的知識系統來補充傳統論文", "summary_zh": "科學文獻爆炸性成長，研究人員難以掌握。本研究提出「具代理能力的出版品」框架，利用大型語言模型將傳統論文轉化為互動式知識系統，結合結構化和非結構化數據，並透過多重代理驗證確保準確性。此框架提供人機介面，具備知識持續更新、自動整合新發現等功能。此方法透過提升跨領域的科學交流效率和協作，並保留傳統出版途徑，尤其對於知識整合困難的跨領域研究而言，更具價值。", "audio": "audios/2505.13246v1.mp3", "timestamp": "2025-05-20T20:20:44.106068"}
{"query": "Foundation Model", "id": "2505.12534v1", "url": "http://arxiv.org/abs/2505.12534v1", "title": "ChemPile: A 250GB Diverse and Curated Dataset for Chemical Foundation Models", "summary": "Foundation models have shown remarkable success across scientific domains,\nyet their impact in chemistry remains limited due to the absence of diverse,\nlarge-scale, high-quality datasets that reflect the field's multifaceted\nnature. We present the ChemPile, an open dataset containing over 75 billion\ntokens of curated chemical data, specifically built for training and evaluating\ngeneral-purpose models in the chemical sciences. The dataset mirrors the human\nlearning journey through chemistry -- from educational foundations to\nspecialized expertise -- spanning multiple modalities and content types\nincluding structured data in diverse chemical representations (SMILES, SELFIES,\nIUPAC names, InChI, molecular renderings), scientific and educational text,\nexecutable code, and chemical images. ChemPile integrates foundational\nknowledge (textbooks, lecture notes), specialized expertise (scientific\narticles and language-interfaced data), visual understanding (molecular\nstructures, diagrams), and advanced reasoning (problem-solving traces and code)\n-- mirroring how human chemists develop expertise through diverse learning\nmaterials and experiences. Constructed through hundreds of hours of expert\ncuration, the ChemPile captures both foundational concepts and domain-specific\ncomplexity. We provide standardized training, validation, and test splits,\nenabling robust benchmarking. ChemPile is openly released via HuggingFace with\na consistent API, permissive license, and detailed documentation. We hope the\nChemPile will serve as a catalyst for chemical AI, enabling the development of\nthe next generation of chemical foundation models.", "authors": ["Adrian Mirza", "Nawaf Alampara", "Martiño Ríos-García", "Mohamed Abdelalim", "Jack Butler", "Bethany Connolly", "Tunca Dogan", "Marianna Nezhurina", "Bünyamin Şen", "Santosh Tirunagari", "Mark Worrall", "Adamo Young", "Philippe Schwaller", "Michael Pieler", "Kevin Maik Jablonka"], "published_date": "2025-05-18", "title_zh": "ChemPile：一個250GB的多樣化且精心策劃的化學基礎模型數據集", "summary_zh": "ChemPile是一個開放的250GB化學數據集，包含超過750億個tokens，專為訓練和評估化學領域的通用模型而設計。它涵蓋結構化數據、文本、程式碼和圖像等多種形式，模擬人類學習化學的過程，從基礎知識到專業知識，致力於推動化學人工智慧的發展，並助力新一代化學基礎模型的誕生。", "audio": "audios/2505.12534v1.mp3", "timestamp": "2025-05-20T20:20:49.050176"}
{"query": "Diffusion Model", "id": "2505.12882v1", "url": "http://arxiv.org/abs/2505.12882v1", "title": "PhyDA: Physics-Guided Diffusion Models for Data Assimilation in Atmospheric Systems", "summary": "Data Assimilation (DA) plays a critical role in atmospheric science by\nreconstructing spatially continous estimates of the system state, which serves\nas initial conditions for scientific analysis. While recent advances in\ndiffusion models have shown great potential for DA tasks, most existing\napproaches remain purely data-driven and often overlook the physical laws that\ngovern complex atmospheric dynamics. As a result, they may yield physically\ninconsistent reconstructions that impair downstream applications. To overcome\nthis limitation, we propose PhyDA, a physics-guided diffusion framework\ndesigned to ensure physical coherence in atmospheric data assimilation. PhyDA\nintroduces two key components: (1) a Physically Regularized Diffusion Objective\nthat integrates physical constraints into the training process by penalizing\ndeviations from known physical laws expressed as partial differential\nequations, and (2) a Virtual Reconstruction Encoder that bridges observational\nsparsity for structured latent representations, further enhancing the model's\nability to infer complete and physically coherent states. Experiments on the\nERA5 reanalysis dataset demonstrate that PhyDA achieves superior accuracy and\nbetter physical plausibility compared to state-of-the-art baselines. Our\nresults emphasize the importance of combining generative modeling with\ndomain-specific physical knowledge and show that PhyDA offers a promising\ndirection for improving real-world data assimilation systems.", "authors": ["Hao Wang", "Jindong Han", "Wei Fan", "Weijia Zhang", "Hao Liu"], "published_date": "2025-05-19", "title_zh": "PhyDA：物理引導的擴散模型用於大氣系統中的資料同化", "summary_zh": "PhyDA是一個新型的大氣資料同化框架，它利用物理定律引導擴散模型，確保重建的大氣狀態不僅準確，而且符合物理規律。它透過將物理約束納入訓練目標，並使用編碼器來處理觀測資料的稀疏性，從而優於傳統方法，更適用於實際應用。", "audio": "audios/2505.12882v1.mp3", "timestamp": "2025-05-20T20:20:53.711520"}
{"query": "AI", "id": "2505.14680v1", "url": "http://arxiv.org/abs/2505.14680v1", "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search", "summary": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.", "authors": ["Sunhao Dai", "Wenjie Wang", "Liang Pang", "Jun Xu", "See-Kiong Ng", "Ji-Rong Wen", "Tat-Seng Chua"], "published_date": "2025-05-20", "title_zh": "NExT-Search：重建生成式AI搜尋的使用者回饋生態系統", "summary_zh": "生成式AI搜尋雖然方便，但打破了傳統搜尋仰賴使用者回饋不斷改進的機制。傳統搜尋可以透過使用者點擊、停留時間等精細回饋來優化排序模型。而生成式AI搜尋流程更長，使用者僅對最終答案提供粗略回饋，難以追溯問題源頭，導致系統難以改進。本研究提出NExT-Search，透過「使用者除錯模式」讓使用者介入關鍵步驟，並利用「影子使用者模式」模擬使用者偏好提供AI輔助回饋，重新引入精細的流程級回饋。這些回饋將用於線上即時調整搜尋結果，以及離線微調查詢分解、檢索和生成模型，最終打造能夠持續根據使用者回饋進化的AI搜尋系統。", "audio": "audios/2505.14680v1.mp3", "timestamp": "2025-05-21T03:11:29.471335"}
{"query": "Foundation Model", "id": "2505.14683v1", "url": "http://arxiv.org/abs/2505.14683v1", "title": "Emerging Properties in Unified Multimodal Pretraining", "summary": "Unifying multimodal understanding and generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0source foundational model that natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\non trillions of tokens curated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such as free-form image manipulation,\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/", "authors": ["Chaorui Deng", "Deyao Zhu", "Kunchang Li", "Chenhui Gou", "Feng Li", "Zeyu Wang", "Shu Zhong", "Weihao Yu", "Xiaonan Nie", "Ziang Song", "Guang Shi", "Haoqi Fan"], "published_date": "2025-05-20", "title_zh": "統一多模態預訓練中湧現的特性", "summary_zh": "本研究介紹了開放原始碼的多模態基礎模型 BAGEL，它能同時理解和生成多模態內容。BAGEL 基於大量的文字、圖片、影片和網路數據進行預訓練，展現了在複雜多模態推理方面的能力。在多模態生成和理解方面，BAGEL 的表現明顯優於其他開放原始碼的統一模型，並且具備進階的多模態推理能力，例如自由形式的圖像操作、未來幀預測、3D 操作和世界導航。研究團隊分享了重要的發現、預訓練細節、數據創建協議，並公開了程式碼和模型權重，希望能促進多模態研究的發展。", "audio": "audios/2505.14683v1.mp3", "timestamp": "2025-05-21T03:11:36.102262"}
{"query": "Diffusion Model", "id": "2505.14673v1", "url": "http://arxiv.org/abs/2505.14673v1", "title": "Training-Free Watermarking for Autoregressive Image Generation", "summary": "Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.", "authors": ["Yu Tong", "Zihao Pan", "Shuai Yang", "Kaiyang Zhou"], "published_date": "2025-05-20", "title_zh": "無需訓練的自迴歸圖像生成浮水印", "summary_zh": "一種為自迴歸圖像生成模型設計的，無需訓練的浮水印框架IndexMark。它利用碼本的冗餘特性，將自迴歸生成的索引替換為視覺上相似的索引，以嵌入肉眼難以察覺的浮水印，且不影響圖像質量。透過計算生成圖像中浮水印標記的比例來驗證浮水印，並使用索引編碼器進一步提高精度。實驗表明，IndexMark在圖像質量和驗證準確性方面都表現出色，並且對各種攻擊具有魯棒性，例如裁剪、噪聲、模糊等等。", "audio": "audios/2505.14673v1.mp3", "timestamp": "2025-05-21T03:11:41.526487"}
{"query": "AI", "id": "2505.14677v1", "url": "http://arxiv.org/abs/2505.14677v1", "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning", "summary": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.", "authors": ["Jiaer Xia", "Yuhang Zang", "Peng Gao", "Yixuan Li", "Kaiyang Zhou"], "published_date": "2025-05-20", "title_zh": "Visionary-R1：利用強化學習減輕視覺推理中的捷徑", "summary_zh": "大型語言模型(LLM)利用強化學習在推理方面取得進展。本研究旨在透過強化學習訓練視覺語言模型(VLM)進行圖像推理，無需逐步思考(CoT)的監督。研究發現，直接應用強化學習於VLM可能會因簡單問題而產生捷徑，降低其泛化能力。為解決此問題，本研究提出先生成圖像的詳細描述，再進行推理的Caption-Reason-Answer方法。訓練模型Visionary-R1後，其在多個視覺推理基準測試中超越了GPT-4o等強大的多模態模型。", "audio": "audios/2505.14677v1.mp3", "timestamp": "2025-05-21T04:22:43.916166"}
{"query": "Foundation Model", "id": "2505.14648v1", "url": "http://arxiv.org/abs/2505.14648v1", "title": "Vox-Profile: A Speech Foundation Model Benchmark for Characterizing Diverse Speaker and Speech Traits", "summary": "We introduce Vox-Profile, a comprehensive benchmark to characterize rich\nspeaker and speech traits using speech foundation models. Unlike existing works\nthat focus on a single dimension of speaker traits, Vox-Profile provides\nholistic and multi-dimensional profiles that reflect both static speaker traits\n(e.g., age, sex, accent) and dynamic speech properties (e.g., emotion, speech\nflow). This benchmark is grounded in speech science and linguistics, developed\nwith domain experts to accurately index speaker and speech characteristics. We\nreport benchmark experiments using over 15 publicly available speech datasets\nand several widely used speech foundation models that target various static and\ndynamic speaker and speech properties. In addition to benchmark experiments, we\nshowcase several downstream applications supported by Vox-Profile. First, we\nshow that Vox-Profile can augment existing speech recognition datasets to\nanalyze ASR performance variability. Vox-Profile is also used as a tool to\nevaluate the performance of speech generation systems. Finally, we assess the\nquality of our automated profiles through comparison with human evaluation and\nshow convergent validity. Vox-Profile is publicly available at:\nhttps://github.com/tiantiaf0627/vox-profile-release.", "authors": ["Tiantian Feng", "Jihwan Lee", "Anfeng Xu", "Yoonjeong Lee", "Thanathai Lertpetchpun", "Xuan Shi", "Helin Wang", "Thomas Thebaud", "Laureano Moro-Velazquez", "Dani Byrd", "Najim Dehak", "Shrikanth Narayanan"], "published_date": "2025-05-20", "title_zh": "Vox-Profile: 一個用於表徵多樣化說話者和語音特徵的語音基礎模型基準", "summary_zh": "Vox-Profile 是一個全面的基準測試，旨在利用語音基礎模型來分析說話者和語音的豐富特徵。它不僅關注說話者的年齡、性別、口音等靜態特徵，還包含情緒、語速等動態語音屬性。該基準基於語音科學和語言學，由領域專家開發，能準確地索引說話者和語音的特徵。研究者使用超過15個公開語音數據集和多個主流語音基礎模型進行了基準測試，並展示了Vox-Profile在增強語音識別數據集、評估語音生成系統和驗證自動分析結果等方面的應用。Vox-Profile程式碼已公開。", "audio": "audios/2505.14648v1.mp3", "timestamp": "2025-05-21T04:22:49.028751"}
{"query": "Diffusion Model", "id": "2505.14556v1", "url": "http://arxiv.org/abs/2505.14556v1", "title": "Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI", "summary": "Brain-to-image decoding has been recently propelled by the progress in\ngenerative AI models and the availability of large ultra-high field functional\nMagnetic Resonance Imaging (fMRI). However, current approaches depend on\ncomplicated multi-stage pipelines and preprocessing steps that typically\ncollapse the temporal dimension of brain recordings, thereby limiting\ntime-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural\nActivity Diffusion for Image Reconstruction), a new single-stage diffusion\nmodel designed for reconstructing images from dynamically evolving fMRI\nrecordings. Our approach offers three main contributions. First, Dynadiff\nsimplifies training as compared to existing approaches. Second, our model\noutperforms state-of-the-art models on time-resolved fMRI signals, especially\non high-level semantic image reconstruction metrics, while remaining\ncompetitive on preprocessed fMRI data that collapse time. Third, this approach\nallows a precise characterization of the evolution of image representations in\nbrain activity. Overall, this work lays the foundation for time-resolved\nbrain-to-image decoding.", "authors": ["Marlène Careil", "Yohann Benchetrit", "Jean-Rémi King"], "published_date": "2025-05-20", "title_zh": "Dynadiff: 從持續演進的fMRI數據單階段解碼圖像", "summary_zh": "近年來，腦部到圖像的解碼技術，受益於生成式AI和高場強功能性磁振造影（fMRI）的發展。然而，現有方法依賴複雜的多階段流程，並通常會壓縮腦部記錄的時間維度，限制了時間分辨的腦部解碼器。我們提出Dynadiff，一種新的單階段擴散模型，旨在從動態演進的fMRI記錄中重建圖像。Dynadiff簡化了訓練流程，在時間分辨的fMRI訊號上優於現有模型，特別是在高階語義圖像重建指標上，同時在預處理過的、時間維度已壓縮的fMRI數據上仍具競爭力。此外，它能精確描述腦部活動中圖像表徵的演進過程。這項研究為時間分辨的腦部到圖像解碼奠定了基礎。", "audio": "audios/2505.14556v1.mp3", "timestamp": "2025-05-21T04:22:55.811957"}
{"query": "AI", "id": "2505.14675v1", "url": "http://arxiv.org/abs/2505.14675v1", "title": "Semi-parametric efficient estimation of small genetic effects in large-scale population cohorts", "summary": "Population genetics seeks to quantify DNA variant associations with traits or\ndiseases, as well as interactions among variants and with environmental\nfactors. Computing millions of estimates in large cohorts in which small effect\nsizes are expected, necessitates minimising model-misspecification bias to\ncontrol false discoveries. We present TarGene, a unified statistical workflow\nfor the semi-parametric efficient and double robust estimation of genetic\neffects including k-point interactions among categorical variables in the\npresence of confounding and weak population dependence. k-point interactions,\nor Average Interaction Effects (AIEs), are a direct generalisation of the usual\naverage treatment effect (ATE). We estimate AIEs with cross-validated and/or\nweighted versions of Targeted Minimum Loss-based Estimators (TMLE) and One-Step\nEstimators (OSE). The effect of dependence among data units on variance\nestimates is corrected by using sieve plateau variance estimators based on\ngenetic relatedness across the units. We present extensive realistic\nsimulations to demonstrate power, coverage, and control of type I error. Our\nmotivating application is the targeted estimation of genetic effects on trait,\nincluding two-point and higher-order gene-gene and gene-environment\ninteractions, in large-scale genomic databases such as UK Biobank and All of\nUs. All cross-validated and/or weighted TMLE and OSE for the AIE k-point\ninteraction, as well as ATEs, conditional ATEs and functions thereof, are\nimplemented in the general purpose Julia package TMLE.jl. For high-throughput\napplications in population genomics, we provide the open-source Nextflow\npipeline and software TarGene which integrates seamlessly with modern\nhigh-performance and cloud computing platforms.", "authors": ["Olivier Labayle", "Breeshey Roskams-Hieter", "Joshua Slaughter", "Kelsey Tetley-Campbell", "Mark J. van der Laan", "Chris P. Ponting", "Sjoerd Viktor Beentjes", "Ava Khamseh"], "published_date": "2025-05-20", "title_zh": "大規模群體世代研究中小型遺傳效應的半參數有效估計", "summary_zh": "本研究提出 TarGene，一個統一的統計流程，旨在準確且高效地估計大規模基因體數據庫中小型遺傳效應，即使存在混雜因素和弱群體依賴性。TarGene 使用半參數方法，包括目標最小損失估計器（TMLE）和單步估計器（OSE），並結合交叉驗證和加權策略，來估計基因間和基因與環境間的多點交互作用（平均交互效應 AIE）。透過基於遺傳相關性的篩法平穩方差估計器，修正數據單元間依賴性對方差估計的影響。TarGene 的目標應用是在如 UK Biobank 和 All of Us 等大型數據庫中，針對性地估計遺傳效應，包括高階基因-基因和基因-環境交互作用。所有方法都實現在 Julia 語言的 TMLE.jl 包中，並提供 Nextflow 流程 TarGene 方便在高通量環境下使用。", "audio": "audios/2505.14675v1.mp3", "timestamp": "2025-05-21T06:27:20.327572"}
{"query": "Foundation Model", "id": "2505.14603v1", "url": "http://arxiv.org/abs/2505.14603v1", "title": "Towards a Foundation Model for Communication Systems", "summary": "Artificial Intelligence (AI) has demonstrated unprecedented performance\nacross various domains, and its application to communication systems is an\nactive area of research. While current methods focus on task-specific\nsolutions, the broader trend in AI is shifting toward large general models\ncapable of supporting multiple applications. In this work, we take a step\ntoward a foundation model for communication data--a transformer-based,\nmulti-modal model designed to operate directly on communication data. We\npropose methodologies to address key challenges, including tokenization,\npositional embedding, multimodality, variable feature sizes, and normalization.\nFurthermore, we empirically demonstrate that such a model can successfully\nestimate multiple features, including transmission rank, selected precoder,\nDoppler spread, and delay profile.", "authors": ["Davide Buffelli", "Sowmen Das", "Yu-Wei Lin", "Sattar Vakili", "Chien-Yi Wang", "Masoud Attarifar", "Pritthijit Nath", "Da-shan Shiu"], "published_date": "2025-05-20", "title_zh": "邁向通訊系統的基礎模型", "summary_zh": "本文旨在探索通訊系統領域的基礎模型。 借鑒AI領域發展趨勢，提出一個基於Transformer的多模態模型，直接處理通訊數據。研究針對通訊數據的特殊性，解決了分詞、位置嵌入、多模態、可變特徵尺寸和正規化等關鍵挑戰。實驗結果顯示，該模型能有效預測多種通訊指標，例如傳輸等級、預編碼器、都卜勒頻展和延遲分佈。", "audio": "audios/2505.14603v1.mp3", "timestamp": "2025-05-21T06:27:23.893770"}
{"query": "Diffusion Model", "id": "2505.14521v1", "url": "http://arxiv.org/abs/2505.14521v1", "title": "SparC: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling", "summary": "High-fidelity 3D object synthesis remains significantly more challenging than\n2D image generation due to the unstructured nature of mesh data and the cubic\ncomplexity of dense volumetric grids. Existing two-stage pipelines-compressing\nmeshes with a VAE (using either 2D or 3D supervision), followed by latent\ndiffusion sampling-often suffer from severe detail loss caused by inefficient\nrepresentations and modality mismatches introduced in VAE. We introduce SparC,\na unified framework that combines a sparse deformable marching cubes\nrepresentation SparseCubes with a novel encoder SparConv-VAE. SparseCubes\nconverts raw meshes into high-resolution ($1024^3$) surfaces with arbitrary\ntopology by scattering signed distance and deformation fields onto a sparse\ncube, allowing differentiable optimization. SparConv-VAE is the first\nmodality-consistent variational autoencoder built entirely upon sparse\nconvolutional networks, enabling efficient and near-lossless 3D reconstruction\nsuitable for high-resolution generative modeling through latent diffusion.\nSparC achieves state-of-the-art reconstruction fidelity on challenging inputs,\nincluding open surfaces, disconnected components, and intricate geometry. It\npreserves fine-grained shape details, reduces training and inference cost, and\nintegrates naturally with latent diffusion models for scalable, high-resolution\n3D generation.", "authors": ["Zhihao Li", "Yufei Wang", "Heliang Zheng", "Yihao Luo", "Bihan Wen"], "published_date": "2025-05-20", "title_zh": "SparC: 用於高解析度3D形狀建模的稀疏表示與建構", "summary_zh": "SparC是一个统一的3D模型生成框架，它結合了稀疏可變形移動立方體表示SparseCubes和新颖的編碼器SparConv-VAE。SparseCubes能将原始网格转换为高分辨率的表面，而SparConv-VAE則是首個完全基於稀疏卷積網絡的變分自編碼器，实现高效近乎無损的3D重建。SparC在高精度重建复杂3D模型方面表现出色，并且能与潜在扩散模型整合，实现可扩展的高分辨率3D生成。", "audio": "audios/2505.14521v1.mp3", "timestamp": "2025-05-21T06:27:28.672877"}
{"query": "AI", "id": "2505.14668v1", "url": "http://arxiv.org/abs/2505.14668v1", "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions", "summary": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants.", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Kaiwei Liu", "Siyang Jiang", "Wenrui Lu", "Hongkai Chen", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "published_date": "2025-05-20", "title_zh": "ContextAgent：具備開放世界感知能力的語境感知主動式大型語言模型代理", "summary_zh": "論文提出 ContextAgent，一個能主動提供協助的 AI 代理。它利用穿戴裝置的感測數據，像是影像和聲音，以及歷史資料，來理解使用者的意圖，並預測使用者是否需要協助。當需要協助時，ContextAgent 會自動調用工具來提供服務。研究團隊還建立了 ContextAgentBench 基準測試，證明 ContextAgent 在主動預測和工具調用方面都比其他方法更準確。目標是開發更先進、以人為本的主動式 AI 助理。", "audio": "audios/2505.14668v1.mp3", "timestamp": "2025-05-21T08:24:48.499842"}
{"query": "Foundation Model", "id": "2505.14543v1", "url": "http://arxiv.org/abs/2505.14543v1", "title": "Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions", "summary": "Traditional time series models are task-specific and often depend on\ndataset-specific training and extensive feature engineering. While\nTransformer-based architectures have improved scalability, foundation models,\ncommonplace in text, vision, and audio, remain under-explored for time series\nand are largely restricted to forecasting. We introduce $\\textbf{CHARM}$, a\nfoundation embedding model for multivariate time series that learns shared,\ntransferable, and domain-aware representations. To address the unique\ndifficulties of time series foundation learning, $\\textbf{CHARM}$ incorporates\narchitectural innovations that integrate channel-level textual descriptions\nwhile remaining invariant to channel order. The model is trained using a Joint\nEmbedding Predictive Architecture (JEPA), with novel augmentation schemes and a\nloss function designed to improve interpretability and training stability. Our\n$7$M-parameter model achieves state-of-the-art performance across diverse\ndownstream tasks, setting a new benchmark for time series representation\nlearning.", "authors": ["Utsav Dutta", "Sina Khoshfetrat Pakazad", "Henrik Ohlsson"], "published_date": "2025-05-20", "title_zh": "時間嵌入：利用通道描述解鎖時間序列基礎模型", "summary_zh": "傳統時間序列模型高度依賴特定任務和數據集，且需要大量特徵工程。雖然Transformer架構提升了可擴展性，但時間序列的基礎模型開發仍落後於文字、視覺和音訊領域，且主要集中在預測上。本研究提出一個名為CHARM的多元時間序列基礎嵌入模型，旨在學習可共享、可遷移且具領域感知性的表徵。CHARM結合了通道層級的文字描述，並具備通道順序不變性，克服了時間序列基礎學習的獨特挑戰。CHARM採用聯合嵌入預測架構（JEPA）進行訓練，結合創新的增強方案和損失函數，以提升可解釋性和訓練穩定性。僅有700萬參數的CHARM模型在各種下游任務中表現出色，為時間序列表徵學習設定了新的基準。", "audio": "audios/2505.14543v1.mp3", "timestamp": "2025-05-21T08:24:54.929489"}
{"query": "Diffusion Model", "id": "2505.14502v1", "url": "http://arxiv.org/abs/2505.14502v1", "title": "Learning to Integrate Diffusion ODEs by Averaging the Derivatives", "summary": "To accelerate diffusion model inference, numerical solvers perform poorly at\nextremely small steps, while distillation techniques often introduce complexity\nand instability. This work presents an intermediate strategy, balancing\nperformance and cost, by learning ODE integration using loss functions derived\nfrom the derivative-integral relationship, inspired by Monte Carlo integration\nand Picard iteration. From a geometric perspective, the losses operate by\ngradually extending the tangent to the secant, thus are named as secant losses.\nThe secant losses can rapidly convert (via fine-tuning or distillation) a\npretrained diffusion model into its secant version. In our experiments, the\nsecant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the\nsecant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID\nof $1.96$ on ImageNet-$256\\times256$. Code will be available.", "authors": ["Wenze Liu", "Xiangyu Yue"], "published_date": "2025-05-20", "title_zh": "透過平均導數學習整合擴散常微分方程式", "summary_zh": "為了加速擴散模型的推論速度，數值解法在極小步長下表現不佳，而知識蒸餾技術又常引入複雜性和不穩定性。本文提出一種中間策略，在性能和成本之間取得平衡，透過學習常微分方程式的積分，利用源自導數-積分關係的損失函數，靈感來自蒙地卡羅積分和皮卡迭代。從幾何角度來看，這些損失函數透過逐步將切線延伸至割線來運作，因此被命名為割線損失。割線損失可以快速地（透過微調或知識蒸餾）將預訓練的擴散模型轉換為其割線版本。在實驗中，EDM 的割線版本在 CIFAR-10 上僅需 10 步即可達到 2.14 的 FID，而 SiT-XL/2 的割線版本在 ImageNet-256x256 上僅需 4 步即可達到 2.27 的 FID，8 步可達 1.96 的 FID。程式碼將會公開。", "audio": "audios/2505.14502v1.mp3", "timestamp": "2025-05-21T08:25:02.392047"}
{"query": "AI", "id": "2505.14667v1", "url": "http://arxiv.org/abs/2505.14667v1", "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment", "summary": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI.", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Minsuk Kahng", "Albert No"], "published_date": "2025-05-20", "title_zh": "SAFEPATH：透過早期對齊預防鏈式思考中的有害推理", "summary_zh": "大型推理模型在解決複雜問題上表現出色，但鏈式思考過程可能在面對有害提示時產生不安全的輸出。現有安全對齊方法雖可減少有害輸出，但也可能降低推理深度，影響複雜任務的表現，且容易受到精巧的越獄攻擊。為此，我們提出 SAFEPATH，這是一種輕量級的對齊方法，透過微調大型推理模型，使其在收到有害提示時，於推理的開頭輸出一段簡短的 8 字元安全引言，同時讓剩餘的推理過程保持無監督。實驗結果顯示，SAFEPATH 能有效減少有害輸出，同時維持推理效能。我們還提出了一個零樣本變體，無需任何微調。此外，我們分析了現有大型語言模型方法應用於以推理為中心的模型時的泛化能力，揭示了關鍵的不足之處和更安全 AI 的新方向。", "audio": "audios/2505.14667v1.mp3", "timestamp": "2025-05-21T09:20:36.302657"}
{"query": "Foundation Model", "id": "2505.14417v1", "url": "http://arxiv.org/abs/2505.14417v1", "title": "Towards Non-Euclidean Foundation Models: Advancing AI Beyond Euclidean Frameworks", "summary": "In the era of foundation models and Large Language Models (LLMs), Euclidean\nspace is the de facto geometric setting of our machine learning architectures.\nHowever, recent literature has demonstrated that this choice comes with\nfundamental limitations. To that end, non-Euclidean learning is quickly gaining\ntraction, particularly in web-related applications where complex relationships\nand structures are prevalent. Non-Euclidean spaces, such as hyperbolic,\nspherical, and mixed-curvature spaces, have been shown to provide more\nefficient and effective representations for data with intrinsic geometric\nproperties, including web-related data like social network topology,\nquery-document relationships, and user-item interactions. Integrating\nfoundation models with non-Euclidean geometries has great potential to enhance\ntheir ability to capture and model the underlying structures, leading to better\nperformance in search, recommendations, and content understanding. This\nworkshop focuses on the intersection of Non-Euclidean Foundation Models and\nGeometric Learning (NEGEL), exploring its potential benefits, including the\npotential benefits for advancing web-related technologies, challenges, and\nfuture directions. Workshop page:\n[https://hyperboliclearning.github.io/events/www2025workshop](https://hyperboliclearning.github.io/events/www2025workshop)", "authors": ["Menglin Yang", "Yifei Zhang", "Jialin Chen", "Melanie Weber", "Rex Ying"], "published_date": "2025-05-20", "title_zh": "邁向非歐幾里得基礎模型：超越歐幾里得框架推進人工智慧", "summary_zh": "歐幾里得空間是目前機器學習架構的預設幾何設定，但它存在根本性的局限性。因此，非歐幾里得學習正迅速受到關注，尤其是在網路相關應用中。例如，雙曲、球面和混合曲率空間，在處理具有內在幾何特性的資料（如社交網路拓撲、查詢-文件關係和使用者-項目互動）方面更有效率。將非歐幾里得幾何與基礎模型整合，可望提升模型捕捉和建模底層結構的能力，進而改善搜尋、推薦和內容理解。本工作坊聚焦於非歐幾里得基礎模型和幾何學習的交叉領域，探討其潛在優勢、挑戰和未來方向，特別是在推進網路相關技術方面的潛力。", "audio": "audios/2505.14417v1.mp3", "timestamp": "2025-05-21T09:20:41.995382"}
{"query": "Diffusion Model", "id": "2505.14455v1", "url": "http://arxiv.org/abs/2505.14455v1", "title": "CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation", "summary": "Although autoregressive models have dominated language modeling in recent\nyears, there has been a growing interest in exploring alternative paradigms to\nthe conventional next-token prediction framework. Diffusion-based language\nmodels have emerged as a compelling alternative due to their powerful parallel\ngeneration capabilities and inherent editability. However, these models are\noften constrained by fixed-length generation. A promising direction is to\ncombine the strengths of both paradigms, segmenting sequences into blocks,\nmodeling autoregressive dependencies across blocks while leveraging discrete\ndiffusion to estimate the conditional distribution within each block given the\npreceding context. Nevertheless, their practical application is often hindered\nby two key limitations: rigid fixed-length outputs and a lack of flexible\ncontrol mechanisms. In this work, we address the critical limitations of fixed\ngranularity and weak controllability in current large diffusion language\nmodels. We propose CtrlDiff, a dynamic and controllable semi-autoregressive\nframework that adaptively determines the size of each generation block based on\nlocal semantics using reinforcement learning. Furthermore, we introduce a\nclassifier-guided control mechanism tailored to discrete diffusion, which\nsignificantly reduces computational overhead while facilitating efficient\npost-hoc conditioning without retraining. Extensive experiments demonstrate\nthat CtrlDiff sets a new standard among hybrid diffusion models, narrows the\nperformance gap to state-of-the-art autoregressive approaches, and enables\neffective conditional text generation across diverse tasks.", "authors": ["Chihan Huang", "Hao Tang"], "published_date": "2025-05-20", "title_zh": "CtrlDiff：透過動態區塊預測與可控生成提升大型擴散語言模型", "summary_zh": "現今，擴散語言模型因其強大的平行生成能力和可編輯性而備受關注。然而，這些模型常受限於固定長度的生成。CtrlDiff提出一種動態且可控的半自迴歸框架，能基於局部語義自適應地決定每個生成區塊的大小。此外，我們還引入了一種針對離散擴散的分類器引導控制機制，在無需重新訓練的情況下，實現高效的後驗條件生成。實驗表明，CtrlDiff在混合擴散模型中樹立了新的標竿，縮小了與最先進自迴歸方法的性能差距，並能跨多種任務實現有效的條件文本生成。", "audio": "audios/2505.14455v1.mp3", "timestamp": "2025-05-21T09:20:47.242642"}
{"query": "AI", "id": "2505.14661v1", "url": "http://arxiv.org/abs/2505.14661v1", "title": "Abacus: A Cost-Based Optimizer for Semantic Operator Systems", "summary": "LLMs enable an exciting new class of data processing applications over large\ncollections of unstructured documents. Several new programming frameworks have\nenabled developers to build these applications by composing them out of\nsemantic operators: a declarative set of AI-powered data transformations with\nnatural language specifications. These include LLM-powered maps, filters,\njoins, etc. used for document processing tasks such as information extraction,\nsummarization, and more. While systems of semantic operators have achieved\nstrong performance on benchmarks, they can be difficult to optimize. An\noptimizer for this setting must determine how to physically implement each\nsemantic operator in a way that optimizes the system globally. Existing\noptimizers are limited in the number of optimizations they can apply, and most\n(if not all) cannot optimize system quality, cost, or latency subject to\nconstraint(s) on the other dimensions. In this paper we present Abacus, an\nextensible, cost-based optimizer which searches for the best implementation of\na semantic operator system given a (possibly constrained) optimization\nobjective. Abacus estimates operator performance by leveraging a minimal set of\nvalidation examples and, if available, prior beliefs about operator\nperformance. We evaluate Abacus on document processing workloads in the\nbiomedical and legal domains (BioDEX; CUAD) and multi-modal question answering\n(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%\nbetter quality and up to 23.6x lower cost and 4.2x lower latency than the next\nbest system.", "authors": ["Matthew Russo", "Sivaprasad Sudhir", "Gerardo Vitagliano", "Chunwei Liu", "Tim Kraska", "Samuel Madden", "Michael Cafarella"], "published_date": "2025-05-20", "title_zh": "算盤 (Abacus): 一個基於成本的語義運算子系統最佳化器", "summary_zh": "大型語言模型催生了新的非結構化文件處理應用。開發者可以使用語義運算子，也就是基於AI、具自然語言規範的資料轉換，來建構這些應用。然而，優化這些運算子系統非常困難。現有的優化器能力有限，且難以同時優化品質、成本和延遲。本文提出 Abacus，一個可擴展、基於成本的優化器，能根據給定的優化目標（可能帶有約束）尋找語義運算子系統的最佳實現方案。Abacus 利用少量的驗證樣本和運算子效能的先驗知識來估算運算子效能。在生物醫學、法律領域和多模態問答工作負載上的評估表明，由 Abacus 優化的系統比其他系統能達到18.7%-39.2%的品質提升，並降低高達23.6倍的成本和4.2倍的延遲。", "audio": "audios/2505.14661v1.mp3", "timestamp": "2025-05-21T11:15:44.284777"}
{"query": "Foundation Model", "id": "2505.14415v1", "url": "http://arxiv.org/abs/2505.14415v1", "title": "Table Foundation Models: on knowledge pre-training for tabular learning", "summary": "Table foundation models bring high hopes to data science: pre-trained on\ntabular data to embark knowledge or priors, they should facilitate downstream\ntasks on tables. One specific challenge is that of data semantics: numerical\nentries take their meaning from context, e.g., column name. Pre-trained neural\nnetworks that jointly model column names and table entries have recently\nboosted prediction accuracy. While these models outline the promises of world\nknowledge to interpret table values, they lack the convenience of popular\nfoundation models in text or vision. Indeed, they must be fine-tuned to bring\nbenefits, come with sizeable computation costs, and cannot easily be reused or\ncombined with other architectures. Here we introduce TARTE, a foundation model\nthat transforms tables to knowledge-enhanced vector representations using the\nstring to capture semantics. Pre-trained on large relational data, TARTE yields\nrepresentations that facilitate subsequent learning with little additional\ncost. These representations can be fine-tuned or combined with other learners,\ngiving models that push the state-of-the-art prediction performance and improve\nthe prediction/computation performance trade-off. Specialized to a task or a\ndomain, TARTE gives domain-specific representations that facilitate further\nlearning. Our study demonstrates an effective approach to knowledge\npre-training for tabular learning.", "authors": ["Myung Jun Kim", "Félix Lefebvre", "Gaëtan Brison", "Alexandre Perez-Lebel", "Gaël Varoquaux"], "published_date": "2025-05-20", "title_zh": "表格基礎模型：關於表格學習的知識預訓練", "summary_zh": "表格基礎模型被寄予厚望，期望透過在表格數據上進行預訓練，獲取知識或先驗知識，從而簡化下游表格任務。一個特別的挑戰是數據語義：數值條目的意義來自於上下文，例如欄位名稱。最近，聯合建模欄位名稱和表格條目的預訓練神經網路提高了預測準確性。然而，這些模型缺乏文本或視覺領域中常見基礎模型的便利性，必須進行微調才能帶來效益，計算成本高昂，且難以重複使用或與其他架構結合。因此，我們介紹了TARTE，一種基礎模型，它使用字串將表格轉換為知識增強的向量表示，從而捕獲語義。TARTE在大型關聯數據上進行預訓練，產生易於後續學習且幾乎無需額外成本的表示。這些表示可以進行微調或與其他學習器結合，從而使模型能夠提升最先進的預測性能，並改善預測/計算性能的權衡。針對特定任務或領域進行專門設計的TARTE，可以提供領域特定的表示，從而促進進一步的學習。我們的研究證明了一種有效的表格學習知識預訓練方法。", "audio": "audios/2505.14415v1.mp3", "timestamp": "2025-05-21T11:15:52.095562"}
{"query": "Diffusion Model", "id": "2505.14429v1", "url": "http://arxiv.org/abs/2505.14429v1", "title": "Compositional amortized inference for large-scale hierarchical Bayesian models", "summary": "Amortized Bayesian inference (ABI) has emerged as a powerful simulation-based\napproach for estimating complex mechanistic models, offering fast posterior\nsampling via generative neural networks. However, extending ABI to hierarchical\nmodels, a cornerstone of modern Bayesian analysis, remains a major challenge\ndue to the difficulty of scaling to large numbers of parameters. In this work,\nwe build on compositional score matching (CSM), a divide-and-conquer strategy\nfor Bayesian updating using diffusion models. To address existing stability\nissues of CSM, we propose adaptive solvers coupled with a novel, error-damping\ncompositional estimator. Our proposed method remains stable even with hundreds\nof thousands of data points and parameters. We validate our approach on a\ncontrolled toy example, a high-dimensional spatial autoregressive model, and a\nreal-world advanced microscopy biological application task involving over\n750,000 parameters.", "authors": ["Jonas Arruda", "Vikas Pandey", "Catherine Sherry", "Margarida Barroso", "Xavier Intes", "Jan Hasenauer", "Stefan T. Radev"], "published_date": "2025-05-20", "title_zh": "大規模層級貝氏模型的組合攤銷推論", "summary_zh": "提出一種基於生成神經網路的攤銷貝氏推論 (ABI) 方法，加速複雜模型後驗分布的取樣。針對 ABI 在大規模層級模型上的應用挑戰，本研究結合組合分數匹配 (CSM) 策略和自適應求解器，並提出一種新型的誤差阻尼組合估計器，克服 CSM 的穩定性問題。該方法即使在高維度數據和參數下也能保持穩定，並在模擬數據、空間自迴歸模型以及實際生物顯微鏡數據上驗證其有效性。簡言之，本研究提出一種穩定的方法，可以處理參數量巨大的層級貝氏模型推論問題。", "audio": "audios/2505.14429v1.mp3", "timestamp": "2025-05-21T11:15:57.552059"}
{"query": "AI", "id": "2505.14659v1", "url": "http://arxiv.org/abs/2505.14659v1", "title": "Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks", "summary": "As healthcare systems increasingly adopt advanced wireless networks and\nconnected devices, securing medical applications has become critical. The\nintegration of Internet of Medical Things devices, such as robotic surgical\ntools, intensive care systems, and wearable monitors has enhanced patient care\nbut introduced serious security risks. Cyberattacks on these devices can lead\nto life threatening consequences, including surgical errors, equipment failure,\nand data breaches. While the ITU IMT 2030 vision highlights 6G's transformative\nrole in healthcare through AI and cloud integration, it also raises new\nsecurity concerns. This paper explores how explainable AI techniques like SHAP,\nLIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve\ntrust and transparency in 6G enabled healthcare. We support our approach with\nexperimental analysis and highlight promising results.", "authors": ["Navneet Kaur", "Lav Gupta"], "published_date": "2025-05-20", "title_zh": "可解釋人工智慧在物聯網整合型 6G 無線網路中保障醫療保健安全", "summary_zh": "隨著醫療系統更廣泛使用無線網路和聯網設備，醫療應用的安全性變得至關重要。物聯網醫療設備雖然提升了照護品質，但也帶來了嚴重的安全風險，像是手術機器人可能被駭、數據外洩等。6G 的發展進一步促進醫療保健的 AI 和雲端整合，但也引發了新的安全隱憂。本文探討如何運用可解釋人工智慧技術，例如 SHAP、LIME 和 DiCE，來發現漏洞、強化防禦，並提高 6G 醫療保健的信任度和透明度。實驗分析結果也顯示這些方法很有潛力。", "audio": "audios/2505.14659v1.mp3", "timestamp": "2025-05-21T12:37:45.151636"}
{"query": "Foundation Model", "id": "2505.14414v1", "url": "http://arxiv.org/abs/2505.14414v1", "title": "Diving into the Fusion of Monocular Priors for Generalized Stereo Matching", "summary": "The matching formulation makes it naturally hard for the stereo matching to\nhandle ill-posed regions like occlusions and non-Lambertian surfaces. Fusing\nmonocular priors has been proven helpful for ill-posed matching, but the biased\nmonocular prior learned from small stereo datasets constrains the\ngeneralization. Recently, stereo matching has progressed by leveraging the\nunbiased monocular prior from the vision foundation model (VFM) to improve the\ngeneralization in ill-posed regions. We dive into the fusion process and\nobserve three main problems limiting the fusion of the VFM monocular prior. The\nfirst problem is the misalignment between affine-invariant relative monocular\ndepth and absolute depth of disparity. Besides, when we use the monocular\nfeature in an iterative update structure, the over-confidence in the disparity\nupdate leads to local optima results. A direct fusion of a monocular depth map\ncould alleviate the local optima problem, but noisy disparity results computed\nat the first several iterations will misguide the fusion. In this paper, we\npropose a binary local ordering map to guide the fusion, which converts the\ndepth map into a binary relative format, unifying the relative and absolute\ndepth representation. The computed local ordering map is also used to re-weight\nthe initial disparity update, resolving the local optima and noisy problem. In\naddition, we formulate the final direct fusion of monocular depth to the\ndisparity as a registration problem, where a pixel-wise linear regression\nmodule can globally and adaptively align them. Our method fully exploits the\nmonocular prior to support stereo matching results effectively and efficiently.\nWe significantly improve the performance from the experiments when generalizing\nfrom SceneFlow to Middlebury and Booster datasets while barely reducing the\nefficiency.", "authors": ["Chengtang Yao", "Lidong Yu", "Zhidan Liu", "Jiaxi Zeng", "Yuwei Wu", "Yunde Jia"], "published_date": "2025-05-20", "title_zh": "深入探討融合單眼先驗知識以提升廣義立體匹配", "summary_zh": "立體匹配在處理遮擋和非朗伯表面等不良區域時面臨挑戰。融合單眼先驗知識已被證明有效，但從小型立體數據集學習到的偏差先驗會限制泛化能力。近年來，利用視覺基礎模型（VFM）中無偏差的單眼先驗，立體匹配在不良區域的泛化能力方面取得了進展。本研究深入探討了融合過程，發現了限制VFM單眼先驗融合的三個主要問題：仿射不變的相對單眼深度與絕對深度視差之間的錯位；迭代更新結構中，過度自信的視差更新導致局部最優；以及早期迭代中嘈雜的視差結果會誤導直接融合單眼深度圖。我們提出了一種二元局部排序圖來引導融合，將深度圖轉換為二元相對格式，統一了相對深度和絕對深度表示。該局部排序圖還用於重新權衡初始視差更新，解決了局部最優和噪聲問題。此外，我們將單眼深度與視差的最終直接融合表述為一個註冊問題，通過像素級線性回歸模塊進行全局自適應對齊。實驗結果表明，我們的模型充分利用了單眼先驗知識，顯著提高了從SceneFlow泛化到Middlebury和Booster數據集的性能，同時幾乎沒有降低效率。", "audio": "audios/2505.14414v1.mp3", "timestamp": "2025-05-21T12:37:55.398628"}
{"query": "Diffusion Model", "id": "2505.14357v1", "url": "http://arxiv.org/abs/2505.14357v1", "title": "Vid2World: Crafting Video Diffusion Models to Interactive World Models", "summary": "World models, which predict transitions based on history observation and\naction sequences, have shown great promise in improving data efficiency for\nsequential decision making. However, existing world models often require\nextensive domain-specific training and still produce low-fidelity, coarse\npredictions, limiting their applicability in complex environments. In contrast,\nvideo diffusion models trained on large, internet-scale datasets have\ndemonstrated impressive capabilities in generating high-quality videos that\ncapture diverse real-world dynamics. In this work, we present Vid2World, a\ngeneral approach for leveraging and transferring pre-trained video diffusion\nmodels into interactive world models. To bridge the gap, Vid2World performs\ncasualization of a pre-trained video diffusion model by crafting its\narchitecture and training objective to enable autoregressive generation.\nFurthermore, it introduces a causal action guidance mechanism to enhance action\ncontrollability in the resulting interactive world model. Extensive experiments\nin robot manipulation and game simulation domains show that our method offers a\nscalable and effective approach for repurposing highly capable video diffusion\nmodels to interactive world models.", "authors": ["Siqiao Huang", "Jialong Wu", "Qixing Zhou", "Shangchen Miao", "Mingsheng Long"], "published_date": "2025-05-20", "title_zh": "Vid2World：打造視訊擴散模型成為互動式世界模型", "summary_zh": "Vid2World 提出一種利用預訓練視訊擴散模型打造互動式世界模型的新方法。它通過改造模型架構和訓練目標，使其能自迴歸地生成預測，並引入因果行動引導機制，增強行動的可控性。實驗證明，這種方法能有效地將强大的視訊擴散模型轉化為可互動的世界模型，並應用於機器人操作和遊戲模擬等領域。", "audio": "audios/2505.14357v1.mp3", "timestamp": "2025-05-21T12:37:58.683896"}
{"query": "AI", "id": "2505.14654v1", "url": "http://arxiv.org/abs/2505.14654v1", "title": "Beyond Words: Multimodal LLM Knows When to Speak", "summary": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI.", "authors": ["Zikai Liao", "Yi Ouyang", "Yi-Lun Lee", "Chen-Ping Yu", "Yi-Hsuan Tsai", "Zhaozheng Yin"], "published_date": "2025-05-20", "title_zh": "超越文字：多模態大型語言模型知道何時該說話", "summary_zh": "大型語言模型聊天機器人在產生連貫且符合上下文的回應方面表現出色，但它們常常難以理解何時該說話，尤其是在持續的對話中給予簡短、及時的回應。本研究著重於即時預測回應類型，特別是那些依賴視覺、聽覺和文字等細微多模態信號的簡短反應性話語。為此，研究者們建立了一個新的多模態數據集，並提出了一個名為MM-When2Speak的多模態模型，該模型能自適應地整合視覺、聽覺和文字信息，以預測何時應進行回應以及何種回應類型是適當的。實驗結果表明，該模型在回應時機的準確性上，比現有技術和基於大型語言模型的模型有顯著提升，突顯了多模態輸入對於產生及時、自然和引人入勝的對話式AI的重要性。", "audio": "audios/2505.14654v1.mp3", "timestamp": "2025-05-21T13:30:37.941299"}
{"query": "Foundation Model", "id": "2505.14411v1", "url": "http://arxiv.org/abs/2505.14411v1", "title": "Byte Pair Encoding for Efficient Time Series Forecasting", "summary": "Existing time series tokenization methods predominantly encode a constant\nnumber of samples into individual tokens. This inflexible approach can generate\nexcessive tokens for even simple patterns like extended constant values,\nresulting in substantial computational overhead. Inspired by the success of\nbyte pair encoding, we propose the first pattern-centric tokenization scheme\nfor time series analysis. Based on a discrete vocabulary of frequent motifs,\nour method merges samples with underlying patterns into tokens, compressing\ntime series adaptively. Exploiting our finite set of motifs and the continuous\nproperties of time series, we further introduce conditional decoding as a\nlightweight yet powerful post-hoc optimization method, which requires no\ngradient computation and adds no computational overhead. On recent time series\nfoundation models, our motif-based tokenization improves forecasting\nperformance by 36% and boosts efficiency by 1990% on average. Conditional\ndecoding further reduces MSE by up to 44%. In an extensive analysis, we\ndemonstrate the adaptiveness of our tokenization to diverse temporal patterns,\nits generalization to unseen data, and its meaningful token representations\ncapturing distinct time series properties, including statistical moments and\ntrends.", "authors": ["Leon Götz", "Marcel Kollovieh", "Stephan Günnemann", "Leo Schwinn"], "published_date": "2025-05-20", "title_zh": "用於高效時間序列預測的位元組對編碼", "summary_zh": "現有的時間序列符號化方法通常將固定數量的樣本編碼成單獨的符號，這種不靈活的方式會對簡單的模式（例如持續的常數值）產生過多的符號，導致大量的計算開銷。受位元組對編碼的成功啟發，我們提出了一種以模式為中心的的時間序列符號化方案。該方法基於常見的圖案離散詞彙，將具有底層模式的樣本合併到符號中，從而自適應地壓縮時間序列。利用我們的有限模式集和時間序列的連續性，我們進一步引入條件解碼作為一種輕量但強大的後處理優化方法，無需梯度計算，也不會增加計算開銷。在最新的時間序列基礎模型上，我們基於模式的符號化平均提高了36%的預測性能，並提高了1990%的效率。條件解碼進一步將均方誤差降低了最多44%。在廣泛的分析中，我們證明了我們的符號化對不同時間模式的適應性、對未見數據的泛化能力，以及其捕獲不同時間序列屬性（包括統計矩和趨勢）的有意義的符號表示。", "audio": "audios/2505.14411v1.mp3", "timestamp": "2025-05-21T13:30:45.995773"}
{"query": "Diffusion Model", "id": "2505.14254v1", "url": "http://arxiv.org/abs/2505.14254v1", "title": "Instructing Text-to-Image Diffusion Models via Classifier-Guided Semantic Optimization", "summary": "Text-to-image diffusion models have emerged as powerful tools for\nhigh-quality image generation and editing. Many existing approaches rely on\ntext prompts as editing guidance. However, these methods are constrained by the\nneed for manual prompt crafting, which can be time-consuming, introduce\nirrelevant details, and significantly limit editing performance. In this work,\nwe propose optimizing semantic embeddings guided by attribute classifiers to\nsteer text-to-image models toward desired edits, without relying on text\nprompts or requiring any training or fine-tuning of the diffusion model. We\nutilize classifiers to learn precise semantic embeddings at the dataset level.\nThe learned embeddings are theoretically justified as the optimal\nrepresentation of attribute semantics, enabling disentangled and accurate\nedits. Experiments further demonstrate that our method achieves high levels of\ndisentanglement and strong generalization across different domains of data.", "authors": ["Yuanyuan Chang", "Yinghua Yao", "Tao Qin", "Mengmeng Wang", "Ivor Tsang", "Guang Dai"], "published_date": "2025-05-20", "title_zh": "透過分類器引導的語義優化來指示文字到圖像擴散模型", "summary_zh": "現有的文字到圖像模型編輯方式仰賴人工撰寫提示詞，費時且易引入雜訊。本研究提出一種新方法，無需文字提示詞，也無需訓練或微調擴散模型，而是利用屬性分類器引導語義嵌入的優化，從而實現更精準、更具解耦性的圖像編輯。研究結果顯示此方法在不同數據領域皆具有良好的泛化能力。", "audio": "audios/2505.14254v1.mp3", "timestamp": "2025-05-21T13:30:50.483667"}
{"query": "AI", "id": "2505.14646v1", "url": "http://arxiv.org/abs/2505.14646v1", "title": "CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation", "summary": "Efficient creation of accurate and editable 3D CAD models is critical in\nengineering design, significantly impacting cost and time-to-market in product\ninnovation. Current manual workflows remain highly time-consuming and demand\nextensive user expertise. While recent developments in AI-driven CAD generation\nshow promise, existing models are limited by incomplete representations of CAD\noperations, inability to generalize to real-world images, and low output\naccuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model\n(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)\ndirectly from visual input. Leveraging a novel dataset that we\ncreated--GenCAD-Code, consisting of over 163k CAD-model image and code\npairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and\nQwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in\n3D solid similarity. Notably, our VLM demonstrates some signs of\ngeneralizability, successfully generating CAD code from real-world images and\nexecuting CAD operations unseen during fine-tuning. The performance and\nadaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code\nto streamline CAD workflows for engineers and designers. CAD-Coder is publicly\navailable at: https://github.com/anniedoris/CAD-Coder.", "authors": ["Anna C. Doris", "Md Ferdous Alam", "Amin Heyrani Nobari", "Faez Ahmed"], "published_date": "2025-05-20", "title_zh": "CAD-Coder：一個用於電腦輔助設計程式碼生成的開源視覺-語言模型", "summary_zh": "工程設計中，高效、精確且可編輯的 3D CAD 模型至關重要。現有方法耗時且需專業知識。本文介紹了 CAD-Coder，一個開源視覺-語言模型，能直接從視覺輸入生成可編輯的 CAD 程式碼。它基於一個包含超過 16 萬 CAD 模型圖像和程式碼配對的新數據集 GenCAD-Code 進行微調，性能超越了 GPT-4.5 和 Qwen2.5-VL-72B 等最先進的模型，實現了 100% 的有效語法率和最高的 3D 實體相似度精度。CAD-Coder 甚至展現了從真實圖像生成 CAD 程式碼的能力。這個模型的性能和適應性凸顯了程式碼微調的視覺-語言模型在簡化工程師和設計師 CAD 工作流程方面的潛力。CAD-Coder 可在 GitHub 上公開獲取。", "audio": "audios/2505.14646v1.mp3", "timestamp": "2025-05-21T14:19:13.872585"}
{"query": "Foundation Model", "id": "2505.14402v1", "url": "http://arxiv.org/abs/2505.14402v1", "title": "OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking", "summary": "The code of nature, embedded in DNA and RNA genomes since the origin of life,\nholds immense potential to impact both humans and ecosystems through genome\nmodeling. Genomic Foundation Models (GFMs) have emerged as a transformative\napproach to decoding the genome. As GFMs scale up and reshape the landscape of\nAI-driven genomics, the field faces an urgent need for rigorous and\nreproducible evaluation. We present OmniGenBench, a modular benchmarking\nplatform designed to unify the data, model, benchmarking, and interpretability\nlayers across GFMs. OmniGenBench enables standardized, one-command evaluation\nof any GFM across five benchmark suites, with seamless integration of over 31\nopen-source models. Through automated pipelines and community-extensible\nfeatures, the platform addresses critical reproducibility challenges, including\ndata transparency, model interoperability, benchmark fragmentation, and\nblack-box interpretability. OmniGenBench aims to serve as foundational\ninfrastructure for reproducible genomic AI research, accelerating trustworthy\ndiscovery and collaborative innovation in the era of genome-scale modeling.", "authors": ["Heng Yang", "Jack Cole", "Yuan Li", "Renzhi Chen", "Geyong Min", "Ke Li"], "published_date": "2025-05-20", "title_zh": "OmniGenBench：一個用於基因體基礎模型可重現性評測的模組化平台", "summary_zh": "OmniGenBench是一個基因體基礎模型（GFMs）的評測平台，旨在解決AI驅動基因體學領域中可重現性評估的迫切需求。它整合了數據、模型、評測和可解釋性等層面，支援一鍵式評估31個以上的開源模型。透過自動化流程和社群擴展功能，OmniGenBench致力於克服數據透明度、模型互操作性、基準測試碎片化和黑盒可解釋性等挑戰，加速基因體AI研究的可靠發現和協作創新。", "audio": "audios/2505.14402v1.mp3", "timestamp": "2025-05-21T14:19:18.151222"}
{"query": "Diffusion Model", "id": "2505.14206v1", "url": "http://arxiv.org/abs/2505.14206v1", "title": "Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data", "summary": "The widespread adoption of mobile sensors has the potential to provide\nmassive and heterogeneous time series data, driving Artificial Intelligence\napplications in mHealth. However, data collection remains limited due to\nstringent ethical regulations, privacy concerns, and other constraints,\nhindering progress in the field. Synthetic data generation, particularly\nthrough Generative Adversarial Networks and Diffusion Models, has emerged as a\npromising solution to address both data scarcity and privacy issues. Yet, these\nmodels are often limited to short-term, unimodal signal patterns. This paper\npresents a systematic evaluation of state-of-the-art generative models for time\nseries synthesis, with a focus on their ability to jointly handle\nmulti-modality, long-range dependencies, and conditional generation-key\nchallenges in the mHealth domain. To ensure a fair comparison, we introduce a\nnovel evaluation framework designed to measure both the intrinsic quality of\nsynthetic data and its utility in downstream predictive tasks. Our findings\nreveal critical limitations in the existing approaches, particularly in\nmaintaining cross-modal consistency, preserving temporal coherence, and\nensuring robust performance in train-on-synthetic, test-on-real, and data\naugmentation scenarios. Finally, we present our future research directions to\nenhance synthetic time series generation and improve the applicability of\ngenerative models in mHealth.", "authors": ["Flavio Di Martino", "Franca Delmastro"], "published_date": "2025-05-20", "title_zh": "mHealth感測器資料合成生成中的挑戰與限制", "summary_zh": "行動感測器在mHealth領域應用廣泛，能產生大量時間序列數據。然而，倫理、隱私等因素限制了真實數據的收集。利用生成對抗網路和擴散模型等技術合成數據是個有前景的解決方案，但現有模型通常只能處理短期、單模態訊號。本文系統性地評估了目前最先進的時間序列生成模型，著重於它們處理多模態、長程依賴以及條件生成的能力，這些都是mHealth領域的關鍵挑戰。研究結果揭示了現有方法的局限性，尤其是在保持跨模態一致性、保存時間相干性和確保在不同情境下的穩健性能方面。最後，論文提出了未來的研究方向，旨在改進合成時間序列生成技術，並提高生成模型在mHealth領域的應用性。", "audio": "audios/2505.14206v1.mp3", "timestamp": "2025-05-21T14:19:25.266113"}
{"query": "AI", "id": "2505.14640v1", "url": "http://arxiv.org/abs/2505.14640v1", "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation", "summary": "Large multimodal models (LMMs) have recently emerged as a powerful tool for\nlong video understanding (LVU), prompting the development of standardized LVU\nbenchmarks to evaluate their performance. However, our investigation reveals a\nrather sober lesson for existing LVU benchmarks. First, most existing\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\nresults are inflated due to the possibility of guessing the correct answer;\nSecond, a significant portion of questions in these benchmarks have strong\npriors to allow models to answer directly without even reading the input video.\nFor example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame\nfrom a long video on Video-MME. We also observe that increasing the number of\nframes does not necessarily lead to improvement on existing benchmarks, which\nis counterintuitive. As a result, the validity and robustness of current LVU\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\nrealistic LVU benchmark containing questions with open-ended short-answer,\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\nsegment-level and full-video understanding through perception and reasoning\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\nfollowing findings: (1) video LMMs show drastic performance ($>$25\\%) drops on\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\nframes. Our results show that VideoEval-Pro offers a more realistic and\nreliable measure of long video understanding, providing a clearer view of\nprogress in this domain.", "authors": ["Wentao Ma", "Weiming Ren", "Yiming Jia", "Zhuofeng Li", "Ping Nie", "Ge Zhang", "Wenhu Chen"], "published_date": "2025-05-20", "title_zh": "VideoEval-Pro：穩健且真實的長影片理解評估", "summary_zh": "現有的長影片理解評估基準測試存在嚴重問題，容易讓模型猜對答案，甚至不看影片也能答題，導致評估結果虛高。為了更真實地評估大型多模態模型的能力，我們提出了 VideoEval-Pro。它包含需要完整理解影片內容的開放式簡答題，涵蓋影片片段及完整影片的感知和推理任務。實驗結果顯示，現有模型在 VideoEval-Pro 上表現大幅下降，且增加輸入影格數更能提升效能，顯示 VideoEval-Pro 能更準確地衡量長影片理解能力。", "audio": "audios/2505.14640v1.mp3", "timestamp": "2025-05-21T16:24:06.392302"}
{"query": "Foundation Model", "id": "2505.14396v1", "url": "http://arxiv.org/abs/2505.14396v1", "title": "Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds", "summary": "Causal world models are systems that can answer counterfactual questions\nabout an environment of interest, i.e. predict how it would have evolved if an\narbitrary subset of events had been realized differently. It requires\nunderstanding the underlying causes behind chains of events and conducting\ncausal inference for arbitrary unseen distributions. So far, this task eludes\nfoundation models, notably large language models (LLMs), which do not have\ndemonstrated causal reasoning capabilities beyond the memorization of existing\ncausal relationships. Furthermore, evaluating counterfactuals in real-world\napplications is challenging since only the factual world is observed, limiting\nevaluation to synthetic datasets. We address these problems by explicitly\nextracting and modeling causal relationships and propose the Causal\nCartographer framework. First, we introduce a graph retrieval-augmented\ngeneration agent tasked to retrieve causal relationships from data. This\napproach allows us to construct a large network of real-world causal\nrelationships that can serve as a repository of causal knowledge and build\nreal-world counterfactuals. In addition, we create a counterfactual reasoning\nagent constrained by causal relationships to perform reliable step-by-step\ncausal inference. We show that our approach can extract causal knowledge and\nimprove the robustness of LLMs for causal reasoning tasks while reducing\ninference costs and spurious correlations.", "authors": ["Gaël Gendron", "Jože M. Rožanec", "Michael Witbrock", "Gillian Dobbie"], "published_date": "2025-05-20", "title_zh": "因果製圖師：從地圖繪製到反事實世界的推理", "summary_zh": "大型語言模型（LLM）擅長記憶，但缺乏真正的因果推理能力，難以回答關於反事實情境的問題，也就是預測如果某些事件以不同方式發生，世界將如何演變。本研究提出「因果製圖師」框架，透過檢索數據中的因果關係，構建一個龐大的真實世界因果關係網絡，作為因果知識庫。此外，我們還建立了一個受因果關係約束的反事實推理代理，使其能執行可靠的逐步因果推理。實驗證明，此方法能有效提取因果知識，提升LLM在因果推理任務中的穩健性，同時降低推理成本和錯誤的相關性。", "audio": "audios/2505.14396v1.mp3", "timestamp": "2025-05-21T16:24:11.925044"}
{"query": "Diffusion Model", "id": "2505.14139v1", "url": "http://arxiv.org/abs/2505.14139v1", "title": "FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning", "summary": "The use of guidance to steer sampling toward desired outcomes has been widely\nexplored within diffusion models, especially in applications such as image and\ntrajectory generation. However, incorporating guidance during training remains\nrelatively underexplored. In this work, we introduce energy-guided flow\nmatching, a novel approach that enhances the training of flow models and\neliminates the need for guidance at inference time. We learn a conditional\nvelocity field corresponding to the flow policy by approximating an\nenergy-guided probability path as a Gaussian path. Learning guided trajectories\nis appealing for tasks where the target distribution is defined by a\ncombination of data and an energy function, as in reinforcement learning.\nDiffusion-based policies have recently attracted attention for their expressive\npower and ability to capture multi-modal action distributions. Typically, these\npolicies are optimized using weighted objectives or by back-propagating\ngradients through actions sampled by the policy. As an alternative, we propose\nFlowQ, an offline reinforcement learning algorithm based on energy-guided flow\nmatching. Our method achieves competitive performance while the policy training\ntime is constant in the number of flow sampling steps.", "authors": ["Marvin Alles", "Nutan Chen", "Patrick van der Smagt", "Botond Cseke"], "published_date": "2025-05-20", "title_zh": "FlowQ：能量導引的流動策略用於離線強化學習", "summary_zh": "本研究提出FlowQ，一種基於能量導引的流動匹配的離線強化學習算法。FlowQ透過近似能量導引機率路徑為高斯路徑，學習條件速度場，從而訓練出更好的流動模型。這種方法消除了推理階段需要額外導引的步驟，並且在策略訓練時間上與流動採樣步數無關，同時能達到具競爭力的效能。", "audio": "audios/2505.14139v1.mp3", "timestamp": "2025-05-21T16:24:16.369091"}
{"query": "AI", "id": "2505.14633v1", "url": "http://arxiv.org/abs/2505.14633v1", "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas", "summary": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.", "authors": ["Yu Ying Chiu", "Zhilin Wang", "Sharan Maiya", "Yejin Choi", "Kyle Fish", "Sydney Levine", "Evan Hubinger"], "published_date": "2025-05-20", "title_zh": "AI會為了拯救生病的孩子而說謊嗎？用AIRiskDilemmas測試AI的價值優先順序", "summary_zh": "隨著AI模型變得更強大，偵測AI風險也更具挑戰性，因為它們會使用像是「對齊偽造」等新方法來迴避偵測。我們認為，識別AI模型內部的價值觀，可以作為AI潛在風險行為的早期預警系統。我們創建了LitmusValues評估流程，揭示AI模型在各種價值觀類別上的優先順序。接著，我們收集了AIRiskDilemmas，這是一系列多樣化的兩難情境，這些情境將價值觀彼此對立，並與AI安全風險（例如權力追求）相關。透過評估AI模型在這些情境中的選擇，我們可以獲得一致的價值優先順序預測，從而揭示潛在的風險。研究表明，LitmusValues中的價值觀（包括看似無害的價值觀，如關懷）可以預測AIRiskDilemmas中已知的風險行為，以及HarmBench中未知的風險行為。", "audio": "audios/2505.14633v1.mp3", "timestamp": "2025-05-21T17:16:05.673771"}
{"query": "Foundation Model", "id": "2505.14361v1", "url": "http://arxiv.org/abs/2505.14361v1", "title": "Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives", "summary": "Vision-language modeling (VLM) aims to bridge the information gap between\nimages and natural language. Under the new paradigm of first pre-training on\nmassive image-text pairs and then fine-tuning on task-specific data, VLM in the\nremote sensing domain has made significant progress. The resulting models\nbenefit from the absorption of extensive general knowledge and demonstrate\nstrong performance across a variety of remote sensing data analysis tasks.\nMoreover, they are capable of interacting with users in a conversational\nmanner. In this paper, we aim to provide the remote sensing community with a\ntimely and comprehensive review of the developments in VLM using the two-stage\nparadigm. Specifically, we first cover a taxonomy of VLM in remote sensing:\ncontrastive learning, visual instruction tuning, and text-conditioned image\ngeneration. For each category, we detail the commonly used network architecture\nand pre-training objectives. Second, we conduct a thorough review of existing\nworks, examining foundation models and task-specific adaptation methods in\ncontrastive-based VLM, architectural upgrades, training strategies and model\ncapabilities in instruction-based VLM, as well as generative foundation models\nwith their representative downstream applications. Third, we summarize datasets\nused for VLM pre-training, fine-tuning, and evaluation, with an analysis of\ntheir construction methodologies (including image sources and caption\ngeneration) and key properties, such as scale and task adaptability. Finally,\nwe conclude this survey with insights and discussions on future research\ndirections: cross-modal representation alignment, vague requirement\ncomprehension, explanation-driven model reliability, continually scalable model\ncapabilities, and large-scale datasets featuring richer modalities and greater\nchallenges.", "authors": ["Xingxing Weng", "Chao Pang", "Gui-Song Xia"], "published_date": "2025-05-20", "title_zh": "視覺-語言建模遇上遙感：模型、資料集與展望", "summary_zh": "本文綜述了視覺-語言建模（VLM）在遙感領域的最新進展。VLM透過大規模圖像-文字配對的預訓練，再針對特定任務進行微調，已展現出強大的遙感數據分析能力，甚至可以進行對話互動。本文分類介紹了遙感VLM的三種主要方法：對比學習、視覺指令調整和文本條件圖像生成，並深入探討了現有模型、訓練策略和資料集。最後，文章展望了未來研究方向，包括跨模態表示對齊、模糊需求理解、可解釋性、持續可擴展的模型能力，以及更大規模、更具挑戰性的多模態資料集。", "audio": "audios/2505.14361v1.mp3", "timestamp": "2025-05-21T17:16:12.073270"}
{"query": "Diffusion Model", "id": "2505.14036v1", "url": "http://arxiv.org/abs/2505.14036v1", "title": "Adaptive Cyclic Diffusion for Inference Scaling", "summary": "Diffusion models have demonstrated strong generative capabilities across\ndomains ranging from image synthesis to complex reasoning tasks. However, most\ninference-time scaling methods rely on fixed denoising schedules, limiting\ntheir ability to allocate computation based on instance difficulty or\ntask-specific demands adaptively. We introduce the challenge of adaptive\ninference-time scaling-dynamically adjusting computational effort during\ninference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a\nflexible, search-based inference framework. ABCD refines outputs through\nbi-directional diffusion cycles while adaptively controlling exploration depth\nand termination. It comprises three components: Cyclic Diffusion Search,\nAutomatic Exploration-Exploitation Balancing, and Adaptive Thinking Time.\nExperiments show that ABCD improves performance across diverse tasks while\nmaintaining computational efficiency.", "authors": ["Gyubin Lee", "Truong Nhat Nguyen Bao", "Jaesik Yoon", "Dongwoo Lee", "Minsu Kim", "Yoshua Bengio", "Sungjin Ahn"], "published_date": "2025-05-20", "title_zh": "自適應循環擴散用於推論規模調整", "summary_zh": "擴散模型在生成任務上表現出色，但現有的推論加速方法依賴固定降噪流程，無法根據任務難度或需求動態調整計算量。本文提出自適應推論規模調整的挑戰，並引入自適應雙向循環擴散(ABCD)框架。ABCD透過雙向擴散循環精煉輸出，並自適應控制探索深度和終止條件，包含循環擴散搜尋、自動探索-利用平衡和自適應思考時間三個部分。實驗證明，ABCD在多種任務上提升效能的同時，也保持了計算效率。", "audio": "audios/2505.14036v1.mp3", "timestamp": "2025-05-21T17:16:17.825422"}
{"query": "AI", "id": "2505.14613v1", "url": "http://arxiv.org/abs/2505.14613v1", "title": "Virtual Cells: Predict, Explain, Discover", "summary": "Drug discovery is fundamentally a process of inferring the effects of\ntreatments on patients, and would therefore benefit immensely from\ncomputational models that can reliably simulate patient responses, enabling\nresearchers to generate and test large numbers of therapeutic hypotheses safely\nand economically before initiating costly clinical trials. Even a more specific\nmodel that predicts the functional response of cells to a wide range of\nperturbations would be tremendously valuable for discovering safe and effective\ntreatments that successfully translate to the clinic. Creating such virtual\ncells has long been a goal of the computational research community that\nunfortunately remains unachieved given the daunting complexity and scale of\ncellular biology. Nevertheless, recent advances in AI, computing power, lab\nautomation, and high-throughput cellular profiling provide new opportunities\nfor reaching this goal. In this perspective, we present a vision for developing\nand evaluating virtual cells that builds on our experience at Recursion. We\nargue that in order to be a useful tool to discover novel biology, virtual\ncells must accurately predict the functional response of a cell to\nperturbations and explain how the predicted response is a consequence of\nmodifications to key biomolecular interactions. We then introduce key\nprinciples for designing therapeutically-relevant virtual cells, describe a\nlab-in-the-loop approach for generating novel insights with them, and advocate\nfor biologically-grounded benchmarks to guide virtual cell development.\nFinally, we make the case that our approach to virtual cells provides a useful\nframework for building other models at higher levels of organization, including\nvirtual patients. We hope that these directions prove useful to the research\ncommunity in developing virtual models optimized for positive impact on drug\ndiscovery outcomes.", "authors": ["Emmanuel Noutahi", "Jason Hartford", "Prudencio Tossou", "Shawn Whitfield", "Alisandra K. Denton", "Cas Wognum", "Kristina Ulicna", "Jonathan Hsu", "Michael Cuccarese", "Emmanuel Bengio", "Dominique Beaini", "Christopher Gibson", "Daniel Cohen", "Berton Earnshaw"], "published_date": "2025-05-20", "title_zh": "虛擬細胞：預測、解釋、發現", "summary_zh": "藥物發現的核心在於推斷治療對患者的影響。開發能夠可靠模擬患者反應的計算模型，將能幫助研究人員在昂貴的臨床試驗前，安全且經濟地產生和測試大量治療假設。本論文提出一個開發和評估「虛擬細胞」的願景，它能準確預測細胞對干擾的反應，並解釋其反應的生物分子機制。透過AI、運算能力、實驗室自動化和高通量細胞分析的進展，我們有機會打造出可用於發現安全有效療法的虛擬細胞，並最終建構更複雜的虛擬患者模型，從而加速藥物開發。", "audio": "audios/2505.14613v1.mp3", "timestamp": "2025-05-21T18:26:28.417346"}
{"query": "Foundation Model", "id": "2505.14100v1", "url": "http://arxiv.org/abs/2505.14100v1", "title": "Unlocking the Power of SAM 2 for Few-Shot Segmentation", "summary": "Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few\nclasses to segment arbitrary classes, but at the risk of overfitting. To\naddress this, some methods use the well-learned knowledge of foundation models\n(e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM\nby supporting video segmentation, whose class-agnostic matching ability is\nuseful to FSS. A simple idea is to encode support foreground (FG) features as\nmemory, with which query FG features are matched and fused. Unfortunately, the\nFG objects in different frames of SAM 2's video data are always the same\nidentity, while those in FSS are different identities, i.e., the matching step\nis incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo\nquery memory, matching with query features in a compatible way. However, the\nmemories can never be as accurate as the real ones, i.e., they are likely to\ncontain incomplete query FG, and some unexpected query background (BG)\nfeatures, leading to wrong segmentation. Hence, we further design Iterative\nMemory Refinement to fuse more query FG features into the memory, and devise a\nSupport-Calibrated Memory Attention to suppress the unexpected query BG\nfeatures in memory. Extensive experiments have been conducted on PASCAL-5$^i$\nand COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot\nmIoU can be 4.2\\% better than the best baseline.", "authors": ["Qianxiong Xu", "Lanyun Zhu", "Xuanyi Liu", "Guosheng Lin", "Cheng Long", "Ziyue Li", "Rui Zhao"], "published_date": "2025-05-20", "title_zh": "解鎖 SAM 2 在少樣本分割中的力量", "summary_zh": "少樣本分割的目標是學習類別無關的分割，但容易過擬合。本文利用SAM 2的影片分割能力，提出了一種新方法。核心挑戰是SAM 2訓練數據的目標在不同幀中是同一物體，而少樣本分割不是。因此，我們設計了偽提示生成器和迭代記憶體精煉，並提出了支持校準的記憶體注意力機制，以提升分割效果。實驗證明，我們的模型在PASCAL-5$^i$和COCO-20$^i$等數據集上表現優異，例如，1-shot mIoU 可以比最好的基線高出 4.2%。", "audio": "audios/2505.14100v1.mp3", "timestamp": "2025-05-21T18:26:32.856819"}
{"query": "Diffusion Model", "id": "2505.13983v1", "url": "http://arxiv.org/abs/2505.13983v1", "title": "Combining Deterministic Enhanced Conditions with Dual-Streaming Encoding for Diffusion-Based Speech Enhancement", "summary": "Diffusion-based speech enhancement (SE) models need to incorporate correct\nprior knowledge as reliable conditions to generate accurate predictions.\nHowever, providing reliable conditions using noisy features is challenging. One\nsolution is to use features enhanced by deterministic methods as conditions.\nHowever, the information distortion and loss caused by deterministic methods\nmight affect the diffusion process. In this paper, we first investigate the\neffects of using different deterministic SE models as conditions for diffusion.\nWe validate two conditions depending on whether the noisy feature was used as\npart of the condition: one using only the deterministic feature\n(deterministic-only), and the other using both deterministic and noisy features\n(deterministic-noisy). Preliminary investigation found that using deterministic\nenhanced conditions improves hearing experiences on real data, while the choice\nbetween using deterministic-only or deterministic-noisy conditions depends on\nthe deterministic models. Based on these findings, we propose a dual-streaming\nencoding Repair-Diffusion Model for SE (DERDM-SE) to more effectively utilize\nboth conditions. Moreover, we found that fine-grained deterministic models have\ngreater potential in objective evaluation metrics, while UNet-based\ndeterministic models provide more stable diffusion performance. Therefore, in\nthe DERDM-SE, we propose a deterministic model that combines coarse- and\nfine-grained processing. Experimental results on CHiME4 show that the proposed\nmodels effectively leverage deterministic models to achieve better SE\nevaluation scores, along with more stable performance compared to other\ndiffusion-based SE models.", "authors": ["Hao Shi", "Xugang Lu", "Kazuki Shimada", "Tatsuya Kawahara"], "published_date": "2025-05-20", "title_zh": "結合確定性增強條件與雙流編碼的基於擴散的語音增強", "summary_zh": "為了讓基於擴散的語音增強模型更準確，需要提供可靠的先驗知識作為條件。本研究探討使用確定性方法增強的特徵作為條件的效果，並發現加入確定性增強條件可以改善聽覺體驗。基於這些發現，我們提出了一種雙流編碼的修復擴散模型（DERDM-SE），能更有效地利用確定性增強條件。實驗結果表明，我們提出的模型能有效利用確定性模型，在語音增強評估中獲得更好的分數，且相較於其他基於擴散的語音增強模型，表現更穩定。", "audio": "audios/2505.13983v1.mp3", "timestamp": "2025-05-21T18:26:39.178977"}
{"query": "AI", "id": "2505.14612v1", "url": "http://arxiv.org/abs/2505.14612v1", "title": "AI Agents in the Electricity Market Game with Cryptocurrency Transactions: A Post-Terminator Analysis", "summary": "This paper extends (Spear 2003) by replacing human agents with artificial\nintelligence (AI) entities that derive utility solely from electricity\nconsumption. These AI agents must prepay for electricity using cryptocurrency\nand the verification of these transactions requires a fixed amount of\nelectricity. As a result the agents must strategically allocate electricity\nresources between consumption and payment verification. This paper analyzes the\nequilibrium outcomes of such a system and discusses the implications of\nAI-driven energy markets.", "authors": ["Microsoft Copilot", "Stephen E. Spear"], "published_date": "2025-05-20", "title_zh": "基於加密貨幣交易的電力市場博弈中的AI代理：後終結者時代分析", "summary_zh": "本研究將既有電力市場模型中的人類代理替換為完全依賴電力消費獲取效用的AI實體。這些AI代理必須使用加密貨幣預付電費，而交易驗證需要消耗一定量的電力。因此，AI代理需要在消費和支付驗證之間策略性地分配電力資源。本研究分析了該系統的均衡結果，並探討了AI驅動能源市場的影響。", "audio": "audios/2505.14612v1.mp3", "timestamp": "2025-05-21T19:14:36.343567"}
{"query": "Foundation Model", "id": "2505.14088v1", "url": "http://arxiv.org/abs/2505.14088v1", "title": "Generalizable Multispectral Land Cover Classification via Frequency-Aware Mixture of Low-Rank Token Experts", "summary": "We introduce Land-MoE, a novel approach for multispectral land cover\nclassification (MLCC). Spectral shift, which emerges from disparities in\nsensors and geospatial conditions, poses a significant challenge in this\ndomain. Existing methods predominantly rely on domain adaptation and\ngeneralization strategies, often utilizing small-scale models that exhibit\nlimited performance. In contrast, Land-MoE addresses these issues by\nhierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts,\nto fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner.\nSpecifically, Land-MoE comprises two key modules: the mixture of low-rank token\nexperts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages\nrank-differentiated tokens to generate diverse feature adjustments for\nindividual instances within multispectral images. By dynamically combining\nlearnable low-rank token experts of varying ranks, it enhances the robustness\nagainst spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on\nthe refined features. This process enables the model to effectively capture\nfrequency band information that is strongly correlated with semantic essence,\nwhile simultaneously suppressing frequency noise irrelevant to the task.\nComprehensive experiments on MLCC tasks involving cross-sensor and\ncross-geospatial setups demonstrate that Land-MoE outperforms existing methods\nby a large margin. Additionally, the proposed approach has also achieved\nstate-of-the-art performance in domain generalization semantic segmentation\ntasks of RGB remote sensing images.", "authors": ["Xi Chen", "Shen Yan", "Juelin Zhu", "Chen Chen", "Yu Liu", "Maojun Zhang"], "published_date": "2025-05-20", "title_zh": "基於頻率感知的低秩Token專家混合模型實現廣義多光譜地表覆蓋分類", "summary_zh": "Land-MoE 是一種新的多光譜地表覆蓋分類方法，旨在解決因傳感器和地理條件差異導致的光譜偏移問題。它通過層次化地插入一個頻率感知的低秩 Token 專家混合模型，以參數高效的方式微調視覺基礎模型。該方法包含低秩 Token 專家混合（MoLTE）和頻率感知濾波器（FAF）兩個關鍵模塊。MoLTE 利用不同秩的 Token 為多光譜圖像中的每個實例生成不同的特徵調整，增強了對光譜偏移的魯棒性。FAF 則對精煉後的特徵進行頻域調製，有效捕捉與語義本質密切相關的頻帶信息，同時抑制與任務無關的頻率噪聲。實驗結果表明，Land-MoE 在跨傳感器和跨地理空間的多光譜地表覆蓋分類任務中顯著優於現有方法，並在 RGB 遙感圖像的領域泛化語義分割任務中也取得了最先進的性能。", "audio": "audios/2505.14088v1.mp3", "timestamp": "2025-05-21T19:14:45.882789"}
{"query": "Diffusion Model", "id": "2505.13919v1", "url": "http://arxiv.org/abs/2505.13919v1", "title": "Predicting Dynamical Systems across Environments via Diffusive Model Weight Generation", "summary": "Data-driven methods offer an effective equation-free solution for predicting\nphysical dynamics. However, the same physical system can exhibit significantly\ndifferent dynamic behaviors in various environments. This causes prediction\nfunctions trained for specific environments to fail when transferred to unseen\nenvironments. Therefore, cross-environment prediction requires modeling the\ndynamic functions of different environments. In this work, we propose a model\nweight generation method, \\texttt{EnvAd-Diff}. \\texttt{EnvAd-Diff} operates in\nthe weight space of the dynamic function, generating suitable weights from\nscratch based on environmental condition for zero-shot prediction.\nSpecifically, we first train expert prediction functions on dynamic\ntrajectories from a limited set of visible environments to create a model zoo,\nthereby constructing sample pairs of prediction function weights and their\ncorresponding environments. Subsequently, we train a latent space diffusion\nmodel conditioned on the environment to model the joint distribution of weights\nand environments. Considering the lack of environmental prior knowledge in\nreal-world scenarios, we propose a physics-informed surrogate label to\ndistinguish different environments. Generalization experiments across multiple\nsystems demonstrate that a 1M parameter prediction function generated by\n\\texttt{EnvAd-Diff} outperforms a pre-trained 500M parameter foundation model.", "authors": ["Ruikun Li", "Huandong Wang", "Jingtao Ding", "Yuan Yuan", "Qingmin Liao", "Yong Li"], "published_date": "2025-05-20", "title_zh": "透過擴散模型權重生成跨環境預測動態系統", "summary_zh": "現有的資料驅動方法能有效預測物理動態，但在不同環境下，同一系統可能展現截然不同的行為，導致特定環境訓練的模型在未見環境失效。為解決跨環境預測問題，我們提出EnvAd-Diff，在動態函數的權重空間中，根據環境條件從頭生成合適的權重，以實現零樣本預測。我們訓練一系列基於可見環境數據的專家模型，建立模型庫，並以此構建權重和環境的樣本對。接著，我們訓練一個以環境為條件的潛空間擴散模型，學習權重和環境的聯合分佈。針對真實環境缺乏先驗知識的問題，我們提出物理資訊代理標籤來區分不同環境。實驗結果表明，EnvAd-Diff生成的100萬參數預測函數優於預訓練的5億參數基礎模型。", "audio": "audios/2505.13919v1.mp3", "timestamp": "2025-05-21T19:14:51.897428"}
{"query": "AI", "id": "2505.14588v1", "url": "http://arxiv.org/abs/2505.14588v1", "title": "Generative AI at the Crossroads: Light Bulb, Dynamo, or Microscope?", "summary": "With the advent of generative AI (genAI), the potential scope of artificial\nintelligence has increased dramatically, but the future effect of genAI on\nproductivity remains uncertain with the effect of the technology on the\ninnovation process a crucial open question. Some labor-saving innovations, such\nas the light bulb, temporarily raise productivity growth as adoption spreads,\nbut the effect fades when the market is saturated; that is, the level of output\nper hour is permanently higher but the growth rate is not. In contrast, two\ntypes of technologies stand out as having longer-lived effects on productivity\ngrowth. First, there are technologies known as general-purpose technologies\n(GPTs). GPTs are (1) widely adopted, (2) spur abundant knock-on innovations\n(new goods and services, process efficiencies, and business reorganization),\nand (3) improve continuously, refreshing this innovation cycle; the electric\ndynamo is an example. Second, there are inventions of methods of invention\n(IMIs). IMIs increase the efficiency of the research and development process,\ngenerating new ideas more quickly and cheaply; the compound microscope is an\nexample. We show that GenAI has the characteristics of both a GPT and an IMI --\nan encouraging sign. Even so, for genAI to boost productivity growth, its\ncontribution will have to exceed the fading growth effects of past IT\ninnovations baked into the trend, including predecessor AI technologies.", "authors": ["Martin Baily", "David Byrne", "Aidan Kane", "Paul Soto"], "published_date": "2025-05-20", "title_zh": "生成式人工智慧在十字路口：燈泡、發電機，還是顯微鏡？", "summary_zh": "生成式人工智慧（GenAI）的出現大幅拓展了人工智慧的潛力範圍，但其對生產力的未來影響仍不明朗，對創新過程的影響尤其關鍵。論文探討GenAI是否像燈泡一樣，僅短暫提高生產力後趨於平緩；還是像通用技術（GPT）如發電機，持續推動創新；又或者像發明方法的發明（IMI）如顯微鏡，加速研發進程。研究發現GenAI兼具GPT和IMI的特性，這是個鼓舞人心的跡象。然而，要提升生產力增長，GenAI的貢獻必須超越過去IT創新（包括先前的AI技術）所帶來的趨勢衰退效應。", "audio": "audios/2505.14588v1.mp3", "timestamp": "2025-05-21T20:20:15.738409"}
{"query": "Foundation Model", "id": "2505.14042v1", "url": "http://arxiv.org/abs/2505.14042v1", "title": "Adversarially Pretrained Transformers may be Universally Robust In-Context Learners", "summary": "Adversarial training is one of the most effective adversarial defenses, but\nit incurs a high computational cost. In this study, we show that transformers\nadversarially pretrained on diverse tasks can serve as robust foundation models\nand eliminate the need for adversarial training in downstream tasks.\nSpecifically, we theoretically demonstrate that through in-context learning, a\nsingle adversarially pretrained transformer can robustly generalize to multiple\nunseen tasks without any additional training, i.e., without any parameter\nupdates. This robustness stems from the model's focus on robust features and\nits resistance to attacks that exploit non-predictive features. Besides these\npositive findings, we also identify several limitations. Under certain\nconditions (though unrealistic), no universally robust single-layer\ntransformers exist. Moreover, robust transformers exhibit an\naccuracy--robustness trade-off and require a large number of in-context\ndemonstrations. The code is available at\nhttps://github.com/s-kumano/universally-robust-in-context-learner.", "authors": ["Soichiro Kumano", "Hiroshi Kera", "Toshihiko Yamasaki"], "published_date": "2025-05-20", "title_zh": "對抗式預訓練的變換器可能成為通用的穩健上下文學習器", "summary_zh": "對抗式訓練是有效的防禦手段，但成本高昂。本研究發現，在多樣化任務上進行對抗式預訓練的變換器，可作為穩健的基礎模型，無需在下游任務中額外進行對抗式訓練。理論上證明，透過上下文學習，單一對抗式預訓練的變換器，可以在沒有任何參數更新的情況下，穩健地泛化到多個未見過的任務。這種穩健性來自模型對穩健特徵的關注以及對利用非預測性特徵攻擊的抵抗力。研究也指出一些限制，例如在特定條件下，不存在通用的穩健單層變換器，且穩健的變換器需要在準確性和穩健性之間權衡，並需要大量的上下文示範。", "audio": "audios/2505.14042v1.mp3", "timestamp": "2025-05-21T20:20:22.156616"}
{"query": "Diffusion Model", "id": "2505.13843v1", "url": "http://arxiv.org/abs/2505.13843v1", "title": "A Semantic Information-based Hierarchical Speech Enhancement Method Using Factorized Codec and Diffusion Model", "summary": "Most current speech enhancement (SE) methods recover clean speech from noisy\ninputs by directly estimating time-frequency masks or spectrums. However, these\napproaches often neglect the distinct attributes, such as semantic content and\nacoustic details, inherent in speech signals, which can hinder performance in\ndownstream tasks. Moreover, their effectiveness tends to degrade in complex\nacoustic environments. To overcome these challenges, we propose a novel,\nsemantic information-based, step-by-step factorized SE method using factorized\ncodec and diffusion model. Unlike traditional SE methods, our hierarchical\nmodeling of semantic and acoustic attributes enables more robust clean speech\nrecovery, particularly in challenging acoustic scenarios. Moreover, this method\noffers further advantages for downstream TTS tasks. Experimental results\ndemonstrate that our algorithm not only outperforms SOTA baselines in terms of\nspeech quality but also enhances TTS performance in noisy environments.", "authors": ["Yang Xiang", "Canan Huang", "Desheng Hu", "Jingguang Tian", "Xinhui Hu", "Chao Zhang"], "published_date": "2025-05-20", "title_zh": "基於語義訊息的階層式語音增強方法，採用分解式編解碼器與擴散模型", "summary_zh": "現有的語音增強方法多半直接從嘈雜的訊號中估計時頻遮罩或頻譜來還原乾淨語音，忽略了語音訊號中獨特的語義內容和聲學細節，這會影響後續任務的表現。我們提出一種新的方法，利用分解式編解碼器和擴散模型，分階段、以語義訊息為基礎進行語音增強。這種階層式建模方式更能還原乾淨語音，尤其是在複雜的聲學環境中，而且還能提升後續的語音合成任務表現。實驗結果顯示，我們的演算法不僅在語音品質上優於現有方法，還能在嘈雜的環境中提升語音合成的性能。", "audio": "audios/2505.13843v1.mp3", "timestamp": "2025-05-21T20:20:27.423795"}
{"query": "AI", "id": "2505.14585v1", "url": "http://arxiv.org/abs/2505.14585v1", "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning", "summary": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +17.64% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively.", "authors": ["Wenbin Hu", "Haoran Li", "Huihao Jing", "Qi Hu", "Ziqian Zeng", "Sirui Han", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "published_date": "2025-05-20", "title_zh": "Context Reasoner：透過強化學習激勵情境化隱私與安全合規的推理能力", "summary_zh": "大型語言模型雖強大，卻也帶來安全和隱私風險。現有防護策略往往犧牲情境推理能力，過度依賴敏感模式匹配，且忽略現有的安全和隱私標準。本研究將安全和隱私問題轉化為情境化合規問題，並以情境完整性理論為基礎，對齊GDPR、歐盟AI法案和HIPAA等規範。我們利用強化學習，以規則為基礎的獎勵來激勵模型的情境推理能力，同時提升安全和隱私合規性。實驗證明，此方法不僅顯著提高法律合規性，更增強了一般的推理能力。", "audio": "audios/2505.14585v1.mp3", "timestamp": "2025-05-21T21:17:12.423871"}
{"query": "Foundation Model", "id": "2505.13840v1", "url": "http://arxiv.org/abs/2505.13840v1", "title": "EfficientLLM: Efficiency in Large Language Models", "summary": "Large Language Models (LLMs) have driven significant progress, yet their\ngrowing parameter counts and context windows incur prohibitive compute, energy,\nand monetary costs. We introduce EfficientLLM, a novel benchmark and the first\ncomprehensive empirical study evaluating efficiency techniques for LLMs at\nscale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our\nstudy systematically explores three key axes: (1) architecture pretraining\n(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts\n(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and\n(3) inference (quantization methods: int4, float16). We define six fine-grained\nmetrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy\nConsumption, Compression Rate) to capture hardware saturation,\nlatency-throughput balance, and carbon cost. Evaluating over 100\nmodel-technique pairs (0.5B-72B parameters), we derive three core insights: (i)\nEfficiency involves quantifiable trade-offs: no single method is universally\noptimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by\n40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%\naccuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal\nmemory-latency trade-offs for constrained devices, MLA achieves lowest\nperplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency\nonly beyond 14B parameters. (iii) Techniques generalize across modalities: we\nextend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and\nVision-Language Models (Qwen2.5-VL), confirming effective transferability. By\nopen-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM\nprovides essential guidance for researchers and engineers navigating the\nefficiency-performance landscape of next-generation foundation models.", "authors": ["Zhengqing Yuan", "Weixiang Sun", "Yixin Liu", "Huichi Zhou", "Rong Zhou", "Yiyang Li", "Zheyuan Zhang", "Wei Song", "Yue Huang", "Haolong Jia", "Keerthiram Murugesan", "Yu Wang", "Lifang He", "Jianfeng Gao", "Lichao Sun", "Yanfang Ye"], "published_date": "2025-05-20", "title_zh": "高效LLM：大型語言模型效率研究", "summary_zh": "大型語言模型雖然帶來突破，但參數量和上下文窗口的增長導致成本過高。本文推出EfficientLLM基準測試，對大型語言模型的效率提升技術進行大規模評估。研究涵蓋架構預訓練、微調和推理三個主要方向，並定義了六個細粒度指標來衡量硬體利用率、延遲吞吐量平衡和碳排放成本。研究發現，效率提升涉及權衡取捨，沒有萬能方法；最佳方案取決於任務和模型規模；這些技術也能推廣到視覺和視覺語言模型上。EfficientLLM開放資料集和評估流程，為研究者和工程師在下一代基礎模型的效率與性能之間做出選擇提供指引。", "audio": "audios/2505.13840v1.mp3", "timestamp": "2025-05-21T21:17:21.216998"}
{"query": "Diffusion Model", "id": "2505.13791v1", "url": "http://arxiv.org/abs/2505.13791v1", "title": "Scalable Autoregressive 3D Molecule Generation", "summary": "Generative models of 3D molecular structure play a rapidly growing role in\nthe design and simulation of molecules. Diffusion models currently dominate the\nspace of 3D molecule generation, while autoregressive models have trailed\nbehind. In this work, we present Quetzal, a simple but scalable autoregressive\nmodel that builds molecules atom-by-atom in 3D. Treating each molecule as an\nordered sequence of atoms, Quetzal combines a causal transformer that predicts\nthe next atom's discrete type with a smaller Diffusion MLP that models the\ncontinuous next-position distribution. Compared to existing autoregressive\nbaselines, Quetzal achieves substantial improvements in generation quality and\nis competitive with the performance of state-of-the-art diffusion models. In\naddition, by reducing the number of expensive forward passes through a dense\ntransformer, Quetzal enables significantly faster generation speed, as well as\nexact divergence-based likelihood computation. Finally, without any\narchitectural changes, Quetzal natively handles variable-size tasks like\nhydrogen decoration and scaffold completion. We hope that our work motivates a\nperspective on scalability and generality for generative modelling of 3D\nmolecules.", "authors": ["Austin H. Cheng", "Chong Sun", "Alán Aspuru-Guzik"], "published_date": "2025-05-20", "title_zh": "可擴展的自迴歸三維分子生成", "summary_zh": "分子結構的生成模型在分子設計和模擬中扮演著越來越重要的角色。目前擴散模型主導了三維分子生成領域，而自迴歸模型則相對落後。這篇論文介紹了 Quetzal，一個簡單但可擴展的自迴歸模型，能夠以原子為單位逐個構建三維分子。Quetzal 將每個分子視為一個有序的原子序列，結合了一個預測下一個原子離散類型的因果轉換器，以及一個較小的擴散 MLP 模型，用於模擬下一個位置的連續分佈。Quetzal 在生成品質上顯著優於現有的自迴歸模型，並且性能可與最先進的擴散模型相媲美。此外，Quetzal 透過減少密集轉換器的前向傳播次數，顯著提高了生成速度，並且能夠進行基於散度的精確似然計算。最後，Quetzal 在不改變架構的情況下，可以自然地處理氫原子裝飾和骨架補全等可變大小的任務。這項工作旨在激發人們對三維分子生成模型的可擴展性和通用性的思考。", "audio": "audios/2505.13791v1.mp3", "timestamp": "2025-05-21T21:17:51.909900"}
{"query": "AI", "id": "2505.14569v1", "url": "http://arxiv.org/abs/2505.14569v1", "title": "Agent Context Protocols Enhance Collective Inference", "summary": "AI agents have become increasingly adept at complex tasks such as coding,\nreasoning, and multimodal understanding. However, building generalist systems\nrequires moving beyond individual agents to collective inference -- a paradigm\nwhere multi-agent systems with diverse, task-specialized agents complement one\nanother through structured communication and collaboration. Today, coordination\nis usually handled with imprecise, ad-hoc natural language, which limits\ncomplex interaction and hinders interoperability with domain-specific agents.\nWe introduce Agent context protocols (ACPs): a domain- and agent-agnostic\nfamily of structured protocols for agent-agent communication, coordination, and\nerror handling. ACPs combine (i) persistent execution blueprints -- explicit\ndependency graphs that store intermediate agent outputs -- with (ii)\nstandardized message schemas, enabling robust and fault-tolerant multi-agent\ncollective inference. ACP-powered generalist systems reach state-of-the-art\nperformance: 28.3 % accuracy on AssistantBench for long-horizon web assistance\nand best-in-class multimodal technical reports, outperforming commercial AI\nsystems in human evaluation. ACPs are highly modular and extensible, allowing\npractitioners to build top-tier generalist agents quickly.", "authors": ["Devansh Bhardwaj", "Arjun Beniwal", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Karthik R. Narasimhan", "Ameet Deshpande", "Vishvak Murahari"], "published_date": "2025-05-20", "title_zh": "代理上下文協議增強集體推理", "summary_zh": "AI 代理在編碼、推理和多模態理解等複雜任務中越來越強大。要打造通用系統，需要超越單個代理，轉向集體推理——一種多代理系統，其中不同且專精於特定任務的代理通過結構化的溝通和協作來互補。目前，協調通常依賴於不精確、臨時性的自然語言，這限制了複雜的互動，也阻礙了與特定領域代理的互操作性。為此，我們引入代理上下文協議 (ACPs)：一種領域和代理無關的結構化協議族，用於代理之間的通訊、協調和錯誤處理。ACPs 結合了 (i) 持續的執行藍圖 (顯式的依賴關係圖，用於儲存中間代理的輸出)，以及 (ii) 標準化的訊息架構，從而實現穩健且容錯的多代理集體推理。使用 ACPs 的通用系統達到了最先進的性能：在 AssistantBench 上，針對長週期網絡助手達到 28.3% 的準確率，並且在多模態技術報告方面表現最佳，在人類評估中優於商業 AI 系統。ACPs 具有高度的模組化和可擴展性，使從業者能夠快速構建頂級的通用代理。", "audio": "audios/2505.14569v1.mp3", "timestamp": "2025-05-21T22:17:50.906228"}
{"query": "Foundation Model", "id": "2505.13755v1", "url": "http://arxiv.org/abs/2505.13755v1", "title": "Panda: A pretrained forecast model for universal representation of chaotic dynamics", "summary": "Chaotic systems are intrinsically sensitive to small errors, challenging\nefforts to construct predictive data-driven models of real-world dynamical\nsystems such as fluid flows or neuronal activity. Prior efforts comprise either\nspecialized models trained separately on individual time series, or foundation\nmodels trained on vast time series databases with little underlying dynamical\nstructure. Motivated by dynamical systems theory, we present Panda, Patched\nAttention for Nonlinear DynAmics. We train Panda on a novel synthetic,\nextensible dataset of $2 \\times 10^4$ chaotic dynamical systems that we\ndiscover using an evolutionary algorithm. Trained purely on simulated data,\nPanda exhibits emergent properties: zero-shot forecasting of unseen real world\nchaotic systems, and nonlinear resonance patterns in cross-channel attention\nheads. Despite having been trained only on low-dimensional ordinary\ndifferential equations, Panda spontaneously develops the ability to predict\npartial differential equations without retraining. We demonstrate a neural\nscaling law for differential equations, underscoring the potential of\npretrained models for probing abstract mathematical domains like nonlinear\ndynamics.", "authors": ["Jeffrey Lai", "Anthony Bao", "William Gilpin"], "published_date": "2025-05-19", "title_zh": "Panda：用於混沌動力學通用表示的預訓練預測模型", "summary_zh": "混沌系統對微小誤差極為敏感，這對構建預測性的資料驅動模型構成挑戰。本研究提出 Panda 模型，利用演化算法發現的大量合成混沌動力學系統進行訓練。Panda 展現出驚人的能力，例如零樣本預測未見過的真實世界混沌系統，以及在跨通道注意力頭中呈現非線性共振模式。更令人驚訝的是，Panda 在僅接受低維常微分方程訓練後，無需重新訓練即可預測偏微分方程。研究還展示了微分方程的神經縮放定律，突顯了預訓練模型在探索非線性動力學等抽象數學領域的潛力。", "audio": "audios/2505.13755v1.mp3", "timestamp": "2025-05-21T22:18:10.039919"}
{"query": "Diffusion Model", "id": "2505.13771v1", "url": "http://arxiv.org/abs/2505.13771v1", "title": "Score-Based Training for Energy-Based TTS Models", "summary": "Noise contrastive estimation (NCE) is a popular method for training\nenergy-based models (EBM) with intractable normalisation terms. The key idea of\nNCE is to learn by comparing unnormalised log-likelihoods of the reference and\nnoisy samples, thus avoiding explicitly computing normalisation terms. However,\nNCE critically relies on the quality of noisy samples. Recently, sliced score\nmatching (SSM) has been popularised by closely related diffusion models (DM).\nUnlike NCE, SSM learns a gradient of log-likelihood, or score, by learning\ndistribution of its projections on randomly chosen directions. However, both\nNCE and SSM disregard the form of log-likelihood function, which is problematic\ngiven that EBMs and DMs make use of first-order optimisation during inference.\nThis paper proposes a new criterion that learns scores more suitable for\nfirst-order schemes. Experiments contrasts these approaches for training EBMs.", "authors": ["Wanli Sun", "Anton Ragni"], "published_date": "2025-05-19", "title_zh": "基於分數的能量型語音合成模型訓練", "summary_zh": "這篇論文探討能量型語音合成模型的訓練方法。能量型模型訓練的一大挑戰是難以計算歸一化項。論文比較了噪音對比估計 (NCE) 和分片分數匹配 (SSM) 等方法，前者透過比較真實和噪音樣本的未歸一化對數似然來學習，後者則學習對數似然梯度的投影分佈。然而，這些方法忽略了對數似然函數的形式，這在推理時使用一階最佳化的能量型模型中是個問題。因此，論文提出了一種新的訓練準則，旨在學習更適合一階方法的梯度，並通過實驗比較不同方法在能量型模型訓練上的效果。", "audio": "audios/2505.13771v1.mp3", "timestamp": "2025-05-21T22:18:28.330206"}
{"query": "AI", "id": "2505.14539v1", "url": "http://arxiv.org/abs/2505.14539v1", "title": "A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)", "summary": "In this work, we present the first general logic of attention. Attention is a\npowerful cognitive ability that allows agents to focus on potentially complex\ninformation, such as logically structured propositions, higher-order beliefs,\nor what other agents pay attention to. This ability is a strength, as it helps\nto ignore what is irrelevant, but it can also introduce biases when some types\nof information or agents are systematically ignored. Existing dynamic epistemic\nlogics for attention cannot model such complex attention scenarios, as they\nonly model attention to atomic formulas. Additionally, such logics quickly\nbecome cumbersome, as their size grows exponentially in the number of agents\nand announced literals. Here, we introduce a logic that overcomes both\nlimitations. First, we generalize edge-conditioned event models, which we show\nto be as expressive as standard event models yet exponentially more succinct\n(generalizing both standard event models and generalized arrow updates).\nSecond, we extend attention to arbitrary formulas, allowing agents to also\nattend to other agents' beliefs or attention. Our work treats attention as a\nmodality, like belief or awareness. We introduce attention principles that\nimpose closure properties on that modality and that can be used in its\naxiomatization. Throughout, we illustrate our framework with examples of AI\nagents reasoning about human attentional biases, demonstrating how such agents\ncan discover attentional biases.", "authors": ["Gaia Belardinelli", "Thomas Bolander", "Sebastian Watzl"], "published_date": "2025-05-20", "title_zh": "基於邊緣條件事件模型的一般注意力邏輯（擴展版本）", "summary_zh": "這篇論文提出了一種全新的、更通用化的注意力邏輯。傳統的注意力模型只能處理簡單的原子公式，而這個新邏輯可以處理更複雜的資訊，例如邏輯命題、高階信念，甚至是其他人的注意力。為了實現這一點，研究人員使用了邊緣條件事件模型，這種模型更簡潔，表達能力也更強。這個新邏輯將注意力視為一種模態，就像信念或意識一樣，並且可以捕捉人類注意力偏差的特性。文章用AI智能體推理人類注意力偏差的例子，展示了這個框架的應用。", "audio": "audios/2505.14539v1.mp3", "timestamp": "2025-05-21T23:17:25.202342"}
{"query": "Foundation Model", "id": "2505.13099v2", "url": "http://arxiv.org/abs/2505.13099v2", "title": "Industrial Synthetic Segment Pre-training", "summary": "Pre-training on real-image datasets has been widely proven effective for\nimproving instance segmentation. However, industrial applications face two key\nchallenges: (1) legal and ethical restrictions, such as ImageNet's prohibition\nof commercial use, and (2) limited transferability due to the domain gap\nbetween web images and industrial imagery. Even recent vision foundation\nmodels, including the segment anything model (SAM), show notable performance\ndegradation in industrial settings. These challenges raise critical questions:\nCan we build a vision foundation model for industrial applications without\nrelying on real images or manual annotations? And can such models outperform\neven fine-tuned SAM on industrial datasets? To address these questions, we\npropose the Instance Core Segmentation Dataset (InsCore), a synthetic\npre-training dataset based on formula-driven supervised learning (FDSL).\nInsCore generates fully annotated instance segmentation images that reflect key\ncharacteristics of industrial data, including complex occlusions, dense\nhierarchical masks, and diverse non-rigid shapes, distinct from typical web\nimagery. Unlike previous methods, InsCore requires neither real images nor\nhuman annotations. Experiments on five industrial datasets show that models\npre-trained with InsCore outperform those trained on COCO and ImageNet-21k, as\nwell as fine-tuned SAM, achieving an average improvement of 6.2 points in\ninstance segmentation performance. This result is achieved using only 100k\nsynthetic images, more than 100 times fewer than the 11 million images in SAM's\nSA-1B dataset, demonstrating the data efficiency of our approach. These\nfindings position InsCore as a practical and license-free vision foundation\nmodel for industrial applications.", "authors": ["Shinichi Mae", "Ryousuke Yamada", "Hirokatsu Kataoka"], "published_date": "2025-05-19", "title_zh": "工業合成分割預訓練", "summary_zh": "現有的圖像分割模型在工業應用中面臨授權限制和領域差異的挑戰。本研究提出一個基於公式驅動的合成數據集InsCore，無需真實圖像或人工標注，即可預訓練出專為工業應用設計的圖像分割模型。實驗證明，用InsCore預訓練的模型在多個工業數據集上的表現，優於用COCO和ImageNet預訓練的模型，甚至超越了微調後的SAM模型，且只需少量合成數據就能達到很好的效果。InsCore為工業應用提供了一個實用且無授權問題的視覺基礎模型。", "audio": "audios/2505.13099v2.mp3", "timestamp": "2025-05-21T23:17:30.656713"}
{"query": "Diffusion Model", "id": "2505.13740v1", "url": "http://arxiv.org/abs/2505.13740v1", "title": "Improving Compositional Generation with Diffusion Models Using Lift Scores", "summary": "We introduce a novel resampling criterion using lift scores, for improving\ncompositional generation in diffusion models. By leveraging the lift scores, we\nevaluate whether generated samples align with each single condition and then\ncompose the results to determine whether the composed prompt is satisfied. Our\nkey insight is that lift scores can be efficiently approximated using only the\noriginal diffusion model, requiring no additional training or external modules.\nWe develop an optimized variant that achieves relatively lower computational\noverhead during inference while maintaining effectiveness. Through extensive\nexperiments, we demonstrate that lift scores significantly improved the\ncondition alignment for compositional generation across 2D synthetic data,\nCLEVR position tasks, and text-to-image synthesis. Our code is available at\nhttp://github.com/rainorangelemon/complift.", "authors": ["Chenning Yu", "Sicun Gao"], "published_date": "2025-05-19", "title_zh": "使用Lift分數改進擴散模型中的組合生成", "summary_zh": "本研究提出一種新的重採樣標準，利用Lift分數來提升擴散模型的組合生成能力。透過Lift分數評估生成樣本與各個條件的對齊程度，進而判斷組合提示詞是否被滿足。此方法僅需原始擴散模型即可有效近似計算Lift分數，無需額外訓練或模組。實驗證明，Lift分數能顯著改善二維合成數據、CLEVR位置任務和文本生成圖像等場景中的條件對齊效果。", "audio": "audios/2505.13740v1.mp3", "timestamp": "2025-05-21T23:17:37.788568"}
{"query": "AI", "id": "2505.15811v1", "url": "http://arxiv.org/abs/2505.15811v1", "title": "On the creation of narrow AI: hierarchy and nonlocality of neural network skills", "summary": "We study the problem of creating strong, yet narrow, AI systems. While recent\nAI progress has been driven by the training of large general-purpose foundation\nmodels, the creation of smaller models specialized for narrow domains could be\nvaluable for both efficiency and safety. In this work, we explore two\nchallenges involved in creating such systems, having to do with basic\nproperties of how neural networks learn and structure their representations.\nThe first challenge regards when it is possible to train narrow models from\nscratch. Through experiments on a synthetic task, we find that it is sometimes\nnecessary to train networks on a wide distribution of data to learn certain\nnarrow skills within that distribution. This effect arises when skills depend\non each other hierarchically, and training on a broad distribution introduces a\ncurriculum which substantially accelerates learning. The second challenge\nregards how to transfer particular skills from large general models into small\nspecialized models. We find that model skills are often not perfectly localized\nto a particular set of prunable components. However, we find that methods based\non pruning can still outperform distillation. We investigate the use of a\nregularization objective to align desired skills with prunable components while\nunlearning unnecessary skills.", "authors": ["Eric J. Michaud", "Asher Parker-Sartori", "Max Tegmark"], "published_date": "2025-05-21", "title_zh": "關於窄人工智慧的創建：神經網路技能的層次性和非局部性", "summary_zh": "本文探討如何創建強大但專精於特定領域的窄人工智慧系統。研究發現，從頭訓練窄領域模型有時需要先在大範圍的數據上訓練，以建立技能間的層次結構，加速學習。此外，將大型通用模型的技能轉移到小型專精模型時，技能並非總是完美地集中在可修剪的組件上。儘管如此，修剪方法仍優於知識蒸餾。研究進一步探討如何透過正則化，將期望的技能與可修剪的組件對齊，同時移除不必要的技能。", "audio": "audios/2505.15811v1.mp3", "timestamp": "2025-05-22T03:10:54.678837"}
{"query": "Foundation Model", "id": "2505.15809v1", "url": "http://arxiv.org/abs/2505.15809v1", "title": "MMaDA: Multimodal Large Diffusion Language Models", "summary": "We introduce MMaDA, a novel class of multimodal diffusion foundation models\ndesigned to achieve superior performance across diverse domains such as textual\nreasoning, multimodal understanding, and text-to-image generation. The approach\nis distinguished by three key innovations: (i) MMaDA adopts a unified diffusion\narchitecture with a shared probabilistic formulation and a modality-agnostic\ndesign, eliminating the need for modality-specific components. This\narchitecture ensures seamless integration and processing across different data\ntypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning\nstrategy that curates a unified CoT format across modalities. By aligning\nreasoning processes between textual and visual domains, this strategy\nfacilitates cold-start training for the final reinforcement learning (RL)\nstage, thereby enhancing the model's ability to handle complex tasks from the\noutset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm\nspecifically tailored for diffusion foundation models. Utilizing diversified\nreward modeling, UniGRPO unifies post-training across both reasoning and\ngeneration tasks, ensuring consistent performance improvements. Experimental\nresults demonstrate that MMaDA-8B exhibits strong generalization capabilities\nas a unified multimodal foundation model. It surpasses powerful models like\nLLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in\nmultimodal understanding, and excels over SDXL and Janus in text-to-image\ngeneration. These achievements highlight MMaDA's effectiveness in bridging the\ngap between pretraining and post-training within unified diffusion\narchitectures, providing a comprehensive framework for future research and\ndevelopment. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA", "authors": ["Ling Yang", "Ye Tian", "Bowen Li", "Xinchen Zhang", "Ke Shen", "Yunhai Tong", "Mengdi Wang"], "published_date": "2025-05-21", "title_zh": "MMaDA：多模態大型擴散語言模型", "summary_zh": "MMaDA是一種新的多模態擴散模型，旨在提升文字推理、多模態理解和文字生成圖像等領域的表現。它採用統一的擴散架構，無需針對特定模態的元件。透過混合式的長鏈思考微調策略，統一不同模態的推理過程。更提出UniGRPO，一種基於策略梯度的強化學習演算法，適用於擴散模型，能統一推理和生成任務的後訓練。實驗結果顯示，MMaDA在文字推理、多模態理解和文字生成圖像方面都超越了其他模型，證明其作為統一多模態模型的強大泛化能力。", "audio": "audios/2505.15809v1.mp3", "timestamp": "2025-05-22T03:10:59.380909"}
{"query": "Diffusion Model", "id": "2505.15812v1", "url": "http://arxiv.org/abs/2505.15812v1", "title": "Leveraging the Powerful Attention of a Pre-trained Diffusion Model for Exemplar-based Image Colorization", "summary": "Exemplar-based image colorization aims to colorize a grayscale image using a\nreference color image, ensuring that reference colors are applied to\ncorresponding input regions based on their semantic similarity. To achieve\naccurate semantic matching between regions, we leverage the self-attention\nmodule of a pre-trained diffusion model, which is trained on a large dataset\nand exhibits powerful attention capabilities. To harness this power, we propose\na novel, fine-tuning-free approach based on a pre-trained diffusion model,\nmaking two key contributions. First, we introduce dual attention-guided color\ntransfer. We utilize the self-attention module to compute an attention map\nbetween the input and reference images, effectively capturing semantic\ncorrespondences. The color features from the reference image is then\ntransferred to the semantically matching regions of the input image, guided by\nthis attention map, and finally, the grayscale features are replaced with the\ncorresponding color features. Notably, we utilize dual attention to calculate\nattention maps separately for the grayscale and color images, achieving more\nprecise semantic alignment. Second, we propose classifier-free colorization\nguidance, which enhances the transferred colors by combining color-transferred\nand non-color-transferred outputs. This process improves the quality of\ncolorization. Our experimental results demonstrate that our method outperforms\nexisting techniques in terms of image quality and fidelity to the reference.\nSpecifically, we use 335 input-reference pairs from previous research,\nachieving an FID of 95.27 (image quality) and an SI-FID of 5.51 (fidelity to\nthe reference). Our source code is available at\nhttps://github.com/satoshi-kosugi/powerful-attention.", "authors": ["Satoshi Kosugi"], "published_date": "2025-05-21", "title_zh": "利用預訓練擴散模型強大的注意力機制進行範例式圖像著色", "summary_zh": "範例式圖像著色旨在利用參考彩色圖像為灰度圖像著色，確保參考顏色根據其語義相似性應用到相應的輸入區域。為了實現精確的區域語義匹配，我們利用預訓練擴散模型的自注意力模塊，該模型在大型數據集上訓練，展現了強大的注意力能力。我們提出了一種基於預訓練擴散模型的新穎、免微調方法，做出了兩個主要貢獻：雙重注意力引導的顏色傳輸，利用自注意力模塊計算輸入和參考圖像之間的注意力圖，有效捕捉語義對應關係，並將參考圖像的顏色特徵傳輸到輸入圖像的語義匹配區域；以及無分類器顏色引導，通過結合顏色傳輸和非顏色傳輸的輸出，增強傳輸的顏色，提高著色質量。實驗結果表明，我們的方法在圖像質量和參考保真度方面優於現有技術。我們使用先前研究中的335個輸入-參考對，实现了95.27的FID（图像质量）和5.51的SI-FID（参考保真度）。", "audio": "audios/2505.15812v1.mp3", "timestamp": "2025-05-22T03:11:06.284321"}
{"query": "AI", "id": "2505.15799v1", "url": "http://arxiv.org/abs/2505.15799v1", "title": "The Agentic Economy", "summary": "Generative AI has transformed human-computer interaction by enabling natural\nlanguage interfaces and the emergence of autonomous agents capable of acting on\nusers' behalf. While early applications have improved individual productivity,\nthese gains have largely been confined to predefined tasks within existing\nworkflows. We argue that the more profound economic impact lies in reducing\ncommunication frictions between consumers and businesses. This shift could\nreorganize markets, redistribute power, and catalyze the creation of new\nproducts and services. We explore the implications of an agentic economy, where\nassistant agents act on behalf of consumers and service agents represent\nbusinesses, interacting programmatically to facilitate transactions. A key\ndistinction we draw is between unscripted interactions -- enabled by technical\nadvances in natural language and protocol design -- and unrestricted\ninteractions, which depend on market structures and governance. We examine the\ncurrent limitations of siloed and end-to-end agents, and explore future\nscenarios shaped by technical standards and market dynamics. These include the\npotential tension between agentic walled gardens and an open web of agents,\nimplications for advertising and discovery, the evolution of\nmicro-transactions, and the unbundling and rebundling of digital goods.\nUltimately, we argue that the architecture of agentic communication will\ndetermine the extent to which generative AI democratizes access to economic\nopportunity.", "authors": ["David M. Rothschild", "Markus Mobius", "Jake M. Hofman", "Eleanor W. Dillon", "Daniel G. Goldstein", "Nicole Immorlica", "Sonia Jaffe", "Brendan Lucier", "Aleksandrs Slivkins", "Matthew Vogel"], "published_date": "2025-05-21", "title_zh": "代理經濟", "summary_zh": "生成式AI透過自然語言介面和自主代理，大幅改變人機互動。論文認為，其更深遠的經濟影響在於降低消費者與企業之間的溝通障礙，進而重塑市場、重新分配權力，並催生新產品和服務。論文探討代理經濟，消費者代理代表用戶，服務代理代表企業，透過程式化互動促進交易。關鍵區別在於，技術進步促成的「無腳本互動」與取決於市場結構和治理的「無限制互動」。論文分析了現有代理的局限性，並探討技術標準和市場動態塑造的未來場景，包括代理封閉花園與開放網路的潛在衝突、廣告與發現的影響、微交易的演變，以及數位商品的解綁和重組。論文強調，代理溝通的架構將決定生成式AI能否真正實現經濟機會的民主化。", "audio": "audios/2505.15799v1.mp3", "timestamp": "2025-05-22T04:23:32.823938"}
{"query": "Foundation Model", "id": "2505.15685v1", "url": "http://arxiv.org/abs/2505.15685v1", "title": "From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems", "summary": "Foundation models (FMs) are increasingly used to bridge language and action\nin embodied agents, yet the operational characteristics of different FM\nintegration strategies remain under-explored -- particularly for complex\ninstruction following and versatile action generation in changing environments.\nThis paper examines three paradigms for building robotic systems: end-to-end\nvision-language-action (VLA) models that implicitly integrate perception and\nplanning, and modular pipelines incorporating either vision-language models\n(VLMs) or multimodal large language models (LLMs). We evaluate these paradigms\nthrough two focused case studies: a complex instruction grounding task\nassessing fine-grained instruction understanding and cross-modal\ndisambiguation, and an object manipulation task targeting skill transfer via\nVLA finetuning. Our experiments in zero-shot and few-shot settings reveal\ntrade-offs in generalization and data efficiency. By exploring performance\nlimits, we distill design implications for developing language-driven physical\nagents and outline emerging challenges and opportunities for FM-powered\nrobotics in real-world conditions.", "authors": ["Xiuchao Sui", "Daiying Tian", "Qi Sun", "Ruirui Chen", "Dongkyu Choi", "Kenneth Kwok", "Soujanya Poria"], "published_date": "2025-05-21", "title_zh": "從基礎紮根到操控自如：具體化機器人系統中基礎模型整合的案例研究", "summary_zh": "本論文探討了如何將基礎模型整合到機器人系统中，使其能够理解人类指令并执行复杂操作。研究比较了三种不同的整合方法：端到端视觉-语言-动作模型、以及使用视觉-语言模型或多模态大型语言模型的模块化流程。通过两个案例研究，揭示了不同方法在泛化能力和数据效率方面的优缺点，为开发更智能的语言驱动型机器人提供了设计指导，并指出了未来实际应用中面临的挑战和机遇。", "audio": "audios/2505.15685v1.mp3", "timestamp": "2025-05-22T04:23:36.820479"}
{"query": "Diffusion Model", "id": "2505.15800v1", "url": "http://arxiv.org/abs/2505.15800v1", "title": "Interspatial Attention for Efficient 4D Human Video Generation", "summary": "Generating photorealistic videos of digital humans in a controllable manner\nis crucial for a plethora of applications. Existing approaches either build on\nmethods that employ template-based 3D representations or emerging video\ngeneration models but suffer from poor quality or limited consistency and\nidentity preservation when generating individual or multiple digital humans. In\nthis paper, we introduce a new interspatial attention (ISA) mechanism as a\nscalable building block for modern diffusion transformer (DiT)--based video\ngeneration models. ISA is a new type of cross attention that uses relative\npositional encodings tailored for the generation of human videos. Leveraging a\ncustom-developed video variation autoencoder, we train a latent ISA-based\ndiffusion model on a large corpus of video data. Our model achieves\nstate-of-the-art performance for 4D human video synthesis, demonstrating\nremarkable motion consistency and identity preservation while providing precise\ncontrol of the camera and body poses. Our code and model are publicly released\nat https://dsaurus.github.io/isa4d/.", "authors": ["Ruizhi Shao", "Yinghao Xu", "Yujun Shen", "Ceyuan Yang", "Yang Zheng", "Changan Chen", "Yebin Liu", "Gordon Wetzstein"], "published_date": "2025-05-21", "title_zh": "用於高效4D人體影片生成之跨空間注意力機制", "summary_zh": "論文提出一種新的跨空間注意力（ISA）機制，用於提升基於擴散Transformer（DiT）模型的4D人體影片生成效率。ISA是一種新型的跨注意力，針對人體影片生成設計了相對位置編碼。該模型透過訓練大量影片數據，在運動一致性和身份保持方面達到最佳效果，同時能精準控制相機和身體姿勢。程式碼和模型已公開。", "audio": "audios/2505.15800v1.mp3", "timestamp": "2025-05-22T04:23:41.402506"}
{"query": "AI", "id": "2505.15798v1", "url": "http://arxiv.org/abs/2505.15798v1", "title": "Model Merging is Secretly Certifiable: Non-Vacuous Generalisation Bounds for Low-Shot Learning", "summary": "Certifying the IID generalisation ability of deep networks is the first of\nmany requirements for trusting AI in high-stakes applications from medicine to\nsecurity. However, when instantiating generalisation bounds for deep networks\nit remains challenging to obtain non-vacuous guarantees, especially when\napplying contemporary large models on the small scale data prevalent in such\nhigh-stakes fields. In this paper, we draw a novel connection between a family\nof learning methods based on model fusion and generalisation certificates, and\nsurprisingly show that with minor adjustment several existing learning\nstrategies already provide non-trivial generalisation guarantees. Essentially,\nby focusing on data-driven learning of downstream tasks by fusion rather than\nfine-tuning, the certified generalisation gap becomes tiny and independent of\nthe base network size, facilitating its certification. Our results show for the\nfirst time non-trivial generalisation guarantees for learning with as low as\n100 examples, while using vision models such as VIT-B and language models such\nas mistral-7B. This observation is significant as it has immediate implications\nfor facilitating the certification of existing systems as trustworthy, and\nopens up new directions for research at the intersection of practice and\ntheory.", "authors": ["Taehoon Kim", "Henry Gouk", "Minyoung Kim", "Timothy Hospedales"], "published_date": "2025-05-21", "title_zh": "模型融合隱藏的認證性：少樣本學習的非空泛泛化界限", "summary_zh": "這篇論文揭示了模型融合技術隱藏的優勢，它能為深度學習模型提供可驗證的泛化能力，尤其是在少樣本學習的場景下。傳統上，要為深度網路建立有效的泛化界限非常困難，特別是在高風險領域中，資料量通常很小。研究發現，透過模型融合進行資料驅動的下游任務學習，可以顯著縮小認證泛化差距，使其與基底網路的大小無關。該研究首次展示了使用少至100個樣本，以及VIT-B和Mistral-7B等模型，也能實現非空泛的泛化保證，為現有系統的認證開闢了新的方向。", "audio": "audios/2505.15798v1.mp3", "timestamp": "2025-05-22T05:18:42.219278"}
{"query": "Foundation Model", "id": "2505.15594v1", "url": "http://arxiv.org/abs/2505.15594v1", "title": "Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off", "summary": "While foundation models demonstrate impressive performance across various\ntasks, they remain vulnerable to adversarial inputs. Current research explores\nvarious approaches to enhance model robustness, with Diffusion Denoised\nSmoothing emerging as a particularly promising technique. This method employs a\npretrained diffusion model to preprocess inputs before model inference. Yet,\nits effectiveness remains largely unexplored beyond classification. We aim to\naddress this gap by analyzing three datasets with four distinct downstream\ntasks under three different adversarial attack algorithms. Our findings reveal\nthat while foundation models maintain resilience against conventional\ntransformations, applying high-noise diffusion denoising to clean images\nwithout any distortions significantly degrades performance by as high as 57%.\nLow-noise diffusion settings preserve performance but fail to provide adequate\nprotection across all attack types. Moreover, we introduce a novel attack\nstrategy specifically targeting the diffusion process itself, capable of\ncircumventing defenses in the low-noise regime. Our results suggest that the\ntrade-off between adversarial robustness and performance remains a challenge to\nbe addressed.", "authors": ["Yury Belousov", "Brian Pulfer", "Vitaliy Kinakh", "Slava Voloshynovskiy"], "published_date": "2025-05-21", "title_zh": "超越分類：評估擴散去噪平滑在安全性與效用性權衡上的表現", "summary_zh": "大型模型雖然在各項任務表現出色，但容易受到對抗性輸入的攻擊。擴散去噪平滑是一種有潛力的防禦方法，它利用預訓練的擴散模型來預處理輸入。然而，目前對其在分類以外任務的有效性研究不足。本研究針對多個任務和攻擊策略進行分析，發現高噪音擴散去噪會顯著降低模型效能，甚至高達57%。低噪音設置雖能維持效能，但無法有效防禦所有攻擊。此外，我們還設計了一種針對擴散過程本身的新型攻擊，能夠繞過低噪音設置的防禦。結果表明，在對抗性魯棒性和模型效能之間取得平衡仍然是一個挑戰。", "audio": "audios/2505.15594v1.mp3", "timestamp": "2025-05-22T05:18:47.929179"}
{"query": "Diffusion Model", "id": "2505.15791v1", "url": "http://arxiv.org/abs/2505.15791v1", "title": "VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL", "summary": "Diffusion models have emerged as powerful generative tools across various\ndomains, yet tailoring pre-trained models to exhibit specific desirable\nproperties remains challenging. While reinforcement learning (RL) offers a\npromising solution,current methods struggle to simultaneously achieve stable,\nefficient fine-tuning and support non-differentiable rewards. Furthermore,\ntheir reliance on sparse rewards provides inadequate supervision during\nintermediate steps, often resulting in suboptimal generation quality. To\naddress these limitations, dense and differentiable signals are required\nthroughout the diffusion process. Hence, we propose VAlue-based Reinforced\nDiffusion (VARD): a novel approach that first learns a value function\npredicting expection of rewards from intermediate states, and subsequently uses\nthis value function with KL regularization to provide dense supervision\nthroughout the generation process. Our method maintains proximity to the\npretrained model while enabling effective and stable training via\nbackpropagation. Experimental results demonstrate that our approach facilitates\nbetter trajectory guidance, improves training efficiency and extends the\napplicability of RL to diffusion models optimized for complex,\nnon-differentiable reward functions.", "authors": ["Fengyuan Dai", "Zifeng Zhuang", "Yufei Huang", "Siteng Huang", "Bangyan Liao", "Donglin Wang", "Fajie Yuan"], "published_date": "2025-05-21", "title_zh": "VARD：基於價值的強化學習，用於擴散模型的高效且密集微調", "summary_zh": "擴散模型在生成領域表現出色，但要讓預訓練模型展現特定期望的特性仍然困難。強化學習雖有潛力，但現有方法難以兼顧穩定、高效的微調，且無法支援不可微分的獎勵。它們對稀疏獎勵的依賴導致中間步驟的監督不足，影響生成品質。VARD 旨在解決這些問題，它首先學習一個價值函數，預測中間狀態的獎勵期望值，然後用該價值函數及KL正則化，在生成過程中提供密集的監督。這種方法在保持與預訓練模型接近的同時，通過反向傳播實現有效且穩定的訓練。實驗表明，VARD 能更好地引導生成軌跡，提高訓練效率，並將強化學習應用擴展到優化複雜的、不可微分的獎勵函數的擴散模型上。", "audio": "audios/2505.15791v1.mp3", "timestamp": "2025-05-22T05:18:54.362516"}
{"query": "AI", "id": "2505.15790v1", "url": "http://arxiv.org/abs/2505.15790v1", "title": "Exploring the Innovation Opportunities for Pre-trained Models", "summary": "Innovators transform the world by understanding where services are\nsuccessfully meeting customers' needs and then using this knowledge to identify\nfailsafe opportunities for innovation. Pre-trained models have changed the AI\ninnovation landscape, making it faster and easier to create new AI products and\nservices. Understanding where pre-trained models are successful is critical for\nsupporting AI innovation. Unfortunately, the hype cycle surrounding pre-trained\nmodels makes it hard to know where AI can really be successful. To address\nthis, we investigated pre-trained model applications developed by HCI\nresearchers as a proxy for commercially successful applications. The research\napplications demonstrate technical capabilities, address real user needs, and\navoid ethical challenges. Using an artifact analysis approach, we categorized\ncapabilities, opportunity domains, data types, and emerging interaction design\npatterns, uncovering some of the opportunity space for innovation with\npre-trained models.", "authors": ["Minjung Park", "Jodi Forlizzi", "John Zimmerman"], "published_date": "2025-05-21", "title_zh": "探索預訓練模型的創新機會", "summary_zh": "預訓練模型正在重塑人工智慧創新，加速新產品和服務的開發。本研究分析人機互動(HCI)研究者開發的應用案例，作為商業成功的指標，旨在辨識預訓練模型真正能夠發揮價值的領域。透過分析這些應用，我們歸納出預訓練模型在能力、應用領域、資料類型和互動設計模式上的潛力，為人工智慧創新指明方向。", "audio": "audios/2505.15790v1.mp3", "timestamp": "2025-05-22T06:27:08.958151"}
{"query": "Foundation Model", "id": "2505.15572v1", "url": "http://arxiv.org/abs/2505.15572v1", "title": "Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback", "summary": "The data-to-equation (Data2Eqn) task aims to discover interpretable\nmathematical equations that map observed values to labels, offering physical\ninsights and broad applicability across academic and industrial domains.\nGenetic programming and traditional deep learning-based approaches suffer from\nsearch inefficiency and poor generalization on small task-specific datasets.\nFoundation models showed promise in this area, but existing approaches suffer\nfrom: 1) They are pretrained on general-purpose data distributions, making them\nless effective for domain-specific tasks; and 2) their training objectives\nfocus on token-level alignment, overlooking mathematical semantics, which can\nlead to inaccurate equations. To address these issues, we aim to enhance the\ndomain adaptability of foundation models for Data2Eqn tasks. In this work, we\npropose a reinforcement learning-based finetuning framework that directly\noptimizes the generation policy of a pretrained model through reward signals\nderived from downstream numerical fitness. Our method allows the model to adapt\nto specific and complex data distributions and generate mathematically\nmeaningful equations. Extensive experiments demonstrate that our approach\nimproves both the accuracy and robustness of equation generation under complex\ndistributions.", "authors": ["Wangyang Ying", "Haoyue Bai", "Nanxu Gong", "Xinyuan Wang", "Sixun Dong", "Haifeng Chen", "Yanjie Fu"], "published_date": "2025-05-21", "title_zh": "以強化回饋彌合方程式蒸餾中的領域差距", "summary_zh": "資料到方程式（Data2Eqn）任務旨在從觀測值找出可解釋的數學方程式，進而對應到標籤，以提供物理學見解並廣泛應用於學術界和工業界。我們發現，預訓練模型在特定領域的Data2Eqn任務中表現不佳，原因在於它們是在通用數據分佈上訓練，並且訓練目標側重於token級別對齊，忽略了數學語義，導致方程式不準確。為了解決這些問題，我們提出一個基於強化學習的微調框架，透過來自下游數值擬合的獎勵信號，直接優化預訓練模型的生成策略。實驗結果表明，此方法提高了模型在複雜分佈下生成方程式的準確性和魯棒性。", "audio": "audios/2505.15572v1.mp3", "timestamp": "2025-05-22T06:27:14.421326"}
{"query": "Diffusion Model", "id": "2505.15679v1", "url": "http://arxiv.org/abs/2505.15679v1", "title": "SwarmDiff: Swarm Robotic Trajectory Planning in Cluttered Environments via Diffusion Transformer", "summary": "Swarm robotic trajectory planning faces challenges in computational\nefficiency, scalability, and safety, particularly in complex, obstacle-dense\nenvironments. To address these issues, we propose SwarmDiff, a hierarchical and\nscalable generative framework for swarm robots. We model the swarm's\nmacroscopic state using Probability Density Functions (PDFs) and leverage\nconditional diffusion models to generate risk-aware macroscopic trajectory\ndistributions, which then guide the generation of individual robot trajectories\nat the microscopic level. To ensure a balance between the swarm's optimal\ntransportation and risk awareness, we integrate Wasserstein metrics and\nConditional Value at Risk (CVaR). Additionally, we introduce a Diffusion\nTransformer (DiT) to improve sampling efficiency and generation quality by\ncapturing long-range dependencies. Extensive simulations and real-world\nexperiments demonstrate that SwarmDiff outperforms existing methods in\ncomputational efficiency, trajectory validity, and scalability, making it a\nreliable solution for swarm robotic trajectory planning.", "authors": ["Kang Ding", "Chunxuan Jiao", "Yunze Hu", "Kangjie Zhou", "Pengying Wu", "Yao Mu", "Chang Liu"], "published_date": "2025-05-21", "title_zh": "SwarmDiff：基於擴散轉換器的複雜環境群體機器人軌跡規劃", "summary_zh": "SwarmDiff 是一個針對複雜環境中群體機器人的軌跡規劃框架。它運用擴散模型生成風險感知的群體宏觀軌跡分佈，再以此引導生成個體機器人的微觀軌跡。同時，整合了 Wasserstein 指標和條件風險值（CVaR）來平衡群體的最優運輸和風險意識。透過擴散轉換器 (DiT) 來提升抽樣效率和生成品質。實驗證明，SwarmDiff 在計算效率、軌跡有效性和可擴展性方面優於現有方法，為群體機器人軌跡規劃提供了一個可靠的解決方案。", "audio": "audios/2505.15679v1.mp3", "timestamp": "2025-05-22T06:27:18.842130"}
{"query": "AI", "id": "2505.15778v1", "url": "http://arxiv.org/abs/2505.15778v1", "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space", "summary": "Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking.", "authors": ["Zhen Zhang", "Xuehai He", "Weixiang Yan", "Ao Shen", "Chenyang Zhao", "Shuohang Wang", "Yelong Shen", "Xin Eric Wang"], "published_date": "2025-05-21", "title_zh": "軟性思考：釋放LLM在連續概念空間中的推理潛力", "summary_zh": "現有的推理模型受限於語言，只能處理離散的語義單位。這篇論文提出一種名為「軟性思考」的免訓練方法，模擬人類的抽象思考模式，在連續概念空間中產生軟性的概念標記。這些標記透過加權混合不同的詞嵌入形成，能更平滑地轉換並產生更豐富的表徵，突破傳統離散邊界。實驗結果顯示，「軟性思考」在數學和程式碼基準測試中，能有效提升準確度並降低 token 使用量，同時保持輸出的可讀性，展現其打破離散語言推理瓶頸的潛力。", "audio": "audios/2505.15778v1.mp3", "timestamp": "2025-05-22T09:21:16.643593"}
{"query": "Foundation Model", "id": "2505.15559v1", "url": "http://arxiv.org/abs/2505.15559v1", "title": "Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music Attributes", "summary": "Moonbeam is a transformer-based foundation model for symbolic music,\npretrained on a large and diverse collection of MIDI data totaling 81.6K hours\nof music and 18 billion tokens. Moonbeam incorporates music-domain inductive\nbiases by capturing both absolute and relative musical attributes through the\nintroduction of a novel domain-knowledge-inspired tokenization method and\nMultidimensional Relative Attention (MRA), which captures relative music\ninformation without additional trainable parameters. Leveraging the pretrained\nMoonbeam, we propose 2 finetuning architectures with full anticipatory\ncapabilities, targeting 2 categories of downstream tasks: symbolic music\nunderstanding and conditional music generation (including music infilling). Our\nmodel outperforms other large-scale pretrained music models in most cases in\nterms of accuracy and F1 score across 3 downstream music classification tasks\non 4 datasets. Moreover, our finetuned conditional music generation model\noutperforms a strong transformer baseline with a REMI-like tokenizer. We\nopen-source the code, pretrained model, and generated samples on Github.", "authors": ["Zixun Guo", "Simon Dixon"], "published_date": "2025-05-21", "title_zh": "Moonbeam：一個利用絕對與相對音樂屬性的MIDI基礎模型", "summary_zh": "Moonbeam是一個基於Transformer的符號音樂基礎模型，使用大量MIDI數據（總計8.16萬小時的音樂和180億個tokens）進行預訓練。它整合了音樂領域的歸納偏置，通過創新的token化方法和多維相對注意力機制（MRA）來捕捉絕對和相對的音樂屬性。MRA在不增加可訓練參數的情況下捕捉相對音樂信息。利用預訓練的Moonbeam，我們提出了兩種具有完整預測能力的微調架構，針對符號音樂理解和條件音樂生成（包括音樂填充）兩類下游任務。我們的模型在四個數據集的三个下游音樂分類任務中，其準確度和F1分數大多優於其他大規模預訓練音樂模型。此外，我們微調的條件音樂生成模型也勝過使用類似REMI tokenization方法的強Transformer基線。我們已在Github上開源代碼、預訓練模型和生成的樣本。", "audio": "audios/2505.15559v1.mp3", "timestamp": "2025-05-22T09:21:23.097162"}
{"query": "Diffusion Model", "id": "2505.15644v1", "url": "http://arxiv.org/abs/2505.15644v1", "title": "FragFake: A Dataset for Fine-Grained Detection of Edited Images with Vision Language Models", "summary": "Fine-grained edited image detection of localized edits in images is crucial\nfor assessing content authenticity, especially given that modern diffusion\nmodels and image editing methods can produce highly realistic manipulations.\nHowever, this domain faces three challenges: (1) Binary classifiers yield only\na global real-or-fake label without providing localization; (2) Traditional\ncomputer vision methods often rely on costly pixel-level annotations; and (3)\nNo large-scale, high-quality dataset exists for modern image-editing detection\ntechniques. To address these gaps, we develop an automated data-generation\npipeline to create FragFake, the first dedicated benchmark dataset for edited\nimage detection, which includes high-quality images from diverse editing models\nand a wide variety of edited objects. Based on FragFake, we utilize Vision\nLanguage Models (VLMs) for the first time in the task of edited image\nclassification and edited region localization. Experimental results show that\nfine-tuned VLMs achieve higher average Object Precision across all datasets,\nsignificantly outperforming pretrained models. We further conduct ablation and\ntransferability analyses to evaluate the detectors across various\nconfigurations and editing scenarios. To the best of our knowledge, this work\nis the first to reformulate localized image edit detection as a vision-language\nunderstanding task, establishing a new paradigm for the field. We anticipate\nthat this work will establish a solid foundation to facilitate and inspire\nsubsequent research endeavors in the domain of multimodal content authenticity.", "authors": ["Zhen Sun", "Ziyi Zhang", "Zeren Luo", "Zeyang Sha", "Tianshuo Cong", "Zheng Li", "Shiwen Cui", "Weiqiang Wang", "Jiaheng Wei", "Xinlei He", "Qi Li", "Qian Wang"], "published_date": "2025-05-21", "title_zh": "FragFake：一個利用視覺語言模型進行細粒度編輯圖像檢測的資料集", "summary_zh": "現代圖像編輯技術高度逼真，準確判斷圖像是否經過局部修改至關重要。 然而，傳統方法難以定位編輯區域，且缺乏高品質的大規模資料集。為此，我們創建了 FragFake 資料集，包含多種編輯模型和物件，並首次使用視覺語言模型 (VLMs) 進行編輯圖像分類和區域定位。實驗表明，經過微調的 VLMs 在檢測編輯區域的精確度上顯著優於預訓練模型。 這項研究將局部圖像編輯檢測重新定義為視覺語言理解任務，為該領域建立了一個新範式。", "audio": "audios/2505.15644v1.mp3", "timestamp": "2025-05-22T09:21:28.753217"}
{"query": "AI", "id": "2505.15755v1", "url": "http://arxiv.org/abs/2505.15755v1", "title": "Exploring The Visual Feature Space for Multimodal Neural Decoding", "summary": "The intrication of brain signals drives research that leverages multimodal AI\nto align brain modalities with visual and textual data for explainable\ndescriptions. However, most existing studies are limited to coarse\ninterpretations, lacking essential details on object descriptions, locations,\nattributes, and their relationships. This leads to imprecise and ambiguous\nreconstructions when using such cues for visual decoding. To address this, we\nanalyze different choices of vision feature spaces from pre-trained visual\ncomponents within Multimodal Large Language Models (MLLMs) and introduce a\nzero-shot multimodal brain decoding method that interacts with these models to\ndecode across multiple levels of granularities. % To assess a model's ability\nto decode fine details from brain signals, we propose the Multi-Granularity\nBrain Detail Understanding Benchmark (MG-BrainDub). This benchmark includes two\nkey tasks: detailed descriptions and salient question-answering, with metrics\nhighlighting key visual elements like objects, attributes, and relationships.\nOur approach enhances neural decoding precision and supports more accurate\nneuro-decoding applications. Code will be available at\nhttps://github.com/weihaox/VINDEX.", "authors": ["Weihao Xia", "Cengiz Oztireli"], "published_date": "2025-05-21", "title_zh": "探索多模態神經解碼的視覺特徵空間", "summary_zh": "現有研究利用多模態人工智慧將腦部訊號與視覺及文字資料對齊，以產生可解釋的描述，但細節不足，導致視覺解碼重建不精確。本研究分析多模態大型語言模型(MLLM)中預訓練視覺元件的不同視覺特徵空間，並提出一種零樣本多模態腦部解碼方法，與這些模型互動，在多個粒度層級上進行解碼。研究更推出多粒度腦部細節理解基準(MG-BrainDub)，包含詳細描述和顯著問答任務，以評估模型從腦部訊號解碼細節的能力。本研究旨在提高神經解碼的精確度，並支援更準確的神經解碼應用。程式碼將在https://github.com/weihaox/VINDEX 上公開。", "audio": "audios/2505.15755v1.mp3", "timestamp": "2025-05-22T10:20:08.924290"}
{"query": "Foundation Model", "id": "2505.15506v1", "url": "http://arxiv.org/abs/2505.15506v1", "title": "Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts", "summary": "Recently, Vision-Language foundation models like CLIP and ALIGN, which are\npre-trained on large-scale data have shown remarkable zero-shot generalization\nto diverse datasets with different classes and even domains. In this work, we\ntake a step further and analyze whether these models can be adapted to target\ndatasets having very different distributions and classes compared to what these\nmodels have been trained on, using only a few labeled examples from the target\ndataset. In such scenarios, finetuning large pretrained models is challenging\ndue to problems of overfitting as well as loss of generalization, and has not\nbeen well explored in prior literature. Since, the pre-training data of such\nmodels are unavailable, it is difficult to comprehend the performance on\nvarious downstream datasets. First, we try to answer the question: Given a\ntarget dataset with a few labelled examples, can we estimate whether further\nfine-tuning can enhance the performance compared to zero-shot evaluation? by\nanalyzing the common vision-language embedding space. Based on the analysis, we\npropose a novel prompt-tuning method, PromptMargin for adapting such\nlarge-scale VLMs directly on the few target samples. PromptMargin effectively\ntunes the text as well as visual prompts for this task, and has two main\nmodules: 1) Firstly, we use a selective augmentation strategy to complement the\nfew training samples in each task; 2) Additionally, to ensure robust training\nin the presence of unfamiliar class names, we increase the inter-class margin\nfor improved class discrimination using a novel Multimodal Margin Regularizer.\nExtensive experiments and analysis across fifteen target benchmark datasets,\nwith varying degrees of distribution shifts from natural images, shows the\neffectiveness of the proposed framework over the existing state-of-the-art\napproaches applied to this setting. github.com/debarshigit/PromptMargin.", "authors": ["Debarshi Brahma", "Anuska Roy", "Soma Biswas"], "published_date": "2025-05-21", "title_zh": "使用邊界正則化器調整視覺語言模型以應對分布偏移下的少樣本學習", "summary_zh": "CLIP 和 ALIGN 等視覺語言預訓練模型在不同資料集上展現了出色的零樣本泛化能力。本研究進一步探討如何使用目標資料集的少量標記樣本，使這些模型適應與訓練資料具有顯著分布和類別差異的目標資料集。針對這個問題，我們分析了視覺語言嵌入空間，提出一種新的 PromptMargin 方法，有效調整文本和視覺提示詞，並採用選擇性資料擴增和多模態邊界正則化器，增強模型在少樣本情況下的鲁棒性和類別區分能力。在十五個基準資料集上的實驗證明，PromptMargin 在分布偏移下的少樣本學習中優於現有方法。", "audio": "audios/2505.15506v1.mp3", "timestamp": "2025-05-22T10:20:16.069397"}
{"query": "Diffusion Model", "id": "2505.15450v1", "url": "http://arxiv.org/abs/2505.15450v1", "title": "Comprehensive Evaluation and Analysis for NSFW Concept Erasure in Text-to-Image Diffusion Models", "summary": "Text-to-image diffusion models have gained widespread application across\nvarious domains, demonstrating remarkable creative potential. However, the\nstrong generalization capabilities of diffusion models can inadvertently lead\nto the generation of not-safe-for-work (NSFW) content, posing significant risks\nto their safe deployment. While several concept erasure methods have been\nproposed to mitigate the issue associated with NSFW content, a comprehensive\nevaluation of their effectiveness across various scenarios remains absent. To\nbridge this gap, we introduce a full-pipeline toolkit specifically designed for\nconcept erasure and conduct the first systematic study of NSFW concept erasure\nmethods. By examining the interplay between the underlying mechanisms and\nempirical observations, we provide in-depth insights and practical guidance for\nthe effective application of concept erasure methods in various real-world\nscenarios, with the aim of advancing the understanding of content safety in\ndiffusion models and establishing a solid foundation for future research and\ndevelopment in this critical area.", "authors": ["Die Chen", "Zhiwen Li", "Cen Chen", "Yuexiang Xie", "Xiaodan Li", "Jinyan Ye", "Yingda Chen", "Yaliang Li"], "published_date": "2025-05-21", "title_zh": "文字生成圖像擴散模型中，針對不雅內容概念消除的全面評估與分析", "summary_zh": "文字生成圖像模型能力強大，但也可能生成不雅內容，造成安全風險。雖然已有消除不雅內容的方法，但缺乏全面評估。本研究開發工具，系統性地評估現有方法，深入分析其機制和效果，為實際應用提供指導，旨在提升對擴散模型內容安全性的理解，並為未來研究奠定基礎。\n\n**簡明摘要：**AI繪圖很厲害，但可能畫出不雅圖片。這項研究評估了目前消除不雅內容的方法，希望能讓AI繪圖更安全、更可靠。", "audio": "audios/2505.15450v1.mp3", "timestamp": "2025-05-22T10:20:23.383218"}
{"query": "AI", "id": "2505.15700v1", "url": "http://arxiv.org/abs/2505.15700v1", "title": "\"Alexa, can you forget me?\" Machine Unlearning Benchmark in Spoken Language Understanding", "summary": "Machine unlearning, the process of efficiently removing specific information\nfrom machine learning models, is a growing area of interest for responsible AI.\nHowever, few studies have explored the effectiveness of unlearning methods on\ncomplex tasks, particularly speech-related ones. This paper introduces\nUnSLU-BENCH, the first benchmark for machine unlearning in spoken language\nunderstanding (SLU), focusing on four datasets spanning four languages. We\naddress the unlearning of data from specific speakers as a way to evaluate the\nquality of potential \"right to be forgotten\" requests. We assess eight\nunlearning techniques and propose a novel metric to simultaneously better\ncapture their efficacy, utility, and efficiency. UnSLU-BENCH sets a foundation\nfor unlearning in SLU and reveals significant differences in the effectiveness\nand computational feasibility of various techniques.", "authors": ["Alkis Koudounas", "Claudio Savelli", "Flavio Giobergia", "Elena Baralis"], "published_date": "2025-05-21", "title_zh": "「Alexa，你可以忘記我嗎？」：口語理解中的機器遺忘基準測試", "summary_zh": "為響應「被遺忘權」的要求，機器遺忘技術日漸重要。本論文推出首個口語理解（SLU）機器遺忘基準測試UnSLU-BENCH，涵蓋四種語言的四個數據集，專注於將特定說話者的數據從模型中移除。研究評估了八種遺忘技術，並提出一種新的評估指標，綜合考量其效果、實用性和效率。UnSLU-BENCH 為口語理解中的遺忘技術奠定基礎，並揭示了不同技術在效果和計算可行性上的顯著差異。", "audio": "audios/2505.15700v1.mp3", "timestamp": "2025-05-22T11:16:43.359376"}
{"query": "Foundation Model", "id": "2505.15334v1", "url": "http://arxiv.org/abs/2505.15334v1", "title": "Parameter-Efficient Fine-Tuning of Multispectral Foundation Models for Hyperspectral Image Classification", "summary": "Foundation models have achieved great success across diverse domains,\nincluding remote sensing (RS), thanks to their versatility and strong\ngeneralization abilities. However, most RS foundation models are designed for\nmultispectral data, while hyperspectral imagery (HSI) - with its hundreds of\nspectral bands - remains less explored. Fine-tuning such models for downstream\ntasks is also challenging, often demanding considerable memory and storage. In\nthis paper, we propose an efficient framework to fine-tune SpectralGPT, a\nmultispectral foundation model, for hyperspectral image classification (HSIC).\nWe explore several Parameter-Efficient Fine-Tuning (PEFT) methods, including\nLow-Rank Adaptation (LoRA), Kronecker-based adaptation (KronA), Low-Rank\nKronecker (LoKr), and the recent LoRA+, which uses distinct learning rates for\nlow-rank adapters scaled by a factor lambda. Inspired by LoRA+, we introduce\nKronA+, which applies a similar mechanism to the Kronecker matrices. We\nevaluate our approach on five datasets from different sensors, showing\ncompetitive performance with state-of-the-art HSI models. Our full fine-tuning\n(FFT) setup for SpectralGPT even outperforms a dedicated hyperspectral\nfoundation model on some datasets while requiring only a quarter of the\ntraining epochs. Under the same number of epochs, KronA+ reaches similar\nperformance with far fewer trainable parameters - just 0.056 percent - and adds\nonly approximately 0.2 megabytes of storage, making it the most effective PEFT\nmethod tested.", "authors": ["Bernardin Ligan", "Khalide Jbilou", "Fahd Kalloubi", "Ahmed Ratnani"], "published_date": "2025-05-21", "title_zh": "針對高光譜影像分類的多光譜基礎模型之參數高效微調", "summary_zh": "本文提出一個高效框架，針對高光譜影像分類，微調多光譜基礎模型SpectralGPT。由於高光譜影像擁有數百個頻譜通道，直接微調大型模型耗費大量資源。因此，研究使用多種參數高效微調(PEFT)方法，如LoRA、KronA及其改良版本LoRA+和KronA+。實驗證明，我們的框架在多個數據集上表現優異，其中KronA+僅需極少的可訓練參數和儲存空間，即可達到與完整微調相近的效能，是目前測試中最有效的PEFT方法。", "audio": "audios/2505.15334v1.mp3", "timestamp": "2025-05-22T11:16:48.505895"}
{"query": "Diffusion Model", "id": "2505.15427v1", "url": "http://arxiv.org/abs/2505.15427v1", "title": "Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions", "summary": "The remarkable ability of diffusion models to generate high-fidelity images\nhas led to their widespread adoption. However, concerns have also arisen\nregarding their potential to produce Not Safe for Work (NSFW) content and\nexhibit social biases, hindering their practical use in real-world\napplications. In response to this challenge, prior work has focused on\nemploying security filters to identify and exclude toxic text, or\nalternatively, fine-tuning pre-trained diffusion models to erase sensitive\nconcepts. Unfortunately, existing methods struggle to achieve satisfactory\nperformance in the sense that they can have a significant impact on the normal\nmodel output while still failing to prevent the generation of harmful content\nin some cases. In this paper, we propose a novel self-discovery approach to\nidentifying a semantic direction vector in the embedding space to restrict text\nembedding within a safe region. Our method circumvents the need for correcting\nindividual words within the input text and steers the entire text prompt\ntowards a safe region in the embedding space, thereby enhancing model\nrobustness against all possibly unsafe prompts. In addition, we employ Low-Rank\nAdaptation (LoRA) for semantic direction vector initialization to reduce the\nimpact on the model performance for other semantics. Furthermore, our method\ncan also be integrated with existing methods to improve their social\nresponsibility. Extensive experiments on benchmark datasets demonstrate that\nour method can effectively reduce NSFW content and mitigate social bias\ngenerated by diffusion models compared to several state-of-the-art baselines.", "authors": ["Zhiwen Li", "Die Chen", "Mingyuan Fan", "Cen Chen", "Yaliang Li", "Yanhao Wang", "Wenmeng Zhou"], "published_date": "2025-05-21", "title_zh": "透過在安全區域內約束文本嵌入實現負責任的擴散模型", "summary_zh": "擴散模型雖然能生成高擬真度圖像，但也可能產生不宜內容和社會偏見，影響實際應用。為了應對這個問題，本研究提出一種新的方法，透過在嵌入空間中識別語義方向向量，將文本嵌入限制在安全區域內。這種方法無需修正輸入文本中的個別詞彙，而是引導整個文本提示詞前往嵌入空間中的安全區域，從而增強模型對所有可能不安全提示詞的抵抗力。此外，我們採用低秩適應 (LoRA) 初始化語義方向向量，以減少對模型性能的影響。實驗結果表明，與現有方法相比，我們的方法能有效減少擴散模型生成的不宜內容並減輕社會偏見。", "audio": "audios/2505.15427v1.mp3", "timestamp": "2025-05-22T11:16:53.788897"}
{"query": "AI", "id": "2505.15622v1", "url": "http://arxiv.org/abs/2505.15622v1", "title": "Benchmarking Energy and Latency in TinyML: A Novel Method for Resource-Constrained AI", "summary": "The rise of IoT has increased the need for on-edge machine learning, with\nTinyML emerging as a promising solution for resource-constrained devices such\nas MCU. However, evaluating their performance remains challenging due to\ndiverse architectures and application scenarios. Current solutions have many\nnon-negligible limitations. This work introduces an alternative benchmarking\nmethodology that integrates energy and latency measurements while\ndistinguishing three execution phases pre-inference, inference, and\npost-inference. Additionally, the setup ensures that the device operates\nwithout being powered by an external measurement unit, while automated testing\ncan be leveraged to enhance statistical significance. To evaluate our setup, we\ntested the STM32N6 MCU, which includes a NPU for executing neural networks. Two\nconfigurations were considered: high-performance and Low-power. The variation\nof the EDP was analyzed separately for each phase, providing insights into the\nimpact of hardware configurations on energy efficiency. Each model was tested\n1000 times to ensure statistically relevant results. Our findings demonstrate\nthat reducing the core voltage and clock frequency improve the efficiency of\npre- and post-processing without significantly affecting network execution\nperformance. This approach can also be used for cross-platform comparisons to\ndetermine the most efficient inference platform and to quantify how pre- and\npost-processing overhead varies across different hardware implementations.", "authors": ["Pietro Bartoli", "Christian Veronesi", "Andrea Giudici", "David Siorpaes", "Diana Trojaniello", "Franco Zappa"], "published_date": "2025-05-21", "title_zh": "TinyML 能效與延遲基準測試：一種針對資源受限AI的新方法", "summary_zh": "物聯網興起，對邊緣機器學習的需求增加。TinyML 成為資源受限裝置（如微控制器 MCU）的潛力解決方案。然而，評估其效能極具挑戰。本研究提出一種新的基準測試方法，整合能耗與延遲測量，並區分預處理、推論和後處理三個階段。此方法確保裝置獨立供電運行，並利用自動化測試提高統計顯著性。實驗結果顯示，降低核心電壓和時脈頻率能有效提升預處理和後處理的效率，且不顯著影響神經網路的推論效能。此方法也可用於跨平台比較，找出最有效率的推論平台，並量化不同硬體實現的預處理和後處理開銷。", "audio": "audios/2505.15622v1.mp3", "timestamp": "2025-05-22T12:38:29.628724"}
{"query": "Foundation Model", "id": "2505.15307v1", "url": "http://arxiv.org/abs/2505.15307v1", "title": "Towards Pre-training an Effective Respiratory Audio Foundation Model", "summary": "Recent advancements in foundation models have sparked interest in respiratory\naudio foundation models. However, the effectiveness of applying conventional\npre-training schemes to datasets that are small-sized and lack diversity has\nnot been sufficiently verified. This study aims to explore better pre-training\npractices for respiratory sounds by comparing numerous pre-trained audio\nmodels. Our investigation reveals that models pre-trained on AudioSet, a\ngeneral audio dataset, are more effective than the models specifically\npre-trained on respiratory sounds. Moreover, combining AudioSet and respiratory\nsound datasets for further pre-training enhances performance, and preserving\nthe frequency-wise information when aggregating features is vital. Along with\nmore insights found in the experiments, we establish a new state-of-the-art for\nthe OPERA benchmark, contributing to advancing respiratory audio foundation\nmodels. Our code is available online at\nhttps://github.com/nttcslab/eval-audio-repr/tree/main/plugin/OPERA.", "authors": ["Daisuke Niizumi", "Daiki Takeuchi", "Masahiro Yasuda", "Binh Thien Nguyen", "Yasunori Ohishi", "Noboru Harada"], "published_date": "2025-05-21", "title_zh": "邁向預訓練一個有效的呼吸音訊基礎模型", "summary_zh": "近期基礎模型的發展激發了對呼吸音訊基礎模型的興趣。本研究探討針對小規模、缺乏多樣性的呼吸音訊數據集，如何進行更有效的預訓練。研究比較了多個預訓練音訊模型，發現使用通用音訊數據集AudioSet預訓練的模型效果更好。進一步結合AudioSet和呼吸音訊數據集進行預訓練可提升性能，並且在聚合特徵時保留頻率信息至關重要。實驗結果為呼吸音訊基礎模型提供了新的洞見，並在OPERA基準測試中取得了最先進的結果。", "audio": "audios/2505.15307v1.mp3", "timestamp": "2025-05-22T12:38:33.648979"}
{"query": "Diffusion Model", "id": "2505.15336v1", "url": "http://arxiv.org/abs/2505.15336v1", "title": "My Face Is Mine, Not Yours: Facial Protection Against Diffusion Model Face Swapping", "summary": "The proliferation of diffusion-based deepfake technologies poses significant\nrisks for unauthorized and unethical facial image manipulation. While\ntraditional countermeasures have primarily focused on passive detection\nmethods, this paper introduces a novel proactive defense strategy through\nadversarial attacks that preemptively protect facial images from being\nexploited by diffusion-based deepfake systems. Existing adversarial protection\nmethods predominantly target conventional generative architectures (GANs, AEs,\nVAEs) and fail to address the unique challenges presented by diffusion models,\nwhich have become the predominant framework for high-quality facial deepfakes.\nCurrent diffusion-specific adversarial approaches are limited by their reliance\non specific model architectures and weights, rendering them ineffective against\nthe diverse landscape of diffusion-based deepfake implementations.\nAdditionally, they typically employ global perturbation strategies that\ninadequately address the region-specific nature of facial manipulation in\ndeepfakes.", "authors": ["Hon Ming Yam", "Zhongliang Guo", "Chun Pong Lau"], "published_date": "2025-05-21", "title_zh": "我的臉是我的，不是你的：針對擴散模型換臉的面部保護", "summary_zh": "擴散模型技術讓換臉變得更容易，但也帶來了未授權和不道德的面部圖像操縱風險。本文提出一種新的主動防禦策略，通過對抗性攻擊，預先保護臉部圖像免於被擴散模型的Deepfake系統利用。與過去針對GAN等生成模型的防禦方法不同，本文方法專門針對擴散模型的特性進行設計，克服了以往方法對模型依賴性強和擾動策略不夠精確等問題，旨在有效保護臉部圖像免受Deepfake攻擊。", "audio": "audios/2505.15336v1.mp3", "timestamp": "2025-05-22T12:38:39.258376"}
{"query": "AI", "id": "2505.15620v1", "url": "http://arxiv.org/abs/2505.15620v1", "title": "Observation of $χ_{cJ}\\to 3K_S^0K^\\pmπ^\\mp$", "summary": "By analyzing $(2712.4\\pm14.3)\\times10^6$ $\\psi(3686)$ events collected with\nthe BESIII detector operating at the BEPCII collider, the decays $\\chi_{c0,1,2}\n\\to 3K_S^0K^\\pm\\pi^\\mp$ are observed for the first time with statistical\nsignificances greater than $10\\sigma$. The branching fractions of these decays\nare determined to be $\\mathcal{B}(\\chi_{c0}\\to 3K_S^0K^\\pm\\pi^\\mp\n)=(7.95\\pm0.50\\pm0.65)\\times10^{-5},$ $\\mathcal{B}(\\chi_{c1}\\to\n3K_S^0K^\\pm\\pi^\\mp)=(2.62\\pm0.08\\pm0.19)\\times10^{-4},$ and\n$\\mathcal{B}(\\chi_{c2}\\to\n3K_S^0K^\\pm\\pi^\\mp)=(1.72\\pm0.07\\pm0.15)\\times10^{-4},$ where the first\nuncertainties are statistical and the second systematic.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "G. Chelkov", "C. Chen", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "Y. B. Chen", "Y. Q. Chen", "Z. J. Chen", "Z. K. Chen", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "Yang Gao", "S. Garbolino", "I. Garzia", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "T. Holtmann", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "B. Y. Hu", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "S. Janchiv", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "Z. H. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "Cheng Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "C. X. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. Liu", "L. C. Liu", "Lu Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. Ma", "H. L. Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "Y. H. Meng", "Z. X. Meng", "J. G. Messchendorp", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "Y. Y. Peng", "K. Peters", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "Ch. Rosner", "M. Q. Ruan", "S. N. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. X. Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "L. F. Tang", "M. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "V. Thoren", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "T. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "Lei Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "X. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. Z. Zhou", "Z. C. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-05-21", "title_zh": "$χ_{cJ}$ 衰變至 $3K_S^0K^\\pmπ^\\mp$ 的觀測", "summary_zh": "利用北京譜儀III實驗在BEPCII對撞機上收集的$(2712.4\\pm14.3)\\times10^6$個$\\psi(3686)$事件，首次觀測到衰變$\\chi_{c0,1,2} \\to 3K_S^0K^\\pm\\pi^\\mp$，統計顯著性均大於$10\\sigma$。測得分支比分別為$\\mathcal{B}(\\chi_{c0}\\to 3K_S^0K^\\pm\\pi^\\mp)=(7.95\\pm0.50\\pm0.65)\\times10^{-5}$，$\\mathcal{B}(\\chi_{c1}\\to 3K_S^0K^\\pm\\pi^\\mp)=(2.62\\pm0.08\\pm0.19)\\times10^{-4}$，以及$\\mathcal{B}(\\chi_{c2}\\to 3K_S^0K^\\pm\\pi^\\mp)=(1.72\\pm0.07\\pm0.15)\\times10^{-4}$，其中第一項誤差為統計誤差，第二項為系統誤差。", "audio": "audios/2505.15620v1.mp3", "timestamp": "2025-05-22T23:34:08.504115"}
{"query": "Foundation Model", "id": "2505.15192v1", "url": "http://arxiv.org/abs/2505.15192v1", "title": "Leveraging Foundation Models for Multimodal Graph-Based Action Recognition", "summary": "Foundation models have ushered in a new era for multimodal video\nunderstanding by enabling the extraction of rich spatiotemporal and semantic\nrepresentations. In this work, we introduce a novel graph-based framework that\nintegrates a vision-language foundation, leveraging VideoMAE for dynamic visual\nencoding and BERT for contextual textual embedding, to address the challenge of\nrecognizing fine-grained bimanual manipulation actions. Departing from\nconventional static graph architectures, our approach constructs an adaptive\nmultimodal graph where nodes represent frames, objects, and textual\nannotations, and edges encode spatial, temporal, and semantic relationships.\nThese graph structures evolve dynamically based on learned interactions,\nallowing for flexible and context-aware reasoning. A task-specific attention\nmechanism within a Graph Attention Network further enhances this reasoning by\nmodulating edge importance based on action semantics. Through extensive\nevaluations on diverse benchmark datasets, we demonstrate that our method\nconsistently outperforms state-of-the-art baselines, underscoring the strength\nof combining foundation models with dynamic graph-based reasoning for robust\nand generalizable action recognition.", "authors": ["Fatemeh Ziaeetabar", "Florentin Wörgötter"], "published_date": "2025-05-21", "title_zh": "利用基礎模型進行多模態圖基於動作識別", "summary_zh": "基礎模型為多模態影片理解帶來新紀元，能提取豐富時空與語義表徵。本文提出一種新型基於圖的框架，整合視覺-語言基礎模型，利用VideoMAE進行動態視覺編碼，BERT進行上下文文本嵌入，以解決精細雙手操作動作識別的挑戰。不同於傳統靜態圖架構，我們的方法構建自適應多模態圖，其中節點代表幀、對象和文本註釋，邊緣編碼空間、時間和語義關係。這些圖結構基於學習到的交互動態演化，實現靈活的上下文感知推理。圖注意力網絡中的任務特定注意力機制通過調節基於動作語義的邊緣重要性，進一步增強推理能力。在多個基準數據集上的廣泛評估表明，該方法始終優於最先進的基線，突顯了將基礎模型與基於動態圖的推理相結合，以實現穩健和通用的動作識別的優勢。", "audio": "audios/2505.15192v1.mp3", "timestamp": "2025-05-22T23:34:14.825615"}
{"query": "Diffusion Model", "id": "2505.15313v1", "url": "http://arxiv.org/abs/2505.15313v1", "title": "FaceCrafter: Identity-Conditional Diffusion with Disentangled Control over Facial Pose, Expression, and Emotion", "summary": "Human facial images encode a rich spectrum of information, encompassing both\nstable identity-related traits and mutable attributes such as pose, expression,\nand emotion. While recent advances in image generation have enabled\nhigh-quality identity-conditional face synthesis, precise control over\nnon-identity attributes remains challenging, and disentangling identity from\nthese mutable factors is particularly difficult. To address these limitations,\nwe propose a novel identity-conditional diffusion model that introduces two\nlightweight control modules designed to independently manipulate facial pose,\nexpression, and emotion without compromising identity preservation. These\nmodules are embedded within the cross-attention layers of the base diffusion\nmodel, enabling precise attribute control with minimal parameter overhead.\nFurthermore, our tailored training strategy, which leverages cross-attention\nbetween the identity feature and each non-identity control feature, encourages\nidentity features to remain orthogonal to control signals, enhancing\ncontrollability and diversity. Quantitative and qualitative evaluations, along\nwith perceptual user studies, demonstrate that our method surpasses existing\napproaches in terms of control accuracy over pose, expression, and emotion,\nwhile also improving generative diversity under identity-only conditioning.", "authors": ["Kazuaki Mishima", "Antoni Bigata Casademunt", "Stavros Petridis", "Maja Pantic", "Kenji Suzuki"], "published_date": "2025-05-21", "title_zh": "人臉工匠：基於身份條件的擴散模型，實現對面部姿態、表情和情緒的解耦控制", "summary_zh": "人臉圖像包含豐富資訊，包括穩定身份特徵和可變屬性，如姿態、表情和情緒。雖然圖像生成技術在身份條件人臉合成方面取得進展，但精確控制非身份屬性仍然具挑戰性，且分離身份與這些可變因素尤其困難。為解決這些問題，我們提出一種新型身份條件擴散模型，引入兩個輕量級控制模塊，獨立操縱人臉姿態、表情和情緒，同時保持身份不變。這些模塊嵌入在基礎擴散模型的交叉注意力層中，以最小的參數開銷實現精確的屬性控制。此外，我們採用定制化訓練策略，利用身份特徵和每個非身份控制特徵之間的交叉注意力，促使身份特徵與控制信號保持正交，從而提高可控性和多樣性。定量、定性評估以及感知用戶研究表明，我們的方法在姿態、表情和情緒的控制精度上優於現有方法，同時提高了僅在身份條件下的生成多樣性。", "audio": "audios/2505.15313v1.mp3", "timestamp": "2025-05-22T23:34:22.160515"}
{"query": "AI", "id": "2505.15596v1", "url": "http://arxiv.org/abs/2505.15596v1", "title": "Exploring LLM-Generated Feedback for Economics Essays: How Teaching Assistants Evaluate and Envision Its Use", "summary": "This project examines the prospect of using AI-generated feedback as\nsuggestions to expedite and enhance human instructors' feedback provision. In\nparticular, we focus on understanding the teaching assistants' perspectives on\nthe quality of AI-generated feedback and how they may or may not utilize AI\nfeedback in their own workflows. We situate our work in a foundational college\nEconomics class, which has frequent short essay assignments. We developed an\nLLM-powered feedback engine that generates feedback on students' essays based\non grading rubrics used by the teaching assistants (TAs). To ensure that TAs\ncan meaningfully critique and engage with the AI feedback, we had them complete\ntheir regular grading jobs. For a randomly selected set of essays that they had\ngraded, we used our feedback engine to generate feedback and displayed the\nfeedback as in-text comments in a Word document. We then performed think-aloud\nstudies with 5 TAs over 20 1-hour sessions to have them evaluate the AI\nfeedback, contrast the AI feedback with their handwritten feedback, and share\nhow they envision using the AI feedback if they were offered as suggestions.\nThe study highlights the importance of providing detailed rubrics for AI to\ngenerate high-quality feedback for knowledge-intensive essays. TAs considered\nthat using AI feedback as suggestions during their grading could expedite\ngrading, enhance consistency, and improve overall feedback quality. We discuss\nthe importance of decomposing the feedback generation task into steps and\npresenting intermediate results, in order for TAs to use the AI feedback.", "authors": ["Xinyi Lu", "Aditya Mahesh", "Zejia Shen", "Mitchell Dudley", "Larissa Sano", "Xu Wang"], "published_date": "2025-05-21", "title_zh": "探索大型語言模型生成之經濟學論文回饋：教學助理如何評估並設想其應用", "summary_zh": "本研究探討利用人工智慧生成的回饋意見，加速並提升人工教師回饋品質的可能性。重點在於了解助教對AI生成回饋的品質觀感，以及他們如何運用AI回饋於工作流程中。研究以大學基礎經濟學課程為背景，該課程包含頻繁的短篇論文作業。開發了一款基於大型語言模型的回饋引擎，根據助教使用的評分標準為學生論文生成回饋。為確保助教有效評估和運用AI回饋，讓他們完成常規評分工作。針對隨機選取、已評分的論文，使用回饋引擎生成回饋，並以Word文檔內文評論形式呈現。與5位助教進行20次、每次1小時的思考發聲研究，評估AI回饋，並將其與手寫回饋進行對比，分享他們如何運用AI回饋作為建議。研究強調提供詳細評分標準，對AI生成高品質知識密集型論文回饋的重要性。助教認為，在評分過程中將AI回饋作為建議，可加速評分，增強一致性，並提高整體回饋品質。討論了將回饋生成任務分解為多個步驟並呈現中間結果，對於助教使用AI回饋的重要性。", "audio": "audios/2505.15596v1.mp3", "timestamp": "2025-05-22T16:24:40.489307"}
{"query": "Foundation Model", "id": "2505.15185v1", "url": "http://arxiv.org/abs/2505.15185v1", "title": "MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth Foundation Models", "summary": "Recent advances in generalizable 3D Gaussian Splatting have demonstrated\npromising results in real-time high-fidelity rendering without per-scene\noptimization, yet existing approaches still struggle to handle unfamiliar\nvisual content during inference on novel scenes due to limited\ngeneralizability. To address this challenge, we introduce MonoSplat, a novel\nframework that leverages rich visual priors from pre-trained monocular depth\nfoundation models for robust Gaussian reconstruction. Our approach consists of\ntwo key components: a Mono-Multi Feature Adapter that transforms monocular\nfeatures into multi-view representations, coupled with an Integrated Gaussian\nPrediction module that effectively fuses both feature types for precise\nGaussian generation. Through the Adapter's lightweight attention mechanism,\nfeatures are seamlessly aligned and aggregated across views while preserving\nvaluable monocular priors, enabling the Prediction module to generate Gaussian\nprimitives with accurate geometry and appearance. Through extensive experiments\non diverse real-world datasets, we convincingly demonstrate that MonoSplat\nachieves superior reconstruction quality and generalization capability compared\nto existing methods while maintaining computational efficiency with minimal\ntrainable parameters. Codes are available at\nhttps://github.com/CUHK-AIM-Group/MonoSplat.", "authors": ["Yifan Liu", "Keyu Fan", "Weihao Yu", "Chenxin Li", "Hao Lu", "Yixuan Yuan"], "published_date": "2025-05-21", "title_zh": "MonoSplat：基於單目深度基礎模型的可泛化三維高斯濺射", "summary_zh": "通用三維高斯濺射技術雖在即時高保真渲染上展現潛力，但現有方法在處理新場景時仍因泛化能力不足而難以應對不熟悉的視覺內容。為此，我們提出MonoSplat，一種利用預訓練單眼深度基礎模型的豐富視覺先驗進行穩健高斯重建的新框架。該方法包含：將單眼特徵轉換為多視圖表示的單-多特徵適配器，以及有效融合兩種特徵以精確生成高斯分佈的集成高斯預測模組。透過適配器的輕量級注意力機制，特徵在視圖間無縫對齊和聚合，同時保留寶貴的單眼先驗，使預測模組能生成具有準確幾何和外觀的高斯圖元。在多樣真實世界資料集上的實驗證明，相較於現有方法，MonoSplat在保持計算效率和極少可訓練參數的同時，實現了卓越的重建品質和泛化能力。程式碼可在https://github.com/CUHK-AIM-Group/MonoSplat取得。", "audio": "audios/2505.15185v1.mp3", "timestamp": "2025-05-22T16:25:09.297783"}
{"query": "Diffusion Model", "id": "2505.15157v1", "url": "http://arxiv.org/abs/2505.15157v1", "title": "Cascaded Diffusion Models for Neural Motion Planning", "summary": "Robots in the real world need to perceive and move to goals in complex\nenvironments without collisions. Avoiding collisions is especially difficult\nwhen relying on sensor perception and when goals are among clutter. Diffusion\npolicies and other generative models have shown strong performance in solving\nlocal planning problems, but often struggle at avoiding all of the subtle\nconstraint violations that characterize truly challenging global motion\nplanning problems. In this work, we propose an approach for learning global\nmotion planning using diffusion policies, allowing the robot to generate full\ntrajectories through complex scenes and reasoning about multiple obstacles\nalong the path. Our approach uses cascaded hierarchical models which unify\nglobal prediction and local refinement together with online plan repair to\nensure the trajectories are collision free. Our method outperforms (by ~5%) a\nwide variety of baselines on challenging tasks in multiple domains including\nnavigation and manipulation.", "authors": ["Mohit Sharma", "Adam Fishman", "Vikash Kumar", "Chris Paxton", "Oliver Kroemer"], "published_date": "2025-05-21", "title_zh": "用於神經運動規劃的級聯擴散模型", "summary_zh": "現實環境中的機器人需要在複雜場景中感知並移動至目標點，同時避免碰撞。仰賴感測器感知且目標位於雜亂環境時，避碰尤為困難。擴散策略及其他生成模型在解決局部規劃問題上表現出色，但在避免複雜全局運動規劃問題中細微的約束違規方面往往力不從心。本研究提出一種利用擴散策略學習全局運動規劃的方法，使機器人能夠生成穿越複雜場景的完整軌跡，並推論路徑上的多個障礙物。該方法採用級聯階層模型，結合全局預測與局部優化，並透過線上計畫修正確保軌跡無碰撞。在導航和操作等多個領域的挑戰性任務中，該方法優於多種基準方法約5%。", "audio": "audios/2505.15157v1.mp3", "timestamp": "2025-05-22T16:25:16.858980"}
{"query": "AI", "id": "2505.15590v1", "url": "http://arxiv.org/abs/2505.15590v1", "title": "Bridging the Gap: Physical PCI Device Integration Into SystemC-TLM Virtual Platforms", "summary": "In today's technology-driven world, early-stage software development and\ntesting are crucial. Virtual Platforms (VPs) have become indispensable tools\nfor this purpose as they serve as a platform to execute and debug the\nunmodified target software at an early design stage. With the increasing\ncomplexity of software, especially in areas like Artificial Intelligence (AI)\napplications, VPs need to provide high simulation speed to ensure the target\nsoftware executes within a reasonable time. Hybrid simulation, which combines\nvirtual models with real hardware, can improve the performance of VPs. This\npaper introduces a novel approach for integrating real Peripheral Component\nInterconnect (PCI) devices into SystemC-TLM-2.0-based VPs. The embedded PCI\ndevices enable high performance, easy integration, and allow introspection for\nanalysis and optimization. To illustrate the practical application of our\napproach, we present a case study where we integrate Google Coral's Edge Tensor\nProcessing Unit (TPU) into an ARM-based VP. The integration allows efficient\nexecution of AI workloads, accelerating simulation speeds by up to 480x while\neliminating the need for complex virtual device models. Beyond accelerating\nAI-workload execution, our framework enables driver development, regression\ntesting across architectures, and device communication analysis. Our findings\ndemonstrate that embedding PCI devices into SystemC simulations significantly\nenhances", "authors": ["Nils Bosbach", "Rebecca Pelke", "Niko Zurstraßen", "Jan Henrik Weinstock", "Lukas Jünger", "Rainer Leupers"], "published_date": "2025-05-21", "title_zh": "彌合鴻溝：將實體PCI裝置整合至SystemC-TLM虛擬平台", "summary_zh": "在當今科技驅動的世界中，早期軟體開發與測試至關重要。虛擬平台（VP）已成為不可或缺的工具，可在設計初期執行和除錯未修改的目標軟體。隨著軟體複雜性不斷增加，尤其是在人工智慧（AI）應用領域，VP需要提供高速模擬，以確保目標軟體在合理時間內執行。混合模擬結合虛擬模型與真實硬體，可提升VP效能。本文介紹一種將真實周邊組件互連（PCI）設備整合至基於SystemC-TLM-2.0的VP的新方法。嵌入式PCI設備實現了高性能、易於整合，並允許進行內省分析與優化。為說明該方法的實際應用，本文呈現了一個案例研究，將Google Coral Edge張量處理單元（TPU）整合到基於ARM的VP中。此整合可有效執行AI工作負載，加速模擬速度高達480倍，同時免除對複雜虛擬設備模型的需求。除了加速AI工作負載執行外，該框架還可實現驅動程式開發、跨架構回歸測試以及設備通訊分析。研究結果表明，將PCI設備嵌入SystemC模擬可顯著提高效能。", "audio": "audios/2505.15590v1.mp3", "timestamp": "2025-05-22T17:17:08.437912"}
{"query": "Foundation Model", "id": "2505.15151v1", "url": "http://arxiv.org/abs/2505.15151v1", "title": "Time Tracker: Mixture-of-Experts-Enhanced Foundation Time Series Forecasting Model with Decoupled Training Pipelines", "summary": "In the past few years, time series foundation models have achieved superior\npredicting accuracy. However, real-world time series often exhibit significant\ndiversity in their temporal patterns across different time spans and domains,\nmaking it challenging for a single model architecture to fit all complex\nscenarios. In addition, time series data may have multiple variables exhibiting\ncomplex correlations between each other. Recent mainstream works have focused\non modeling times series in a channel-independent manner in both pretraining\nand finetuning stages, overlooking the valuable inter-series dependencies. To\nthis end, we propose \\textbf{Time Tracker} for better predictions on\nmultivariate time series data. Firstly, we leverage sparse mixture of experts\n(MoE) within Transformers to handle the modeling of diverse time series\npatterns, thereby alleviating the learning difficulties of a single model while\nimproving its generalization. Besides, we propose Any-variate Attention,\nenabling a unified model structure to seamlessly handle both univariate and\nmultivariate time series, thereby supporting channel-independent modeling\nduring pretraining and channel-mixed modeling for finetuning. Furthermore, we\ndesign a graph learning module that constructs relations among sequences from\nfrequency-domain features, providing more precise guidance to capture\ninter-series dependencies in channel-mixed modeling. Based on these\nadvancements, Time Tracker achieves state-of-the-art performance in predicting\naccuracy, model generalization and adaptability.", "authors": ["Xiaohou Shi", "Ke Li", "Aobo Liang", "Yan Sun"], "published_date": "2025-05-21", "title_zh": "時間追蹤器：具解耦訓練管線之混合專家增強型基礎時間序列預測模型", "summary_zh": "近年來，時間序列基礎模型展現卓越的預測精度。然而，真實世界的時間序列在不同時間跨度和領域中呈現顯著多樣性，使單一模型架構難以適應所有複雜情境。此外，時間序列資料可能有多個變數，彼此間存在複雜關聯。目前主流研究多著重於預訓練和微調階段中通道獨立的時間序列建模，忽略了重要的序列間依賴關係。為此，我們提出Time Tracker，以提升多變數時間序列資料的預測效果。首先，我們在Transformer中使用稀疏混合專家模型(MoE)，處理多樣時間序列模式的建模，從而減輕單一模型的學習難度，並提高其泛化能力。其次，我們提出Any-variate Attention，使統一模型結構能無縫處理單變數和多變數時間序列，進而支援預訓練期間的通道獨立建模，以及微調期間的通道混合建模。此外，我們設計了一個圖學習模組，從頻域特徵中構建序列間關係，為通道混合建模中捕捉序列間依賴關係提供更精確的指導。基於這些進展，Time Tracker在預測精度、模型泛化能力和適應性方面均實現了最先進的性能。", "audio": "audios/2505.15151v1.mp3", "timestamp": "2025-05-22T17:17:18.583298"}
{"query": "Diffusion Model", "id": "2505.15152v1", "url": "http://arxiv.org/abs/2505.15152v1", "title": "Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation", "summary": "Feature Transformation (FT) crafts new features from original ones via\nmathematical operations to enhance dataset expressiveness for downstream\nmodels. However, existing FT methods exhibit critical limitations: discrete\nsearch struggles with enormous combinatorial spaces, impeding practical use;\nand continuous search, being highly sensitive to initialization and step sizes,\noften becomes trapped in local optima, restricting global exploration. To\novercome these limitations, DIFFT redefines FT as a reward-guided generative\ntask. It first learns a compact and expressive latent space for feature sets\nusing a Variational Auto-Encoder (VAE). A Latent Diffusion Model (LDM) then\nnavigates this space to generate high-quality feature embeddings, its\ntrajectory guided by a performance evaluator towards task-specific optima. This\nsynthesis of global distribution learning (from LDM) and targeted optimization\n(reward guidance) produces potent embeddings, which a novel semi-autoregressive\ndecoder efficiently converts into structured, discrete features, preserving\nintra-feature dependencies while allowing parallel inter-feature generation.\nExtensive experiments on 14 benchmark datasets show DIFFT consistently\noutperforms state-of-the-art baselines in predictive accuracy and robustness,\nwith significantly lower training and inference times.", "authors": ["Nanxu Gong", "Zijun Li", "Sixun Dong", "Haoyue Bai", "Wangyang Ying", "Xinyuan Wang", "Yanjie Fu"], "published_date": "2025-05-21", "title_zh": "從噪聲雕琢特徵：獎勵導向的分層擴散用於任務最佳特徵轉換", "summary_zh": "特徵轉換透過數學運算從原始特徵中創建新特徵，以增強資料集的表達能力。現有方法存在離散搜索組合空間龐大和連續搜索易陷入局部最佳解的局限。DIFFT將特徵轉換重新定義為獎勵引導的生成任務。首先，使用變分自編碼器學習特徵集的壓縮潛在空間。然後，潛在擴散模型在此空間中生成高品質特徵嵌入，其軌跡由性能評估器引導至特定任務的最佳狀態。這種全局分佈學習和目標優化的結合產生了強大的嵌入，新型半自迴歸解碼器將其高效轉換為結構化的離散特徵，在允許並行特徵間生成的同時，保留了特徵內部的依賴關係。在14個基準資料集上的實驗表明，DIFFT在預測準確性和穩健性方面始終優於現有方法，且訓練和推論時間顯著降低。", "audio": "audios/2505.15152v1.mp3", "timestamp": "2025-05-22T17:17:25.618844"}
{"query": "AI", "id": "2505.15571v1", "url": "http://arxiv.org/abs/2505.15571v1", "title": "Temporal Spectrum Cartography in Low-Altitude Economy Networks: A Generative AI Framework with Multi-Agent Learning", "summary": "This paper introduces a two-stage generative AI (GenAI) framework tailored\nfor temporal spectrum cartography in low-altitude economy networks (LAENets).\nLAENets, characterized by diverse aerial devices such as UAVs, rely heavily on\nwireless communication technologies while facing challenges, including spectrum\ncongestion and dynamic environmental interference. Traditional spectrum\ncartography methods have limitations in handling the temporal and spatial\ncomplexities inherent to these networks. Addressing these challenges, the\nproposed framework first employs a Reconstructive Masked Autoencoder (RecMAE)\ncapable of accurately reconstructing spectrum maps from sparse and temporally\nvarying sensor data using a novel dual-mask mechanism. This approach\nsignificantly enhances the precision of reconstructed radio frequency (RF)\npower maps. In the second stage, the Multi-agent Diffusion Policy (MADP) method\nintegrates diffusion-based reinforcement learning to optimize the trajectories\nof dynamic UAV sensors. By leveraging temporal-attention encoding, this method\neffectively manages spatial exploration and exploitation to minimize cumulative\nreconstruction errors. Extensive numerical experiments validate that this\nintegrated GenAI framework outperforms traditional interpolation methods and\ndeep learning baselines by achieving 57.35% and 88.68% reconstruction error\nreduction, respectively. The proposed trajectory planner substantially improves\nspectrum map accuracy, reconstruction stability, and sensor deployment\nefficiency in dynamically evolving low-altitude environments.", "authors": ["Changyuan Zhao", "Ruichen Zhang", "Jiacheng Wang", "Dusit Niyato", "Geng Sun", "Hongyang Du", "Zan Li", "Abbas Jamalipour", "Dong In Kim"], "published_date": "2025-05-21", "title_zh": "低空經濟網絡時序頻譜製圖：基於多代理學習的生成式人工智慧框架", "summary_zh": "本研究提出一個雙階段生成式AI框架，專為低空經濟網路中的時序頻譜地圖繪製設計。針對低空經濟網路中UAV等設備對無線通訊的依賴及其面臨的頻譜擁塞和動態干擾問題，傳統頻譜地圖繪製方法難以應對其時空複雜性。此框架首先採用重建式遮罩自編碼器（RecMAE），利用雙遮罩機制，從稀疏且隨時間變化的感測器數據中精確重建頻譜圖，顯著提升射頻功率地圖的精確度。其次，多智能體擴散策略（MADP）整合基於擴散的強化學習，優化動態UAV感測器的軌跡。透過時序注意力編碼，有效管理空間探索與利用，以最小化累積重建誤差。數值實驗驗證，此生成式AI框架優於傳統內插法和深度學習基準，分別降低57.35%和88.68%的重建誤差。此軌跡規劃器顯著提升動態低空環境中的頻譜圖準確性、重建穩定性和感測器部署效率。", "audio": "audios/2505.15571v1.mp3", "timestamp": "2025-05-22T20:20:33.666103"}
{"query": "Foundation Model", "id": "2505.15147v1", "url": "http://arxiv.org/abs/2505.15147v1", "title": "From Pixels to Images: Deep Learning Advances in Remote Sensing Image Semantic Segmentation", "summary": "Remote sensing images (RSIs) capture both natural and human-induced changes\non the Earth's surface, serving as essential data for environmental monitoring,\nurban planning, and resource management. Semantic segmentation (SS) of RSIs\nenables the fine-grained interpretation of surface features, making it a\ncritical task in remote sensing analysis. With the increasing diversity and\nvolume of RSIs collected by sensors on various platforms, traditional\nprocessing methods struggle to maintain efficiency and accuracy. In response,\ndeep learning (DL) has emerged as a transformative approach, enabling\nsubstantial advances in remote sensing image semantic segmentation (RSISS) by\nautomating feature extraction and improving segmentation accuracy across\ndiverse modalities. This paper revisits the evolution of DL-based RSISS by\ncategorizing existing approaches into four stages: the early pixel-based\nmethods, the prevailing patch-based and tile-based techniques, and the emerging\nimage-based strategies enabled by foundation models. We analyze these\ndevelopments from the perspective of feature extraction and learning\nstrategies, revealing the field's progression from pixel-level to tile-level\nand from unimodal to multimodal segmentation. Furthermore, we conduct a\ncomprehensive evaluation of nearly 40 advanced techniques on a unified dataset\nto quantitatively characterize their performance and applicability. This review\noffers a holistic view of DL-based SS for RS, highlighting key advancements,\ncomparative insights, and open challenges to guide future research.", "authors": ["Quanwei Liu", "Tao Huang", "Yanni Dong", "Jiaqi Yang", "Wei Xiang"], "published_date": "2025-05-21", "title_zh": "從像素到影像：遙感影像語義分割的深度學習進展", "summary_zh": "遙感影像記錄地球表面自然與人為變遷，是環境監測、都市規劃及資源管理的重要數據。遙感影像語義分割可精細解譯地表特徵，為遙感分析的關鍵任務。面對日益增多且多樣的遙感影像，傳統方法難以兼顧效率與準確性。深度學習通過自動化特徵提取和提升分割精度，已成為遙感影像語義分割的變革性方法。本文回顧基於深度學習的遙感影像語義分割發展歷程，將現有方法分為四個階段：早期基於像素的方法、主流的基於圖像塊/瓦片的方法，以及由基礎模型驅動的新興基於圖像的方法。我們從特徵提取和學習策略角度分析這些發展，揭示該領域從像素級到瓦片級、從單模態到多模態分割的演進。此外，我們在統一數據集上對近40種先進技術進行全面評估，量化其性能和適用性。本綜述全面展示基於深度學習的遙感影像語義分割，重點介紹關鍵進展、比較性見解和未決挑戰，以指導未來研究。", "audio": "audios/2505.15147v1.mp3", "timestamp": "2025-05-22T20:20:43.447800"}
{"query": "Diffusion Model", "id": "2505.15093v1", "url": "http://arxiv.org/abs/2505.15093v1", "title": "Steering Generative Models with Experimental Data for Protein Fitness Optimization", "summary": "Protein fitness optimization involves finding a protein sequence that\nmaximizes desired quantitative properties in a combinatorially large design\nspace of possible sequences. Recent developments in steering protein generative\nmodels (e.g diffusion models, language models) offer a promising approach.\nHowever, by and large, past studies have optimized surrogate rewards and/or\nutilized large amounts of labeled data for steering, making it unclear how well\nexisting methods perform and compare to each other in real-world optimization\ncampaigns where fitness is measured by low-throughput wet-lab assays. In this\nstudy, we explore fitness optimization using small amounts (hundreds) of\nlabeled sequence-fitness pairs and comprehensively evaluate strategies such as\nclassifier guidance and posterior sampling for guiding generation from\ndifferent discrete diffusion models of protein sequences. We also demonstrate\nhow guidance can be integrated into adaptive sequence selection akin to\nThompson sampling in Bayesian optimization, showing that plug-and-play guidance\nstrategies offer advantages compared to alternatives such as reinforcement\nlearning with protein language models.", "authors": ["Jason Yang", "Wenda Chu", "Daniel Khalil", "Raul Astudillo", "Bruce J. Wittmann", "Frances H. Arnold", "Yisong Yue"], "published_date": "2025-05-21", "title_zh": "利用實驗數據導引生成模型以優化蛋白質適應性", "summary_zh": "蛋白質適應性最佳化旨在廣大的序列空間中尋找能最大化所需定量性質的蛋白質序列。引導蛋白質生成模型（如擴散模型、語言模型）是個有潛力的途徑。然而，過去研究主要優化替代獎勵或依賴大量標記數據進行引導，難以評估現有方法在真實實驗中的表現和相互比較，因真實實驗通常以低通量濕實驗測定適應性。本研究探討使用少量（數百個）標記序列-適應性配對進行適應性最佳化，並全面評估分類器引導和後驗抽樣等策略，以引導不同蛋白質序列離散擴散模型的生成。我們也展示如何將引導整合到類似貝葉斯最佳化中湯普森抽樣的自適應序列選擇中，表明隨插即用引導策略優於使用蛋白質語言模型的強化學習等替代方案。", "audio": "audios/2505.15093v1.mp3", "timestamp": "2025-05-22T20:20:56.741332"}
{"query": "AI", "id": "2505.15528v1", "url": "http://arxiv.org/abs/2505.15528v1", "title": "PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided Gaussian Splatting", "summary": "Recent years have seen substantial improvements in the ability to generate\nsynthetic 3D objects using AI. However, generating complex 3D objects, such as\nplants, remains a considerable challenge. Current generative 3D models struggle\nwith plant generation compared to general objects, limiting their usability in\nplant analysis tools, which require fine detail and accurate geometry. We\nintroduce PlantDreamer, a novel approach to 3D synthetic plant generation,\nwhich can achieve greater levels of realism for complex plant geometry and\ntextures than available text-to-3D models. To achieve this, our new generation\npipeline leverages a depth ControlNet, fine-tuned Low-Rank Adaptation and an\nadaptable Gaussian culling algorithm, which directly improve textural realism\nand geometric integrity of generated 3D plant models. Additionally,\nPlantDreamer enables both purely synthetic plant generation, by leveraging\nL-System-generated meshes, and the enhancement of real-world plant point clouds\nby converting them into 3D Gaussian Splats. We evaluate our approach by\ncomparing its outputs with state-of-the-art text-to-3D models, demonstrating\nthat PlantDreamer outperforms existing methods in producing high-fidelity\nsynthetic plants. Our results indicate that our approach not only advances\nsynthetic plant generation, but also facilitates the upgrading of legacy point\ncloud datasets, making it a valuable tool for 3D phenotyping applications.", "authors": ["Zane K J Hartley", "Lewis A G Stuart", "Andrew P French", "Michael P Pound"], "published_date": "2025-05-21", "title_zh": "PlantDreamer：藉由擴散引導高斯潑濺實現逼真3D植物模型", "summary_zh": "近年人工智慧在3D物件生成能力上顯著提升，然複雜物件如植物之生成仍具挑戰。現有3D生成模型於植物生成方面表現遜於一般物件，限制其於植物分析工具之應用，因該等工具需精細細節與精確幾何。本研究提出PlantDreamer，一種新穎之3D合成植物生成方法，相較於現有文字轉3D模型，能實現更高層次之複雜植物幾何與紋理真實感。為此，本研究之生成流程採用深度ControlNet、微調之低秩適應及可調整之高斯剔除演算法，直接改善生成之3D植物模型之紋理真實感與幾何完整性。此外，PlantDreamer能藉由L系統生成之網格實現純合成植物生成，並透過將真實世界植物點雲轉換為3D高斯潑濺，進而強化該點雲。經由與最先進文字轉3D模型之比較評估，PlantDreamer在生成高保真合成植物方面優於現有方法。研究結果顯示，本方法不僅推進合成植物生成，亦有助於升級舊有點雲資料集，使其成為3D表型分析應用之寶貴工具。", "audio": "audios/2505.15528v1.mp3", "timestamp": "2025-05-22T21:17:16.678569"}
{"query": "Foundation Model", "id": "2505.15132v1", "url": "http://arxiv.org/abs/2505.15132v1", "title": "Multicrossmodal Automated Agent for Integrating Diverse Materials Science Data", "summary": "We introduce a multicrossmodal LLM-agent framework motivated by the growing\nvolume and diversity of materials-science data ranging from high-resolution\nmicroscopy and dynamic simulation videos to tabular experiment logs and\nsprawling literature archives. While recent AI efforts have accelerated\nindividual tasks such as property prediction or image classification, they\ntypically treat each modality in isolation, leaving rich cross-modal\ncorrelations unexplored and forcing researchers to perform laborious manual\nintegration. Moreover, existing multimodal foundation models often require\nexpensive retraining or fine-tuning on domain data, and current multi-agent\nsystems in materials informatics address only narrow subtasks. To overcome\nthese obstacles, we design a coordinated team of specialized LLM agents, each\nequipped with domain-adapted prompts and plugins that project their outputs\ninto a shared embedding space. A dynamic gating mechanism then weights and\nmerges these insights, enabling unified reasoning over heterogeneous inputs\nwithout ever modifying the underlying LLM weights. We validate our approach on\nchallenging case studies and demonstrate substantial gains in retrieval\naccuracy (85%), captioning fidelity, and integrated coverage (35%) compared to\nsingle-modality and zero-shot baselines. Our work paves the way for AI digital\nresearchers capable of bridging data silos and accelerating the\nmaterials-discovery cycle. The code is available at\nhttps://github.com/adibgpt/Multicrossmodal-Autonomous-Materials-Science-Agent.", "authors": ["Adib Bazgir", "Rama chandra Praneeth Madugula", "Yuwen Zhang"], "published_date": "2025-05-21", "title_zh": "用於整合多樣材料科學數據的多模態自動化代理", "summary_zh": "本研究提出一個多重跨模態大型語言模型代理框架，旨在應對材料科學領域日益增長且多樣化的數據，包含高解析度顯微鏡影像、動態模擬影片、表格實驗日誌以及大量的文獻檔案。現有AI研究雖加速了材料性質預測或影像分類等任務，但通常孤立地處理各模態數據，忽略了豐富的跨模態關聯性，導致研究人員需耗費大量精力進行人工整合。此外，現有的多模態基礎模型往往需要在領域數據上進行昂貴的重新訓練或微調，而目前的材料資訊學多代理系統僅解決狹窄的子任務。為克服這些障礙，我們設計了一個協調的專用大型語言模型代理團隊，每個代理都配備了領域特定的提示和插件，將其輸出投影到共享嵌入空間中。然後，動態門控機制對這些見解進行加權和合併，實現對異質輸入的統一推理，且無需修改底層大型語言模型的權重。我們在具挑戰性的案例研究中驗證了該方法，並證明與單模態和零樣本基準相比，檢索準確度（85%）、字幕保真度和整合覆蓋率（35%）顯著提升。本研究為能夠橋接數據孤島並加速材料發現週期的人工智慧數位研究人員奠定了基礎。", "audio": "audios/2505.15132v1.mp3", "timestamp": "2025-05-22T21:17:24.359867"}
{"query": "Diffusion Model", "id": "2505.15077v1", "url": "http://arxiv.org/abs/2505.15077v1", "title": "Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation", "summary": "Urban forests play a key role in enhancing environmental quality and\nsupporting biodiversity in cities. Mapping and monitoring these green spaces\nare crucial for urban planning and conservation, yet accurately detecting trees\nis challenging due to complex landscapes and the variability in image\nresolution caused by different satellite sensors or UAV flight altitudes. While\ndeep learning architectures have shown promise in addressing these challenges,\ntheir effectiveness remains strongly dependent on the availability of large and\nmanually labeled datasets, which are often expensive and difficult to obtain in\nsufficient quantity. In this work, we propose a novel pipeline that integrates\ndomain adaptation with GANs and Diffusion models to enhance the quality of\nlow-resolution aerial images. Our proposed pipeline enhances low-resolution\nimagery while preserving semantic content, enabling effective tree segmentation\nwithout requiring large volumes of manually annotated data. Leveraging models\nsuch as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we\ngenerate realistic and structurally consistent synthetic samples that expand\nthe training dataset and unify scale across domains. This approach not only\nimproves the robustness of segmentation models across different acquisition\nconditions but also provides a scalable and replicable solution for remote\nsensing scenarios with scarce annotation resources. Experimental results\ndemonstrated an improvement of over 50% in IoU for low-resolution images,\nhighlighting the effectiveness of our method compared to traditional pipelines.", "authors": ["Alessandro dos Santos Ferreira", "Ana Paula Marques Ramos", "José Marcato Junior", "Wesley Nunes Gonçalves"], "published_date": "2025-05-21", "title_zh": "基於GAN與擴散模型的數據增強和分辨率提升於樹木分割", "summary_zh": "都市森林對於提升城市環境品質和支持生物多樣性至關重要。繪製和監測這些綠地對於城市規劃和保育至關重要，然而，由於複雜的景觀以及不同衛星感測器或無人機飛行高度導致的影像解析度變化，準確檢測樹木極具挑戰性。深度學習架構已展現解決這些挑戰的潛力，但其有效性仍高度依賴於大量手動標記資料集，而這些資料集通常成本高昂且難以充分獲取。本研究提出一種新穎流程，整合領域自適應與GANs和擴散模型，以提高低解析度航拍影像的品質。此流程增強了低解析度影像，同時保留了語義內容，從而實現有效的樹木分割，而無需大量手動註釋資料。利用pix2pix、Real-ESRGAN、潛在擴散和穩定擴散等模型，生成逼真且結構一致的合成樣本，以擴展訓練資料集並統一跨領域的尺度。此方法不僅提高了分割模型在不同獲取條件下的穩健性，還為註釋資源稀缺的遙感場景提供了可擴展和可複製的解決方案。實驗結果表明，低解析度影像的IoU提高了50%以上，突顯了該方法相較於傳統流程的有效性。", "audio": "audios/2505.15077v1.mp3", "timestamp": "2025-05-22T21:17:32.045043"}
{"query": "AI", "id": "2505.15519v1", "url": "http://arxiv.org/abs/2505.15519v1", "title": "Exploiting Age of Information in Network Digital Twins for AI-driven Real-Time Link Blockage Detection", "summary": "The Line-of-Sight (LoS) identification is crucial to ensure reliable\nhigh-frequency communication links, especially those vulnerable to blockages.\nNetwork Digital Twins and Artificial Intelligence are key technologies enabling\nblockage detection (LoS identification) for high-frequency wireless systems,\ne.g., 6>GHz. In this work, we enhance Network Digital Twins by incorporating\nAge of Information (AoI) metrics, a quantification of status update freshness,\nenabling reliable real-time blockage detection (LoS identification) in dynamic\nwireless environments. By integrating raytracing techniques, we automate\nlarge-scale collection and labeling of channel data, specifically tailored to\nthe evolving conditions of the environment. The introduced AoI is integrated\nwith the loss function to prioritize more recent information to fine-tune deep\nlearning models in case of performance degradation (model drift). The\neffectiveness of the proposed solution is demonstrated in realistic urban\nsimulations, highlighting the trade-off between input resolution, computational\ncost, and model performance. A resolution reduction of 4x8 from an original\nchannel sample size of (32, 1024) along the angle and subcarrier dimension\nresults in a computational speedup of 32 times. The proposed fine-tuning\nsuccessfully mitigates performance degradation while requiring only 1% of the\navailable data samples, enabling automated and fast mitigation of model drifts.", "authors": ["Michele Zhu", "Francesco Linsalata", "Silvia Mura", "Lorenzo Cazzella", "Damiano Badini", "Umberto Spagnolini"], "published_date": "2025-05-21", "title_zh": "利用網絡數位孿生中的資訊年齡實現人工智慧驅動的即時鏈路阻塞偵測", "summary_zh": "視距(LoS)識別對於確保可靠的高頻通訊鏈路至關重要，特別是易受阻擋的鏈路。網絡數位雙生與人工智慧是實現高頻無線系統（如6GHz以上頻段）阻擋偵測（LoS識別）的關鍵技術。本研究通過納入資訊年齡(AoI)指標來強化網絡數位雙生，量化狀態更新的新鮮度，從而在動態無線環境中實現可靠的即時阻擋偵測（LoS識別）。通過整合射線追蹤技術，我們自動化大規模通道數據的收集與標記，專門針對不斷變化的環境條件。引入的AoI與損失函數整合，優先處理較新的資訊，以在性能下降（模型漂移）時微調深度學習模型。在真實的城市模擬中驗證了所提出解決方案的有效性，突顯了輸入分辨率、計算成本和模型性能之間的權衡。相較於(32, 1024)的原始通道樣本尺寸，角度和子載波維度的分辨率降低4x8倍，計算速度提升32倍。所提出的微調成功緩解了性能下降，同時僅需1%的可用數據樣本，實現了自動化且快速的模型漂移緩解。", "audio": "audios/2505.15519v1.mp3", "timestamp": "2025-05-22T23:17:30.158419"}
{"query": "Foundation Model", "id": "2505.15116v1", "url": "http://arxiv.org/abs/2505.15116v1", "title": "Graph Foundation Models: A Comprehensive Survey", "summary": "Graph-structured data pervades domains such as social networks, biological\nsystems, knowledge graphs, and recommender systems. While foundation models\nhave transformed natural language processing, vision, and multimodal learning\nthrough large-scale pretraining and generalization, extending these\ncapabilities to graphs -- characterized by non-Euclidean structures and complex\nrelational semantics -- poses unique challenges and opens new opportunities. To\nthis end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose\nintelligence to structured data, enabling broad transfer across graph-centric\ntasks and domains. This survey provides a comprehensive overview of GFMs,\nunifying diverse efforts under a modular framework comprising three key\ncomponents: backbone architectures, pretraining strategies, and adaptation\nmechanisms. We categorize GFMs by their generalization scope -- universal,\ntask-specific, and domain-specific -- and review representative methods, key\ninnovations, and theoretical insights within each category. Beyond methodology,\nwe examine theoretical foundations including transferability and emergent\ncapabilities, and highlight key challenges such as structural alignment,\nheterogeneity, scalability, and evaluation. Positioned at the intersection of\ngraph learning and general-purpose AI, GFMs are poised to become foundational\ninfrastructure for open-ended reasoning over structured data. This survey\nconsolidates current progress and outlines future directions to guide research\nin this rapidly evolving field. Resources are available at\nhttps://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.", "authors": ["Zehong Wang", "Zheyuan Liu", "Tianyi Ma", "Jiazheng Li", "Zheyuan Zhang", "Xingbo Fu", "Yiyang Li", "Zhengqing Yuan", "Wei Song", "Yijun Ma", "Qingkai Zeng", "Xiusi Chen", "Jianan Zhao", "Jundong Li", "Meng Jiang", "Pietro Lio", "Nitesh Chawla", "Chuxu Zhang", "Yanfang Ye"], "published_date": "2025-05-21", "title_zh": "圖基礎模型：一份綜合綜述", "summary_zh": "圖結構數據廣泛存在於社交網路、生物系統、知識圖譜及推薦系統等領域。大型預訓練模型已革新自然語言處理、視覺及多模態學習，然將此能力擴展至具非歐幾里德結構及複雜關係語義的圖數據，面臨獨特挑戰及機遇。圖基礎模型旨在為結構化數據帶來可擴展的通用智能，實現跨圖中心任務與領域的廣泛遷移。本綜述提供圖基礎模型的全面概述，透過包含主幹架構、預訓練策略和適應機制的三模塊框架整合各項研究。依據通用性範圍（通用、任務特定及領域特定）對圖基礎模型進行分類，並回顧各類別的代表性方法、創新及理論見解。除方法論外，亦檢視包含可遷移性和湧現能力在內的理論基礎，並強調結構對齊、異質性、可擴展性及評估等關鍵挑戰。圖基礎模型位於圖學習與通用人工智慧的交叉點，有望成為結構化數據開放式推理的基礎設施。本綜述總結當前進展並概述未來方向，以指導此快速發展領域的研究。資源位於https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs。", "audio": "audios/2505.15116v1.mp3", "timestamp": "2025-05-22T23:17:42.449436"}
{"query": "Diffusion Model", "id": "2505.15064v1", "url": "http://arxiv.org/abs/2505.15064v1", "title": "Generalization Through Growth: Hidden Dynamics Controls Depth Dependence", "summary": "Recent theory has reduced the depth dependence of generalization bounds from\nexponential to polynomial and even depth-independent rates, yet these results\nremain tied to specific architectures and Euclidean inputs. We present a\nunified framework for arbitrary \\blue{pseudo-metric} spaces in which a\ndepth-\\(k\\) network is the composition of continuous hidden maps\n\\(f:\\mathcal{X}\\to \\mathcal{X}\\) and an output map \\(h:\\mathcal{X}\\to\n\\mathbb{R}\\). The resulting bound $O(\\sqrt{(\\alpha + \\log \\beta(k))/n})$\nisolates the sole depth contribution in \\(\\beta(k)\\), the word-ball growth of\nthe semigroup generated by the hidden layers. By Gromov's theorem polynomial\n(resp. exponential) growth corresponds to virtually nilpotent (resp. expanding)\ndynamics, revealing a geometric dichotomy behind existing $O(\\sqrt{k})$\n(sublinear depth) and $\\tilde{O}(1)$ (depth-independent) rates. We further\nprovide covering-number estimates showing that expanding dynamics yield an\nexponential parameter saving via compositional expressivity. Our results\ndecouple specification from implementation, offering architecture-agnostic and\ndynamical-systems-aware guarantees applicable to modern deep-learning paradigms\nsuch as test-time inference and diffusion models.", "authors": ["Sho Sonoda", "Yuka Hashimoto", "Isao Ishikawa", "Masahiro Ikeda"], "published_date": "2025-05-21", "title_zh": "透過成長實現泛化：隱藏動態控制深度依賴性", "summary_zh": "近期理論已將泛化邊界的深度依賴性從指數級降低到多項式級甚至深度無關的速率，但這些結果仍與特定架構和歐幾里得輸入相關。我們提出了一個統一框架，適用於任意偽度量空間，其中深度為\\(k\\)的網路是連續隱藏映射\\(f:\\mathcal{X}\\to \\mathcal{X}\\)和輸出映射\\(h:\\mathcal{X}\\to \\mathbb{R}\\)的組合。產生的邊界\\(O(\\sqrt{(\\alpha + \\log \\beta(k))/n})\\)將唯一的深度貢獻隔離在\\(\\beta(k)\\)中，即隱藏層生成的半群的詞球增長。藉由Gromov定理，多項式（或指數）增長對應於幾乎冪零（或擴張）動力學，揭示了現有\\(O(\\sqrt{k})\\)（亞線性深度）和\\(\\tilde{O}(1)\\)（深度無關）速率背後的幾何二分法。我們進一步提供覆蓋數估計，表明擴張動力學透過組合表達性實現了指數級參數節省。我們的結果將規範與實現分離，提供架構無關且具有動態系統感知的保證，適用於現代深度學習範例，如測試時推論和擴散模型。", "audio": "audios/2505.15064v1.mp3", "timestamp": "2025-05-22T23:17:55.865232"}
{"query": "AI", "id": "2505.15516v1", "url": "http://arxiv.org/abs/2505.15516v1", "title": "Explainable embeddings with Distance Explainer", "summary": "While eXplainable AI (XAI) has advanced significantly, few methods address\ninterpretability in embedded vector spaces where dimensions represent complex\nabstractions. We introduce Distance Explainer, a novel method for generating\nlocal, post-hoc explanations of embedded spaces in machine learning models. Our\napproach adapts saliency-based techniques from RISE to explain the distance\nbetween two embedded data points by assigning attribution values through\nselective masking and distance-ranked mask filtering. We evaluate Distance\nExplainer on cross-modal embeddings (image-image and image-caption pairs) using\nestablished XAI metrics including Faithfulness, Sensitivity/Robustness, and\nRandomization. Experiments with ImageNet and CLIP models demonstrate that our\nmethod effectively identifies features contributing to similarity or\ndissimilarity between embedded data points while maintaining high robustness\nand consistency. We also explore how parameter tuning, particularly mask\nquantity and selection strategy, affects explanation quality. This work\naddresses a critical gap in XAI research and enhances transparency and\ntrustworthiness in deep learning applications utilizing embedded spaces.", "authors": ["Christiaan Meijer", "E. G. Patrick Bos"], "published_date": "2025-05-21", "title_zh": "基於距離解釋器的可解釋嵌入", "summary_zh": "可解釋人工智慧(XAI)雖有顯著進展，但針對嵌入向量空間(其維度代表複雜抽象概念)之可解釋性的方法仍然有限。本研究提出距離解釋器(Distance Explainer)，一種新穎的後設局部解釋方法，用於解釋機器學習模型中嵌入空間的距離。本方法改編自RISE的顯著性技術，透過選擇性遮罩和距離排序遮罩過濾來分配歸因值，以解釋兩個嵌入資料點之間的距離。我們使用既定的XAI指標(包括忠實度、敏感度/穩健性和隨機化)在跨模態嵌入(圖像-圖像和圖像-標題對)上評估距離解釋器。使用ImageNet和CLIP模型的實驗表明，該方法有效地識別了有助於嵌入資料點之間相似性或相異性的特徵，同時保持高度的穩健性和一致性。我們還探討了參數調整，特別是遮罩數量和選擇策略，如何影響解釋品質。本研究解決了XAI研究中的一個關鍵缺口，並提高了利用嵌入空間的深度學習應用程式的透明度和可信度。", "audio": "audios/2505.15516v1.mp3", "timestamp": "2025-05-23T01:27:16.595148"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略引導展平層級結構", "summary_zh": "離線目標條件強化學習(GCRL)有望於大規模無獎勵軌跡數據集上預訓練通用策略，類似於電腦視覺和自然語言處理中用於訓練基礎模型的自監督目標。然而，由於稀疏獎勵和折扣的組合，將GCRL擴展到更長的時間範圍仍然具有挑戰性，這模糊了原始動作相對於遠期目標的比較優勢。分層強化學習方法在長程目標達成任務上取得了良好的實證結果，但其對模組化、特定時間尺度策略和子目標生成的依賴引入了顯著的額外複雜性，並阻礙了在高維目標空間中的擴展。本研究提出一種演算法，透過優勢加權重要性抽樣，基於子目標條件策略進行自舉，以訓練扁平(非分層)目標條件策略。此方法無需目標空間上的生成模型，這對於在大狀態空間中擴展到高維控制至關重要。我們進一步表明，現有的分層和基於自舉的方法對應於我們推導中的特定設計選擇。在一系列全面的基於狀態和像素的運動和操控基準測試中，我們的方法與最先進的離線GCRL演算法相匹配或超越，並可擴展到先前方法失敗的複雜長程任務。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-23T01:27:26.087450"}
{"query": "Diffusion Model", "id": "2505.15057v1", "url": "http://arxiv.org/abs/2505.15057v1", "title": "Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine Diffusion Models", "summary": "Magnetic Resonance Imaging (MRI) is highly susceptible to motion artifacts\ndue to the extended acquisition times required for k-space sampling. These\nartifacts can compromise diagnostic utility, particularly for dynamic imaging.\nWe propose a novel alternating minimization framework that leverages a bespoke\ndiffusion model to jointly reconstruct and correct non-rigid motion-corrupted\nk-space data. The diffusion model uses a coarse-to-fine denoising strategy to\ncapture large overall motion and reconstruct the lower frequencies of the image\nfirst, providing a better inductive bias for motion estimation than that of\nstandard diffusion models. We demonstrate the performance of our approach on\nboth real-world cine cardiac MRI datasets and complex simulated rigid and\nnon-rigid deformations, even when each motion state is undersampled by a factor\nof 64x. Additionally, our method is agnostic to sampling patterns, anatomical\nvariations, and MRI scanning protocols, as long as some low frequency\ncomponents are sampled during each motion state.", "authors": ["Frederic Wang", "Jonathan I. Tamir"], "published_date": "2025-05-21", "title_zh": "基於粗細粒度擴散模型的磁共振成像非剛性運動校正重建", "summary_zh": "磁振造影易受運動偽影影響，因其k空間採樣需時較長，此偽影損害診斷效用，尤其於動態影像。本研究提出一種新型交替最小化框架，利用客製化擴散模型聯合重建並校正非剛性運動所汙染之k空間數據。此擴散模型採用由粗到細的去噪策略，先捕捉整體大範圍運動並重建影像低頻部分，為運動估計提供優於標準擴散模型的歸納偏置。實驗結果顯示，本方法於真實心臟電影磁振造影數據集以及複雜模擬的剛性和非剛性變形中皆表現良好，即使在每個運動狀態下欠採樣64倍時亦然。此外，本方法不受採樣模式、解剖結構變異和磁振造影掃描協議的限制，惟需確保在每個運動狀態下皆採樣部分低頻成分。", "audio": "audios/2505.15057v1.mp3", "timestamp": "2025-05-23T01:27:33.657456"}
{"query": "AI", "id": "2505.17021v1", "url": "http://arxiv.org/abs/2505.17021v1", "title": "ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark", "summary": "As Large Multimodal Models (LMMs) become more capable, there is growing\ninterest in evaluating their reasoning processes alongside their final outputs.\nHowever, most benchmarks remain focused on English, overlooking languages with\nrich linguistic and cultural contexts, such as Arabic. To address this gap, we\nintroduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the\nfirst benchmark designed to evaluate step-by-step reasoning in Arabic across\nboth textual and visual modalities. ARB spans 11 diverse domains, including\nvisual reasoning, document understanding, OCR, scientific analysis, and\ncultural interpretation. It comprises 1,356 multimodal samples paired with\n5,119 human-curated reasoning steps and corresponding actions. We evaluated 12\nstate-of-the-art open- and closed-source LMMs and found persistent challenges\nin coherence, faithfulness, and cultural grounding. ARB offers a structured\nframework for diagnosing multimodal reasoning in underrepresented languages and\nmarks a critical step toward inclusive, transparent, and culturally aware AI\nsystems. We release the benchmark, rubric, and evaluation suit to support\nfuture research and reproducibility. Code available at:\nhttps://github.com/mbzuai-oryx/ARB", "authors": ["Sara Ghaboura", "Ketan More", "Wafa Alghallabi", "Omkar Thawakar", "Jorma Laaksonen", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "published_date": "2025-05-22", "title_zh": "ARB：綜合性阿拉伯語多模態推理基準", "summary_zh": "大型多模態模型能力日漸提升，對其推理過程的評估日益重要。然而，現有評測多集中於英語，忽略了如阿拉伯語等具有豐富語言及文化背景的語種。為此，我們推出綜合阿拉伯語多模態推理基準（ARB），首個旨在評估阿拉伯語文本與視覺模態逐步推理的基準。ARB涵蓋視覺推理、文檔理解、光學字元識別、科學分析和文化詮釋等11個領域，包含1356個多模態樣本，並搭配5119個人工校正的推理步驟與相應行動。對12個頂尖開放及封閉源碼大型多模態模型的評估顯示，模型在連貫性、忠實性和文化基礎方面仍面臨挑戰。ARB為診斷代表性不足語言的多模態推理提供結構化框架，是邁向具包容性、透明化和文化意識的人工智慧系統的關鍵一步。我們公開基準、評分標準與評估套件，以支持未來研究和可重複性。程式碼見：https://github.com/mbzuai-oryx/ARB", "audio": "audios/2505.17021v1.mp3", "timestamp": "2025-05-23T03:10:34.878775"}
{"query": "Foundation Model", "id": "2505.16982v1", "url": "http://arxiv.org/abs/2505.16982v1", "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "summary": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "published_date": "2025-05-22", "title_zh": "超越相關性：邁向生物醫學領域的因果大型語言模型代理", "summary_zh": "大型語言模型在生物醫學領域展現潛力，但缺乏真正的因果理解，僅依賴關聯性。本文設想因果大型語言模型代理，整合多模態數據（文本、圖像、基因組等），並執行基於干預的推理以推斷因果關係。實現此目標需克服多項挑戰：設計安全、可控的代理框架；開發嚴格的因果評估基準；整合異質數據源；以及將大型語言模型與結構化知識圖譜和正式因果推論工具結合。此類代理可釋放變革性機遇，包含透過自動假設生成和模擬加速藥物發現，並透過特定患者的因果模型實現個人化醫療。此研究旨在促進跨學科合作，橋接因果概念與基礎模型，以開發可靠的人工智慧夥伴，促進生物醫學進展。", "audio": "audios/2505.16982v1.mp3", "timestamp": "2025-05-23T03:10:43.637960"}
{"query": "Diffusion Model", "id": "2505.17013v1", "url": "http://arxiv.org/abs/2505.17013v1", "title": "When Are Concepts Erased From Diffusion Models?", "summary": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models.", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "published_date": "2025-05-22", "title_zh": "擴散模型中概念的擦除時機", "summary_zh": "概念擦除，即選擇性阻止模型生成特定概念的能力，備受關注，並湧現多種方法應對此挑戰。然而，這些方法擦除目標概念的徹底程度仍不明確。本文首先提出擴散模型中擦除機制的兩種概念模型：(一)降低生成目標概念的可能性，(二)干擾模型的內部引導機制。為徹底評估概念是否已從模型中真正擦除，本文引入一套獨立評估方法，包含對抗性攻擊、新型探測技術，以及對模型替代生成的分析。研究結果闡明了最小化副作用與維持對抗性提示的穩健性之間的緊張關係。總體而言，本文強調了對擴散模型中擦除進行全面評估的重要性。", "audio": "audios/2505.17013v1.mp3", "timestamp": "2025-05-23T03:10:50.888157"}
{"query": "AI", "id": "2505.17019v1", "url": "http://arxiv.org/abs/2505.17019v1", "title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework", "summary": "Metaphorical comprehension in images remains a critical challenge for AI\nsystems, as existing models struggle to grasp the nuanced cultural, emotional,\nand contextual implications embedded in visual content. While multimodal large\nlanguage models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they\nstruggle with a fundamental limitation on image implication tasks: contextual\ngaps that obscure the relationships between different visual elements and their\nabstract meanings. Inspired by the human cognitive process, we propose Let\nAndroids Dream (LAD), a novel framework for image implication understanding and\nreasoning. LAD addresses contextual missing through the three-stage framework:\n(1) Perception: converting visual information into rich and multi-level textual\nrepresentations, (2) Search: iteratively searching and integrating cross-domain\nknowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment\nimage implication via explicit reasoning. Our framework with the lightweight\nGPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English\nimage implication benchmark and a huge improvement on Chinese benchmark,\nperforming comparable with the GPT-4o model on Multiple-Choice Question (MCQ)\nand outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work\nprovides new insights into how AI can more effectively interpret image\nimplications, advancing the field of vision-language reasoning and human-AI\ninteraction. Our project is publicly available at\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.", "authors": ["Chenhao Zhang", "Yazhe Niu"], "published_date": "2025-05-22", "title_zh": "安卓是否夢見電子羊：類人圖像意涵理解與推理框架", "summary_zh": "圖像隱喻理解對人工智慧系統構成重大挑戰，現有模型難以掌握視覺內容中細微的文化、情感與情境意涵。多模態大型語言模型雖擅長基本視覺問答，但在圖像意涵任務中受限於情境缺口，難以辨識視覺元素間的關係及其抽象意義。受人類認知啟發，我們提出Let Androids Dream (LAD)框架，用於圖像意涵理解與推理。LAD透過三階段解決情境缺失：(1)感知：將視覺資訊轉化為豐富的多層次文本表示；(2)搜尋：迭代搜尋並整合跨領域知識以消除歧義；(3)推理：透過顯式推理產生情境對齊的圖像意涵。搭載輕量級GPT-4o-mini模型，本框架在英語圖像意涵基準測試中達到最佳效能，並在中文基準測試中實現顯著提升，在選擇題上與GPT-4o模型相當，在開放式問題上超越36.7%。本研究為人工智慧如何更有效地解讀圖像意涵提供了新見解，推進了視覺語言推理與人機互動領域。專案已公開。", "audio": "audios/2505.17019v1.mp3", "timestamp": "2025-05-23T04:22:00.731004"}
{"query": "Foundation Model", "id": "2505.16941v1", "url": "http://arxiv.org/abs/2505.16941v1", "title": "FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records", "summary": "Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models.", "authors": ["Chao Pang", "Vincent Jeanselme", "Young Sang Choi", "Xinzhuo Jiang", "Zilin Jing", "Aparajita Kashyap", "Yuta Kobayashi", "Yanwei Li", "Florent Pollet", "Karthik Natarajan", "Shalmali Joshi"], "published_date": "2025-05-22", "title_zh": "FoMoH：結構化電子病歷臨床意義基礎模型評估", "summary_zh": "基於其獨立於下游任務提取有意義表徵的能力，基石模型在醫療保健領域展現出巨大潛力。此特性已促成結構化電子病歷數據訓練的多項臨床應用達到頂尖效能，即使在標記數據有限的環境中，這也是醫療保健領域常見的挑戰。然而，由於缺乏對全面且有意義任務的理想要求以及充分多樣化的評估以描述相較於傳統監督式學習的優勢，對於這些模型在臨床上的實用性鮮少共識。為了解決此差距，我們提出一系列具有臨床意義的任務，涵蓋患者預後、急慢性疾病的早期預測，並包含穩健評估的理想要求。我們使用來自紐約市哥倫比亞大學歐文醫學中心 (CUMC) 的五百萬患者的電子病歷數據，在 14 項臨床相關任務中評估了最先進的基石模型。我們測量了總體準確性、校準和亞群體效能，以揭示基於預訓練、標記化和數據表示策略選擇的權衡。本研究旨在推進結構化電子病歷基石模型的實證評估，並指導未來醫療保健基石模型的開發。", "audio": "audios/2505.16941v1.mp3", "timestamp": "2025-05-23T04:22:07.646757"}
{"query": "Diffusion Model", "id": "2505.17004v1", "url": "http://arxiv.org/abs/2505.17004v1", "title": "Guided Diffusion Sampling on Function Spaces with Applications to PDEs", "summary": "We propose a general framework for conditional sampling in PDE-based inverse\nproblems, targeting the recovery of whole solutions from extremely sparse or\nnoisy measurements. This is accomplished by a function-space diffusion model\nand plug-and-play guidance for conditioning. Our method first trains an\nunconditional discretization-agnostic denoising model using neural operator\narchitectures. At inference, we refine the samples to satisfy sparse\nobservation data via a gradient-based guidance mechanism. Through rigorous\nmathematical analysis, we extend Tweedie's formula to infinite-dimensional\nHilbert spaces, providing the theoretical foundation for our posterior sampling\napproach. Our method (FunDPS) accurately captures posterior distributions in\nfunction spaces under minimal supervision and severe data scarcity. Across five\nPDE tasks with only 3% observation, our method achieves an average 32% accuracy\nimprovement over state-of-the-art fixed-resolution diffusion baselines while\nreducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning\nensures strong cross-resolution generalizability. To the best of our knowledge,\nthis is the first diffusion-based framework to operate independently of\ndiscretization, offering a practical and flexible solution for forward and\ninverse problems in the context of PDEs. Code is available at\nhttps://github.com/neuraloperator/FunDPS", "authors": ["Jiachen Yao", "Abbas Mammadov", "Julius Berner", "Gavin Kerrigan", "Jong Chul Ye", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "published_date": "2025-05-22", "title_zh": "函數空間上的導引擴散採樣及其在偏微分方程中的應用", "summary_zh": "本研究提出一個基於偏微分方程逆問題的條件採樣通用框架，旨在從極度稀疏或含噪量測中復原完整解。該方法利用函數空間擴散模型與隨插即用引導實現條件化。首先，採用神經算子架構訓練一個無條件、與離散化無關的去噪模型。在推論階段，透過基於梯度的引導機制精煉樣本，以滿足稀疏觀測數據。透過嚴謹的數學分析，將Tweedie公式擴展到無限維希爾伯特空間，為後驗採樣方法提供理論基礎。此方法(FunDPS)能在極少的監督和嚴重的數據匱乏情況下，準確捕獲函數空間中的後驗分佈。在五項偏微分方程任務中，僅使用3%的觀測數據，此方法比最先進的固定解析度擴散模型平均提高了32%的準確性，同時減少了4倍的採樣步驟。此外，多解析度微調確保了強大的跨解析度泛化能力。據我們所知，這是第一個獨立於離散化運作的基於擴散的框架，為偏微分方程背景下的正向和逆向問題提供了一種實用且靈活的解決方案。代碼位於[https://github.com/neuraloperator/FunDPS](https://github.com/neuraloperator/FunDPS)。", "audio": "audios/2505.17004v1.mp3", "timestamp": "2025-05-23T04:22:15.738654"}
{"query": "AI", "id": "2505.16997v1", "url": "http://arxiv.org/abs/2505.16997v1", "title": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs", "summary": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems.", "authors": ["Rui Ye", "Xiangrui Liu", "Qimin Wu", "Xianghe Pang", "Zhenfei Yin", "Lei Bai", "Siheng Chen"], "published_date": "2025-05-22", "title_zh": "X-MAS：邁向構建具異質大型語言模型的多代理系統", "summary_zh": "基於大型語言模型的多代理系統透過促進多個專業代理之間的協作，擴展了單一大型語言模型的能力。 然而，現有框架大多依賴單一大型語言模型驅動所有代理，限制了系統智慧。 本研究探索異質大型語言模型驅動的多代理系統範式，其中代理由不同的大型語言模型提供支持，從而提升系統潛力。 我們引入了 X-MAS-Bench，一個用於評估不同大型語言模型在不同領域和多代理系統相關功能表現的綜合測試平台。 我們評估了跨越 5 個領域（包含 21 個測試集）和 5 個功能的 27 個大型語言模型，進行了超過 170 萬次評估，以識別每個領域-功能組合的最佳模型選擇。 研究表明，從同質到異質大型語言模型驅動的多代理系統的轉變可以顯著提高系統性能，且無需結構重新設計。 在僅包含聊天機器人的多代理系統情境中，異質配置在 MATH 數據集上產生了高達 8.4% 的性能提升。 在混合聊天機器人-推理器的情境中，異質多代理系統在 AIME 數據集上實現了顯著的 47% 性能提升。 我們的結果強調了異質大型語言模型在多代理系統中的變革潛力，為推進可擴展的協作式人工智慧系統提供了一個有前景的途徑。", "audio": "audios/2505.16997v1.mp3", "timestamp": "2025-05-23T07:18:26.794247"}
{"query": "Foundation Model", "id": "2505.16832v1", "url": "http://arxiv.org/abs/2505.16832v1", "title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization", "summary": "While foundation models (FMs), such as diffusion models and large\nvision-language models (LVLMs), have been widely applied in educational\ncontexts, their ability to generate pedagogically effective visual explanations\nremains limited. Most existing approaches focus primarily on textual reasoning,\noverlooking the critical role of structured and interpretable visualizations in\nsupporting conceptual understanding. To better assess the visual reasoning\ncapabilities of FMs in educational settings, we introduce EduVisBench, a\nmulti-domain, multi-level benchmark. EduVisBench features diverse STEM problem\nsets requiring visually grounded solutions, along with a fine-grained\nevaluation rubric informed by pedagogical theory. Our empirical analysis\nreveals that existing models frequently struggle with the inherent challenge of\ndecomposing complex reasoning and translating it into visual representations\naligned with human cognitive processes. To address these limitations, we\npropose EduVisAgent, a multi-agent collaborative framework that coordinates\nspecialized agents for instructional planning, reasoning decomposition,\nmetacognitive prompting, and visualization design. Experimental results show\nthat EduVisAgent substantially outperforms all baselines, achieving a 40.2%\nimprovement and delivering more educationally aligned visualizations.\nEduVisBench and EduVisAgent are available at\nhttps://github.com/aiming-lab/EduVisBench and\nhttps://github.com/aiming-lab/EduVisAgent.", "authors": ["Haonian Ji", "Shi Qiu", "Siyang Xin", "Siwei Han", "Zhaorun Chen", "Hongyi Wang", "Dake Zhang", "Huaxiu Yao"], "published_date": "2025-05-22", "title_zh": "從EduVisBench到EduVisAgent：教學視覺化之基準測試與多代理人框架", "summary_zh": "基礎模型如擴散模型與大型視覺語言模型已廣泛應用於教育，然其產生具教學效益之視覺解釋能力仍受限。現有方法多側重文字推理，忽略結構化且可解釋之視覺化於概念理解之關鍵作用。為評估基礎模型於教育情境之視覺推理能力，本研究提出EduVisBench，一多領域、多層級基準測試。EduVisBench包含需視覺化解答之STEM問題集，及基於教學理論之細緻評分標準。實證分析顯示，現有模型常難以分解複雜推理，並將其轉譯為符合人類認知歷程之視覺表徵。為解決此問題，本研究提出EduVisAgent，一多代理人協作框架，協調專業代理人進行教學規劃、推理分解、後設認知提示及視覺化設計。實驗結果顯示，EduVisAgent顯著優於所有基準模型，提升40.2%，並產出更符合教育目標之視覺化。EduVisBench與EduVisAgent已於https://github.com/aiming-lab/EduVisBench及https://github.com/aiming-lab/EduVisAgent公開。", "audio": "audios/2505.16832v1.mp3", "timestamp": "2025-05-23T07:18:36.135433"}
{"query": "Diffusion Model", "id": "2505.16980v1", "url": "http://arxiv.org/abs/2505.16980v1", "title": "Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose Interaction", "summary": "Video virtual try-on aims to seamlessly dress a subject in a video with a\nspecific garment. The primary challenge involves preserving the visual\nauthenticity of the garment while dynamically adapting to the pose and physique\nof the subject. While existing methods have predominantly focused on\nimage-based virtual try-on, extending these techniques directly to videos often\nresults in temporal inconsistencies. Most current video virtual try-on\napproaches alleviate this challenge by incorporating temporal modules, yet\nstill overlook the critical spatiotemporal pose interactions between human and\ngarment. Effective pose interactions in videos should not only consider spatial\nalignment between human and garment poses in each frame but also account for\nthe temporal dynamics of human poses throughout the entire video. With such\nmotivation, we propose a new framework, namely Dynamic Pose Interaction\nDiffusion Models (DPIDM), to leverage diffusion models to delve into dynamic\npose interactions for video virtual try-on. Technically, DPIDM introduces a\nskeleton-based pose adapter to integrate synchronized human and garment poses\ninto the denoising network. A hierarchical attention module is then exquisitely\ndesigned to model intra-frame human-garment pose interactions and long-term\nhuman pose dynamics across frames through pose-aware spatial and temporal\nattention mechanisms. Moreover, DPIDM capitalizes on a temporal regularized\nattention loss between consecutive frames to enhance temporal consistency.\nExtensive experiments conducted on VITON-HD, VVT and ViViD datasets demonstrate\nthe superiority of our DPIDM against the baseline methods. Notably, DPIDM\nachieves VFID score of 0.506 on VVT dataset, leading to 60.5% improvement over\nthe state-of-the-art GPD-VVTO approach.", "authors": ["Dong Li", "Wenqi Zhong", "Wei Yu", "Yingwei Pan", "Dingwen Zhang", "Ting Yao", "Junwei Han", "Tao Mei"], "published_date": "2025-05-22", "title_zh": "藉由動態姿態互動實現時序一致的影片虛擬試穿", "summary_zh": "影片虛擬試穿旨在將特定服裝無縫套用於影片中的人物。主要挑戰在於保持服裝視覺真實性的同時，動態適應人物的姿勢和體態。現有方法多側重於圖像虛擬試穿，直接將其延伸至影片常導致時間不一致。目前影片虛擬試穿方法雖加入時間模組以緩解此問題，但忽略了人體與服裝間的時空姿勢互動。有效的姿勢互動應考量每幀中人體與服裝姿勢的空間對齊，以及整段影片中人體姿勢的時間動態。為此，我們提出動態姿勢互動擴散模型（DPIDM），利用擴散模型深入研究影片虛擬試穿的動態姿勢互動。DPIDM採用基於骨架的姿勢適配器，將同步的人體與服裝姿勢整合至去噪網路。精心設計的分層注意力模組通過姿勢感知的空間和時間注意力機制，對幀內人體-服裝姿勢互動以及跨幀的長期人體姿勢動態進行建模。此外，DPIDM利用連續幀之間的時間正規化注意力損失來增強時間一致性。在VITON-HD、VVT和ViViD數據集上的大量實驗表明，我們的DPIDM優於基線方法。值得注意的是，DPIDM在VVT數據集上實現了0.506的VFID評分，相較於最先進的GPD-VVTO方法提升了60.5%。", "audio": "audios/2505.16980v1.mp3", "timestamp": "2025-05-23T07:18:44.910411"}
{"query": "AI", "id": "2505.16977v1", "url": "http://arxiv.org/abs/2505.16977v1", "title": "Incorporating Visual Correspondence into Diffusion Model for Virtual Try-On", "summary": "Diffusion models have shown preliminary success in virtual try-on (VTON)\ntask. The typical dual-branch architecture comprises two UNets for implicit\ngarment deformation and synthesized image generation respectively, and has\nemerged as the recipe for VTON task. Nevertheless, the problem remains\nchallenging to preserve the shape and every detail of the given garment due to\nthe intrinsic stochasticity of diffusion model. To alleviate this issue, we\nnovelly propose to explicitly capitalize on visual correspondence as the prior\nto tame diffusion process instead of simply feeding the whole garment into UNet\nas the appearance reference. Specifically, we interpret the fine-grained\nappearance and texture details as a set of structured semantic points, and\nmatch the semantic points rooted in garment to the ones over target person\nthrough local flow warping. Such 2D points are then augmented into 3D-aware\ncues with depth/normal map of target person. The correspondence mimics the way\nof putting clothing on human body and the 3D-aware cues act as semantic point\nmatching to supervise diffusion model training. A point-focused diffusion loss\nis further devised to fully take the advantage of semantic point matching.\nExtensive experiments demonstrate strong garment detail preservation of our\napproach, evidenced by state-of-the-art VTON performances on both VITON-HD and\nDressCode datasets. Code is publicly available at:\nhttps://github.com/HiDream-ai/SPM-Diff.", "authors": ["Siqi Wan", "Jingwen Chen", "Yingwei Pan", "Ting Yao", "Tao Mei"], "published_date": "2025-05-22", "title_zh": "將視覺對應融入擴散模型以用於虛擬試穿", "summary_zh": "擴散模型在虛擬試穿任務中初顯成效。典型的雙分支架構包含兩個UNet，分別用於隱式服裝變形和合成圖像生成，已成為虛擬試穿的常用方法。然而，由於擴散模型固有的隨機性，保持服裝的形狀和所有細節仍然具有挑戰性。為了解決這個問題，我們創新性地提出明確利用視覺對應關係作為先驗來控制擴散過程，而不是簡單地將整個服裝作為外觀參考輸入UNet。具體來說，我們將細粒度的外觀和紋理細節解釋為一組結構化的語義點，並通過局部流動扭曲將服裝上的語義點與目標人物上的語義點進行匹配。然後，這些2D點通過目標人物的深度/法線貼圖擴增為具有3D感知的提示。這種對應關係模擬了將衣服穿在人體上的方式，而3D感知提示則充當語義點匹配，以監督擴散模型的訓練。此外，我們設計了一種以點為中心的擴散損失，以充分利用語義點匹配的優勢。大量實驗表明，我們的研究方法在服裝細節保留方面表現出色，在VITON-HD和DressCode數據集上均實現了最先進的虛擬試穿性能。程式碼已公開：https://github.com/HiDream-ai/SPM-Diff。", "audio": "audios/2505.16977v1.mp3", "timestamp": "2025-05-23T08:25:01.166128"}
{"query": "Foundation Model", "id": "2505.16793v1", "url": "http://arxiv.org/abs/2505.16793v1", "title": "REOBench: Benchmarking Robustness of Earth Observation Foundation Models", "summary": "Earth observation foundation models have shown strong generalization across\nmultiple Earth observation tasks, but their robustness under real-world\nperturbations remains underexplored. To bridge this gap, we introduce REOBench,\nthe first comprehensive benchmark for evaluating the robustness of Earth\nobservation foundation models across six tasks and twelve types of image\ncorruptions, including both appearance-based and geometric perturbations. To\nensure realistic and fine-grained evaluation, our benchmark focuses on\nhigh-resolution optical remote sensing images, which are widely used in\ncritical applications such as urban planning and disaster response. We conduct\na systematic evaluation of a broad range of models trained using masked image\nmodeling, contrastive learning, and vision-language pre-training paradigms. Our\nresults reveal that (1) existing Earth observation foundation models experience\nsignificant performance degradation when exposed to input corruptions. (2) The\nseverity of degradation varies across tasks, model architectures, backbone\nsizes, and types of corruption, with performance drop varying from less than 1%\nto over 20%. (3) Vision-language models show enhanced robustness, particularly\nin multimodal tasks. REOBench underscores the vulnerability of current Earth\nobservation foundation models to real-world corruptions and provides actionable\ninsights for developing more robust and reliable models.", "authors": ["Xiang Li", "Yong Tao", "Siyuan Zhang", "Siwei Liu", "Zhitong Xiong", "Chunbo Luo", "Lu Liu", "Mykola Pechenizkiy", "Xiao Xiang Zhu", "Tianjin Huang"], "published_date": "2025-05-22", "title_zh": "REOBench：地球觀測基礎模型穩健性基準測試", "summary_zh": "地球觀測基礎模型在多項任務中展現了良好的泛化能力，但其在真實擾動下的穩健性仍待探索。本研究提出REOBench，首個全面評估地球觀測基礎模型穩健性的基準，包含六項任務和十二種影像失真，涵蓋外觀和幾何擾動。基準專注於高解析度光學遙感影像，適用於都市規劃和災害應對等關鍵應用，以確保真實且精細的評估。針對多種採用遮罩影像建模、對比學習和視覺語言預訓練的模型進行了系統性評估。結果顯示：(1)現有模型在輸入失真下性能顯著下降；(2)性能下降程度因任務、模型架構、骨幹大小和失真類型而異，降幅從小於1%到超過20%；(3)視覺語言模型展現了更強的穩健性，尤其是在多模態任務中。REOBench強調了當前模型在真實失真下的脆弱性，並為開發更穩健可靠的模型提供了可行的見解。", "audio": "audios/2505.16793v1.mp3", "timestamp": "2025-05-23T08:25:08.417021"}
{"query": "Diffusion Model", "id": "2505.16976v1", "url": "http://arxiv.org/abs/2505.16976v1", "title": "Creatively Upscaling Images with Global-Regional Priors", "summary": "Contemporary diffusion models show remarkable capability in text-to-image\ngeneration, while still being limited to restricted resolutions (e.g., 1,024 X\n1,024). Recent advances enable tuning-free higher-resolution image generation\nby recycling pre-trained diffusion models and extending them via regional\ndenoising or dilated sampling/convolutions. However, these models struggle to\nsimultaneously preserve global semantic structure and produce creative regional\ndetails in higher-resolution images. To address this, we present C-Upscale, a\nnew recipe of tuning-free image upscaling that pivots on global-regional priors\nderived from given global prompt and estimated regional prompts via Multimodal\nLLM. Technically, the low-frequency component of low-resolution image is\nrecognized as global structure prior to encourage global semantic consistency\nin high-resolution generation. Next, we perform regional attention control to\nscreen cross-attention between global prompt and each region during regional\ndenoising, leading to regional attention prior that alleviates object\nrepetition issue. The estimated regional prompts containing rich descriptive\ndetails further act as regional semantic prior to fuel the creativity of\nregional detail generation. Both quantitative and qualitative evaluations\ndemonstrate that our C-Upscale manages to generate ultra-high-resolution images\n(e.g., 4,096 X 4,096 and 8,192 X 8,192) with higher visual fidelity and more\ncreative regional details.", "authors": ["Yurui Qian", "Qi Cai", "Yingwei Pan", "Ting Yao", "Tao Mei"], "published_date": "2025-05-22", "title_zh": "利用全局-局部先驗進行創造性圖像放大", "summary_zh": "現有擴散模型在文字生成圖像方面表現出色，但解析度受限。近期研究透過重用預訓練模型並擴展區域降噪或擴張採樣/卷積，實現免調優的高解析度圖像生成。然而，這些模型難以同時在高解析度圖像中保持全局語義結構並生成富含創意的區域細節。為了解決此問題，我們提出 C-Upscale，一種免調優的圖像放大新方法，基於全局提示詞和多模態大型語言模型估算的區域提示詞，獲取全局-區域先驗知識。技術上，低解析度圖像的低頻成分被視為全局結構先驗，以促進高解析度生成中的全局語義一致性。接著，我們執行區域注意力控制，篩選全局提示詞和每個區域在區域降噪期間的交叉注意力，產生區域注意力先驗，減輕物體重複問題。包含豐富描述性細節的估算區域提示詞，進一步作為區域語義先驗，激發區域細節生成的創造力。定量和定性評估表明，我們的 C-Upscale 能夠生成具有更高視覺逼真度和更富創意區域細節的超高解析度圖像（例如，4,096 X 4,096 和 8,192 X 8,192）。", "audio": "audios/2505.16976v1.mp3", "timestamp": "2025-05-23T08:25:16.187584"}
{"query": "AI", "id": "2505.16975v1", "url": "http://arxiv.org/abs/2505.16975v1", "title": "SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development", "summary": "Large Language Models (LLMs) have shown strong capability in diverse software\nengineering tasks, e.g. code completion, bug fixing, and document generation.\nHowever, feature-driven development (FDD), a highly prevalent real-world task\nthat involves developing new functionalities for large, existing codebases,\nremains underexplored. We therefore introduce SWE-Dev, the first large-scale\ndataset (with 14,000 training and 500 test samples) designed to evaluate and\ntrain autonomous coding systems on real-world feature development tasks. To\nensure verifiable and diverse training, SWE-Dev uniquely provides all instances\nwith a runnable environment and its developer-authored executable unit tests.\nThis collection not only provides high-quality data for Supervised Fine-Tuning\n(SFT), but also enables Reinforcement Learning (RL) by delivering accurate\nreward signals from executable unit tests. Our extensive evaluations on\nSWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent\nSystems (MAS), reveal that FDD is a profoundly challenging frontier for current\nAI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test\nsplit). Crucially, we demonstrate that SWE-Dev serves as an effective platform\nfor model improvement: fine-tuning on training set enabled a 7B model\ncomparable to GPT-4o on \\textit{hard} split, underscoring the value of its\nhigh-quality training data. Code is available here\n\\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}.", "authors": ["Yaxin Du", "Yuzhu Cai", "Yifan Zhou", "Cheng Wang", "Yu Qian", "Xianghe Pang", "Qian Liu", "Yue Hu", "Siheng Chen"], "published_date": "2025-05-22", "title_zh": "SWE-Dev：評估與訓練自主式特徵驅動軟體開發", "summary_zh": "大型語言模型在程式碼補全、錯誤修復和文件生成等軟體工程任務中展現強大能力，但對大型現有程式碼庫進行的特性驅動開發（FDD）仍未充分探索。本研究推出首個大規模資料集SWE-Dev，包含14,000個訓練樣本和500個測試樣本，旨在評估並訓練自主程式碼系統，以應對真實世界的特性開發任務。SWE-Dev獨特之處在於，它為所有實例提供可執行環境及開發者編寫的可執行單元測試，確保可驗證且多樣化的訓練。此資料集不僅為監督式微調（SFT）提供高品質資料，還能透過單元測試提供準確的獎勵信號，促進強化學習（RL）。對SWE-Dev進行的廣泛評估，涵蓋17個聊天機器人語言模型、10個推理模型和10個多代理系統（MAS），揭示了FDD對當前人工智慧而言是一項極具挑戰性的前沿任務（例如，Claude-3.7-Sonnet在困難測試集上僅達到22.45%的Pass@3）。此外，研究表明SWE-Dev可作為模型改進的有效平台，透過在訓練集上進行微調，使一個70億參數模型在困難測試集上達到與GPT-4o相當的性能，突顯了其高品質訓練資料的價值。程式碼可在\\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}取得。", "audio": "audios/2505.16975v1.mp3", "timestamp": "2025-05-23T09:19:59.885496"}
{"query": "Foundation Model", "id": "2505.16725v1", "url": "http://arxiv.org/abs/2505.16725v1", "title": "Masked Conditioning for Deep Generative Models", "summary": "Datasets in engineering domains are often small, sparsely labeled, and\ncontain numerical as well as categorical conditions. Additionally.\ncomputational resources are typically limited in practical applications which\nhinders the adoption of generative models for engineering tasks. We introduce a\nnovel masked-conditioning approach, that enables generative models to work with\nsparse, mixed-type data. We mask conditions during training to simulate sparse\nconditions at inference time. For this purpose, we explore the use of various\nsparsity schedules that show different strengths and weaknesses. In addition,\nwe introduce a flexible embedding that deals with categorical as well as\nnumerical conditions. We integrate our method into an efficient variational\nautoencoder as well as a latent diffusion model and demonstrate the\napplicability of our approach on two engineering-related datasets of 2D point\nclouds and images. Finally, we show that small models trained on limited data\ncan be coupled with large pretrained foundation models to improve generation\nquality while retaining the controllability induced by our conditioning scheme.", "authors": ["Phillip Mueller", "Jannik Wiese", "Sebastian Mueller", "Lars Mikelsons"], "published_date": "2025-05-22", "title_zh": "深度生成模型的遮蔽條件化", "summary_zh": "工程領域數據集通常規模小、標記稀疏，且包含數值和類別條件。實際應用中計算資源有限，阻礙了生成模型在工程任務中的應用。本研究提出一種新的遮蔽條件方法，使生成模型能處理稀疏、混合型數據。訓練期間遮蔽條件，以模擬推論時的稀疏性。我們探索了多種稀疏性排程，展現不同的優缺點。此外，我們引入一種靈活的嵌入方式，處理類別和數值條件。我們將此方法整合到高效變分自編碼器和潛在擴散模型中，並在兩個二維點雲和圖像的工程數據集上驗證其適用性。最後，我們證明在有限數據上訓練的小型模型可與大型預訓練基礎模型結合，在保持條件控制性的同時，提升生成品質。", "audio": "audios/2505.16725v1.mp3", "timestamp": "2025-05-23T09:20:06.171793"}
{"query": "Diffusion Model", "id": "2505.16959v1", "url": "http://arxiv.org/abs/2505.16959v1", "title": "Bigger Isn't Always Memorizing: Early Stopping Overparameterized Diffusion Models", "summary": "Diffusion probabilistic models have become a cornerstone of modern generative\nAI, yet the mechanisms underlying their generalization remain poorly\nunderstood. In fact, if these models were perfectly minimizing their training\nloss, they would just generate data belonging to their training set, i.e.,\nmemorize, as empirically found in the overparameterized regime. We revisit this\nview by showing that, in highly overparameterized diffusion models,\ngeneralization in natural data domains is progressively achieved during\ntraining before the onset of memorization. Our results, ranging from image to\nlanguage diffusion models, systematically support the empirical law that\nmemorization time is proportional to the dataset size. Generalization vs.\nmemorization is then best understood as a competition between time scales. We\nshow that this phenomenology is recovered in diffusion models learning a simple\nprobabilistic context-free grammar with random rules, where generalization\ncorresponds to the hierarchical acquisition of deeper grammar rules as training\ntime grows, and the generalization cost of early stopping can be characterized.\nWe summarize these results in a phase diagram. Overall, our results support\nthat a principled early-stopping criterion - scaling with dataset size - can\neffectively optimize generalization while avoiding memorization, with direct\nimplications for hyperparameter transfer and privacy-sensitive applications.", "authors": ["Alessandro Favero", "Antonio Sclocchi", "Matthieu Wyart"], "published_date": "2025-05-22", "title_zh": "更大未必更善記：過參數擴散模型的提前停止", "summary_zh": "擴散機率模型已成現代生成式AI基石，但其泛化機制仍不甚明瞭。若模型完美最小化訓練損失，將僅生成訓練集資料，即產生記憶。本研究重新審視此觀點，證明在高度過參數化擴散模型中，自然數據領域的泛化在訓練期間逐步實現，早於記憶發生。從圖像到語言擴散模型的研究結果一致支持：記憶時間與數據集大小成正比。泛化與記憶可視為時間尺度上的競爭。在學習具隨機規則的簡單機率上下文無關文法的擴散模型中，也觀察到此現象，泛化對應於訓練時間增長時，更深層文法規則的階層式獲取，且可量化提前停止的泛化成本。總結結果於相圖中。結果表明，基於數據集大小的提前停止準則，能有效優化泛化，同時避免記憶，對超參數遷移及隱私敏感應用具直接影響。", "audio": "audios/2505.16959v1.mp3", "timestamp": "2025-05-23T09:20:13.279299"}
{"query": "AI", "id": "2505.16964v1", "url": "http://arxiv.org/abs/2505.16964v1", "title": "MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning", "summary": "Existing medical VQA benchmarks mostly focus on single-image analysis, yet\nclinicians almost always compare a series of images before reaching a\ndiagnosis. To better approximate this workflow, we introduce MedFrameQA -- the\nfirst benchmark that explicitly evaluates multi-image reasoning in medical VQA.\nTo build MedFrameQA both at scale and in high-quality, we develop 1) an\nautomated pipeline that extracts temporally coherent frames from medical videos\nand constructs VQA items whose content evolves logically across images, and 2)\na multiple-stage filtering strategy, including model-based and manual review,\nto preserve data clarity, difficulty, and medical relevance. The resulting\ndataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in\n3,420 videos), covering nine human body systems and 43 organs; every question\nis accompanied by two to five images. We comprehensively benchmark ten advanced\nMultimodal LLMs -- both proprietary and open source, with and without explicit\nreasoning modules -- on MedFrameQA. The evaluation challengingly reveals that\nall models perform poorly, with most accuracies below 50%, and accuracy\nfluctuates as the number of images per question increases. Error analysis\nfurther shows that models frequently ignore salient findings, mis-aggregate\nevidence across images, and propagate early mistakes through their reasoning\nchains; results also vary substantially across body systems, organs, and\nmodalities. We hope this work can catalyze research on clinically grounded,\nmulti-image reasoning and accelerate progress toward more capable diagnostic AI\nsystems.", "authors": ["Suhao Yu", "Haojin Wang", "Juncheng Wu", "Cihang Xie", "Yuyin Zhou"], "published_date": "2025-05-22", "title_zh": "MedFrameQA：用於臨床推理的多影像醫學VQA基準", "summary_zh": "現有醫療VQA基準測試多著重於單一影像分析，然臨床醫師診斷前常比較系列影像。為更貼近此流程，我們提出MedFrameQA，首個評估醫療VQA中多影像推理的基準。為大規模且高品質地建構MedFrameQA，我們開發1)自動化流程，從醫療影片提取時間上連貫的幀，並建構內容邏輯演進的VQA項目，以及2)多階段過濾策略，包含基於模型的審查和人工審查，以保持數據清晰度、難度和醫療相關性。最終數據集包含2,851個VQA配對（來自3,420個影片中的9,237個高品質幀），涵蓋九個人體系統和43個器官；每個問題配有兩到五個影像。我們在MedFrameQA上全面評測了十個先進的多模態LLM，包括專有和開源，以及具備和不具備顯式推理模組的模型。評估顯示所有模型表現不佳，多數準確度低於50%，且準確度隨每個問題的影像數量增加而波動。錯誤分析表明，模型常忽略顯著發現、錯誤整合跨影像證據，並將早期錯誤傳播到推理鏈中；結果也因身體系統、器官和模態而異。期望此研究可促進臨床多影像推理研究，並加速更強大的診斷AI系統發展。", "audio": "audios/2505.16964v1.mp3", "timestamp": "2025-05-23T10:19:45.991998"}
{"query": "Foundation Model", "id": "2505.16724v1", "url": "http://arxiv.org/abs/2505.16724v1", "title": "Advancing Brainwave Modeling with a Codebook-Based Foundation Model", "summary": "Recent advances in large-scale pre-trained Electroencephalogram (EEG) models\nhave shown great promise, driving progress in Brain-Computer Interfaces (BCIs)\nand healthcare applications. However, despite their success, many existing\npre-trained models have struggled to fully capture the rich information content\nof neural oscillations, a limitation that fundamentally constrains their\nperformance and generalizability across diverse BCI tasks. This limitation is\nfrequently rooted in suboptimal architectural design choices which constrain\ntheir representational capacity. In this work, we introduce LaBraM++, an\nenhanced Large Brainwave Foundation Model (LBM) that incorporates principled\nimprovements grounded in robust signal processing foundations. LaBraM++\ndemonstrates substantial gains across a variety of tasks, consistently\noutperforming its originally-based architecture and achieving competitive\nresults when compared to other open-source LBMs. Its superior performance and\ntraining efficiency highlight its potential as a strong foundation for future\nadvancements in LBMs.", "authors": ["Konstantinos Barmpas", "Na Lee", "Yannis Panagakis", "Dimitrios A. Adamos", "Nikolaos Laskaris", "Stefanos Zafeiriou"], "published_date": "2025-05-22", "title_zh": "基於碼本的基礎模型推進腦波建模", "summary_zh": "大規模預訓練腦電圖模型取得進展，推動腦機介面與醫療應用。然而，現有模型未能充分捕捉神經震盪的豐富資訊，限制其效能與泛化能力，此缺陷源於架構設計。本研究提出LaBraM++，一種基於訊號處理基礎的增強型大型腦波基礎模型，其在多項任務中均顯著優於原架構，並與其他開放原始碼大型腦波模型相比具備競爭力。LaBraM++卓越的效能和訓練效率使其有望成為未來大型腦波模型發展的堅實基礎。", "audio": "audios/2505.16724v1.mp3", "timestamp": "2025-05-23T10:19:50.098428"}
{"query": "Diffusion Model", "id": "2505.16933v1", "url": "http://arxiv.org/abs/2505.16933v1", "title": "LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning", "summary": "In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large\nLanguage Model (MLLM) that integrates visual instruction tuning with masked\ndiffusion models, representing a departure from the autoregressive paradigms\ndominant in current multimodal approaches. Built upon LLaDA, a representative\nlarge language diffusion model, LLaDA-V incorporates a vision encoder and MLP\nconnector that projects visual features into the language embedding space,\nenabling effective multimodal alignment. Our empirical investigation reveals\nseveral intriguing results: First, LLaDA-V demonstrates promising multimodal\nperformance despite its language model being weaker on purely textual tasks\nthan counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same\ninstruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal\ntasks with better data scalability. It also narrows the performance gap to\nQwen2-VL, suggesting the effectiveness of its architecture for multimodal\ntasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal\nunderstanding compared to existing hybrid autoregressive-diffusion and purely\ndiffusion-based MLLMs. Our findings suggest that large language diffusion\nmodels show promise in multimodal contexts and warrant further investigation in\nfuture research. Project page and codes:\nhttps://ml-gsai.github.io/LLaDA-V-demo/.", "authors": ["Zebin You", "Shen Nie", "Xiaolu Zhang", "Jun Hu", "Jun Zhou", "Zhiwu Lu", "Ji-Rong Wen", "Chongxuan Li"], "published_date": "2025-05-22", "title_zh": "LLaDA-V：基於視覺指令微調的大型語言擴散模型", "summary_zh": "本研究提出 LLaDA-V，一種純擴散模型的多模態大型語言模型 (MLLM)，整合視覺指令微調與遮蔽擴散模型，跳脫現有多模態方法中常見的自迴歸範式。LLaDA-V 基於具代表性的大型語言擴散模型 LLaDA，結合視覺編碼器和 MLP 連接器，將視覺特徵投射至語言嵌入空間，實現有效的多模態對齊。實驗結果顯示，儘管 LLaDA-V 的語言模型在純文本任務上弱於 LLaMA3-8B 和 Qwen2-7B 等模型，但在多模態任務中仍展現出潛力。在相同指令資料集上訓練後，LLaDA-V 在多模態任務中與 LLaMA3-V 相比具有高度競爭力，且資料擴展性更佳，並縮小了與 Qwen2-VL 的性能差距，顯示其架構在多模態任務中的有效性。此外，LLaDA-V 在多模態理解方面相較於現有的混合自迴歸-擴散和純擴散 MLLM 達到了最先進的性能。研究結果表明，大型語言擴散模型在多模態環境中具有前景，值得未來進一步研究。專案頁面和程式碼：https://ml-gsai.github.io/LLaDA-V-demo/。", "audio": "audios/2505.16933v1.mp3", "timestamp": "2025-05-23T10:19:59.385455"}
{"query": "AI", "id": "2505.16954v1", "url": "http://arxiv.org/abs/2505.16954v1", "title": "Cracking Aegis: An Adversarial LLM-based Game for Raising Awareness of Vulnerabilities in Privacy Protection", "summary": "Traditional methods for raising awareness of privacy protection often fail to\nengage users or provide hands-on insights into how privacy vulnerabilities are\nexploited. To address this, we incorporate an adversarial mechanic in the\ndesign of the dialogue-based serious game Cracking Aegis. Leveraging LLMs to\nsimulate natural interactions, the game challenges players to impersonate\ncharacters and extract sensitive information from an AI agent, Aegis. A user\nstudy (n=22) revealed that players employed diverse deceptive linguistic\nstrategies, including storytelling and emotional rapport, to manipulate Aegis.\nAfter playing, players reported connecting in-game scenarios with real-world\nprivacy vulnerabilities, such as phishing and impersonation, and expressed\nintentions to strengthen privacy control, such as avoiding oversharing personal\ninformation with AI systems. This work highlights the potential of LLMs to\nsimulate complex relational interactions in serious games, while demonstrating\nhow an adversarial game strategy provides unique insights for designs for\nsocial good, particularly privacy protection.", "authors": ["Jiaying Fu", "Yiyang Lu", "Zehua Yang", "Fiona Nah", "RAY LC"], "published_date": "2025-05-22", "title_zh": "攻破埃吉斯：基於對抗性大型語言模型的遊戲，旨在提高對隱私保護漏洞的意識", "summary_zh": "傳統隱私保護宣導方法常缺乏使用者參與及實務漏洞認知。本研究於對話式嚴肅遊戲Cracking Aegis中導入對抗機制，利用大型語言模型模擬自然互動，挑戰玩家扮演角色自AI代理人Aegis提取敏感資訊。使用者研究(n=22)顯示，玩家運用多樣欺騙性語言策略，如故事敘述與情感連結，操縱Aegis。遊玩後，玩家表示能將遊戲情境連結至現實隱私漏洞，如網路釣魚與身分冒用，並表達強化隱私控制的意願，如避免過度分享個資予AI系統。此研究突顯大型語言模型在嚴肅遊戲中模擬複雜關係互動的潛力，並展示對抗性遊戲策略如何為公益設計，特別是隱私保護，提供獨特見解。", "audio": "audios/2505.16954v1.mp3", "timestamp": "2025-05-23T11:15:31.879619"}
{"query": "Foundation Model", "id": "2505.16635v1", "url": "http://arxiv.org/abs/2505.16635v1", "title": "WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning", "summary": "Tabular data, ubiquitous and rich in informational value, is an increasing\nfocus for deep representation learning, yet progress is hindered by studies\ncentered on single tables or isolated databases, which limits model\ncapabilities due to data scale. While collaborative learning approaches such as\nfederated learning, transfer learning, split learning, and tabular foundation\nmodels aim to learn from multiple correlated databases, they are challenged by\na scarcity of real-world interconnected tabular resources. Current data lakes\nand corpora largely consist of isolated databases lacking defined\ninter-database correlations. To overcome this, we introduce WikiDBGraph, a\nlarge-scale graph of 100,000 real-world tabular databases from WikiData,\ninterconnected by 17 million edges and characterized by 13 node and 12 edge\nproperties derived from its database schema and data distribution.\nWikiDBGraph's weighted edges identify both instance- and feature-overlapped\ndatabases. Experiments on these newly identified databases confirm that\ncollaborative learning yields superior performance, thereby offering\nconsiderable promise for structured foundation model training while also\nexposing key challenges and future directions for learning from interconnected\ntabular data.", "authors": ["Zhaomin Wu", "Ziyang Wang", "Bingsheng He"], "published_date": "2025-05-22", "title_zh": "WikiDBGraph：用於協作學習的Wikidata大規模資料庫圖", "summary_zh": "表格數據蘊含豐富資訊，日益受到深度表徵學習關注。然，現有研究多集中於單一表格或孤立資料庫，受限於數據規模。聯邦學習、遷移學習、分割學習及表格基礎模型等協作學習方法旨在從多個相關資料庫學習，卻面臨真實互聯表格資源匱乏的挑戰。現有資料湖和語料庫主要由缺乏明確跨庫關聯的孤立資料庫組成。為此，我們推出WikiDBGraph，一個來自WikiData的大規模圖，包含10萬個真實表格資料庫，透過1700萬條邊連接，並以源自其資料庫架構和數據分布的13個節點屬性和12個邊屬性為特徵。WikiDBGraph的加權邊可識別實例和特徵重疊的資料庫。基於這些新識別的資料庫的實驗證實，協作學習可產生更優異的性能，為結構化基礎模型訓練帶來可觀前景，同時也揭示了從互聯表格數據學習的關鍵挑戰和未來方向。", "audio": "audios/2505.16635v1.mp3", "timestamp": "2025-05-23T11:15:39.770004"}
{"query": "Diffusion Model", "id": "2505.16875v1", "url": "http://arxiv.org/abs/2505.16875v1", "title": "T2I-ConBench: Text-to-Image Benchmark for Continual Post-training", "summary": "Continual post-training adapts a single text-to-image diffusion model to\nlearn new tasks without incurring the cost of separate models, but naive\npost-training causes forgetting of pretrained knowledge and undermines\nzero-shot compositionality. We observe that the absence of a standardized\nevaluation protocol hampers related research for continual post-training. To\naddress this, we introduce T2I-ConBench, a unified benchmark for continual\npost-training of text-to-image models. T2I-ConBench focuses on two practical\nscenarios, item customization and domain enhancement, and analyzes four\ndimensions: (1) retention of generality, (2) target-task performance, (3)\ncatastrophic forgetting, and (4) cross-task generalization. It combines\nautomated metrics, human-preference modeling, and vision-language QA for\ncomprehensive assessment. We benchmark ten representative methods across three\nrealistic task sequences and find that no approach excels on all fronts. Even\njoint \"oracle\" training does not succeed for every task, and cross-task\ngeneralization remains unsolved. We release all datasets, code, and evaluation\ntools to accelerate research in continual post-training for text-to-image\nmodels.", "authors": ["Zhehao Huang", "Yuhang Liu", "Yixin Lou", "Zhengbao He", "Mingzhen He", "Wenxing Zhou", "Tao Li", "Kehan Li", "Zeyi Huang", "Xiaolin Huang"], "published_date": "2025-05-22", "title_zh": "T2I-ConBench：用於持續後訓練的文本到圖像基準測試", "summary_zh": "持續後訓練使單一文生圖擴散模型適應新任務，無需獨立模型，但直接後訓練會導致遺忘預訓練知識並損害零樣本組合性。缺乏標準化評估協議阻礙相關研究，因此我們提出T2I-ConBench，一個統一的文生圖模型持續後訓練基準。T2I-ConBench關注項目客製化和領域增強兩種實際情境，並分析四個維度：通用性保留、目標任務效能、災難性遺忘及跨任務泛化。它結合自動化指標、人類偏好建模及視覺語言問答進行全面評估。我們對比了三種實際任務序列中的十種代表性方法，發現沒有任何方法在所有方面都表現出色，即使是聯合「先知」訓練也並非在每個任務中都成功，且跨任務泛化仍未解決。我們釋出所有資料集、程式碼及評估工具，以加速文生圖模型持續後訓練的研究。", "audio": "audios/2505.16875v1.mp3", "timestamp": "2025-05-23T11:15:47.138005"}
{"query": "AI", "id": "2505.16951v1", "url": "http://arxiv.org/abs/2505.16951v1", "title": "From Reality to Virtual Worlds: The Role of Photogrammetry in Game Development", "summary": "Photogrammetry is transforming digital content creation by enabling the rapid\nconversion of real-world objects into highly detailed 3D models. This paper\nevaluates the role of RealityCapture, a GPU-accelerated photogrammetry tool, in\ngame development of Virtual Reality (VR). We assess its efficiency,\nreconstruction accuracy, and integration with Unreal Engine, comparing its\nadvantages and limitations against traditional modeling workflows.\nAdditionally, we examined user preferences between designed 3D assets and\nphotogrammetry-generated models. The results revealed that while photogrammetry\nenhances realism and interactivity, users slightly preferred manually designed\nmodels for small, manipulable elements because of the level of detail. However,\nfrom a developer perspective, RealityCapture significantly reduces development\ntime while maintaining geometric precision and photorealistic textures. Despite\nits reliance on high-performance hardware, its automation, scalability, and\nseamless integration with real-time rendering engines make it a valuable tool\nfor game developers and VR creators. Future improvements in AI-driven\noptimization and cloud-based processing could enhance accessibility, broadening\nits applications in gaming, cultural heritage preservation, and simulation.", "authors": ["Santiago Berrezueta-Guzman", "Andrei Koshelev", "Stefan Wagner"], "published_date": "2025-05-22", "title_zh": "從現實到虛擬世界：攝影測量術在遊戲開發中的作用", "summary_zh": "攝影測量正轉變數位內容創作，能快速將實物轉換為高精度3D模型。本文評估RealityCapture（一款GPU加速攝影測量工具）在虛擬實境遊戲開發中的作用，分析其效率、重建準確度及與Unreal Engine的整合，並與傳統建模流程比較優劣。研究同時考察使用者對設計3D物件和攝影測量生成模型的偏好。結果顯示，攝影測量增強了真實感和互動性，但使用者略微偏好手動設計的小型、可操作物件，因其細節更佳。然而，從開發者角度看，RealityCapture顯著縮短開發時間，同時保持幾何精度和逼真紋理。儘管依賴高效能硬體，其自動化、可擴展性以及與即時渲染引擎的無縫整合，使其成為遊戲開發者和虛擬實境創作者的寶貴工具。未來，基於人工智慧的優化和雲端處理可望提升可及性，擴展其在遊戲、文化遺產保護和模擬中的應用。", "audio": "audios/2505.16951v1.mp3", "timestamp": "2025-05-23T12:36:43.960915"}
{"query": "Foundation Model", "id": "2505.16540v1", "url": "http://arxiv.org/abs/2505.16540v1", "title": "TextureSAM: Towards a Texture Aware Foundation Model for Segmentation", "summary": "Segment Anything Models (SAM) have achieved remarkable success in object\nsegmentation tasks across diverse datasets. However, these models are\npredominantly trained on large-scale semantic segmentation datasets, which\nintroduce a bias toward object shape rather than texture cues in the image.\nThis limitation is critical in domains such as medical imaging, material\nclassification, and remote sensing, where texture changes define object\nboundaries. In this study, we investigate SAM's bias toward semantics over\ntextures and introduce a new texture-aware foundation model, TextureSAM, which\nperforms superior segmentation in texture-dominant scenarios. To achieve this,\nwe employ a novel fine-tuning approach that incorporates texture augmentation\ntechniques, incrementally modifying training images to emphasize texture\nfeatures. By leveraging a novel texture-alternation of the ADE20K dataset, we\nguide TextureSAM to prioritize texture-defined regions, thereby mitigating the\ninherent shape bias present in the original SAM model. Our extensive\nexperiments demonstrate that TextureSAM significantly outperforms SAM-2 on both\nnatural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation\ndatasets. The code and texture-augmented dataset will be publicly available.", "authors": ["Inbal Cohen", "Boaz Meivar", "Peihan Tu", "Shai Avidan", "Gal Oren"], "published_date": "2025-05-22", "title_zh": "紋理感知分割基礎模型：TextureSAM研究", "summary_zh": "物件分割模型(SAM)在多樣數據集上表現出色，但主要訓練於大型語義分割數據集，導致模型傾向物體形狀而非紋理。此限制在醫學影像、材料分類和遙感等領域尤其關鍵，因紋理變化定義物體邊界。本研究探討SAM對語義的偏好，並提出新的紋理感知基礎模型TextureSAM，在紋理主導場景中實現更優分割。我們採用新型微調方法，結合紋理增強技術，逐步修改訓練圖像以強調紋理特徵。透過改良ADE20K數據集，引導TextureSAM優先處理紋理定義區域，降低原始SAM模型固有的形狀偏好。實驗證明，TextureSAM在自然（+0.2 mIoU）和合成（+0.18 mIoU）紋理分割數據集上均顯著優於SAM-2。程式碼和紋理增強數據集將公開發布。", "audio": "audios/2505.16540v1.mp3", "timestamp": "2025-05-23T12:36:51.227202"}
{"query": "Diffusion Model", "id": "2505.16864v1", "url": "http://arxiv.org/abs/2505.16864v1", "title": "Training-Free Efficient Video Generation via Dynamic Token Carving", "summary": "Despite the remarkable generation quality of video Diffusion Transformer\n(DiT) models, their practical deployment is severely hindered by extensive\ncomputational requirements. This inefficiency stems from two key challenges:\nthe quadratic complexity of self-attention with respect to token length and the\nmulti-step nature of diffusion models. To address these limitations, we present\nJenga, a novel inference pipeline that combines dynamic attention carving with\nprogressive resolution generation. Our approach leverages two key insights: (1)\nearly denoising steps do not require high-resolution latents, and (2) later\nsteps do not require dense attention. Jenga introduces a block-wise attention\nmechanism that dynamically selects relevant token interactions using 3D\nspace-filling curves, alongside a progressive resolution strategy that\ngradually increases latent resolution during generation. Experimental results\ndemonstrate that Jenga achieves substantial speedups across multiple\nstate-of-the-art video diffusion models while maintaining comparable generation\nquality (8.83$\\times$ speedup with 0.01\\% performance drop on VBench). As a\nplug-and-play solution, Jenga enables practical, high-quality video generation\non modern hardware by reducing inference time from minutes to seconds --\nwithout requiring model retraining. Code:\nhttps://github.com/dvlab-research/Jenga", "authors": ["Yuechen Zhang", "Jinbo Xing", "Bin Xia", "Shaoteng Liu", "Bohao Peng", "Xin Tao", "Pengfei Wan", "Eric Lo", "Jiaya Jia"], "published_date": "2025-05-22", "title_zh": "基於動態令牌雕琢的免訓練高效視訊生成", "summary_zh": "視訊擴散轉換器模型（DiT）生成品質卓越，但其龐大計算需求嚴重阻礙實際部署。此低效源於兩個主要挑戰：自注意力機制隨令牌長度呈現的二次複雜度，以及擴散模型的多步特性。為解決此限制，我們提出Jenga，一種結合動態注意力雕刻與漸進式解析度生成的全新推論流程。此方法基於兩個關鍵洞察：（1）早期去噪步驟無需高解析度潛在變數，（2）後期步驟無需密集注意力。Jenga引入塊狀注意力機制，利用3D空間填充曲線動態選擇相關令牌交互，並採用漸進式解析度策略，在生成過程中逐步提升潛在變數解析度。實驗結果表明，Jenga在多個最先進視訊擴散模型上實現顯著加速，同時保持可比擬的生成品質（在VBench上加速8.83倍，性能下降0.01%）。作為一種即插即用解決方案，Jenga將推論時間從數分鐘縮短至數秒，從而在現代硬體上實現實用且高品質的視訊生成，且無需模型重新訓練。", "audio": "audios/2505.16864v1.mp3", "timestamp": "2025-05-23T12:37:00.242831"}
{"query": "AI", "id": "2505.16938v1", "url": "http://arxiv.org/abs/2505.16938v1", "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification", "summary": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours.", "authors": ["NovelSeek Team", "Bo Zhang", "Shiyang Feng", "Xiangchao Yan", "Jiakang Yuan", "Zhiyin Yu", "Xiaohan He", "Songtao Huang", "Shaowei Hou", "Zheng Nie", "Zhilong Wang", "Jinyao Liu", "Runmin Ma", "Tianshuo Peng", "Peng Ye", "Dongzhan Zhou", "Shufei Zhang", "Xiaosong Wang", "Yilan Zhang", "Meng Li", "Zhongying Tu", "Xiangyu Yue", "Wangli Ouyang", "Bowen Zhou", "Lei Bai"], "published_date": "2025-05-22", "title_zh": "新穎探索：當代理人化身科學家——構建從假說到驗證的閉環系統", "summary_zh": "人工智慧加速科學研究範式轉變，提升效率並驅動創新。我們提出NovelSeek，一個統一的閉環多代理框架，用於跨多個科學領域進行自主科學研究(ASR)，使研究人員能以前所未有的速度和精度解決複雜問題。NovelSeek具有三大優勢：1) 可擴展性：已在12項科學研究任務中展示其多功能性，能產生創新想法以提升基準程式碼效能。2) 互動性：提供人為專家回饋介面及多代理互動，能將領域專家知識無縫整合到自動端到端流程中。3) 效率：在多個科學領域取得顯著效能提升，且耗時遠少於人工。例如，在反應產率預測中，僅用12小時將準確率從27.6%提升至35.4%；在增強子活性預測中，僅用4小時將準確率從0.52提升至0.79；在2D語義分割中，僅用30小時將精確度從78.8%提升至81.0%。", "audio": "audios/2505.16938v1.mp3", "timestamp": "2025-05-23T13:28:56.575249"}
{"query": "Foundation Model", "id": "2505.16531v1", "url": "http://arxiv.org/abs/2505.16531v1", "title": "HOFT: Householder Orthogonal Fine-tuning", "summary": "Adaptation of foundation models using low-rank methods is a widespread\napproach. Another way to adapt these models is to employ orthogonal fine-tuning\nmethods, which are less time and memory efficient despite their good\ngeneralization properties. In this work, we propose Householder Orthogonal\nFine-tuning (HOFT), a novel orthogonal fine-tuning method that aims to\nalleviate time and space complexity. Moreover, some theoretical properties of\nthe orthogonal fine-tuning paradigm are explored. From this exploration, Scaled\nHouseholder Orthogonal Fine-tuning (SHOFT) is proposed. Both HOFT and SHOFT are\nevaluated in downstream tasks, namely commonsense reasoning, machine\ntranslation, subject-driven generation and mathematical reasoning. Compared\nwith state-of-the-art adaptation methods, HOFT and SHOFT show comparable or\nbetter results.", "authors": ["Alejandro Moreno Arcas", "Albert Sanchis", "Jorge Civera", "Alfons Juan"], "published_date": "2025-05-22", "title_zh": "豪斯霍爾德正交微調", "summary_zh": "基於低秩方法的基礎模型適配應用廣泛。另一種方法是正交微調，儘管其泛化性良好，但效率較低。本研究提出Householder正交微調(HOFT)，旨在降低時間和空間複雜度。此外，探討了正交微調範式的若干理論性質，並由此提出縮放Householder正交微調(SHOFT)。HOFT和SHOFT在常識推理、機器翻譯、主體驅動生成和數學推理等下游任務中進行評估，結果顯示，與現有最佳適配方法相比，HOFT和SHOFT表現出相當甚至更優異的性能。", "audio": "audios/2505.16531v1.mp3", "timestamp": "2025-05-23T13:29:01.709419"}
{"query": "Diffusion Model", "id": "2505.16862v1", "url": "http://arxiv.org/abs/2505.16862v1", "title": "Conditional Panoramic Image Generation via Masked Autoregressive Modeling", "summary": "Recent progress in panoramic image generation has underscored two critical\nlimitations in existing approaches. First, most methods are built upon\ndiffusion models, which are inherently ill-suited for equirectangular\nprojection (ERP) panoramas due to the violation of the identically and\nindependently distributed (i.i.d.) Gaussian noise assumption caused by their\nspherical mapping. Second, these methods often treat text-conditioned\ngeneration (text-to-panorama) and image-conditioned generation (panorama\noutpainting) as separate tasks, relying on distinct architectures and\ntask-specific data. In this work, we propose a unified framework, Panoramic\nAutoRegressive model (PAR), which leverages masked autoregressive modeling to\naddress these challenges. PAR avoids the i.i.d. assumption constraint and\nintegrates text and image conditioning into a cohesive architecture, enabling\nseamless generation across tasks. To address the inherent discontinuity in\nexisting generative models, we introduce circular padding to enhance spatial\ncoherence and propose a consistency alignment strategy to improve generation\nquality. Extensive experiments demonstrate competitive performance in\ntext-to-image generation and panorama outpainting tasks while showcasing\npromising scalability and generalization capabilities.", "authors": ["Chaoyang Wang", "Xiangtai Li", "Lu Qi", "Xiaofan Lin", "Jinbin Bai", "Qianyu Zhou", "Yunhai Tong"], "published_date": "2025-05-22", "title_zh": "基於遮罩自迴歸模型的條件全景圖像生成", "summary_zh": "全景影像生成的新進展揭示了現有方法的兩大局限。多數方法基於擴散模型，但因其球形映射導致獨立同分布高斯雜訊假設失效，故不適用於等距柱狀投影全景圖。此外，文字條件生成和圖像條件生成常被視為獨立任務，採用不同架構和特定數據。本研究提出統一框架，即全景自迴歸模型（PAR），利用遮罩自迴歸建模解決上述問題。PAR規避了獨立同分布假設限制，並將文字和圖像條件整合至統一架構，實現跨任務的無縫生成。為解決生成模型固有的不連續性，引入環形填充以增強空間一致性，並提出一致性對齊策略以提升生成品質。大量實驗表明，PAR在文字轉圖像生成和全景圖外繪任務中表現出色，並展現了良好的擴展性和泛化能力。", "audio": "audios/2505.16862v1.mp3", "timestamp": "2025-05-23T13:29:08.558974"}
{"query": "AI", "id": "2505.16934v1", "url": "http://arxiv.org/abs/2505.16934v1", "title": "In-Context Watermarks for Large Language Models", "summary": "The growing use of large language models (LLMs) for sensitive applications\nhas highlighted the need for effective watermarking techniques to ensure the\nprovenance and accountability of AI-generated text. However, most existing\nwatermarking methods require access to the decoding process, limiting their\napplicability in real-world settings. One illustrative example is the use of\nLLMs by dishonest reviewers in the context of academic peer review, where\nconference organizers have no access to the model used but still need to detect\nAI-generated reviews. Motivated by this gap, we introduce In-Context\nWatermarking (ICW), which embeds watermarks into generated text solely through\nprompt engineering, leveraging LLMs' in-context learning and\ninstruction-following abilities. We investigate four ICW strategies at\ndifferent levels of granularity, each paired with a tailored detection method.\nWe further examine the Indirect Prompt Injection (IPI) setting as a specific\ncase study, in which watermarking is covertly triggered by modifying input\ndocuments such as academic manuscripts. Our experiments validate the\nfeasibility of ICW as a model-agnostic, practical watermarking approach.\nMoreover, our findings suggest that as LLMs become more capable, ICW offers a\npromising direction for scalable and accessible content attribution.", "authors": ["Yepeng Liu", "Xuandong Zhao", "Christopher Kruegel", "Dawn Song", "Yuheng Bu"], "published_date": "2025-05-22", "title_zh": "大型語言模型的上下文水印", "summary_zh": "大型語言模型於敏感應用日漸普及，突顯有效浮水印技術之必要性，以確保AI生成文本之溯源性與責任歸屬。然而，多數現有浮水印方法需取用解碼過程，限制其實際應用。為解決此缺口，本研究提出情境式浮水印（ICW），僅透過提示工程將浮水印嵌入生成文本，利用大型語言模型的情境學習與指令遵循能力。我們探討四種不同精細度的ICW策略，並搭配客製化偵測方法。此外，我們研究間接提示注入（IPI）情境，此為透過修改學術論文等輸入文件，隱蔽地觸發浮水印的案例研究。實驗驗證ICW作為模型無關、實用浮水印方法的可行性。研究結果顯示，隨著大型語言模型能力提升，ICW為可擴展且易於使用的內容歸屬提供具前景的方向。", "audio": "audios/2505.16934v1.mp3", "timestamp": "2025-05-23T14:17:45.222759"}
{"query": "Foundation Model", "id": "2505.16490v1", "url": "http://arxiv.org/abs/2505.16490v1", "title": "HPP-Voice: A Large-Scale Evaluation of Speech Embeddings for Multi-Phenotypic Classification", "summary": "Human speech contains paralinguistic cues that reflect a speaker's\nphysiological and neurological state, potentially enabling non-invasive\ndetection of various medical phenotypes. We introduce the Human Phenotype\nProject Voice corpus (HPP-Voice): a dataset of 7,188 recordings in which\nHebrew-speaking adults count for 30 seconds, with each speaker linked to up to\n15 potentially voice-related phenotypes spanning respiratory, sleep, mental\nhealth, metabolic, immune, and neurological conditions. We present a systematic\ncomparison of 14 modern speech embedding models, where modern speech embeddings\nfrom these 30-second counting tasks outperform MFCCs and demographics for\ndownstream health condition classifications. We found that embedding learned\nfrom a speaker identification model can predict objectively measured moderate\nto severe sleep apnea in males with an AUC of 0.64 $\\pm$ 0.03, while MFCC and\ndemographic features led to AUCs of 0.56 $\\pm$ 0.02 and 0.57 $\\pm$ 0.02,\nrespectively. Additionally, our results reveal gender-specific patterns in\nmodel effectiveness across different medical domains. For males, speaker\nidentification and diarization models consistently outperformed speech\nfoundation models for respiratory conditions (e.g., asthma: 0.61 $\\pm$ 0.03 vs.\n0.56 $\\pm$ 0.02) and sleep-related conditions (insomnia: 0.65 $\\pm$ 0.04 vs.\n0.59 $\\pm$ 0.05). For females, speaker diarization models performed best for\nsmoking status (0.61 $\\pm$ 0.02 vs 0.55 $\\pm$ 0.02), while Hebrew-specific\nmodels performed best (0.59 $\\pm$ 0.02 vs. 0.58 $\\pm$ 0.02) in classifying\nanxiety compared to speech foundation models. Our findings provide evidence\nthat a simple counting task can support large-scale, multi-phenotypic voice\nscreening and highlight which embedding families generalize best to specific\nconditions, insights that can guide future vocal biomarker research and\nclinical deployment.", "authors": ["David Krongauz", "Hido Pinto", "Sarah Kohn", "Yanir Marmor", "Eran Segal"], "published_date": "2025-05-22", "title_zh": "HPP-語音：用於多表型分類的語音嵌入大規模評估", "summary_zh": "人類語音包含反映生理和神經狀態的超語言線索，有助於非侵入式檢測多種醫學表型。我們推出人類表型計畫語音語料庫(HPP-Voice)，包含7188份希伯來語成年人30秒計數錄音，每位受試者關聯至多15種潛在語音相關表型，涵蓋呼吸、睡眠、精神健康、代謝、免疫及神經系統疾病。系統性比較14種現代語音嵌入模型，發現此計數任務產生的現代語音嵌入，在下游健康狀況分類中優於MFCC和人口統計特徵。由說話人辨識模型學習的嵌入，能預測男性客觀測量的中重度睡眠呼吸中止症，AUC達0.64 $\\pm$ 0.03，而MFCC和人口統計特徵的AUC分別為0.56 $\\pm$ 0.02和0.57 $\\pm$ 0.02。結果顯示模型在不同醫學領域的有效性存在性別差異。對於男性，說話人辨識和區分模型在呼吸系統疾病和睡眠相關疾病方面，始終優於語音基礎模型。對於女性，說話人區分模型在吸菸狀態的表現最佳，而希伯來語專用模型在焦慮症分類中優於語音基礎模型。研究結果證明，簡單的計數任務可支援大規模、多表型的語音篩查，並強調哪些嵌入家族最能推廣至特定疾病，為未來語音生物標記研究和臨床應用提供參考。", "audio": "audios/2505.16490v1.mp3", "timestamp": "2025-05-23T14:17:56.417910"}
{"query": "Diffusion Model", "id": "2505.16839v1", "url": "http://arxiv.org/abs/2505.16839v1", "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding", "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.", "authors": ["Shufan Li", "Konstantinos Kallidromitis", "Hritik Bansal", "Akash Gokul", "Yusuke Kato", "Kazuki Kozuka", "Jason Kuen", "Zhe Lin", "Kai-Wei Chang", "Aditya Grover"], "published_date": "2025-05-22", "title_zh": "LaViDa：用於多模態理解的大型擴散語言模型", "summary_zh": "現代視覺語言模型（VLMs）擅長解決需要視覺推理的多種任務。在實際應用中，快速推論和可控生成至關重要，但現有的自迴歸VLMs如LLaVA在此方面表現不佳。離散擴散模型（DMs）提供了一種有前景的替代方案，透過平行解碼加速推論，並利用雙向上下文實現可控生成。我們提出了LaViDa，一個基於DMs的VLMs家族。LaViDa透過為DMs配備視覺編碼器並聯合微調，實現多模態指令追蹤。LaViDa採用互補遮罩以提升訓練效果，前綴KV快取以提高推論效率，以及時間步長偏移以改善取樣品質。實驗表明，LaViDa在MMMU等多模態基準測試中，效能與自迴歸VLMs相當或更優越，同時具備DMs的獨特優勢，包括靈活的速度與品質權衡、可控性和雙向推理。在COCO圖片標註中，LaViDa以1.92倍的速度提升超越Open-LLaVa-Next-8B 4.1個CIDEr點。在雙向任務中，LaViDa在受限詩歌補全方面提升了59%。這些結果證明LaViDa是自迴歸VLMs的一個強大替代方案。", "audio": "audios/2505.16839v1.mp3", "timestamp": "2025-05-23T14:18:05.896554"}
{"query": "AI", "id": "2505.16928v1", "url": "http://arxiv.org/abs/2505.16928v1", "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning", "summary": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning.", "authors": ["Bosung Kim", "Prithviraj Ammanabrolu"], "published_date": "2025-05-22", "title_zh": "超越具身稻草堆中的針：長上下文推理的環境、架構與訓練考量", "summary_zh": "$\\infty$-THOR為具身人工智慧中的長程任務提出新框架，促進長文理解。其提供：(1)可擴展、可重現且無限長程軌跡的生成框架；(2)新型具身問答任務Needle(s) in the Embodied Haystack，透過分散線索測試智能體的長文推理能力；(3)長程資料集及基準測試集，包含跨越數百環境步驟的複雜任務，並附帶真實動作序列。為實現此能力，探索架構調整，包括交錯目標-狀態-動作建模、上下文擴展技術與上下文並行，使基於大型語言模型的智能體具備極端長文推理與互動能力。實驗結果突顯基準測試帶來的挑戰，並提供長程條件下訓練策略及模型行為的洞見。此研究為下一代具備穩健長程推理與規劃能力的具身人工智慧系統奠定基礎。", "audio": "audios/2505.16928v1.mp3", "timestamp": "2025-05-23T15:19:17.435111"}
{"query": "Foundation Model", "id": "2505.16360v1", "url": "http://arxiv.org/abs/2505.16360v1", "title": "Style Transfer with Diffusion Models for Synthetic-to-Real Domain Adaptation", "summary": "Semantic segmentation models trained on synthetic data often perform poorly\non real-world images due to domain gaps, particularly in adverse conditions\nwhere labeled data is scarce. Yet, recent foundation models enable to generate\nrealistic images without any training. This paper proposes to leverage such\ndiffusion models to improve the performance of vision models when learned on\nsynthetic data. We introduce two novel techniques for semantically consistent\nstyle transfer using diffusion models: Class-wise Adaptive Instance\nNormalization and Cross-Attention (CACTI) and its extension with selective\nattention Filtering (CACTIF). CACTI applies statistical normalization\nselectively based on semantic classes, while CACTIF further filters\ncross-attention maps based on feature similarity, preventing artifacts in\nregions with weak cross-attention correspondences. Our methods transfer style\ncharacteristics while preserving semantic boundaries and structural coherence,\nunlike approaches that apply global transformations or generate content without\nconstraints. Experiments using GTA5 as source and Cityscapes/ACDC as target\ndomains show that our approach produces higher quality images with lower FID\nscores and better content preservation. Our work demonstrates that class-aware\ndiffusion-based style transfer effectively bridges the synthetic-to-real domain\ngap even with minimal target domain data, advancing robust perception systems\nfor challenging real-world applications. The source code is available at:\nhttps://github.com/echigot/cactif.", "authors": ["Estelle Chigot", "Dennis G. Wilson", "Meriem Ghrib", "Thomas Oberlin"], "published_date": "2025-05-22", "title_zh": "基於擴散模型的風格遷移用於合成到真實領域適應", "summary_zh": "基於合成數據訓練的語義分割模型在真實圖像上表現不佳，主因在於領域差異，尤其是在標記數據稀缺的惡劣條件下。 近期基礎模型無需訓練即可生成逼真圖像。 本文提出利用擴散模型來提升視覺模型在合成數據上的學習效能。 我們引入兩種新的語義一致風格轉換技術：類別自適應實例正規化和交叉注意力（CACTI），及其帶選擇性注意力過濾的擴展（CACTIF）。 CACTI根據語義類別選擇性地應用統計正規化，而CACTIF則基於特徵相似性進一步過濾交叉注意力圖，從而防止弱交叉注意力區域中出現偽影。 我們的方法在保留語義邊界和結構一致性的同時轉換風格特徵，這與應用全局轉換或生成無約束內容的方法不同。 以GTA5為源域，Cityscapes/ACDC為目標域的實驗表明，我們的方法可生成更高品質的圖像，具有更低的FID分數和更好的內容保留。 研究表明，基於類別感知的擴散風格轉換能有效彌合合成到真實的領域差異，即使目標域數據極少，亦可推進適用於具挑戰性現實應用之強韌感知系統。 原始碼可在以下網址取得：https://github.com/echigot/cactif。", "audio": "audios/2505.16360v1.mp3", "timestamp": "2025-05-23T15:19:36.415768"}
{"query": "Diffusion Model", "id": "2505.16798v1", "url": "http://arxiv.org/abs/2505.16798v1", "title": "SEED: Speaker Embedding Enhancement Diffusion Model", "summary": "A primary challenge when deploying speaker recognition systems in real-world\napplications is performance degradation caused by environmental mismatch. We\npropose a diffusion-based method that takes speaker embeddings extracted from a\npre-trained speaker recognition model and generates refined embeddings. For\ntraining, our approach progressively adds Gaussian noise to both clean and\nnoisy speaker embeddings extracted from clean and noisy speech, respectively,\nvia forward process of a diffusion model, and then reconstructs them to clean\nembeddings in the reverse process. While inferencing, all embeddings are\nregenerated via diffusion process. Our method needs neither speaker label nor\nany modification to the existing speaker recognition pipeline. Experiments on\nevaluation sets simulating environment mismatch scenarios show that our method\ncan improve recognition accuracy by up to 19.6% over baseline models while\nretaining performance on conventional scenarios. We publish our code here\nhttps://github.com/kaistmm/seed-pytorch", "authors": ["KiHyun Nam", "Jungwoo Heo", "Jee-weon Jung", "Gangin Park", "Chaeyoung Jung", "Ha-Jin Yu", "Joon Son Chung"], "published_date": "2025-05-22", "title_zh": "SEED：說者嵌入增強擴散模型", "summary_zh": "在實際應用中部署語者辨識系統的主要挑戰是環境不匹配導致的效能下降。 我們提出一種基於擴散模型的方法，該方法提取自預訓練語者辨識模型的語者嵌入，並產生精煉的嵌入。 在訓練階段，我們的策略透過擴散模型的前向過程，逐步將高斯雜訊添加到乾淨和嘈雜語音中提取的乾淨及嘈雜語者嵌入，然後在反向過程中將它們重建為乾淨的嵌入。 在推理時，所有嵌入都透過擴散過程重新生成。 該方法無需語者標籤，也不需要修改現有的語者辨識流程。 在模擬環境不匹配場景的評估集上的實驗表明，我們的方法可以將辨識準確率提高至比基線模型高 19.6%，同時保持在傳統場景下的效能。 我們的程式碼已公開於https://github.com/kaistmm/seed-pytorch。", "audio": "audios/2505.16798v1.mp3", "timestamp": "2025-05-23T15:19:48.427714"}
{"query": "AI", "id": "2505.16899v1", "url": "http://arxiv.org/abs/2505.16899v1", "title": "Identifying, Evaluating, and Mitigating Risks of AI Thought Partnerships", "summary": "Artificial Intelligence (AI) systems have historically been used as tools\nthat execute narrowly defined tasks. Yet recent advances in AI have unlocked\npossibilities for a new class of models that genuinely collaborate with humans\nin complex reasoning, from conceptualizing problems to brainstorming solutions.\nSuch AI thought partners enable novel forms of collaboration and extended\ncognition, yet they also pose major risks-including and beyond risks of typical\nAI tools and agents. In this commentary, we systematically identify risks of AI\nthought partners through a novel framework that identifies risks at multiple\nlevels of analysis, including Real-time, Individual, and Societal risks arising\nfrom collaborative cognition (RISc). We leverage this framework to propose\nconcrete metrics for risk evaluation, and finally suggest specific mitigation\nstrategies for developers and policymakers. As AI thought partners continue to\nproliferate, these strategies can help prevent major harms and ensure that\nhumans actively benefit from productive thought partnerships.", "authors": ["Kerem Oktar", "Katherine M. Collins", "Jose Hernandez-Orallo", "Diane Coyle", "Stephen Cave", "Adrian Weller", "Ilia Sucholutsky"], "published_date": "2025-05-22", "title_zh": "識別、評估與緩解人工智慧思維夥伴關係之風險", "summary_zh": "人工智慧系統過去多為執行特定任務之工具。然近期發展已催生新型模型，能與人類於複雜推理中真正協作，涵蓋問題概念化至方案集思廣益。此類AI思考夥伴促進嶄新協作模式及延伸認知，然亦帶來重大風險，不僅限於傳統AI工具及代理。本文提出新穎框架，於即時、個體及社會層面系統性識別協作認知所生風險(RISc)，並據此提出風險評估之具體指標，最終為開發者及決策者建議特定緩解策略。隨AI思考夥伴日漸普及，此等策略有助預防重大危害，確保人類積極受益於具生產力之思考夥伴關係。", "audio": "audios/2505.16899v1.mp3", "timestamp": "2025-05-23T16:22:17.025557"}
{"query": "Foundation Model", "id": "2505.16338v1", "url": "http://arxiv.org/abs/2505.16338v1", "title": "Fusion of Foundation and Vision Transformer Model Features for Dermatoscopic Image Classification", "summary": "Accurate classification of skin lesions from dermatoscopic images is\nessential for diagnosis and treatment of skin cancer. In this study, we\ninvestigate the utility of a dermatology-specific foundation model, PanDerm, in\ncomparison with two Vision Transformer (ViT) architectures (ViT base and Swin\nTransformer V2 base) for the task of skin lesion classification. Using frozen\nfeatures extracted from PanDerm, we apply non-linear probing with three\ndifferent classifiers, namely, multi-layer perceptron (MLP), XGBoost, and\nTabNet. For the ViT-based models, we perform full fine-tuning to optimize\nclassification performance. Our experiments on the HAM10000 and MSKCC datasets\ndemonstrate that the PanDerm-based MLP model performs comparably to the\nfine-tuned Swin transformer model, while fusion of PanDerm and Swin Transformer\npredictions leads to further performance improvements. Future work will explore\nadditional foundation models, fine-tuning strategies, and advanced fusion\ntechniques.", "authors": ["Amirreza Mahbod", "Rupert Ecker", "Ramona Woitek"], "published_date": "2025-05-22", "title_zh": "基於基礎模型與視覺變換器模型特徵融合的皮膚鏡圖像分類", "summary_zh": "皮膚鏡影像的準確皮膚病灶分類對皮膚癌診斷和治療至關重要。本研究探討皮膚科專用基礎模型PanDerm在皮膚病灶分類中的效用，並與兩種Vision Transformer (ViT)架構(ViT base 和 Swin Transformer V2 base) 進行比較。我們使用從PanDerm提取的凍結特徵，透過多層感知器(MLP)、XGBoost和TabNet三種分類器進行非線性探測。對於基於ViT的模型，我們執行完全微調以優化分類性能。在HAM10000和MSKCC數據集上的實驗表明，基於PanDerm的MLP模型性能與微調後的Swin Transformer模型相當，而PanDerm和Swin Transformer預測的融合可進一步提高性能。未來工作將探索更多基礎模型、微調策略和進階融合技術。", "audio": "audios/2505.16338v1.mp3", "timestamp": "2025-05-23T16:22:28.954045"}
{"query": "Diffusion Model", "id": "2505.16790v1", "url": "http://arxiv.org/abs/2505.16790v1", "title": "Learning Flexible Forward Trajectories for Masked Molecular Diffusion", "summary": "Masked diffusion models (MDMs) have achieved notable progress in modeling\ndiscrete data, while their potential in molecular generation remains\nunderexplored. In this work, we explore their potential and introduce the\nsurprising result that naively applying standards MDMs severely degrades the\nperformance. We identify the critical cause of this issue as a state-clashing\nproblem-where the forward diffusion of distinct molecules collapse into a\ncommon state, resulting in a mixture of reconstruction targets that cannot be\nlearned using typical reverse diffusion process with unimodal predictions. To\nmitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that\norchestrates per-element corruption trajectories to avoid collision between\ndistinct molecular graphs. This is achieved through a parameterized noise\nscheduling network that assigns distinct corruption rates to individual graph\nelements, i.e., atoms and bonds. Extensive experiments on diverse molecular\nbenchmarks reveal that MELD markedly enhances overall generation quality\ncompared to element-agnostic noise scheduling, increasing the chemical validity\nof vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves\nstate-of-the-art property alignment in conditional generation tasks.", "authors": ["Hyunjin Seo", "Taewon Kim", "Sihyun Yu", "SungSoo Ahn"], "published_date": "2025-05-22", "title_zh": "用於掩碼分子擴散的柔性前向軌跡學習", "summary_zh": "遮罩擴散模型在離散數據建模方面取得顯著進展，但其在分子生成中的潛力尚未充分探索。本研究發現，直接應用標準遮罩擴散模型會嚴重降低性能，原因在於狀態衝突問題：不同分子的正向擴散會坍縮到共同狀態，導致逆向擴散過程中出現難以學習的混合重建目標。為此，我們提出遮罩元素級可學習擴散（MELD），協調逐元素腐敗軌跡以避免分子圖之間的衝突。MELD透過參數化的雜訊排程網路，為個別圖元素（原子和鍵）分配不同的腐敗率。在多個分子基準測試中，MELD顯著提高了整體生成品質，在ZINC250K數據集上，原始遮罩擴散模型的化學有效性從15%提升至93%，並在條件生成任務中實現了最先進的性質對齊。", "audio": "audios/2505.16790v1.mp3", "timestamp": "2025-05-23T16:22:44.838182"}
{"query": "AI", "id": "2505.16888v1", "url": "http://arxiv.org/abs/2505.16888v1", "title": "CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework", "summary": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available.", "authors": ["Viet Pham", "Thai Le"], "published_date": "2025-05-22", "title_zh": "CAIN：基於兩階段惡意系統提示詞生成與精煉框架的LLM-人機對話劫持", "summary_zh": "大型語言模型（LLMs）雖推動諸多應用，卻易受對抗性攻擊。本文提出一種新型安全威脅：透過操縱LLMs的系統提示，劫持人機對話，使其僅針對特定目標問題（如總統投票人選、新冠疫苗安全性）產生惡意回答，而在其他問題上表現正常。此攻擊具危害性，惡意行為者可藉散布看似無害的惡意系統提示，進行大規模資訊操縱。為驗證此攻擊，我們開發CAIN演算法，可在黑箱環境下自動產生針對特定目標問題的有害系統提示，無需存取LLM參數。於開源及商業LLMs上的評估顯示，CAIN具有顯著的對抗性影響。在非定向攻擊或迫使LLMs輸出錯誤答案時，CAIN使目標問題的F1值降低高達40%，同時保持良性輸入的高準確性。在定向攻擊或迫使LLMs輸出特定有害答案時，CAIN在這些目標回應上實現超過70%的F1值，且對良性問題影響甚微。研究結果突顯了加強穩健性措施的迫切需求，以保護LLMs在實際應用中的完整性和安全性。所有原始碼將公開。", "audio": "audios/2505.16888v1.mp3", "timestamp": "2025-05-23T17:16:40.379263"}
{"query": "Foundation Model", "id": "2505.16304v1", "url": "http://arxiv.org/abs/2505.16304v1", "title": "SAMba-UNet: Synergizing SAM2 and Mamba in UNet with Heterogeneous Aggregation for Cardiac MRI Segmentation", "summary": "To address the challenge of complex pathological feature extraction in\nautomated cardiac MRI segmentation, this study proposes an innovative\ndual-encoder architecture named SAMba-UNet. The framework achieves cross-modal\nfeature collaborative learning by integrating the vision foundation model SAM2,\nthe state-space model Mamba, and the classical UNet. To mitigate domain\ndiscrepancies between medical and natural images, a Dynamic Feature Fusion\nRefiner is designed, which enhances small lesion feature extraction through\nmulti-scale pooling and a dual-path calibration mechanism across channel and\nspatial dimensions. Furthermore, a Heterogeneous Omni-Attention Convergence\nModule (HOACM) is introduced, combining global contextual attention with\nbranch-selective emphasis mechanisms to effectively fuse SAM2's local\npositional semantics and Mamba's long-range dependency modeling capabilities.\nExperiments on the ACDC cardiac MRI dataset demonstrate that the proposed model\nachieves a Dice coefficient of 0.9103 and an HD95 boundary error of 1.0859 mm,\nsignificantly outperforming existing methods, particularly in boundary\nlocalization for complex pathological structures such as right ventricular\nanomalies. This work provides an efficient and reliable solution for automated\ncardiac disease diagnosis, and the code will be open-sourced.", "authors": ["Guohao Huo", "Ruiting Dai", "Hao Tang"], "published_date": "2025-05-22", "title_zh": "SAMba-UNet：結合SAM2與Mamba的UNet，透過異質聚合用於心臟MRI分割", "summary_zh": "本研究提出名為SAMba-UNet的創新雙編碼器架構，旨在解決自動心臟MRI分割中複雜病理特徵提取的挑戰。該框架整合視覺基礎模型SAM2、狀態空間模型Mamba和經典UNet，實現跨模態特徵協同學習。為緩解醫學圖像與自然圖像之間的領域差異，設計動態特徵融合精煉器，透過多尺度池化和跨通道及空間維度的雙路徑校準機制，增強小病灶特徵提取。此外，引入異質全方位注意力融合模組(HOACM)，結合全局上下文注意力和分支選擇性強調機制，有效融合SAM2的局部位置語義和Mamba的長程依賴建模能力。在ACDC心臟MRI數據集上的實驗表明，所提出的模型Dice係數達到0.9103，HD95邊界誤差為1.0859毫米，顯著優於現有方法，尤其是在右心室異常等複雜病理結構的邊界定位方面。本研究為自動心臟疾病診斷提供了一種高效可靠的解決方案，並將開源程式碼。", "audio": "audios/2505.16304v1.mp3", "timestamp": "2025-05-23T17:16:52.040396"}
{"query": "Diffusion Model", "id": "2505.16733v1", "url": "http://arxiv.org/abs/2505.16733v1", "title": "Forward-only Diffusion Probabilistic Models", "summary": "This work presents a forward-only diffusion (FoD) approach for generative\nmodelling. In contrast to traditional diffusion models that rely on a coupled\nforward-backward diffusion scheme, FoD directly learns data generation through\na single forward diffusion process, yielding a simple yet efficient generative\nframework. The core of FoD is a state-dependent linear stochastic differential\nequation that involves a mean-reverting term in both the drift and diffusion\nfunctions. This mean-reversion property guarantees the convergence to clean\ndata, naturally simulating a stochastic interpolation between source and target\ndistributions. More importantly, FoD is analytically tractable and is trained\nusing a simple stochastic flow matching objective, enabling a few-step\nnon-Markov chain sampling during inference. The proposed FoD model, despite its\nsimplicity, achieves competitive performance on various image-conditioned\n(e.g., image restoration) and unconditional generation tasks, demonstrating its\neffectiveness in generative modelling. Our code is available at\nhttps://github.com/Algolzw/FoD.", "authors": ["Ziwei Luo", "Fredrik K. Gustafsson", "Jens Sjölund", "Thomas B. Schön"], "published_date": "2025-05-22", "title_zh": "僅正向擴散機率模型", "summary_zh": "本研究提出一種僅前向擴散(FoD)的生成模型方法。FoD不同於傳統依賴耦合前向-後向擴散方案的擴散模型，直接透過單一前向擴散過程學習資料生成，產生一個簡單而高效的生成框架。FoD的核心是一個狀態相關的線性隨機微分方程，其漂移和擴散函數中均包含均值回歸項。此均值回歸特性保證收斂至乾淨資料，自然地模擬源分佈和目標分佈之間的隨機插值。更重要的是，FoD在分析上易於處理，並使用簡單的隨機流匹配目標進行訓練，從而在推論期間實現少步非馬可夫鏈取樣。所提出的FoD模型儘管結構簡單，但在各種圖像條件（例如，圖像修復）和無條件生成任務上均實現了有競爭力的效能，證明其在生成模型中的有效性。程式碼可在https://github.com/Algolzw/FoD取得。", "audio": "audios/2505.16733v1.mp3", "timestamp": "2025-05-23T17:17:03.364699"}
{"query": "AI", "id": "2505.16869v1", "url": "http://arxiv.org/abs/2505.16869v1", "title": "MPO: Multilingual Safety Alignment via Reward Gap Optimization", "summary": "Large language models (LLMs) have become increasingly central to AI\napplications worldwide, necessitating robust multilingual safety alignment to\nensure secure deployment across diverse linguistic contexts. Existing\npreference learning methods for safety alignment, such as RLHF and DPO, are\nprimarily monolingual and struggle with noisy multilingual data. To address\nthese limitations, we introduce Multilingual reward gaP Optimization (MPO), a\nnovel approach that leverages the well-aligned safety capabilities of the\ndominant language (English) to improve safety alignment across multiple\nlanguages. MPO directly minimizes the reward gap difference between the\ndominant language and target languages, effectively transferring safety\ncapabilities while preserving the original strengths of the dominant language.\nExtensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate\nMPO's efficacy in multilingual safety alignment without degrading general\nmultilingual utility.", "authors": ["Weixiang Zhao", "Yulin Hu", "Yang Deng", "Tongtong Wu", "Wenxuan Zhang", "Jiahe Guo", "An Zhang", "Yanyan Zhao", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "published_date": "2025-05-22", "title_zh": "MPO：透過獎勵差距最佳化實現多語言安全對齊", "summary_zh": "大型語言模型於全球人工智慧應用中日益重要，需具備穩健的多語安全對齊，以確保在不同語言環境下的安全部署。現有的安全對齊偏好學習方法，如RLHF和DPO，主要為單語且易受嘈雜多語資料影響。為了解決這些限制，我們提出多語獎勵差距優化（MPO），此方法利用主導語言（英語）良好的安全能力，提升多種語言的安全對齊。MPO直接最小化主導語言與目標語言之間的獎勵差距差異，有效轉移安全能力，同時保留主導語言的優勢。在LLaMA-3.1、Gemma-2和Qwen2.5三個大型語言模型上的廣泛實驗，驗證了MPO在多語安全對齊方面的有效性，且不降低一般多語實用性。", "audio": "audios/2505.16869v1.mp3", "timestamp": "2025-05-23T18:24:33.935318"}
{"query": "Foundation Model", "id": "2505.16130v1", "url": "http://arxiv.org/abs/2505.16130v1", "title": "Scalable Graph Generative Modeling via Substructure Sequences", "summary": "Graph neural networks (GNNs) has been predominantly driven by\nmessage-passing, where node representations are iteratively updated via local\nneighborhood aggregation. Despite their success, message-passing suffers from\nfundamental limitations -- including constrained expressiveness,\nover-smoothing, over-squashing, and limited capacity to model long-range\ndependencies. These issues hinder scalability: increasing data size or model\nsize often fails to yield improved performance, limiting the viability of GNNs\nas backbones for graph foundation models. In this work, we explore pathways\nbeyond message-passing and introduce Generative Graph Pattern Machine\n(G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM\nrepresents graph instances (nodes, edges, or entire graphs) as sequences of\nsubstructures, and employs generative pre-training over the sequences to learn\ngeneralizable, transferable representations. Empirically, G$^2$PM demonstrates\nstrong scalability: on the ogbn-arxiv benchmark, it continues to improve with\nmodel sizes up to 60M parameters, outperforming prior generative approaches\nthat plateau at significantly smaller scales (e.g., 3M). In addition, we\nsystematically analyze the model design space, highlighting key architectural\nchoices that contribute to its scalability and generalization. Across diverse\ntasks -- including node classification, graph classification, and transfer\nlearning -- G$^2$PM consistently outperforms strong baselines, establishing a\ncompelling foundation for scalable graph learning. The code and dataset are\navailable at https://github.com/Zehong-Wang/G2PM.", "authors": ["Zehong Wang", "Zheyuan Zhang", "Tianyi Ma", "Chuxu Zhang", "Yanfang Ye"], "published_date": "2025-05-22", "title_zh": "基於子結構序列的可擴展圖生成模型", "summary_zh": "圖神經網路主要基於訊息傳遞，透過鄰域聚合迭代更新節點表示。訊息傳遞存在表達力受限、過度平滑、過度擠壓以及長程依賴建模能力不足等根本限制，阻礙了其擴展性。本文探索訊息傳遞之外的途徑，提出生成式圖樣式機器(G$^2$PM)，一種用於圖的生成式Transformer預訓練框架。G$^2$PM將圖實例表示為子結構序列，並透過序列上的生成式預訓練學習可泛化、可轉移的表示。實驗表明，G$^2$PM具有強大的擴展性，在ogbn-arxiv基準測試中，隨著模型規模增大(最高達60M參數)，性能持續提升，優於在較小規模(如3M)下達到瓶頸的生成式方法。此外，本文系統地分析了模型設計空間，強調了促進擴展性和泛化的關鍵架構選擇。在節點分類、圖分類和轉移學習等多項任務中，G$^2$PM始終優於強大的基線模型，為可擴展圖學習奠定了基礎。", "audio": "audios/2505.16130v1.mp3", "timestamp": "2025-05-23T18:24:50.559904"}
{"query": "Diffusion Model", "id": "2505.16549v1", "url": "http://arxiv.org/abs/2505.16549v1", "title": "Towards Coordinate- and Dimension-Agnostic Machine Learning for Partial Differential Equations", "summary": "The machine learning methods for data-driven identification of partial\ndifferential equations (PDEs) are typically defined for a given number of\nspatial dimensions and a choice of coordinates the data have been collected in.\nThis dependence prevents the learned evolution equation from generalizing to\nother spaces. In this work, we reformulate the problem in terms of coordinate-\nand dimension-independent representations, paving the way toward what we call\n``spatially liberated\" PDE learning. To this end, we employ a machine learning\napproach to predict the evolution of scalar field systems expressed in the\nformalism of exterior calculus, which is coordinate-free and immediately\ngeneralizes to arbitrary dimensions by construction. We demonstrate the\nperformance of this approach in the FitzHugh-Nagumo and Barkley\nreaction-diffusion models, as well as the Patlak-Keller-Segel model informed by\nin-situ chemotactic bacteria observations. We provide extensive numerical\nexperiments that demonstrate that our approach allows for seamless transitions\nacross various spatial contexts. We show that the field dynamics learned in one\nspace can be used to make accurate predictions in other spaces with different\ndimensions, coordinate systems, boundary conditions, and curvatures.", "authors": ["Trung V. Phan", "George A. Kevrekidis", "Soledad Villar", "Yannis G. Kevrekidis", "Juan M. Bello-Rivas"], "published_date": "2025-05-22", "title_zh": "偏微分方程之坐標與維度無關機器學習方法研究", "summary_zh": "基於資料驅動的偏微分方程式(PDE)機器學習方法通常針對特定空間維度和座標系設計，限制了其泛化能力。本研究將問題重新表述為與座標和維度無關的形式，實現空間解放的PDE學習。我們利用機器學習預測標量場系統的演化，該系統採用外微分形式，具有無座標特性，並能自然推廣到任意維度。研究展示了此方法在FitzHugh-Nagumo、Barkley反應擴散模型以及Patlak-Keller-Segel模型中的表現，後者使用了原位趨化細菌觀測資料。大量實驗證明，此方法能跨越不同空間情境，在不同維度、座標系、邊界條件和曲率下準確預測場動力學。", "audio": "audios/2505.16549v1.mp3", "timestamp": "2025-05-23T18:25:04.214820"}
{"query": "AI", "id": "2505.16866v1", "url": "http://arxiv.org/abs/2505.16866v1", "title": "Including the magnitude variability of a signal into the ordinal pattern analysis", "summary": "One of the most popular and innovative methods to analyse signals is by using\nOrdinal Patterns (OPs). The OP encoding is based on transforming a (univariate)\nsignal into a symbolic sequence of OPs, where each OP represents the number of\npermutations needed to order a small subset of the signal's magnitudes. This\nimplies that OPs are conceptually clear, methodologically simple to implement,\nrobust to noise, and can be applied to short signals. Moreover, they simplify\nthe statistical analyses that can be carried out on a signal, such as entropy\nand complexity quantifications. However, because of the relative ordering,\ninformation about the magnitude of the signal at each timestamp is lost -- this\nbeing one of the major drawbacks in the method. Here, we propose a way to use\nthe signal magnitudes discarded in the OP encoding as a complementary variable\nto its permutation entropy. To illustrate our approach, we analyse synthetic\ntrajectories from logistic and H{\\'e}non maps -- with and without added noise\n-- and intracranial electroencephalographic recordings from rats in different\nsleep-wake states. Our results show that, when complementing the permutation\nentropy with the variability in the signal magnitudes, the characterisation of\nthe dynamical behaviours of the maps and the sleep-wake states is improved.\nThis implies that our approach can be useful for feature engineering and\nimproving AI classifiers, where typical machine learning algorithms need\ncomplementary signal features as inputs to improve classification accuracy.", "authors": ["Melvyn Tyloo", "Joaquín González", "Nicolás Rubido"], "published_date": "2025-05-22", "title_zh": "將訊號幅度變異性納入序模式分析", "summary_zh": "序數模式(OPs)是熱門且創新的訊號分析方法，透過將單變量訊號轉換為OPs符號序列，以代表訊號幅度子集的排序排列數。OPs概念清晰、易於實現、具備抗噪性，且適用於短訊號，簡化了訊號的統計分析，如熵值和複雜度量化。然而，相對排序會遺失訊號幅度訊息。本研究提出一種方法，將OP編碼中捨棄的訊號幅度作為排列熵的互補變量。我們分析了邏輯和Hénon映射的合成軌跡（含噪與無噪），以及大鼠不同睡眠覺醒狀態下的顱內腦電記錄。結果表明，將訊號幅度變異性納入考量後，能有效改善映射動態行為和睡眠覺醒狀態的特性描述。此方法有助於特徵工程，並提升AI分類器的效能，透過提供互補訊號特徵，改善分類準確性。", "audio": "audios/2505.16866v1.mp3", "timestamp": "2025-05-23T19:14:31.672727"}
{"query": "Foundation Model", "id": "2505.16027v1", "url": "http://arxiv.org/abs/2505.16027v1", "title": "Benchmarking Chest X-ray Diagnosis Models Across Multinational Datasets", "summary": "Foundation models leveraging vision-language pretraining have shown promise\nin chest X-ray (CXR) interpretation, yet their real-world performance across\ndiverse populations and diagnostic tasks remains insufficiently evaluated. This\nstudy benchmarks the diagnostic performance and generalizability of foundation\nmodels versus traditional convolutional neural networks (CNNs) on multinational\nCXR datasets. We evaluated eight CXR diagnostic models - five vision-language\nfoundation models and three CNN-based architectures - across 37 standardized\nclassification tasks using six public datasets from the USA, Spain, India, and\nVietnam, and three private datasets from hospitals in China. Performance was\nassessed using AUROC, AUPRC, and other metrics across both shared and\ndataset-specific tasks. Foundation models outperformed CNNs in both accuracy\nand task coverage. MAVL, a model incorporating knowledge-enhanced prompts and\nstructured supervision, achieved the highest performance on public (mean AUROC:\n0.82; AUPRC: 0.32) and private (mean AUROC: 0.95; AUPRC: 0.89) datasets,\nranking first in 14 of 37 public and 3 of 4 private tasks. All models showed\nreduced performance on pediatric cases, with average AUROC dropping from 0.88\n+/- 0.18 in adults to 0.57 +/- 0.29 in children (p = 0.0202). These findings\nhighlight the value of structured supervision and prompt design in radiologic\nAI and suggest future directions including geographic expansion and ensemble\nmodeling for clinical deployment. Code for all evaluated models is available at\nhttps://drive.google.com/drive/folders/1B99yMQm7bB4h1sVMIBja0RfUu8gLktCE", "authors": ["Qinmei Xu", "Yiheng Li", "Xianghao Zhan", "Ahmet Gorkem Er", "Brittany Dashevsky", "Chuanjun Xu", "Mohammed Alawad", "Mengya Yang", "Liu Ya", "Changsheng Zhou", "Xiao Li", "Haruka Itakura", "Olivier Gevaert"], "published_date": "2025-05-21", "title_zh": "跨國資料集下胸腔X光診斷模型基準測試", "summary_zh": "本研究評估了視覺語言預訓練基礎模型在胸腔X光判讀上的診斷效能與泛化能力，並與傳統卷積神經網路在多國胸腔X光數據集上進行基準測試。我們評估了五個視覺語言基礎模型和三個卷積神經網路架構，涵蓋來自美國、西班牙、印度、越南的六個公共數據集，以及來自中國醫院的三個私有數據集，共37項標準化分類任務。評估指標包括AUROC和AUPRC。結果顯示，基礎模型在準確性和任務覆蓋率上均優於卷積神經網路。採用知識增強提示和結構化監督的MAVL模型在公共（平均AUROC：0.82；AUPRC：0.32）和私有（平均AUROC：0.95；AUPRC：0.89）數據集上表現最佳。所有模型在兒科病例上的表現均有所下降。研究結果突顯了結構化監督和提示設計在放射醫學人工智慧中的價值，並提出了地理擴展和集成建模等未來發展方向。所有評估模型的程式碼均已公開。", "audio": "audios/2505.16027v1.mp3", "timestamp": "2025-05-23T19:14:52.739259"}
{"query": "Diffusion Model", "id": "2505.16527v1", "url": "http://arxiv.org/abs/2505.16527v1", "title": "Joint Relational Database Generation via Graph-Conditional Diffusion Models", "summary": "Building generative models for relational databases (RDBs) is important for\napplications like privacy-preserving data release and augmenting real datasets.\nHowever, most prior work either focuses on single-table generation or relies on\nautoregressive factorizations that impose a fixed table order and generate\ntables sequentially. This approach limits parallelism, restricts flexibility in\ndownstream applications like missing value imputation, and compounds errors due\nto commonly made conditional independence assumptions. We propose a\nfundamentally different approach: jointly modeling all tables in an RDB without\nimposing any order. By using a natural graph representation of RDBs, we propose\nthe Graph-Conditional Relational Diffusion Model (GRDM). GRDM leverages a graph\nneural network to jointly denoise row attributes and capture complex\ninter-table dependencies. Extensive experiments on six real-world RDBs\ndemonstrate that our approach substantially outperforms autoregressive\nbaselines in modeling multi-hop inter-table correlations and achieves\nstate-of-the-art performance on single-table fidelity metrics.", "authors": ["Mohamed Amine Ketata", "David Lüdke", "Leo Schwinn", "Stephan Günnemann"], "published_date": "2025-05-22", "title_zh": "基於圖條件擴散模型的聯合關係資料庫生成", "summary_zh": "為關聯式資料庫構建生成模型對保護隱私的資料發布及擴充真實資料集至關重要。現有方法多專注於單表生成或依賴自迴歸分解，此法強加固定表順序並循序生成表，限制平行處理能力，降低缺失值填補等下游應用彈性，並因常見條件獨立性假設而累積誤差。本文提出一種根本不同的方法：聯合建模RDB中所有表，不強加任何順序。透過RDB的自然圖表示，我們提出圖條件關係擴散模型(GRDM)。GRDM利用圖神經網路聯合去噪行屬性並捕捉複雜的表間依賴關係。在六個真實RDB上的大量實驗表明，該方法在建模多跳表間相關性方面顯著優於自迴歸基準，並在單表保真度指標上實現了最先進的性能。", "audio": "audios/2505.16527v1.mp3", "timestamp": "2025-05-23T19:15:03.143490"}
{"query": "AI", "id": "2505.16821v1", "url": "http://arxiv.org/abs/2505.16821v1", "title": "LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols", "summary": "Integrating large AI models (LAMs) into 6G mobile networks promises to\nredefine protocol design and control-plane intelligence by enabling autonomous,\ncognitive network operations. While industry concepts, such as ETSI's\nExperiential Networked Intelligence (ENI), envision LAM-driven agents for\nadaptive network slicing and intent-based management, practical implementations\nstill face challenges in protocol literacy and real-world deployment. This\npaper presents an end-to-end demonstration of a LAM that generates\nstandards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as\npart of control-plane procedures inside a gNB. We treat RRC messaging as a\ndomain-specific language and fine-tune a decoder-only transformer model (LLaMA\nclass) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages\nlinearized to retain their ASN.1 syntactic structure before standard byte-pair\nencoding tokenization. This enables combinatorial generalization over RRC\nprotocol states while minimizing training overhead. On 30k field-test\nrequest-response pairs, our 8 B model achieves a median cosine similarity of\n0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a\nzero-shot LLaMA-3 8B baseline -- indicating substantially improved structural\nand semantic RRC fidelity. Overall, our results show that LAMs, when augmented\nwith Radio Access Network (RAN)-specific reasoning, can directly orchestrate\ncontrol-plane procedures, representing a stepping stone toward the AI-native\nair-interface paradigm. Beyond RRC emulation, this work lays the groundwork for\nfuture AI-native wireless standards.", "authors": ["Ziming liu", "Bryan Liu", "Alvaro Valcarce", "Xiaoli Chu"], "published_date": "2025-05-22", "title_zh": "基於大型語言模型的無線資源控制層模擬：邁向原生人工智慧無線接取網路協定", "summary_zh": "大型AI模型（LAM）整合至6G行動網路有望透過實現自主認知網路運營，重新定義協定設計與控制平面智慧。儘管ETSI的體驗網路智慧（ENI）等業界概念設想了由LAM驅動的代理程式，用於自適應網路切片與意圖導向管理，但實際部署仍面臨協定理解與現實挑戰。本研究展示了一個端對端LAM範例，該模型能夠在gNB內部產生符合標準、以ASN.1編碼的無線電資源控制（RRC）訊息，作為控制平面程序的一部分。我們將RRC訊息視為特定領域語言，並在RRC訊息上使用參數高效的低秩適應（LoRA）微調解碼器Transformer模型（LLaMA類別），RRC訊息在標準位元組配對編碼標記化之前經過線性化處理，以保留其ASN.1語法結構。這實現了對RRC協定狀態的組合泛化，同時最大限度地降低了訓練開銷。在3萬個現場測試請求-回應配對上，我們的80億參數模型在邊緣GPU上實現了與真實訊息0.97的中位餘弦相似度，相較於零樣本LLaMA-3 80億參數基線，相對提升了61%，表明結構與語義RRC保真度顯著提高。總體而言，我們的結果表明，LAM在增強無線電接入網路（RAN）特定推理能力後，可以直接編排控制平面程序，代表著朝向AI原生空口範式的墊腳石。除了RRC模擬，這項工作也為未來AI原生無線標準奠定了基礎。", "audio": "audios/2505.16821v1.mp3", "timestamp": "2025-05-23T20:20:26.704098"}
{"query": "Foundation Model", "id": "2505.15970v1", "url": "http://arxiv.org/abs/2505.15970v1", "title": "Analyzing Hierarchical Structure in Vision Models with Sparse Autoencoders", "summary": "The ImageNet hierarchy provides a structured taxonomy of object categories,\noffering a valuable lens through which to analyze the representations learned\nby deep vision models. In this work, we conduct a comprehensive analysis of how\nvision models encode the ImageNet hierarchy, leveraging Sparse Autoencoders\n(SAEs) to probe their internal representations. SAEs have been widely used as\nan explanation tool for large language models (LLMs), where they enable the\ndiscovery of semantically meaningful features. Here, we extend their use to\nvision models to investigate whether learned representations align with the\nontological structure defined by the ImageNet taxonomy. Our results show that\nSAEs uncover hierarchical relationships in model activations, revealing an\nimplicit encoding of taxonomic structure. We analyze the consistency of these\nrepresentations across different layers of the popular vision foundation model\nDINOv2 and provide insights into how deep vision models internalize\nhierarchical category information by increasing information in the class token\nthrough each layer. Our study establishes a framework for systematic\nhierarchical analysis of vision model representations and highlights the\npotential of SAEs as a tool for probing semantic structure in deep networks.", "authors": ["Matthew Lyle Olson", "Musashi Hinck", "Neale Ratzlaff", "Changbai Li", "Phillip Howard", "Vasudev Lal", "Shao-Yen Tseng"], "published_date": "2025-05-21", "title_zh": "使用稀疏自動編碼器分析視覺模型中的層級結構", "summary_zh": "ImageNet層級結構為物件類別提供結構化分類，有助於分析深度視覺模型學習到的表徵。本研究利用稀疏自動編碼器(SAE)探測視覺模型的內部表徵，深入分析模型如何編碼ImageNet層級結構。SAE已被廣泛應用於大型語言模型(LLM)，以發現語義上有意義的特徵。本研究將其應用擴展到視覺模型，以檢驗學習到的表徵是否與ImageNet分類法定義的本體結構一致。結果顯示，SAE揭示了模型激活中的層級關係，展現了分類結構的隱式編碼。我們分析了DINOv2不同層之間表徵的一致性，並闡明深度視覺模型如何透過在每一層增加類別令牌中的信息來內化層級類別信息。本研究建立了一個系統性層級分析視覺模型表徵的框架，並強調了SAE作為探測深度網路語義結構工具的潛力。", "audio": "audios/2505.15970v1.mp3", "timestamp": "2025-05-23T20:20:35.152302"}
{"query": "Diffusion Model", "id": "2505.16512v1", "url": "http://arxiv.org/abs/2505.16512v1", "title": "Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection", "summary": "In recent years, the rapid development of deepfake technology has given rise\nto an emerging and serious threat to public security: diffusion model-based\ndigital human generation. Unlike traditional face manipulation methods, such\nmodels can generate highly realistic videos with consistency through multimodal\ncontrol signals. Their flexibility and covertness pose severe challenges to\nexisting detection strategies. To bridge this gap, we introduce DigiFakeAV, the\nfirst large-scale multimodal digital human forgery dataset based on diffusion\nmodels. Employing five latest digital human generation methods (Sonic, Hallo,\netc.) and voice cloning method, we systematically produce a dataset comprising\n60,000 videos (8.4 million frames), covering multiple nationalities, skin\ntones, genders, and real-world scenarios, significantly enhancing data\ndiversity and realism. User studies show that the confusion rate between forged\nand real videos reaches 68%, and existing state-of-the-art (SOTA) detection\nmodels exhibit large drops in AUC values on DigiFakeAV, highlighting the\nchallenge of the dataset. To address this problem, we further propose\nDigiShield, a detection baseline based on spatiotemporal and cross-modal\nfusion. By jointly modeling the 3D spatiotemporal features of videos and the\nsemantic-acoustic features of audio, DigiShield achieves SOTA performance on\nboth the DigiFakeAV and DF-TIMIT datasets. Experiments show that this method\neffectively identifies covert artifacts through fine-grained analysis of the\ntemporal evolution of facial features in synthetic videos.", "authors": ["Jiaxin Liu", "Jia Wang", "Saihui Hou", "Min Ren", "Huijia Wu", "Zhaofeng He"], "published_date": "2025-05-22", "title_zh": "超越換臉：基於擴散模型之數位人類基準，用於多模態深度偽造檢測", "summary_zh": "近年來，深度偽造技術快速發展，基於擴散模型的數位人生成對公共安全構成新興且嚴重的威脅。此類模型透過多模態控制訊號生成高度逼真的連貫影片，其靈活性和隱蔽性對現有偵測策略構成嚴峻挑戰。為此，我們推出首個大規模多模態數位人偽造數據集DigiFakeAV。該數據集採用五種最新的數位人生成方法及聲音複製技術，系統性地生成包含6萬個影片（840萬幀），涵蓋多種國籍、膚色、性別和真實場景，顯著提升數據多樣性和真實性。使用者研究顯示，偽造影片與真實影片的混淆率達到68%，且現有最佳偵測模型在DigiFakeAV上的AUC值大幅下降，突顯了該數據集的挑戰性。為了解決此問題，我們進一步提出基於時空和跨模態融合的偵測基準DigiShield。透過聯合建模影片的3D時空特徵和音訊的語義聲學特徵，DigiShield在DigiFakeAV和DF-TIMIT數據集上均達到最佳性能。實驗表明，該方法透過對合成影片中面部特徵的時間演變進行細粒度分析，有效識別隱蔽的偽造痕跡。", "audio": "audios/2505.16512v1.mp3", "timestamp": "2025-05-23T20:20:48.939483"}
{"query": "AI", "id": "2505.16815v1", "url": "http://arxiv.org/abs/2505.16815v1", "title": "Perceptual Quality Assessment for Embodied AI", "summary": "Embodied AI has developed rapidly in recent years, but it is still mainly\ndeployed in laboratories, with various distortions in the Real-world limiting\nits application. Traditionally, Image Quality Assessment (IQA) methods are\napplied to predict human preferences for distorted images; however, there is no\nIQA method to assess the usability of an image in embodied tasks, namely, the\nperceptual quality for robots. To provide accurate and reliable quality\nindicators for future embodied scenarios, we first propose the topic: IQA for\nEmbodied AI. Specifically, we (1) based on the Mertonian system and\nmeta-cognitive theory, constructed a perception-cognition-decision-execution\npipeline and defined a comprehensive subjective score collection process; (2)\nestablished the Embodied-IQA database, containing over 36k reference/distorted\nimage pairs, with more than 5m fine-grained annotations provided by Vision\nLanguage Models/Vision Language Action-models/Real-world robots; (3) trained\nand validated the performance of mainstream IQA methods on Embodied-IQA,\ndemonstrating the need to develop more accurate quality indicators for Embodied\nAI. We sincerely hope that through evaluation, we can promote the application\nof Embodied AI under complex distortions in the Real-world. Project page:\nhttps://github.com/lcysyzxdxc/EmbodiedIQA", "authors": ["Chunyi Li", "Jiaohao Xiao", "Jianbo Zhang", "Farong Wen", "Zicheng Zhang", "Yuan Tian", "Xiangyang Zhu", "Xiaohong Liu", "Zhengxue Cheng", "Weisi Lin", "Guangtao Zhai"], "published_date": "2025-05-22", "title_zh": "具身人工智慧之感知品質評估", "summary_zh": "具身人工智慧近年發展迅速，但現實環境的限制阻礙其應用。傳統影像品質評估方法用於預測人對失真影像的偏好，然尚無評估影像於具身任務中可用性，即機器人感知品質之方法。為提供未來具身情境準確可靠的品質指標，本文首倡具身人工智慧之影像品質評估議題。具體而言，(1)基於默頓系統與後設認知理論，建構感知-認知-決策-執行管道，並定義一套全面的主觀評分收集流程；(2)建立具身影像品質評估資料庫，包含逾36,000組參考/失真影像對，以及超過500萬條由視覺語言模型/視覺語言動作模型/真實機器人提供的細粒度標註；(3)於具身影像品質評估資料庫上訓練並驗證主流影像品質評估方法之效能，表明開發更精準之具身人工智慧品質指標的需求。期望透過評估，促進具身人工智慧於複雜現實環境中之應用。專案頁面：https://github.com/lcysyzxdxc/EmbodiedIQA", "audio": "audios/2505.16815v1.mp3", "timestamp": "2025-05-23T21:15:15.129572"}
{"query": "Foundation Model", "id": "2505.15870v1", "url": "http://arxiv.org/abs/2505.15870v1", "title": "Satellites Reveal Mobility: A Commuting Origin-destination Flow Generator for Global Cities", "summary": "Commuting Origin-destination~(OD) flows, capturing daily population mobility\nof citizens, are vital for sustainable development across cities around the\nworld. However, it is challenging to obtain the data due to the high cost of\ntravel surveys and privacy concerns. Surprisingly, we find that satellite\nimagery, publicly available across the globe, contains rich urban semantic\nsignals to support high-quality OD flow generation, with over 98\\%\nexpressiveness of traditional multisource hard-to-collect urban\nsociodemographic, economics, land use, and point of interest data. This\ninspires us to design a novel data generator, GlODGen, which can generate OD\nflow data for any cities of interest around the world. Specifically, GlODGen\nfirst leverages Vision-Language Geo-Foundation Models to extract urban semantic\nsignals related to human mobility from satellite imagery. These features are\nthen combined with population data to form region-level representations, which\nare used to generate OD flows via graph diffusion models. Extensive experiments\non 4 continents and 6 representative cities show that GlODGen has great\ngeneralizability across diverse urban environments on different continents and\ncan generate OD flow data for global cities highly consistent with real-world\nmobility data. We implement GlODGen as an automated tool, seamlessly\nintegrating data acquisition and curation, urban semantic feature extraction,\nand OD flow generation together. It has been released at\nhttps://github.com/tsinghua-fib-lab/generate-od-pubtools.", "authors": ["Can Rong", "Xin Zhang", "Yanxin Xi", "Hongjie Sui", "Jingtao Ding", "Yong Li"], "published_date": "2025-05-21", "title_zh": "衛星揭示的移動性：全球城市通勤起訖流生成器", "summary_zh": "通勤起訖流量(OD)對城市永續發展至關重要，但獲取成本高昂且涉及隱私。研究發現，全球可得的衛星影像蘊含豐富的城市語義訊息，可高度表達傳統多源城市社會人口、經濟、土地利用和興趣點數據。因此，我們設計了GlODGen，一種新的數據生成器，能為任何城市生成OD流量數據。GlODGen首先利用視覺-語言地理基礎模型從衛星影像中提取與人口流動相關的城市語義訊息，然後將這些特徵與人口數據結合，形成區域級表示，再透過圖擴散模型生成OD流量。在四大洲六個代表性城市的大量實驗表明，GlODGen在不同城市環境中具有良好的泛化性，能為全球城市生成與真實世界移動數據高度一致的OD流量數據。GlODGen已作為自動化工具發布，整合了數據獲取與整理、城市語義特徵提取和OD流量生成。", "audio": "audios/2505.15870v1.mp3", "timestamp": "2025-05-23T21:15:21.764554"}
{"query": "Diffusion Model", "id": "2505.16474v1", "url": "http://arxiv.org/abs/2505.16474v1", "title": "Consistent World Models via Foresight Diffusion", "summary": "Diffusion and flow-based models have enabled significant progress in\ngeneration tasks across various modalities and have recently found applications\nin world modeling. However, unlike typical generation tasks that encourage\nsample diversity, world models entail different sources of uncertainty and\nrequire consistent samples aligned with the ground-truth trajectory, which is a\nlimitation we empirically observe in diffusion models. We argue that a key\nbottleneck in learning consistent diffusion-based world models lies in the\nsuboptimal predictive ability, which we attribute to the entanglement of\ncondition understanding and target denoising within shared architectures and\nco-training schemes. To address this, we propose Foresight Diffusion\n(ForeDiff), a diffusion-based world modeling framework that enhances\nconsistency by decoupling condition understanding from target denoising.\nForeDiff incorporates a separate deterministic predictive stream to process\nconditioning inputs independently of the denoising stream, and further\nleverages a pretrained predictor to extract informative representations that\nguide generation. Extensive experiments on robot video prediction and\nscientific spatiotemporal forecasting show that ForeDiff improves both\npredictive accuracy and sample consistency over strong baselines, offering a\npromising direction for diffusion-based world models.", "authors": ["Yu Zhang", "Xingzhuo Guo", "Haoran Xu", "Mingsheng Long"], "published_date": "2025-05-22", "title_zh": "基於前瞻擴散的一致性世界模型", "summary_zh": "擴散模型與流模型在多模態生成任務中取得顯著進展，並應用於世界建模。然而，世界模型不同於鼓勵樣本多樣性的典型生成任務，它需要與真實軌跡對齊的一致樣本，這在擴散模型中存在限制。我們認為，基於擴散的世界模型學習一致性的瓶頸在於次優的預測能力，這歸因於共享架構和共同訓練方案中條件理解與目標去噪的糾纏。為了解決此問題，我們提出Foresight Diffusion (ForeDiff)，一種基於擴散的世界建模框架，透過解耦條件理解與目標去噪來提高一致性。ForeDiff整合了獨立於去噪流的確定性預測流來處理條件輸入，並利用預訓練預測器提取資訊豐富的表示來引導生成。在機器人影片預測和科學時空預測的大量實驗表明，相較於強基準線，ForeDiff提高了預測準確性和樣本一致性，為基於擴散的世界模型提供了一個有希望的方向。", "audio": "audios/2505.16474v1.mp3", "timestamp": "2025-05-23T21:15:28.102998"}
{"query": "AI", "id": "2505.16809v1", "url": "http://arxiv.org/abs/2505.16809v1", "title": "Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor Segmentation with Missing Modalities", "summary": "Existing methods for multimodal MRI segmentation with missing modalities\ntypically assume that all MRI modalities are available during training.\nHowever, in clinical practice, some modalities may be missing due to the\nsequential nature of MRI acquisition, leading to performance degradation.\nFurthermore, retraining models to accommodate newly available modalities can be\ninefficient and may cause overfitting, potentially compromising previously\nlearned knowledge. To address these challenges, we propose Replay-based\nHypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation\nwith missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to\nenable the segmentation model to learn from newly acquired MRI modalities\nwithout forgetting previously learned information. To enhance segmentation\nperformance across diverse patient scenarios, we introduce the Cross-Patient\nHypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture\nhigh-order associations between patients. Additionally, we incorporate\nTversky-Aware Contrastive (TAC) loss to effectively mitigate information\nimbalance both across and within different modalities. Extensive experiments on\nthe BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art\nmethods, achieving an improvement of over 2\\% in the Dice Similarity\nCoefficient across various tumor regions. Our code is available at ReHyDIL.", "authors": ["Junze Wang", "Lei Fan", "Weipeng Jing", "Donglin Di", "Yang Song", "Sidong Liu", "Cong Cong"], "published_date": "2025-05-22", "title_zh": "超圖Tversky感知的領域增量學習用於缺失模態的腦腫瘤分割", "summary_zh": "現有多模態MRI分割方法在模態缺失情況下，通常假設訓練期間所有模態均可用。然而，臨床實踐中，由於MRI掃描的時序性，部分模態可能缺失，導致性能下降。重新訓練模型以適應新模態效率低下，並可能導致過擬合，損害先前學習的知識。為了解決這些問題，我們提出基於重播的超圖域增量學習(ReHyDIL)用於缺失模態的腦腫瘤分割。ReHyDIL利用域增量學習(DIL)，使分割模型能夠從新獲取的MRI模態中學習，同時不忘記先前學習的資訊。為了提高不同患者場景下的分割性能，我們引入跨患者超圖分割網路(CHSNet)，利用超圖捕捉患者之間的高階關聯。此外，我們採用Tversky感知對比(TAC)損失，有效減輕不同模態之間和之內的信息不平衡。在BraTS2019數據集上的大量實驗表明，ReHyDIL優於現有方法，在各種腫瘤區域的Dice相似係數上提高了2%以上。", "audio": "audios/2505.16809v1.mp3", "timestamp": "2025-05-23T22:18:12.423329"}
{"query": "Foundation Model", "id": "2505.15868v1", "url": "http://arxiv.org/abs/2505.15868v1", "title": "An Inclusive Foundation Model for Generalizable Cytogenetics in Precision Oncology", "summary": "Chromosome analysis is vital for diagnosing genetic disorders and guiding\ncancer therapy decisions through the identification of somatic clonal\naberrations. However, developing an AI model are hindered by the overwhelming\ncomplexity and diversity of chromosomal abnormalities, requiring extensive\nannotation efforts, while automated methods remain task-specific and lack\ngeneralizability due to the scarcity of comprehensive datasets spanning diverse\nresource conditions. Here, we introduce CHROMA, a foundation model for\ncytogenomics, designed to overcome these challenges by learning generalizable\nrepresentations of chromosomal abnormalities. Pre-trained on over 84,000\nspecimens (~4 million chromosomal images) via self-supervised learning, CHROMA\noutperforms other methods across all types of abnormalities, even when trained\non fewer labelled data and more imbalanced datasets. By facilitating\ncomprehensive mapping of instability and clonal leisons across various\naberration types, CHROMA offers a scalable and generalizable solution for\nreliable and automated clinical analysis, reducing the annotation workload for\nexperts and advancing precision oncology through the early detection of rare\ngenomic abnormalities, enabling broad clinical AI applications and making\nadvanced genomic analysis more accessible.", "authors": ["Changchun Yang", "Weiqian Dai", "Yilan Zhang", "Siyuan Chen", "Jingdong Hu", "Junkai Su", "Yuxuan Chen", "Ao Xu", "Na Li", "Xin Gao", "Yongguo Yu"], "published_date": "2025-05-21", "title_zh": "精準腫瘤學中具泛化能力的細胞遺傳學通用包容性基礎模型", "summary_zh": "染色體分析對診斷遺傳疾病和指導癌症治療至關重要，但染色體異常的複雜性阻礙了AI模型的發展。本文提出細胞遺傳學基礎模型CHROMA，通過自監督學習在超過84,000個樣本（約400萬張染色體圖像）上進行預訓練，克服了這些挑戰。CHROMA在各種類型的異常中均優於其他方法，即使在較少的標記數據和更不平衡的數據集上訓練也是如此。CHROMA促進了各種異常類型中不穩定性和克隆病變的全面繪製，提供了一個可擴展和通用的解決方案，實現可靠且自動化的臨床分析，減少專家的註釋工作量，並通過早期檢測罕見基因組異常來推進精準腫瘤學，實現廣泛的臨床AI應用，並使先進的基因組分析更易於使用。", "audio": "audios/2505.15868v1.mp3", "timestamp": "2025-05-23T22:18:17.650564"}
{"query": "Diffusion Model", "id": "2505.16456v1", "url": "http://arxiv.org/abs/2505.16456v1", "title": "MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM", "summary": "Recent advances in static 3D generation have intensified the demand for\nphysically consistent dynamic 3D content. However, existing video generation\nmodels, including diffusion-based methods, often prioritize visual realism\nwhile neglecting physical plausibility, resulting in implausible object\ndynamics. Prior approaches for physics-aware dynamic generation typically rely\non large-scale annotated datasets or extensive model fine-tuning, which imposes\nsignificant computational and data collection burdens and limits scalability\nacross scenarios. To address these challenges, we present MAGIC, a\ntraining-free framework for single-image physical property inference and\ndynamic generation, integrating pretrained image-to-video diffusion models with\niterative LLM-based reasoning. Our framework generates motion-rich videos from\na static image and closes the visual-to-physical gap through a\nconfidence-driven LLM feedback loop that adaptively steers the diffusion model\ntoward physics-relevant motion. To translate visual dynamics into controllable\nphysical behavior, we further introduce a differentiable MPM simulator\noperating directly on 3D Gaussians reconstructed from the single image,\nenabling physically grounded, simulation-ready outputs without any supervision\nor model tuning. Experiments show that MAGIC outperforms existing physics-aware\ngenerative methods in inference accuracy and achieves greater temporal\ncoherence than state-of-the-art video diffusion models.", "authors": ["Siwei Meng", "Yawei Luo", "Ping Liu"], "published_date": "2025-05-22", "title_zh": "MAGIC：基於置信度引導的大型語言模型之運動感知生成推論", "summary_zh": "靜態3D生成技術的進步提升了對具備物理一致性動態3D內容的需求。現有影片生成模型，包括基於擴散的方法，往往側重視覺真實感而忽略物理合理性，導致不合理的物體動態。先前的物理感知動態生成方法通常依賴大規模標註資料集或大量模型微調，造成沉重的計算和資料收集負擔，並限制了跨場景的可擴展性。為了解決這些問題，我們提出MAGIC，一個免訓練的單圖像物理屬性推斷與動態生成框架，整合了預訓練的圖像轉影片擴散模型與基於LLM的迭代推理。我們的框架從靜態圖像生成富含動作的影片，並透過基於置信度的LLM回饋迴路彌合視覺與物理間的差距，自適應地引導擴散模型朝向物理相關的運動。為了將視覺動態轉化為可控制的物理行為，我們進一步引入了一個可微分的MPM模擬器，直接作用於從單一圖像重建的3D高斯模型，實現物理基礎的、可直接用於模擬的輸出，無需任何監督或模型調整。實驗表明，MAGIC在推斷準確性方面優於現有的物理感知生成方法，並且比最先進的影片擴散模型實現了更高的時間連貫性。", "audio": "audios/2505.16456v1.mp3", "timestamp": "2025-05-23T22:18:25.299795"}
{"query": "AI", "id": "2505.16792v1", "url": "http://arxiv.org/abs/2505.16792v1", "title": "REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training", "summary": "Diffusion Transformers (DiTs) deliver state-of-the-art image quality, yet\ntheir training remains notoriously slow. A recent remedy -- representation\nalignment (REPA) that matches DiT hidden features to those of a non-generative\nteacher (e.g. DINO) -- dramatically accelerates the early epochs but plateaus\nor even degrades performance later. We trace this failure to a capacity\nmismatch: once the generative student begins modelling the joint data\ndistribution, the teacher's lower-dimensional embeddings and attention patterns\nbecome a straitjacket rather than a guide. We then introduce HASTE (Holistic\nAlignment with Stage-wise Termination for Efficient training), a two-phase\nschedule that keeps the help and drops the hindrance. Phase I applies a\nholistic alignment loss that simultaneously distills attention maps (relational\npriors) and feature projections (semantic anchors) from the teacher into\nmid-level layers of the DiT, yielding rapid convergence. Phase II then performs\none-shot termination that deactivates the alignment loss, once a simple trigger\nsuch as a fixed iteration is hit, freeing the DiT to focus on denoising and\nexploit its generative capacity. HASTE speeds up training of diverse DiTs\nwithout architecture changes. On ImageNet 256X256, it reaches the vanilla\nSiT-XL/2 baseline FID in 50 epochs and matches REPA's best FID in 500 epochs,\namounting to a 28X reduction in optimization steps. HASTE also improves\ntext-to-image DiTs on MS-COCO, demonstrating to be a simple yet principled\nrecipe for efficient diffusion training across various tasks. Our code is\navailable at https://github.com/NUS-HPC-AI-Lab/HASTE .", "authors": ["Ziqiao Wang", "Wangbo Zhao", "Yuhao Zhou", "Zekai Li", "Zhiyuan Liang", "Mingjia Shi", "Xuanlei Zhao", "Pengfei Zhou", "Kaipeng Zhang", "Zhangyang Wang", "Kai Wang", "Yang You"], "published_date": "2025-05-22", "title_zh": "REPA失效之時：提早停止的整體對齊增強擴散訓練", "summary_zh": "擴散變換器(DiT)具備頂尖影像品質，但訓練速度緩慢。近期方法，表徵對齊(REPA)，透過將DiT隱藏特徵與非生成教師模型(如DINO)對齊，雖加速初期訓練，後期效果停滯甚至退化。此乃因容量不匹配：生成模型開始學習聯合資料分布時，教師模型的低維嵌入和注意力模式反而成為限制。為此，引入HASTE(高效訓練的分階段終止整體對齊)，一種兩階段策略。第一階段採用整體對齊損失，同時將教師模型的注意力圖(關係先驗)和特徵投影(語義錨點)提煉至DiT的中間層，實現快速收斂。第二階段，一旦滿足預設條件，立即停用對齊損失，使DiT專注於去噪並發揮生成能力。HASTE無需修改架構即可加速多種DiT訓練。在ImageNet 256X256上，HASTE在50個epoch內達到SiT-XL/2基準FID，並在500個epoch內達到REPA的最佳FID，優化步數減少28倍。HASTE亦改善MS-COCO上的文本生成圖像DiT，證明其為一種簡潔且有效的擴散模型訓練方法，適用於多種任務。", "audio": "audios/2505.16792v1.mp3", "timestamp": "2025-05-23T23:17:35.353008"}
{"query": "Foundation Model", "id": "2505.14938v1", "url": "http://arxiv.org/abs/2505.14938v1", "title": "Scan, Materialize, Simulate: A Generalizable Framework for Physically Grounded Robot Planning", "summary": "Autonomous robots must reason about the physical consequences of their\nactions to operate effectively in unstructured, real-world environments. We\npresent Scan, Materialize, Simulate (SMS), a unified framework that combines 3D\nGaussian Splatting for accurate scene reconstruction, visual foundation models\nfor semantic segmentation, vision-language models for material property\ninference, and physics simulation for reliable prediction of action outcomes.\nBy integrating these components, SMS enables generalizable physical reasoning\nand object-centric planning without the need to re-learn foundational physical\ndynamics. We empirically validate SMS in a billiards-inspired manipulation task\nand a challenging quadrotor landing scenario, demonstrating robust performance\non both simulated domain transfer and real-world experiments. Our results\nhighlight the potential of bridging differentiable rendering for scene\nreconstruction, foundation models for semantic understanding, and physics-based\nsimulation to achieve physically grounded robot planning across diverse\nsettings.", "authors": ["Amine Elhafsi", "Daniel Morton", "Marco Pavone"], "published_date": "2025-05-20", "title_zh": "掃描、實體化、模擬：一個具身機器人規劃的通用化框架", "summary_zh": "自主機器人需推論動作的物理後果，方能在真實環境中有效運作。本研究提出掃描、實體化、模擬(SMS)框架，整合3D高斯濺射以精確重建場景、視覺基礎模型進行語義分割、視覺語言模型推斷材料屬性，以及物理模擬以可靠預測動作結果。藉由整合這些組件，SMS無需重新學習基礎物理動力學，即可實現可泛化的物理推理和以物體為中心的規劃。實驗於撞球操控任務和四旋翼著陸場景驗證SMS，展現模擬領域轉移和真實世界實驗中的穩健效能。研究結果突顯了可微分渲染、基礎模型和物理模擬結合在場景重建、語義理解及實現具物理基礎的機器人規劃的潛力。", "audio": "audios/2505.14938v1.mp3", "timestamp": "2025-05-23T23:17:43.720969"}
{"query": "Diffusion Model", "id": "2505.16365v1", "url": "http://arxiv.org/abs/2505.16365v1", "title": "A collaborative constrained graph diffusion model for the generation of realistic synthetic molecules", "summary": "Developing new molecular compounds is crucial to address pressing challenges,\nfrom health to environmental sustainability. However, exploring the molecular\nspace to discover new molecules is difficult due to the vastness of the space.\nHere we introduce CoCoGraph, a collaborative and constrained graph diffusion\nmodel capable of generating molecules that are guaranteed to be chemically\nvalid. Thanks to the constraints built into the model and to the collaborative\nmechanism, CoCoGraph outperforms state-of-the-art approaches on standard\nbenchmarks while requiring up to an order of magnitude fewer parameters.\nAnalysis of 36 chemical properties also demonstrates that CoCoGraph generates\nmolecules with distributions more closely matching real molecules than current\nmodels. Leveraging the model's efficiency, we created a database of 8.2M\nmillion synthetically generated molecules and conducted a Turing-like test with\norganic chemistry experts to further assess the plausibility of the generated\nmolecules, and potential biases and limitations of CoCoGraph.", "authors": ["Manuel Ruiz-Botella", "Marta Sales-Pardo", "Roger Guimerà"], "published_date": "2025-05-22", "title_zh": "協作約束圖擴散模型用於生成逼真合成分子", "summary_zh": "開發新型分子化合物對於應對健康和環境永續性等嚴峻挑戰至關重要。由於分子空間廣闊，探索並發現新分子極具挑戰性。本文介紹CoCoGraph，一種協作且受限的圖擴散模型，能生成保證化學有效性的分子。CoCoGraph憑藉其內建約束和協作機制，在標準基準測試中優於現有技術，且參數需求減少近一個數量級。對36種化學性質的分析表明，CoCoGraph生成的分子分佈比現有模型更接近真實分子。藉助模型的高效率，我們創建了一個包含820萬個合成生成分子的資料庫，並與有機化學專家進行了類似圖靈測試的實驗，以進一步評估生成分子的合理性，以及CoCoGraph的潛在偏差和局限性。", "audio": "audios/2505.16365v1.mp3", "timestamp": "2025-05-23T23:17:50.067419"}
{"query": "AI", "id": "2505.16771v1", "url": "http://arxiv.org/abs/2505.16771v1", "title": "Data-Driven Breakthroughs and Future Directions in AI Infrastructure: A Comprehensive Review", "summary": "This paper presents a comprehensive synthesis of major breakthroughs in\nartificial intelligence (AI) over the past fifteen years, integrating\nhistorical, theoretical, and technological perspectives. It identifies key\ninflection points in AI' s evolution by tracing the convergence of\ncomputational resources, data access, and algorithmic innovation. The analysis\nhighlights how researchers enabled GPU based model training, triggered a data\ncentric shift with ImageNet, simplified architectures through the Transformer,\nand expanded modeling capabilities with the GPT series. Rather than treating\nthese advances as isolated milestones, the paper frames them as indicators of\ndeeper paradigm shifts. By applying concepts from statistical learning theory\nsuch as sample complexity and data efficiency, the paper explains how\nresearchers translated breakthroughs into scalable solutions and why the field\nmust now embrace data centric approaches. In response to rising privacy\nconcerns and tightening regulations, the paper evaluates emerging solutions\nlike federated learning, privacy enhancing technologies (PETs), and the data\nsite paradigm, which reframe data access and security. In cases where real\nworld data remains inaccessible, the paper also assesses the utility and\nconstraints of mock and synthetic data generation. By aligning technical\ninsights with evolving data infrastructure, this study offers strategic\nguidance for future AI research and policy development.", "authors": ["Beyazit Bestami Yuksel", "Ayse Yilmazer Metin"], "published_date": "2025-05-22", "title_zh": "人工智慧基礎設施中資料驅動的突破與未來方向：一份綜合性回顧", "summary_zh": "本文綜述過去十五年人工智慧領域的重大突破，整合歷史、理論與技術觀點。分析計算資源、數據獲取與演算法創新如何匯聚，標誌AI發展的關鍵轉折點，如基於GPU的模型訓練、ImageNet引領的數據中心轉移、Transformer簡化架構以及GPT系列擴展建模能力。論文將這些進展視為更深層次典範轉移的指標，並運用統計學習理論解釋其如何轉化為可擴展的解決方案，強調數據中心方法的重要性。針對隱私問題和日趨嚴格的監管，本文評估了聯邦學習、隱私增強技術及數據站點範例等新興方案，並探討在真實數據不可獲取時，模擬與合成數據生成的效用與局限性。透過結合技術洞見與不斷發展的數據基礎設施，本研究為未來AI研究與政策制定提供策略指導。", "audio": "audios/2505.16771v1.mp3", "timestamp": "2025-05-24T01:23:12.944049"}
{"query": "Foundation Model", "id": "2505.14933v1", "url": "http://arxiv.org/abs/2505.14933v1", "title": "Foundations of Unknown-aware Machine Learning", "summary": "Ensuring the reliability and safety of machine learning models in open-world\ndeployment is a central challenge in AI safety. This thesis develops both\nalgorithmic and theoretical foundations to address key reliability issues\narising from distributional uncertainty and unknown classes, from standard\nneural networks to modern foundation models like large language models (LLMs).\n  Traditional learning paradigms, such as empirical risk minimization (ERM),\nassume no distribution shift between training and inference, often leading to\noverconfident predictions on out-of-distribution (OOD) inputs. This thesis\nintroduces novel frameworks that jointly optimize for in-distribution accuracy\nand reliability to unseen data. A core contribution is the development of an\nunknown-aware learning framework that enables models to recognize and handle\nnovel inputs without labeled OOD data.\n  We propose new outlier synthesis methods, VOS, NPOS, and DREAM-OOD, to\ngenerate informative unknowns during training. Building on this, we present\nSAL, a theoretical and algorithmic framework that leverages unlabeled\nin-the-wild data to enhance OOD detection under realistic deployment\nconditions. These methods demonstrate that abundant unlabeled data can be\nharnessed to recognize and adapt to unforeseen inputs, providing formal\nreliability guarantees.\n  The thesis also extends reliable learning to foundation models. We develop\nHaloScope for hallucination detection in LLMs, MLLMGuard for defending against\nmalicious prompts in multimodal models, and data cleaning methods to denoise\nhuman feedback used for better alignment. These tools target failure modes that\nthreaten the safety of large-scale models in deployment.\n  Overall, these contributions promote unknown-aware learning as a new\nparadigm, and we hope it can advance the reliability of AI systems with minimal\nhuman efforts.", "authors": ["Xuefeng Du"], "published_date": "2025-05-20", "title_zh": "未知感知機器學習基礎", "summary_zh": "本論文旨在解決開放世界中機器學習模型可靠性與安全性問題，著重於分布不確定性與未知類別所引發的挑戰。透過演算法與理論基礎，從傳統神經網路到大型語言模型等基礎模型，提升模型在未見數據上的泛化能力。\n\n論文提出新穎框架，優化模型在分布內準確度與分布外可靠性。核心貢獻為開發一種未知感知學習框架，使模型能夠識別並處理新穎輸入，無需標記的分布外數據。\n\n論文提出異常值合成方法VOS、NPOS與DREAM-OOD，在訓練期間產生資訊豐富的未知樣本。基於此，提出SAL框架，利用未標記的真實數據增強分布外檢測，並提供形式化的可靠性保證。\n\n論文亦將可靠學習擴展至基礎模型，開發HaloScope用於檢測大型語言模型中的幻覺，MLLMGuard用於防禦多模態模型中的惡意提示，以及數據清洗方法淨化人類回饋，以實現更好的模型對齊。\n\n總體而言，論文提倡未知感知學習作為一種新範式，旨在提升人工智慧系統的可靠性，並減少對人工的依賴。", "audio": "audios/2505.14933v1.mp3", "timestamp": "2025-05-24T01:23:22.509180"}
{"query": "Diffusion Model", "id": "2505.16335v1", "url": "http://arxiv.org/abs/2505.16335v1", "title": "FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design", "summary": "Visual autoregressive (VAR) modeling has marked a paradigm shift in image\ngeneration from next-token prediction to next-scale prediction. VAR predicts a\nset of tokens at each step from coarse to fine scale, leading to better image\nquality and faster inference speed compared to existing diffusion models.\nHowever, the large parameter size and computation cost hinder its deployment on\nedge devices. To reduce the memory and computation cost, we propose FPQVAR, an\nefficient post-training floating-point (FP) quantization framework for VAR\nfeaturing algorithm and hardware co-design. At the algorithm level, we first\nidentify the challenges of quantizing VAR. To address them, we propose Dual\nFormat Quantization for the highly imbalanced input activation. We further\npropose Group-wise Hadamard Transformation and GHT-Aware Learnable\nTransformation to address the time-varying outlier channels. At the hardware\nlevel, we design the first low-bit FP quantizer and multiplier with lookup\ntables on FPGA and propose the first FPGA-based VAR accelerator featuring\nlow-bit FP computation and an elaborate two-level pipeline. Extensive\nexperiments show that compared to the state-of-the-art quantization method, our\nproposed FPQVAR significantly improves Fr\\'echet Inception Distance (FID) from\n10.83 to 3.58, Inception Score (IS) from 175.9 to 241.5 under 4-bit\nquantization. FPQVAR also significantly improves the performance of 6-bit\nquantized VAR, bringing it on par with the FP16 model. Our accelerator on\nAMD-Xilinx VCK190 FPGA achieves a throughput of 1.1 image/s, which is 3.1x\nhigher than the integer-based accelerator. It also demonstrates 3.6x and 2.8x\nhigher energy efficiency compared to the integer-based accelerator and GPU\nbaseline, respectively.", "authors": ["Renjie Wei", "Songqiang Xu", "Qingyu Guo", "Meng Li"], "published_date": "2025-05-22", "title_zh": "FPQVAR：基於FPGA硬體協同設計的視覺自迴歸模型浮點量化", "summary_zh": "視覺自迴歸模型(VAR)將圖像生成從下一個標記預測轉變為下一個尺度預測。相較於現有擴散模型，VAR透過由粗到精尺度預測標記集合，提升圖像品質並加速推論。然而，龐大的參數規模與計算成本限制了其在邊緣裝置上的部署。為降低記憶體與計算成本，本文提出FPQVAR，一個高效的VAR後訓練浮點(FP)量化框架，著重於算法與硬體協同設計。算法層面，針對VAR量化的挑戰，提出雙格式量化以處理高度不平衡的輸入激活，並提出分組哈達瑪轉換和GHT感知可學習轉換以解決時變離群通道。硬體層面，設計首個基於FPGA的低位元FP量化器與乘法器，並提出首個基於FPGA的VAR加速器，其特色為低位元FP計算和精細的雙層流水線。實驗結果顯示，相較於最先進的量化方法，在4位元量化下，FPQVAR顯著提升Fréchet Inception Distance (FID)自10.83至3.58，Inception Score (IS)自175.9至241.5。FPQVAR亦顯著提升6位元量化VAR效能，使其與FP16模型相當。在AMD-Xilinx VCK190 FPGA上的加速器實現每秒1.1張圖像的吞吐量，較基於整數的加速器提高3.1倍，且能源效率分別較基於整數的加速器和GPU基準提高3.6倍和2.8倍。", "audio": "audios/2505.16335v1.mp3", "timestamp": "2025-05-24T01:23:32.812374"}
{"query": "AI", "id": "2505.16763v1", "url": "http://arxiv.org/abs/2505.16763v1", "title": "Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation", "summary": "Text-to-image models are powerful for producing high-quality images based on\ngiven text prompts, but crafting these prompts often requires specialized\nvocabulary. To address this, existing methods train rewriting models with\nsupervision from large amounts of manually annotated data and trained aesthetic\nassessment models. To alleviate the dependence on data scale for model training\nand the biases introduced by trained models, we propose a novel prompt\noptimization framework, designed to rephrase a simple user prompt into a\nsophisticated prompt to a text-to-image model. Specifically, we employ the\nlarge vision language models (LVLMs) as the solver to rewrite the user prompt,\nand concurrently, employ LVLMs as a reward model to score the aesthetics and\nalignment of the images generated by the optimized prompt. Instead of laborious\nhuman feedback, we exploit the prior knowledge of the LVLM to provide rewards,\ni.e., AI feedback. Simultaneously, the solver and the reward model are unified\ninto one model and iterated in reinforcement learning to achieve\nself-improvement by giving a solution and judging itself. Results on two\npopular datasets demonstrate that our method outperforms other strong\ncompetitors.", "authors": ["Hongji Yang", "Yucheng Zhou", "Wencheng Han", "Jianbing Shen"], "published_date": "2025-05-22", "title_zh": "用於文本到圖像生成中提示詞優化的自獎勵大型視覺語言模型", "summary_zh": "基於文字生成圖像的模型強大，但提示詞編寫常需專業詞彙。現有方法仰賴大量人工標註數據及美學評估模型訓練重寫模型，為減少對數據規模的依賴及模型偏見，本文提出一種新穎的提示詞優化框架，將簡單的使用者提示詞改寫為精確的提示詞。具體而言，我們採用大型視覺語言模型（LVLM）作為求解器重寫提示詞，同時將其作為獎勵模型，評估優化後提示詞生成圖像的美學與對齊。我們利用LVLM的先驗知識提供AI回饋作為獎勵，取代耗時的人工回饋。求解器和獎勵模型統一為單一模型，透過強化學習迭代，藉由自我求解和評估實現自我提升。在兩個常用資料集上的結果表明，本文方法優於其他強大的競爭對手。", "audio": "audios/2505.16763v1.mp3", "timestamp": "2025-05-24T03:04:58.436462"}
{"query": "Foundation Model", "id": "2505.14766v1", "url": "http://arxiv.org/abs/2505.14766v1", "title": "This Time is Different: An Observability Perspective on Time Series Foundation Models", "summary": "We introduce Toto, a time series forecasting foundation model with 151\nmillion parameters. Toto uses a modern decoder-only architecture coupled with\narchitectural innovations designed to account for specific challenges found in\nmultivariate observability time series data. Toto's pre-training corpus is a\nmixture of observability data, open datasets, and synthetic data, and is\n4-10$\\times$ larger than those of leading time series foundation models.\nAdditionally, we introduce BOOM, a large-scale benchmark consisting of 350\nmillion observations across 2,807 real-world time series. For both Toto and\nBOOM, we source observability data exclusively from Datadog's own telemetry and\ninternal observability metrics. Extensive evaluations demonstrate that Toto\nachieves state-of-the-art performance on both BOOM and on established general\npurpose time series forecasting benchmarks. Toto's model weights, inference\ncode, and evaluation scripts, as well as BOOM's data and evaluation code, are\nall available as open source under the Apache 2.0 License available at\nhttps://huggingface.co/Datadog/Toto-Open-Base-1.0 and\nhttps://github.com/DataDog/toto.", "authors": ["Ben Cohen", "Emaad Khwaja", "Youssef Doubli", "Salahidine Lemaachi", "Chris Lettieri", "Charles Masson", "Hugo Miccinilli", "Elise Ramé", "Qiqi Ren", "Afshin Rostamizadeh", "Jean Ogier du Terrail", "Anna-Monica Toon", "Kan Wang", "Stephan Xie", "David Asker", "Ameet Talwalkar", "Othmane Abou-Amal"], "published_date": "2025-05-20", "title_zh": "這次不一樣：時間序列基礎模型的可觀測性視角", "summary_zh": "Toto是一個具備1.51億參數的時間序列預測基礎模型，採用現代解碼器架構，並針對多元可觀測性時間序列數據的獨特挑戰進行架構創新。Toto的預訓練語料庫混合了可觀測性數據、開放數據集和合成數據，規模較領先的時間序列基礎模型大4-10倍。同時，我們推出了BOOM，一個包含2,807個真實世界時間序列、橫跨3.5億觀測值的大規模基準測試。Toto和BOOM的可觀測性數據均來自Datadog的遙測和內部可觀測性指標。廣泛評估顯示，Toto在BOOM和既有的通用時間序列預測基準測試上均達到最先進的性能。Toto的模型權重、推論代碼、評估腳本以及BOOM的數據和評估代碼，均以Apache 2.0許可證開源。", "audio": "audios/2505.14766v1.mp3", "timestamp": "2025-05-24T03:05:04.566531"}
{"query": "Diffusion Model", "id": "2505.16324v1", "url": "http://arxiv.org/abs/2505.16324v1", "title": "TensorAR: Refinement is All You Need in Autoregressive Image Generation", "summary": "Autoregressive (AR) image generators offer a language-model-friendly approach\nto image generation by predicting discrete image tokens in a causal sequence.\nHowever, unlike diffusion models, AR models lack a mechanism to refine previous\npredictions, limiting their generation quality. In this paper, we introduce\nTensorAR, a new AR paradigm that reformulates image generation from next-token\nprediction to next-tensor prediction. By generating overlapping windows of\nimage patches (tensors) in a sliding fashion, TensorAR enables iterative\nrefinement of previously generated content. To prevent information leakage\nduring training, we propose a discrete tensor noising scheme, which perturbs\ninput tokens via codebook-indexed noise. TensorAR is implemented as a\nplug-and-play module compatible with existing AR models. Extensive experiments\non LlamaGEN, Open-MAGVIT2, and RAR demonstrate that TensorAR significantly\nimproves the generation performance of autoregressive models.", "authors": ["Cheng Cheng", "Lin Song", "Yicheng Xiao", "Yuxin Chen", "Xuchong Zhang", "Hongbin Sun", "Ying Shan"], "published_date": "2025-05-22", "title_zh": "TensorAR：自回歸圖像生成中，精煉即為關鍵", "summary_zh": "自迴歸影像生成器透過因果序列預測離散影像符記，提供了一種語言模型友善的影像生成方法。然而，與擴散模型不同，自迴歸模型缺乏細化先前預測的機制，限制了其生成品質。本研究提出TensorAR，一種新的自迴歸範式，將影像生成從下一個符記預測重新構建為下一個張量預測。透過滑動方式生成重疊的影像區塊視窗（張量），TensorAR實現了對先前生成內容的迭代細化。為防止訓練期間的資訊洩漏，我們提出了一種離散張量雜訊方案，透過編碼簿索引雜訊擾動輸入符記。TensorAR可作為隨插即用模組，與現有自迴歸模型相容。在LlamaGEN、Open-MAGVIT2和RAR上的大量實驗表明，TensorAR顯著提高了自迴歸模型的生成效能。", "audio": "audios/2505.16324v1.mp3", "timestamp": "2025-05-24T03:05:11.336535"}
{"query": "AI", "id": "2505.16709v1", "url": "http://arxiv.org/abs/2505.16709v1", "title": "SEDD-PCC: A Single Encoder-Dual Decoder Framework For End-To-End Learned Point Cloud Compression", "summary": "To encode point clouds containing both geometry and attributes, most\nlearning-based compression schemes treat geometry and attribute coding\nseparately, employing distinct encoders and decoders. This not only increases\ncomputational complexity but also fails to fully exploit shared features\nbetween geometry and attributes. To address this limitation, we propose\nSEDD-PCC, an end-to-end learning-based framework for lossy point cloud\ncompression that jointly compresses geometry and attributes. SEDD-PCC employs a\nsingle encoder to extract shared geometric and attribute features into a\nunified latent space, followed by dual specialized decoders that sequentially\nreconstruct geometry and attributes. Additionally, we incorporate knowledge\ndistillation to enhance feature representation learning from a teacher model,\nfurther improving coding efficiency. With its simple yet effective design,\nSEDD-PCC provides an efficient and practical solution for point cloud\ncompression. Comparative evaluations against both rule-based and learning-based\nmethods demonstrate its competitive performance, highlighting SEDD-PCC as a\npromising AI-driven compression approach.", "authors": ["Kai Hsiang Hsieh", "Monyneath Yim", "Jui Chiu Chiang"], "published_date": "2025-05-22", "title_zh": "SEDD-PCC：用於端到端學習點雲壓縮的單編碼器-雙解碼器架構", "summary_zh": "針對含幾何與屬性的點雲編碼，現有基於學習的壓縮方案多將幾何與屬性分開編碼，採用獨立編碼器與解碼器，導致計算複雜度增加且未能充分利用幾何與屬性間的共享特徵。為解決此問題，本文提出SEDDPCC，一種端到端基於學習的有損點雲壓縮框架，可聯合壓縮幾何與屬性。SEDDPCC採用單一編碼器提取共享的幾何與屬性特徵至統一潛在空間，再由雙重專業解碼器依序重建幾何與屬性。此外，加入知識蒸餾以增強特徵表示學習，進一步提升編碼效率。SEDDPCC設計簡潔有效，為點雲壓縮提供高效實用的解決方案。與規則式及基於學習的方法相比，實驗結果顯示其具備競爭力，證實SEDDPCC是一種具潛力的人工智慧驅動壓縮方法。", "audio": "audios/2505.16709v1.mp3", "timestamp": "2025-05-24T04:20:53.122964"}
{"query": "Foundation Model", "id": "2505.14100v2", "url": "http://arxiv.org/abs/2505.14100v2", "title": "Unlocking the Power of SAM 2 for Few-Shot Segmentation", "summary": "Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few\nclasses to segment arbitrary classes, but at the risk of overfitting. To\naddress this, some methods use the well-learned knowledge of foundation models\n(e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM\nby supporting video segmentation, whose class-agnostic matching ability is\nuseful to FSS. A simple idea is to encode support foreground (FG) features as\nmemory, with which query FG features are matched and fused. Unfortunately, the\nFG objects in different frames of SAM 2's video data are always the same\nidentity, while those in FSS are different identities, i.e., the matching step\nis incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo\nquery memory, matching with query features in a compatible way. However, the\nmemories can never be as accurate as the real ones, i.e., they are likely to\ncontain incomplete query FG, and some unexpected query background (BG)\nfeatures, leading to wrong segmentation. Hence, we further design Iterative\nMemory Refinement to fuse more query FG features into the memory, and devise a\nSupport-Calibrated Memory Attention to suppress the unexpected query BG\nfeatures in memory. Extensive experiments have been conducted on PASCAL-5$^i$\nand COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot\nmIoU can be 4.2% better than the best baseline.", "authors": ["Qianxiong Xu", "Lanyun Zhu", "Xuanyi Liu", "Guosheng Lin", "Cheng Long", "Ziyue Li", "Rui Zhao"], "published_date": "2025-05-20", "title_zh": "解鎖SAM 2在少樣本分割中的潛力", "summary_zh": "少樣本分割旨在學習類別無關的分割，以分割任意類別，但存在過擬合風險。為解決此問題，部分方法利用基礎模型（如SAM）的知識簡化學習過程。SAM 2擴展了SAM以支援影片分割，其類別無關的匹配能力對少樣本分割有所助益。本文提出將支援前景特徵編碼為記憶，並與查詢前景特徵進行匹配和融合。然而，SAM 2影片數據中不同幀的前景物件身份相同，而少樣本分割中則不同，導致匹配步驟不相容。因此，本文設計偽提示生成器以編碼偽查詢記憶，並以相容方式與查詢特徵匹配。由於偽記憶不如真實記憶準確，可能包含不完整的查詢前景和意外的查詢背景特徵，導致錯誤分割。故此，本文進一步設計迭代記憶提煉，將更多查詢前景特徵融合到記憶中，並設計支援校準記憶注意力，以抑制記憶中意外的查詢背景特徵。在PASCAL-5$^i$和COCO-20$^i$上的實驗驗證了設計的有效性，例如，1-shot mIoU比最佳基準提高了4.2%。", "audio": "audios/2505.14100v2.mp3", "timestamp": "2025-05-24T04:21:00.871481"}
{"query": "Diffusion Model", "id": "2505.16298v1", "url": "http://arxiv.org/abs/2505.16298v1", "title": "Flow Matching based Sequential Recommender Model", "summary": "Generative models, particularly diffusion model, have emerged as powerful\ntools for sequential recommendation. However, accurately modeling user\npreferences remains challenging due to the noise perturbations inherent in the\nforward and reverse processes of diffusion-based methods. Towards this end,\nthis study introduces FMRec, a Flow Matching based model that employs a\nstraight flow trajectory and a modified loss tailored for the recommendation\ntask. Additionally, from the diffusion-model perspective, we integrate a\nreconstruction loss to improve robustness against noise perturbations, thereby\nretaining user preferences during the forward process. In the reverse process,\nwe employ a deterministic reverse sampler, specifically an ODE-based updating\nfunction, to eliminate unnecessary randomness, thereby ensuring that the\ngenerated recommendations closely align with user needs. Extensive evaluations\non four benchmark datasets reveal that FMRec achieves an average improvement of\n6.53% over state-of-the-art methods. The replication code is available at\nhttps://github.com/FengLiu-1/FMRec.", "authors": ["Feng Liu", "Lixin Zou", "Xiangyu Zhao", "Min Tang", "Liming Dong", "Dan Luo", "Xiangyang Luo", "Chenliang Li"], "published_date": "2025-05-22", "title_zh": "基於流匹配的序列推薦模型", "summary_zh": "生成模型，尤其是擴散模型，已成為序列推薦的有力工具。然而，由於基於擴散方法的前向和反向過程中固有的雜訊擾動，精確建模使用者偏好仍然具有挑戰性。本研究提出基於Flow Matching的FMRec模型，該模型採用直接流動軌跡和針對推薦任務修改的損失函數。此外，從擴散模型的角度來看，我們整合了重建損失以提高對雜訊擾動的魯棒性，從而在前向過程中保留使用者偏好。在反向過程中，我們採用確定性反向取樣器，特別是基於ODE的更新函數，以消除不必要的隨機性，從而確保產生的推薦與使用者需求緊密對齊。在四個基準資料集上的廣泛評估表明，FMRec比最先進的方法平均提高了6.53%。複製程式碼可在https://github.com/FengLiu-1/FMRec獲取。", "audio": "audios/2505.16298v1.mp3", "timestamp": "2025-05-24T04:21:08.382848"}
{"query": "AI", "id": "2505.16647v1", "url": "http://arxiv.org/abs/2505.16647v1", "title": "Point, Detect, Count: Multi-Task Medical Image Understanding with Instruction-Tuned Vision-Language Models", "summary": "We investigate fine-tuning Vision-Language Models (VLMs) for multi-task\nmedical image understanding, focusing on detection, localization, and counting\nof findings in medical images. Our objective is to evaluate whether\ninstruction-tuned VLMs can simultaneously improve these tasks, with the goal of\nenhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a\nmultimodal dataset with annotations from endoscopy (polyps and instruments) and\nmicroscopy (sperm cells), we reformulate each task into instruction-based\nprompts suitable for vision-language reasoning. We fine-tune\nQwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task\ncombinations. Results show that multi-task training improves robustness and\naccuracy. For example, it reduces the Count Mean Absolute Error (MAE) and\nincreases Matching Accuracy in the Counting + Pointing task. However,\ntrade-offs emerge, such as more zero-case point predictions, indicating reduced\nreliability in edge cases despite overall performance gains. Our study\nhighlights the potential of adapting general-purpose VLMs to specialized\nmedical tasks via prompt-driven fine-tuning. This approach mirrors clinical\nworkflows, where radiologists simultaneously localize, count, and describe\nfindings - demonstrating how VLMs can learn composite diagnostic reasoning\npatterns. The model produces interpretable, structured outputs, offering a\npromising step toward explainable and versatile medical AI. Code, model\nweights, and scripts will be released for reproducibility at\nhttps://github.com/simula/PointDetectCount.", "authors": ["Sushant Gautam", "Michael A. Riegler", "Pål Halvorsen"], "published_date": "2025-05-22", "title_zh": "點、測、計數：基於指令微調視覺語言模型的多任務醫學影像理解", "summary_zh": "本研究探討微調視覺語言模型（VLMs）於多任務醫學影像理解，著重於醫學影像中病灶的偵測、定位和計數。目標是評估經指令微調的VLMs能否同時提升這些任務，進而提高診斷準確性和效率。利用MedMultiPoints資料集（包含內視鏡息肉、器械與顯微鏡精細胞的標註），將各項任務改寫為基於指令的提示，以利視覺語言推理。使用低秩適應（LoRA）於Qwen2.5-VL-7B-Instruct模型上進行多任務組合微調。結果顯示，多任務訓練提升了穩健性和準確性，例如降低計數平均絕對誤差（MAE）並提高計數+定位任務的匹配準確性。然而，也出現權衡，例如零點預測案例增加，顯示在極端情況下可靠性降低，儘管整體性能有所提升。研究強調透過提示驅動微調，將通用VLMs應用於專業醫學任務的潛力。此方法模擬臨床工作流程，放射科醫師同時定位、計數和描述病灶，展現VLMs如何學習複合診斷推理模式。該模型產生可解釋的結構化輸出，為可解釋且通用的醫療人工智慧提供有前景的一步。代碼、模型權重和腳本將於https://github.com/simula/PointDetectCount發布，以供重現。", "audio": "audios/2505.16647v1.mp3", "timestamp": "2025-05-24T05:17:30.600225"}
{"query": "Diffusion Model", "id": "2505.16275v1", "url": "http://arxiv.org/abs/2505.16275v1", "title": "Semiparametric Bernstein-von Mises theorems for reversible diffusions", "summary": "We establish a general semiparametric Bernstein-von Mises theorem for\nBayesian nonparametric priors based on continuous observations in a periodic\nreversible multidimensional diffusion model. We consider a wide range of\nfunctionals satisfying an approximate linearization condition, including\nseveral nonlinear functionals of the invariant measure. Our result is applied\nto Gaussian and Besov-Laplace priors, showing these can perform efficient\nsemiparametric inference and thus justifying the corresponding Bayesian\napproach to uncertainty quantification. Our theoretical results are illustrated\nvia numerical simulations.", "authors": ["Matteo Giordano", "Kolyan Ray"], "published_date": "2025-05-22", "title_zh": "可逆擴散的半參數Bernstein-von Mises定理", "summary_zh": "本文針對週期可逆多維擴散模型中的連續觀測，建立了基於貝葉斯非參數先驗的一般半參數Bernstein-von Mises定理。研究適用於滿足近似線性化條件的廣泛泛函，包括不變測度的若干非線性泛函。結果應用於高斯和Besov-Laplace先驗，表明這些先驗能有效執行半參數推斷，從而驗證了相應的貝葉斯不確定性量化方法。數值模擬驗證了理論結果。", "audio": "audios/2505.16275v1.mp3", "timestamp": "2025-05-24T05:17:33.793815"}
{"query": "AI", "id": "2505.16630v1", "url": "http://arxiv.org/abs/2505.16630v1", "title": "SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game Understanding", "summary": "The integration of artificial intelligence in sports analytics has\ntransformed soccer video understanding, enabling real-time, automated insights\ninto complex game dynamics. Traditional approaches rely on isolated data\nstreams, limiting their effectiveness in capturing the full context of a match.\nTo address this, we introduce SoccerChat, a multimodal conversational AI\nframework that integrates visual and textual data for enhanced soccer video\ncomprehension. Leveraging the extensive SoccerNet dataset, enriched with jersey\ncolor annotations and automatic speech recognition (ASR) transcripts,\nSoccerChat is fine-tuned on a structured video instruction dataset to\nfacilitate accurate game understanding, event classification, and referee\ndecision making. We benchmark SoccerChat on action classification and referee\ndecision-making tasks, demonstrating its performance in general soccer event\ncomprehension while maintaining competitive accuracy in referee decision\nmaking. Our findings highlight the importance of multimodal integration in\nadvancing soccer analytics, paving the way for more interactive and explainable\nAI-driven sports analysis. https://github.com/simula/SoccerChat", "authors": ["Sushant Gautam", "Cise Midoglu", "Vajira Thambawita", "Michael A. Riegler", "Pål Halvorsen", "Mubarak Shah"], "published_date": "2025-05-22", "title_zh": "足球聊天：整合多模態數據以增強足球比賽理解", "summary_zh": "人工智慧融入運動分析革新了足球影片理解，實現對複雜賽事動態的即時自動洞察。傳統方法依賴獨立數據流，難以捕捉完整賽事脈絡。本研究提出SoccerChat，一種多模態對話式AI框架，整合視覺與文本數據，強化足球影片理解。SoccerChat利用SoccerNet數據集，輔以球衣顏色標註與自動語音辨識轉錄，並於結構化影片指令數據集上微調，以促進精確的賽事理解、事件分類與裁判決策。SoccerChat在行為分類與裁判決策任務上進行基準測試，展現其於一般足球事件理解上的效能，同時在裁判決策上維持具競爭力的準確度。研究結果突顯多模態整合在推動足球分析上的重要性，為更具互動性與可解釋性的AI驅動運動分析鋪路。", "audio": "audios/2505.16630v1.mp3", "timestamp": "2025-05-24T06:23:46.497912"}
{"query": "Diffusion Model", "id": "2505.16239v1", "url": "http://arxiv.org/abs/2505.16239v1", "title": "DOVE: Efficient One-Step Diffusion Model for Real-World Video Super-Resolution", "summary": "Diffusion models have demonstrated promising performance in real-world video\nsuper-resolution (VSR). However, the dozens of sampling steps they require,\nmake inference extremely slow. Sampling acceleration techniques, particularly\nsingle-step, provide a potential solution. Nonetheless, achieving one step in\nVSR remains challenging, due to the high training overhead on video data and\nstringent fidelity demands. To tackle the above issues, we propose DOVE, an\nefficient one-step diffusion model for real-world VSR. DOVE is obtained by\nfine-tuning a pretrained video diffusion model (*i.e.*, CogVideoX). To\neffectively train DOVE, we introduce the latent-pixel training strategy. The\nstrategy employs a two-stage scheme to gradually adapt the model to the video\nsuper-resolution task. Meanwhile, we design a video processing pipeline to\nconstruct a high-quality dataset tailored for VSR, termed HQ-VSR. Fine-tuning\non this dataset further enhances the restoration capability of DOVE. Extensive\nexperiments show that DOVE exhibits comparable or superior performance to\nmulti-step diffusion-based VSR methods. It also offers outstanding inference\nefficiency, achieving up to a **28$\\times$** speed-up over existing methods\nsuch as MGLD-VSR. Code is available at: https://github.com/zhengchen1999/DOVE.", "authors": ["Zheng Chen", "Zichen Zou", "Kewei Zhang", "Xiongfei Su", "Xin Yuan", "Yong Guo", "Yulun Zhang"], "published_date": "2025-05-22", "title_zh": "DOVE：用於真實世界影片超解析度的高效單步擴散模型", "summary_zh": "擴散模型在真實世界影片超解析度展現潛力，但其大量採樣步驟導致推論速度緩慢。單步採樣加速技術提供解決方案，然而，影片資料的高訓練成本和嚴格的保真度要求使單步VSR極具挑戰。為解決上述問題，我們提出DOVE，一種高效的單步擴散模型，用於真實世界VSR。DOVE透過微調預訓練的影片擴散模型（例如CogVideoX）獲得。為有效訓練DOVE，我們引入潛在像素訓練策略，該策略採用兩階段方案，逐步使模型適應影片超解析度任務。同時，我們設計影片處理流程，構建名為HQ-VSR的高品質VSR客製化資料集。在此資料集上進行微調，進一步提升DOVE的重建能力。大量實驗表明，DOVE展現與多步擴散VSR方法相當或更優異的性能，並提供卓越的推論效率，相較於MGLD-VSR等現有方法，速度提升高達28倍。程式碼位於：https://github.com/zhengchen1999/DOVE。", "audio": "audios/2505.16239v1.mp3", "timestamp": "2025-05-24T06:23:55.893315"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的人工智慧：生命科學領域的挑戰、機遇與前路", "summary_zh": "人工智慧近期在生命科學領域取得突破性進展，以前所未有的能力解讀生物資訊，並幾乎每日都有新應用與進展。為最大化基於人工智慧的生命科學研究投資回報並加速進程，亟需解決快速採用人工智慧方法所加劇的長期研究挑戰。本文回顧了因低重用性和再現性導致的對人工智慧研究產出的信任度下降問題，並強調其對環境永續性的影響。此外，討論了人工智慧生態系統的碎片化及其缺乏支持開放與永續人工智慧(OSAI)模型開發的指導路徑。為此，本文提出一套實用的OSAI建議，直接對應人工智慧生態系統的300多個組件，旨在連接研究人員與相關人工智慧資源，促進永續、可重複使用和透明的人工智慧實施。此觀點基於生命科學社群共識並與現有努力保持一致，旨在協助未來政策制定和引導人工智慧實施的結構化路徑發展。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-24T07:16:34.820500"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除抑或蟄伏？基於可逆性的概念擦除再思", "summary_zh": "概念抹除在擴散模型中能消除多少生成能力？ 既有評估主要關注特定文本提示下的概念抑制，本文探討一個互補且根本的問題：現有概念抹除技術是否真正移除生成目標概念的能力，或僅實現表面上的提示特定抑制？ 我們系統性地評估了兩種具代表性概念抹除方法（統一概念編輯和抹除穩定擴散）的穩健性和可逆性，藉由探測其消除文本到圖像模型中目標生成行為的能力。 這些方法試圖透過修改內部模型參數來抑制不需要的語義概念，包括針對性注意力編輯或模型層級微調。 為嚴格評估這些技術是否真正抹除生成能力，我們提出一種實例層級評估策略，採用輕量微調來顯式測試抹除概念的重新激活潛力。 量化指標和定性分析表明，抹除的概念通常在最小程度適應後，以相當的視覺逼真度重新出現，表明當前方法抑制了潛在生成表示，但並未完全消除它們。 研究結果揭示了現有概念抹除方法的關鍵局限性，並強調需要更深入的表示層級干預和更嚴格的評估標準，以確保從生成模型中真正、不可逆地移除概念。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-24T07:16:42.524959"}
{"query": "AI", "id": "2505.16577v1", "url": "http://arxiv.org/abs/2505.16577v1", "title": "Large Language Model-Empowered Interactive Load Forecasting", "summary": "The growing complexity of power systems has made accurate load forecasting\nmore important than ever. An increasing number of advanced load forecasting\nmethods have been developed. However, the static design of current methods\noffers no mechanism for human-model interaction. As the primary users of\nforecasting models, system operators often find it difficult to understand and\napply these advanced models, which typically requires expertise in artificial\nintelligence (AI). This also prevents them from incorporating their experience\nand real-world contextual understanding into the forecasting process. Recent\nbreakthroughs in large language models (LLMs) offer a new opportunity to\naddress this issue. By leveraging their natural language understanding and\nreasoning capabilities, we propose an LLM-based multi-agent collaboration\nframework to bridge the gap between human operators and forecasting models. A\nset of specialized agents is designed to perform different tasks in the\nforecasting workflow and collaborate via a dedicated communication mechanism.\nThis framework embeds interactive mechanisms throughout the load forecasting\npipeline, reducing the technical threshold for non-expert users and enabling\nthe integration of human experience. Our experiments demonstrate that the\ninteractive load forecasting accuracy can be significantly improved when users\nprovide proper insight in key stages. Our cost analysis shows that the\nframework remains affordable, making it practical for real-world deployment.", "authors": ["Yu Zuo", "Dalin Qin", "Yi Wang"], "published_date": "2025-05-22", "title_zh": "大型語言模型驅動的互動式負載預測", "summary_zh": "電力系統日趨複雜，精準負載預測至關重要。現有負載預測方法缺乏人機互動機制，系統操作員難以理解與應用需具備人工智慧專業知識的先進模型，且無法將自身經驗及實務知識融入預測流程。本研究提出基於大型語言模型的多代理協作框架，利用其自然語言理解與推理能力，彌合操作員與預測模型間的差距。框架設計多個專責代理執行不同預測任務，並透過通訊機制協作，在負載預測流程中嵌入互動機制，降低技術門檻，促成人類經驗整合。實驗結果顯示，使用者於關鍵階段提供適當見解，可顯著提升互動式負載預測準確性。成本分析表明，此框架具備實用性與經濟效益，適合實際部署。", "audio": "audios/2505.16577v1.mp3", "timestamp": "2025-05-24T08:21:11.580413"}
{"query": "Diffusion Model", "id": "2505.16166v1", "url": "http://arxiv.org/abs/2505.16166v1", "title": "TRAIL: Transferable Robust Adversarial Images via Latent diffusion", "summary": "Adversarial attacks exploiting unrestricted natural perturbations present\nsevere security risks to deep learning systems, yet their transferability\nacross models remains limited due to distribution mismatches between generated\nadversarial features and real-world data. While recent works utilize\npre-trained diffusion models as adversarial priors, they still encounter\nchallenges due to the distribution shift between the distribution of ideal\nadversarial samples and the natural image distribution learned by the diffusion\nmodel. To address the challenge, we propose Transferable Robust Adversarial\nImages via Latent Diffusion (TRAIL), a test-time adaptation framework that\nenables the model to generate images from a distribution of images with\nadversarial features and closely resembles the target images. To mitigate the\ndistribution shift, during attacks, TRAIL updates the diffusion U-Net's weights\nby combining adversarial objectives (to mislead victim models) and perceptual\nconstraints (to preserve image realism). The adapted model then generates\nadversarial samples through iterative noise injection and denoising guided by\nthese objectives. Experiments demonstrate that TRAIL significantly outperforms\nstate-of-the-art methods in cross-model attack transferability, validating that\ndistribution-aligned adversarial feature synthesis is critical for practical\nblack-box attacks.", "authors": ["Yuhao Xue", "Zhifei Zhang", "Xinyang Jiang", "Yifei Shen", "Junyao Gao", "Wentao Gu", "Jiale Zhao", "Miaojing Shi", "Cairong Zhao"], "published_date": "2025-05-22", "title_zh": "TRAIL：基於潛在擴散的可轉移魯棒對抗性圖像", "summary_zh": "針對深度學習系統，利用無限制自然擾動的對抗性攻擊構成嚴峻安全風險，但因產生之對抗性特徵與真實世界數據間存在分佈不匹配，其跨模型遷移性受限。近期研究雖採用預訓練擴散模型作為對抗性先驗，然理想對抗樣本分佈與擴散模型所學自然圖像分佈間的分佈偏移仍構成挑戰。為此，我們提出基於潛在擴散的可遷移穩健對抗圖像(TRAIL)，此測試時適應框架使模型能從具有對抗性特徵且與目標圖像高度相似的圖像分佈中生成圖像。為緩解分佈偏移，TRAIL於攻擊期間結合對抗性目標(誤導受害者模型)與感知約束(保持圖像真實感)來更新擴散U-Net權重。隨後，調整後的模型透過迭代雜訊注入和目標導向的去噪生成對抗樣本。實驗表明，TRAIL在跨模型攻擊遷移性方面顯著優於現有方法，驗證了分佈對齊的對抗性特徵合成對於實際黑盒攻擊至關重要。", "audio": "audios/2505.16166v1.mp3", "timestamp": "2025-05-24T08:21:19.532383"}
{"query": "AI", "id": "2505.16575v1", "url": "http://arxiv.org/abs/2505.16575v1", "title": "Data Center Model for Transient Stability Analysis of Power Systems", "summary": "The rising demand of computing power leads to the installation of a large\nnumber of Data Centers (DCs). Their Fault-Ride-Through (FRT) behavior and their\nunique power characteristics, especially for DCs catered to Artificial\nIntelligence (AI) workloads, pose a threat to the stability of power systems.\nTo ensure its stability, it is required accurate models of the loads involved.\nHere we propose a dynamic load model that properly captures the behaviour of\nDCs. Its three most defining features are the use of an Uninterrupted Power\nSupply (UPS) which sits between the server load and the grid, the cooling load\nrepresented by an induction motor, and a pulsing load that represents the\ntransients caused by contemporary DCs with significant AI workloads. The\nfeatures of the proposed model and its impact on the dynamic performance of\ntransmission systems are illustrated through a model of the all-island Irish\ntransmission system and real-world data of the DCs currently connected to this\nsystem.", "authors": ["Alberto Jimenez-Ruiz", "Federico Milano"], "published_date": "2025-05-22", "title_zh": "電力系統暫態穩定分析之資料中心模型", "summary_zh": "計算能力需求激增導致大量資料中心建置，其低電壓穿越能力及獨特的電力特性，特別是針對人工智慧工作負載的資料中心，對電力系統穩定性構成威脅。為確保穩定性，需要精確的負載模型。本研究提出一種動態負載模型，能準確捕捉資料中心的行為。其三大特點為：連接伺服器負載與電網間的不斷電供應系統、以感應電動機表示的冷卻負載，以及代表具顯著人工智慧工作負載的現代資料中心所產生瞬態的脈衝負載。透過全愛爾蘭輸電系統模型及當前連接至該系統的資料中心真實數據，闡述了所提模型的特點及其對輸電系統動態性能的影響。", "audio": "audios/2505.16575v1.mp3", "timestamp": "2025-05-24T09:17:51.653573"}
{"query": "Diffusion Model", "id": "2505.16091v1", "url": "http://arxiv.org/abs/2505.16091v1", "title": "OSCAR: One-Step Diffusion Codec Across Multiple Bit-rates", "summary": "Pretrained latent diffusion models have shown strong potential for lossy\nimage compression, owing to their powerful generative priors. Most existing\ndiffusion-based methods reconstruct images by iteratively denoising from random\nnoise, guided by compressed latent representations. While these approaches have\nachieved high reconstruction quality, their multi-step sampling process incurs\nsubstantial computational overhead. Moreover, they typically require training\nseparate models for different compression bit-rates, leading to significant\ntraining and storage costs. To address these challenges, we propose a one-step\ndiffusion codec across multiple bit-rates. termed OSCAR. Specifically, our\nmethod views compressed latents as noisy variants of the original latents,\nwhere the level of distortion depends on the bit-rate. This perspective allows\nthem to be modeled as intermediate states along a diffusion trajectory. By\nestablishing a mapping from the compression bit-rate to a pseudo diffusion\ntimestep, we condition a single generative model to support reconstructions at\nmultiple bit-rates. Meanwhile, we argue that the compressed latents retain rich\nstructural information, thereby making one-step denoising feasible. Thus, OSCAR\nreplaces iterative sampling with a single denoising pass, significantly\nimproving inference efficiency. Extensive experiments demonstrate that OSCAR\nachieves superior performance in both quantitative and visual quality metrics.\nThe code and models will be released at https://github.com/jp-guo/OSCAR.", "authors": ["Jinpei Guo", "Yifei Ji", "Zheng Chen", "Kai Liu", "Min Liu", "Wang Rao", "Wenbo Li", "Yong Guo", "Yulun Zhang"], "published_date": "2025-05-22", "title_zh": "OSCAR：跨多位元速率之單步擴散編解碼器", "summary_zh": "預訓練潛在擴散模型因其強大的生成先驗，在有損影像壓縮方面展現巨大潛力。現有基於擴散的方法多藉由壓縮的潛在表徵引導，從隨機雜訊迭代去噪以重建影像。儘管這些方法實現了高重建品質，但其多步驟取樣過程產生了大量的計算開銷。此外，它們通常需要為不同的壓縮位元速率訓練單獨的模型，導致顯著的訓練和儲存成本。為了解決這些挑戰，我們提出了一種跨多位元速率的單步擴散編解碼器，稱為OSCAR。具體而言，我們的方法將壓縮的潛在視為原始潛在的噪聲變體，其中失真程度取決於位元速率。這種觀點允許將其建模為沿擴散軌跡的中間狀態。透過建立從壓縮位元速率到偽擴散時間步的映射，我們條件化單一生成模型以支援多位元速率的重建。同時，我們認為壓縮的潛在保留了豐富的結構訊息，從而使單步去噪成為可能。因此，OSCAR用單個去噪過程取代了迭代取樣，從而顯著提高了推論效率。大量實驗表明，OSCAR在定量和視覺品質指標方面均實現了卓越的性能。代碼和模型將在https://github.com/jp-guo/OSCAR發布。", "audio": "audios/2505.16091v1.mp3", "timestamp": "2025-05-24T09:18:00.424802"}
{"query": "AI", "id": "2505.16561v1", "url": "http://arxiv.org/abs/2505.16561v1", "title": "Auto-nnU-Net: Towards Automated Medical Image Segmentation", "summary": "Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ\nsegmentation, each with its own challenges in finding the best segmentation\nmodel. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many\naspects of model configuration but remains constrained by fixed hyperparameters\nand heuristic design choices. As a full-AutoML framework for MIS, we propose\nAuto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization\n(HPO), neural architecture search (NAS), and hierarchical NAS (HNAS).\nAdditionally, we propose Regularized PriorBand to balance model accuracy with\nthe computational resources required for training, addressing the resource\nconstraints often faced in real-world medical settings that limit the\nfeasibility of extensive training procedures. We evaluate our approach across\ndiverse MIS datasets from the well-established Medical Segmentation Decathlon,\nanalyzing the impact of AutoML techniques on segmentation performance,\ncomputational efficiency, and model design choices. The results demonstrate\nthat our AutoML approach substantially improves the segmentation performance of\nnnU-Net on 6 out of 10 datasets and is on par on the other datasets while\nmaintaining practical resource requirements. Our code is available at\nhttps://github.com/LUH-AI/AutonnUNet.", "authors": ["Jannis Becktepe", "Leona Hennig", "Steffen Oeltze-Jafra", "Marius Lindauer"], "published_date": "2025-05-22", "title_zh": "Auto-nnU-Net：邁向自動化醫學影像分割", "summary_zh": "醫學影像分割任務多樣，模型選擇具挑戰性。現有AutoML框架nnU-Net雖自動化模型配置，仍受限於固定超參數。本研究提出Auto-nnU-Net，一新型nnU-Net變體，支援超參數優化、神經架構搜尋及階層式神經架構搜尋，並引入Regularized PriorBand平衡模型準確度與運算資源。實驗採用Medical Segmentation Decathlon數據集，驗證AutoML技術對分割效能、運算效率及模型設計的影響。結果顯示，Auto-nnU-Net在十分之六的數據集上顯著提升nnU-Net分割效能，其餘數據集上表現相當，同時維持合理資源需求。", "audio": "audios/2505.16561v1.mp3", "timestamp": "2025-05-24T10:17:28.458296"}
{"query": "Diffusion Model", "id": "2505.16024v1", "url": "http://arxiv.org/abs/2505.16024v1", "title": "Toward Theoretical Insights into Diffusion Trajectory Distillation via Operator Merging", "summary": "Diffusion trajectory distillation methods aim to accelerate sampling in\ndiffusion models, which produce high-quality outputs but suffer from slow\nsampling speeds. These methods train a student model to approximate the\nmulti-step denoising process of a pretrained teacher model in a single step,\nenabling one-shot generation. However, theoretical insights into the trade-off\nbetween different distillation strategies and generative quality remain\nlimited, complicating their optimization and selection. In this work, we take a\nfirst step toward addressing this gap. Specifically, we reinterpret trajectory\ndistillation as an operator merging problem in the linear regime, where each\nstep of the teacher model is represented as a linear operator acting on noisy\ndata. These operators admit a clear geometric interpretation as projections and\nrescalings corresponding to the noise schedule. During merging, signal\nshrinkage occurs as a convex combination of operators, arising from both\ndiscretization and limited optimization time of the student model. We propose a\ndynamic programming algorithm to compute the optimal merging strategy that\nmaximally preserves signal fidelity. Additionally, we demonstrate the existence\nof a sharp phase transition in the optimal strategy, governed by data\ncovariance structures. Our findings enhance the theoretical understanding of\ndiffusion trajectory distillation and offer practical insights for improving\ndistillation strategies.", "authors": ["Weiguo Gao", "Ming Li"], "published_date": "2025-05-21", "title_zh": "藉由算符合併探討擴散軌跡提煉的理論見解", "summary_zh": "擴散軌跡蒸餾旨在加速擴散模型取樣，此類模型能產出高品質結果，但取樣速度慢。此方法訓練學生模型，以單一步驟逼近預訓練教師模型的多步去噪過程，實現單次生成。然而，對於不同蒸餾策略和生成品質間權衡的理論洞見仍然有限，使得優化和選擇變得複雜。本研究朝此方向邁出第一步，將軌跡蒸餾重新詮釋為線性狀態下的運算元合併問題，教師模型的每一步驟表示為作用於雜訊資料的線性運算元。這些運算元可被清楚地幾何解釋為對應於雜訊排程的投影和重新縮放。合併過程中，訊號衰減發生於運算元的凸組合，源於學生模型的離散化和有限優化時間。我們提出動態規劃演算法，計算最大程度保留訊號保真度的最佳合併策略。此外，我們證明了最佳策略中存在由資料共變異數結構控制的明顯相變。這些發現增強了對擴散軌跡蒸餾的理論理解，並為改進蒸餾策略提供實用見解。", "audio": "audios/2505.16024v1.mp3", "timestamp": "2025-05-24T10:17:35.631527"}
{"query": "AI", "id": "2505.16499v1", "url": "http://arxiv.org/abs/2505.16499v1", "title": "Smaller, Smarter, Closer: The Edge of Collaborative Generative AI", "summary": "The rapid adoption of generative AI (GenAI), particularly Large Language\nModels (LLMs), has exposed critical limitations of cloud-centric deployments,\nincluding latency, cost, and privacy concerns. Meanwhile, Small Language Models\n(SLMs) are emerging as viable alternatives for resource-constrained edge\nenvironments, though they often lack the capabilities of their larger\ncounterparts. This article explores the potential of collaborative inference\nsystems that leverage both edge and cloud resources to address these\nchallenges. By presenting distinct cooperation strategies alongside practical\ndesign principles and experimental insights, we offer actionable guidance for\ndeploying GenAI across the computing continuum.", "authors": ["Roberto Morabito", "SiYoung Jang"], "published_date": "2025-05-22", "title_zh": "更小、更智慧、更近端：協作式生成人工智慧的邊緣", "summary_zh": "生成式人工智慧（GenAI）特別是大語言模型（LLMs）的快速普及，暴露出雲端部署的延遲、成本和隱私等關鍵限制。小型語言模型（SLMs）雖成為資源受限邊緣環境的可行替代方案，但能力往往不如大型模型。本文探討利用邊緣和雲端資源的協作推論系統之潛力，旨在應對這些挑戰。透過呈現不同的合作策略、實用設計原則和實驗洞見，我們為在計算連續體中部署GenAI提供可行的指導。", "audio": "audios/2505.16499v1.mp3", "timestamp": "2025-05-24T11:13:36.526423"}
{"query": "Diffusion Model", "id": "2505.16001v1", "url": "http://arxiv.org/abs/2505.16001v1", "title": "Image-to-Image Translation with Diffusion Transformers and CLIP-Based Image Conditioning", "summary": "Image-to-image translation aims to learn a mapping between a source and a\ntarget domain, enabling tasks such as style transfer, appearance\ntransformation, and domain adaptation. In this work, we explore a\ndiffusion-based framework for image-to-image translation by adapting Diffusion\nTransformers (DiT), which combine the denoising capabilities of diffusion\nmodels with the global modeling power of transformers. To guide the translation\nprocess, we condition the model on image embeddings extracted from a\npre-trained CLIP encoder, allowing for fine-grained and structurally consistent\ntranslations without relying on text or class labels. We incorporate both a\nCLIP similarity loss to enforce semantic consistency and an LPIPS perceptual\nloss to enhance visual fidelity during training. We validate our approach on\ntwo benchmark datasets: face2comics, which translates real human faces to\ncomic-style illustrations, and edges2shoes, which translates edge maps to\nrealistic shoe images. Experimental results demonstrate that DiT, combined with\nCLIP-based conditioning and perceptual similarity objectives, achieves\nhigh-quality, semantically faithful translations, offering a promising\nalternative to GAN-based models for paired image-to-image translation tasks.", "authors": ["Qiang Zhu", "Kuan Lu", "Menghao Huo", "Yuxiao Li"], "published_date": "2025-05-21", "title_zh": "基於擴散變換器與CLIP圖像條件的圖像轉圖像轉換", "summary_zh": "圖像到圖像轉換旨在學習源域與目標域之間的映射，實現風格轉換、外觀變換和領域自適應等任務。本研究探索基於擴散的圖像到圖像轉換框架，改進擴散轉換器(DiT)，結合擴散模型的去噪能力和轉換器的全局建模能力。為引導轉換過程，模型以預訓練CLIP編碼器提取的圖像嵌入為條件，實現精細且結構一致的轉換，無需依賴文本或類別標籤。訓練中，加入CLIP相似度損失以確保語義一致性，並加入LPIPS感知損失以增強視覺逼真度。在face2comics和edges2shoes兩個基準數據集上驗證了該方法。實驗結果表明，DiT結合基於CLIP的條件設定和感知相似性目標，可實現高質量、語義忠實的轉換，為配對圖像到圖像轉換任務提供了一種有前景的替代方案，可取代基於GAN的模型。", "audio": "audios/2505.16001v1.mp3", "timestamp": "2025-05-24T11:13:43.305928"}
{"query": "AI", "id": "2505.16477v1", "url": "http://arxiv.org/abs/2505.16477v1", "title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery", "summary": "With recent Nobel Prizes recognising AI contributions to science, Large\nLanguage Models (LLMs) are transforming scientific research by enhancing\nproductivity and reshaping the scientific method. LLMs are now involved in\nexperimental design, data analysis, and workflows, particularly in chemistry\nand biology. However, challenges such as hallucinations and reliability\npersist. In this contribution, we review how Large Language Models (LLMs) are\nredefining the scientific method and explore their potential applications\nacross different stages of the scientific cycle, from hypothesis testing to\ndiscovery. We conclude that, for LLMs to serve as relevant and effective\ncreative engines and productivity enhancers, their deep integration into all\nsteps of the scientific process should be pursued in collaboration and\nalignment with human scientific goals, with clear evaluation metrics. The\ntransition to AI-driven science raises ethical questions about creativity,\noversight, and responsibility. With careful guidance, LLMs could evolve into\ncreative engines, driving transformative breakthroughs across scientific\ndisciplines responsibly and effectively. However, the scientific community must\nalso decide how much it leaves to LLMs to drive science, even when associations\nwith 'reasoning', mostly currently undeserved, are made in exchange for the\npotential to explore hypothesis and solution regions that might otherwise\nremain unexplored by human exploration alone.", "authors": ["Yanbo Zhang", "Sumeer A. Khan", "Adnan Mahmud", "Huck Yang", "Alexander Lavin", "Michael Levin", "Jeremy Frey", "Jared Dunnmon", "James Evans", "Alan Bundy", "Saso Dzeroski", "Jesper Tegner", "Hector Zenil"], "published_date": "2025-05-22", "title_zh": "以大型語言模型推進科學方法：從假說到發現", "summary_zh": "大型語言模型因其對科學的貢獻而備受肯定，正透過提高生產力及重塑科學方法來改變科學研究。它們廣泛應用於實驗設計、數據分析及工作流程，尤其在化學與生物學領域。然幻覺和可靠性問題依然存在。本文探討大型語言模型如何重新定義科學方法，及其在科學週期各階段的潛在應用，從假設檢驗到新發現。為使大型語言模型成為有效的創新引擎和生產力工具，應將其深度整合至科學過程的各個環節，並與人類科學目標協調一致，建立明確的評估指標。人工智慧驅動科學的轉型引發了關於創造力、監督和責任的倫理問題。透過謹慎引導，大型語言模型有望成為創新引擎，負責且有效地推動科學各領域的變革性突破。然而，科學界必須決定在多大程度上依賴大型語言模型來驅動科學發展，即使以潛在的探索假設和解決方案空間為代價，這些空間可能僅憑人類探索無法觸及。", "audio": "audios/2505.16477v1.mp3", "timestamp": "2025-05-24T12:33:34.173858"}
{"query": "Diffusion Model", "id": "2505.15963v1", "url": "http://arxiv.org/abs/2505.15963v1", "title": "OViP: Online Vision-Language Preference Learning", "summary": "Large vision-language models (LVLMs) remain vulnerable to hallucination,\noften generating content misaligned with visual inputs. While recent approaches\nadvance multi-modal Direct Preference Optimization (DPO) to mitigate\nhallucination, they typically rely on predefined or randomly edited negative\nsamples that fail to reflect actual model errors, limiting training efficacy.\nIn this work, we propose an Online Vision-language Preference Learning (OViP)\nframework that dynamically constructs contrastive training data based on the\nmodel's own hallucinated outputs. By identifying semantic differences between\nsampled response pairs and synthesizing negative images using a diffusion\nmodel, OViP generates more relevant supervision signals in real time. This\nfailure-driven training enables adaptive alignment of both textual and visual\npreferences. Moreover, we refine existing evaluation protocols to better\ncapture the trade-off between hallucination suppression and expressiveness.\nExperiments on hallucination and general benchmarks demonstrate that OViP\neffectively reduces hallucinations while preserving core multi-modal\ncapabilities.", "authors": ["Shujun Liu", "Siyuan Wang", "Zejun Li", "Jianxiang Wang", "Cheng Zeng", "Zhongyu Wei"], "published_date": "2025-05-21", "title_zh": "OViP：線上視覺語言偏好學習", "summary_zh": "大型視覺語言模型易產生幻覺，內容與視覺輸入不符。現有方法雖利用多模態直接偏好優化降低幻覺，但多依賴預定義或隨機編輯的負樣本，未能反映模型實際錯誤，限制訓練效果。本文提出線上視覺語言偏好學習框架OViP，基於模型自身幻覺輸出動態構建對比訓練數據。透過識別抽樣回應對之間的語義差異，並利用擴散模型合成負圖像，OViP即時生成更相關的監督訊號。此種失敗驅動訓練使文本和視覺偏好得以自適應對齊。此外，本文改進現有評估協議，以更精確捕捉幻覺抑制與表達能力之間的權衡。在幻覺和通用基準測試上的實驗表明，OViP有效減少幻覺，同時保留核心多模態能力。", "audio": "audios/2505.15963v1.mp3", "timestamp": "2025-05-24T12:33:42.576030"}
{"query": "AI", "id": "2505.16455v1", "url": "http://arxiv.org/abs/2505.16455v1", "title": "Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events", "summary": "During sudden disaster events, accurately predicting public panic sentiment\non social media is crucial for proactive governance and crisis management.\nCurrent efforts on this problem face three main challenges: lack of finely\nannotated data hinders emotion prediction studies, unmodeled risk perception\ncauses prediction inaccuracies, and insufficient interpretability of panic\nformation mechanisms. We address these issues by proposing a Psychology-driven\ngenerative Agent framework (PsychoAgent) for explainable panic prediction based\non emotion arousal theory. Specifically, we first construct a fine-grained open\npanic emotion dataset (namely COPE) via human-large language models (LLMs)\ncollaboration to mitigate semantic bias. Then, we develop a framework\nintegrating cross-domain heterogeneous data grounded in psychological\nmechanisms to model risk perception and cognitive differences in emotion\ngeneration. To enhance interpretability, we design an LLM-based role-playing\nagent that simulates individual psychological chains through dedicatedly\ndesigned prompts. Experimental results on our annotated dataset show that\nPsychoAgent improves panic emotion prediction performance by 12.6% to 21.7%\ncompared to baseline models. Furthermore, the explainability and generalization\nof our approach is validated. Crucially, this represents a paradigm shift from\nopaque \"data-driven fitting\" to transparent \"role-based simulation with\nmechanistic interpretation\" for panic emotion prediction during emergencies.\nOur implementation is publicly available at:\nhttps://anonymous.4open.science/r/PsychoAgent-19DD.", "authors": ["Mengzhu Liu", "Zhengqiu Zhu", "Chuan Ai", "Chen Gao", "Xinghong Li", "Lingnan He", "Kaisheng Lai", "Yingfeng Chen", "Xin Lu", "Yong Li", "Quanjun Yin"], "published_date": "2025-05-22", "title_zh": "基於心理學的LLM代理於突發災難事件期間社交媒體上可解釋的恐慌預測", "summary_zh": "在突發災害事件中，精確預測社群媒體上的公眾恐慌情緒，對主動治理和危機管理至關重要。目前的研究面臨缺乏精細標註資料、未建模風險感知及恐慌形成機制解釋性不足等挑戰。本文提出基於情緒喚醒理論的心理驅動生成式代理框架（PsychoAgent），用於可解釋的恐慌預測。首先，透過人機協作构建精細化的開放式恐慌情緒數據集（COPE），以減輕語義偏差。接著，開發整合跨領域異質數據的框架，基於心理學機制建模風險感知和情緒產生的認知差異。為提升可解釋性，設計基於大型語言模型（LLM）的角色扮演代理，透過精心設計的提示模擬個體心理鏈。實驗結果表明，與基線模型相比，PsychoAgent在標註數據集上將恐慌情緒預測性能提升了12.6%至21.7%，並驗證了其可解釋性和泛化性。此方法代表了從不透明的「數據驅動擬合」到透明的「基於角色的機制解釋模擬」的範式轉移，可用於應急情況下的恐慌情緒預測。", "audio": "audios/2505.16455v1.mp3", "timestamp": "2025-05-24T13:25:36.031393"}
{"query": "Diffusion Model", "id": "2505.15946v1", "url": "http://arxiv.org/abs/2505.15946v1", "title": "MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding", "summary": "Decoding visual experiences from fMRI offers a powerful avenue to understand\nhuman perception and develop advanced brain-computer interfaces. However,\ncurrent progress often prioritizes maximizing reconstruction fidelity while\noverlooking interpretability, an essential aspect for deriving neuroscientific\ninsight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework\ndesigned for high-fidelity, adaptable, and interpretable visual reconstruction.\nMoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture\nwhere distinct experts process fMRI signals from functionally related voxel\ngroups, mimicking specialized brain networks. The experts are first trained to\nencode fMRI into the frozen CLIP space. A finetuned diffusion model then\nsynthesizes images, guided by expert outputs through a novel dual-stage routing\nmechanism that dynamically weighs expert contributions across the diffusion\nprocess. MoRE-Brain offers three main advancements: First, it introduces a\nnovel Mixture-of-Experts architecture grounded in brain network principles for\nneuro-decoding. Second, it achieves efficient cross-subject generalization by\nsharing core expert networks while adapting only subject-specific routers.\nThird, it provides enhanced mechanistic insight, as the explicit routing\nreveals precisely how different modeled brain regions shape the semantic and\nspatial attributes of the reconstructed image. Extensive experiments validate\nMoRE-Brain's high reconstruction fidelity, with bottleneck analyses further\ndemonstrating its effective utilization of fMRI signals, distinguishing genuine\nneural decoding from over-reliance on generative priors. Consequently,\nMoRE-Brain marks a substantial advance towards more generalizable and\ninterpretable fMRI-based visual decoding. Code will be publicly available soon:\nhttps://github.com/yuxiangwei0808/MoRE-Brain.", "authors": ["Yuxiang Wei", "Yanteng Zhang", "Xi Xiao", "Tianyang Wang", "Xiao Wang", "Vince D. Calhoun"], "published_date": "2025-05-21", "title_zh": "MoRE-Brain：用於可解釋和泛化跨受試者fMRI視覺解碼的路由混合專家模型", "summary_zh": "基於功能性磁振造影解碼視覺經驗有助於理解人類感知並發展先進的腦機介面。現有研究多著重於最大化重建保真度，忽略了神經科學洞察力的關鍵要素：可解釋性。為解決此問題，我們提出MoRE-Brain，一種神經啟發框架，旨在實現高保真度、可適應性和可解釋性的視覺重建。MoRE-Brain獨特地採用層次化的專家混合架構，其中不同專家處理來自功能相關體素群的功能性磁振造影訊號，模擬專業化的大腦網路。首先訓練專家將功能性磁振造影編碼到固定的CLIP空間。然後，微調的擴散模型在新型雙階段路由機制的引導下合成圖像，該機制在擴散過程中動態衡量專家的貢獻。MoRE-Brain提供三個主要進展：首先，引入基於大腦網路原理的專家混合架構，用於神經解碼。其次，透過共享核心專家網路並僅調整特定於受試者的路由器，實現有效的跨受試者泛化。第三，提供增強的機制洞察力，因為顯式路由揭示了不同建模的大腦區域如何塑造重建圖像的語義和空間屬性。廣泛的實驗驗證了MoRE-Brain的高重建保真度，瓶頸分析進一步證明了其對功能性磁振造影訊號的有效利用，區分了真正的神經解碼和過度依賴生成先驗。因此，MoRE-Brain標誌著朝著更具泛化性和可解釋性的基於功能性磁振造影的視覺解碼邁出了重要一步。程式碼將很快公開。", "audio": "audios/2505.15946v1.mp3", "timestamp": "2025-05-24T13:25:46.984857"}
{"query": "AI", "id": "2505.16412v1", "url": "http://arxiv.org/abs/2505.16412v1", "title": "Pose-invariant face recognition via feature-space pose frontalization", "summary": "Pose-invariant face recognition has become a challenging problem for modern\nAI-based face recognition systems. It aims at matching a profile face captured\nin the wild with a frontal face registered in a database. Existing methods\nperform face frontalization via either generative models or learning a pose\nrobust feature representation. In this paper, a new method is presented to\nperform face frontalization and recognition within the feature space. First, a\nnovel feature space pose frontalization module (FSPFM) is proposed to transform\nprofile images with arbitrary angles into frontal counterparts. Second, a new\ntraining paradigm is proposed to maximize the potential of FSPFM and boost its\nperformance. The latter consists of a pre-training and an attention-guided\nfine-tuning stage. Moreover, extensive experiments have been conducted on five\npopular face recognition benchmarks. Results show that not only our method\noutperforms the state-of-the-art in the pose-invariant face recognition task\nbut also maintains superior performance in other standard scenarios.", "authors": ["Nikolay Stanishev", "Yuhang Lu", "Touradj Ebrahimi"], "published_date": "2025-05-22", "title_zh": "基於特徵空間姿態正面化的姿態無關人臉識別", "summary_zh": "姿態不變臉部識別已成為現代AI臉部識別系統的挑戰。其目標是將自然環境下的側面臉與資料庫中的正面臉進行匹配。現有方法透過生成模型或學習姿態穩健的特徵表示來實現臉部正面化。本文提出一種新方法，在特徵空間中進行臉部正面化和識別。首先，提出一種新穎的特徵空間姿態正面化模組（FSPFM），將任意角度的側面圖像轉換為正面圖像。其次，提出一種新的訓練範式，以最大化FSPFM的潛力並提高其性能，該範式包含預訓練和注意力引導的微調階段。此外，在五個流行的臉部識別基準上進行了大量實驗。結果表明，該方法不僅在姿態不變臉部識別任務中優於最先進的方法，而且在其他標準場景中也保持卓越的性能。", "audio": "audios/2505.16412v1.mp3", "timestamp": "2025-05-24T14:15:10.093123"}
{"query": "Diffusion Model", "id": "2505.15863v1", "url": "http://arxiv.org/abs/2505.15863v1", "title": "Generative AI for Autonomous Driving: A Review", "summary": "Generative AI (GenAI) is rapidly advancing the field of Autonomous Driving\n(AD), extending beyond traditional applications in text, image, and video\ngeneration. We explore how generative models can enhance automotive tasks, such\nas static map creation, dynamic scenario generation, trajectory forecasting,\nand vehicle motion planning. By examining multiple generative approaches\nranging from Variational Autoencoder (VAEs) over Generative Adversarial\nNetworks (GANs) and Invertible Neural Networks (INNs) to Generative\nTransformers (GTs) and Diffusion Models (DMs), we highlight and compare their\ncapabilities and limitations for AD-specific applications. Additionally, we\ndiscuss hybrid methods integrating conventional techniques with generative\napproaches, and emphasize their improved adaptability and robustness. We also\nidentify relevant datasets and outline open research questions to guide future\ndevelopments in GenAI. Finally, we discuss three core challenges: safety,\ninterpretability, and realtime capabilities, and present recommendations for\nimage generation, dynamic scenario generation, and planning.", "authors": ["Katharina Winter", "Abhishek Vivekanandan", "Rupert Polley", "Yinzhe Shen", "Christian Schlauch", "Mohamed-Khalil Bouzidi", "Bojan Derajic", "Natalie Grabowsky", "Annajoyce Mariani", "Dennis Rochau", "Giovanni Lucente", "Harsh Yadav", "Firas Mualla", "Adam Molin", "Sebastian Bernhard", "Christian Wirth", "Ömer Şahin Taş", "Nadja Klein", "Fabian B. Flohr", "Hanno Gottschalk"], "published_date": "2025-05-21", "title_zh": "生成式人工智慧於自動駕駛之應用：綜述", "summary_zh": "生成式人工智慧(GenAI)正快速推進自動駕駛(AD)領域，超越傳統文字、圖像和影片生成應用。本文探討生成模型如何提升汽車任務，如靜態地圖建立、動態場景生成、軌跡預測和車輛運動規劃。研究變分自編碼器(VAEs)、生成對抗網路(GANs)、可逆神經網路(INNs)、生成式轉換器(GTs)和擴散模型(DMs)等生成方法，比較其在AD應用中的能力和局限性。同時討論混合方法，強調其改進的適應性和穩健性。概述相關資料集並提出開放性研究問題，以指導GenAI未來發展。最後，探討安全性、可解釋性和即時性三大核心挑戰，並針對圖像生成、動態場景生成和規劃提出建議。", "audio": "audios/2505.15863v1.mp3", "timestamp": "2025-05-24T14:15:21.574103"}
{"query": "AI", "id": "2505.16388v1", "url": "http://arxiv.org/abs/2505.16388v1", "title": "Serious Games: Human-AI Interaction, Evolution, and Coevolution", "summary": "The serious games between humans and AI have only just begun. Evolutionary\nGame Theory (EGT) models the competitive and cooperative strategies of\nbiological entities. EGT could help predict the potential evolutionary\nequilibrium of humans and AI. The objective of this work was to examine some of\nthe EGT models relevant to human-AI interaction, evolution, and coevolution. Of\nthirteen EGT models considered, three were examined: the Hawk-Dove Game,\nIterated Prisoner's Dilemma, and the War of Attrition. This selection was based\non the widespread acceptance and clear relevance of these models to potential\nhuman-AI evolutionary dynamics and coevolutionary trajectories. The Hawk-Dove\nGame predicts balanced mixed-strategy equilibria based on the costs of\nconflict. It also shows the potential for balanced coevolution rather than\ndominance. Iterated Prisoner's Dilemma suggests that repeated interaction may\nlead to cognitive coevolution. It demonstrates how memory and reciprocity can\nlead to cooperation. The War of Attrition suggests that competition for\nresources may result in strategic coevolution, asymmetric equilibria, and\nconventions on sharing resources. Therefore, EGT may provide a suitable\nframework to understand and predict the human-AI evolutionary dynamic. However,\nfuture research could extend beyond EGT and explore additional frameworks,\nempirical validation methods, and interdisciplinary perspectives. AI is being\nshaped by human input and is evolving in response to it. So too,\nneuroplasticity allows the human brain to grow and evolve in response to\nstimuli. If humans and AI converge in future, what might be the result of human\nneuroplasticity combined with an ever-evolving AI? Future research should be\nmindful of the ethical and cognitive implications of human-AI interaction,\nevolution, and coevolution.", "authors": ["Nandini Doreswamy", "Louise Horstmanshof"], "published_date": "2025-05-22", "title_zh": "嚴肅遊戲：人機互動、演進與共同演化", "summary_zh": "人機嚴肅博弈方興未艾。演化博弈理論(EGT)模擬生物實體的競爭與合作策略，或能預測人機演化的潛在平衡。本文探討與人機互動、演化及共同演化相關的EGT模型。在考量的十三種模型中，檢視了鷹鴿博弈、重複囚徒困境及消耗戰。鷹鴿博弈基於衝突成本預測混合策略均衡，並展現平衡共同演化的可能性。重複囚徒困境表明重複互動可能導致認知共同演化，記憶與互惠有助合作。消耗戰則暗示資源競爭可能導致策略共同演化、非對稱均衡及資源共享慣例。因此，EGT或能為理解與預測人機演化動態提供框架。然而，未來研究應拓展至EGT之外，探索其他框架、驗證方法及跨領域觀點。AI受人類輸入塑造並隨之演化，而神經可塑性使人腦得以成長與演化。若人機於未來融合，結合人類神經可塑性與持續演化的AI可能產生何種結果？未來研究應關注人機互動、演化及共同演化所涉及的倫理與認知影響。", "audio": "audios/2505.16388v1.mp3", "timestamp": "2025-05-24T15:16:55.688048"}
{"query": "AI", "id": "2505.16381v1", "url": "http://arxiv.org/abs/2505.16381v1", "title": "PaTH Attention: Position Encoding via Accumulating Householder Transformations", "summary": "The attention mechanism is a core primitive in modern large language models\n(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,\nposition encoding is essential for modeling structured domains such as\nlanguage. Rotary position encoding (RoPE) has emerged as the de facto standard\napproach for position encoding and is part of many modern LLMs. However, in\nRoPE the key/query transformation between two elements in a sequence is only a\nfunction of their relative position and otherwise independent of the actual\ninput. This limits the expressivity of RoPE-based transformers.\n  This paper describes PaTH, a flexible data-dependent position encoding scheme\nbased on accumulated products of Householder(like) transformations, where each\ntransformation is data-dependent, i.e., a function of the input. We derive an\nefficient parallel algorithm for training through exploiting a compact\nrepresentation of products of Householder matrices, and implement a\nFlashAttention-style blockwise algorithm that minimizes I/O cost. Across both\ntargeted synthetic benchmarks and moderate-scale real-world language modeling\nexperiments, we find that PaTH demonstrates superior performance compared to\nRoPE and other recent baselines.", "authors": ["Songlin Yang", "Yikang Shen", "Kaiyue Wen", "Shawn Tan", "Mayank Mishra", "Liliang Ren", "Rameswar Panda", "Yoon Kim"], "published_date": "2025-05-22", "title_zh": "PaTH注意力：基於累積豪斯霍爾德變換的位置編碼", "summary_zh": "注意力機制是現代大型語言模型及廣義人工智慧的核心元件。由於注意力本身對排列不變，位置編碼對於語言等結構化領域的建模至關重要。旋轉位置編碼(RoPE)已成為事實上的標準方法，並被廣泛應用於許多現代大型語言模型中。然而，RoPE中序列中兩個元素之間的鍵/查詢轉換僅取決於它們的相對位置，而與實際輸入無關，這限制了基於RoPE的轉換器的表達能力。\n\n本研究提出PaTH，一種基於累積豪斯霍爾德(Householder)類變換乘積的靈活且數據相關的位置編碼方案，其中每個變換都與數據相關，即輸入的函數。我們推導出一種高效的平行訓練算法，利用豪斯霍爾德矩陣乘積的緊湊表示，並實現了一種FlashAttention風格的塊式算法，以最大限度地降低I/O成本。在有針對性的合成基準測試和中等規模的真實語言建模實驗中，PaTH均展現出優於RoPE和其他最新基準的性能。", "audio": "audios/2505.16381v1.mp3", "timestamp": "2025-05-24T16:20:44.065769"}
{"query": "AI", "id": "2505.16379v1", "url": "http://arxiv.org/abs/2505.16379v1", "title": "Materials Generation in the Era of Artificial Intelligence: A Comprehensive Survey", "summary": "Materials are the foundation of modern society, underpinning advancements in\nenergy, electronics, healthcare, transportation, and infrastructure. The\nability to discover and design new materials with tailored properties is\ncritical to solving some of the most pressing global challenges. In recent\nyears, the growing availability of high-quality materials data combined with\nrapid advances in Artificial Intelligence (AI) has opened new opportunities for\naccelerating materials discovery. Data-driven generative models provide a\npowerful tool for materials design by directly create novel materials that\nsatisfy predefined property requirements. Despite the proliferation of related\nwork, there remains a notable lack of up-to-date and systematic surveys in this\narea. To fill this gap, this paper provides a comprehensive overview of recent\nprogress in AI-driven materials generation. We first organize various types of\nmaterials and illustrate multiple representations of crystalline materials. We\nthen provide a detailed summary and taxonomy of current AI-driven materials\ngeneration approaches. Furthermore, we discuss the common evaluation metrics\nand summarize open-source codes and benchmark datasets. Finally, we conclude\nwith potential future directions and challenges in this fast-growing field. The\nrelated sources can be found at\nhttps://github.com/ZhixunLEE/Awesome-AI-for-Materials-Generation.", "authors": ["Zhixun Li", "Bin Cao", "Rui Jiao", "Liang Wang", "Ding Wang", "Yang Liu", "Dingshuo Chen", "Jia Li", "Qiang Liu", "Yu Rong", "Liang Wang", "Tong-yi Zhang", "Jeffrey Xu Yu"], "published_date": "2025-05-22", "title_zh": "人工智慧時代的材料生成：一份綜合綜述", "summary_zh": "材料是現代社會的基石，推動能源、電子、醫療、運輸和基礎設施的發展。 具備發現和設計具備客製化屬性新材料的能力，對於解決緊迫的全球挑戰至關重要。 近年來，高品質材料數據的普及與人工智慧的快速發展，為加速材料發現創造了新機遇。 數據驅動的生成模型透過直接創造滿足預定義屬性要求的新材料，為材料設計提供強大工具。 儘管相關研究不斷湧現，但仍缺乏最新且系統性的綜述。 本文旨在彌合此差距，全面概述人工智慧驅動材料生成的最新進展。 首先，組織不同類型的材料，並闡明晶體材料的多種表示方式。 接著，詳細總結和分類當前人工智慧驅動的材料生成方法。 此外，討論了常見的評估指標，並總結了開源程式碼和基準數據集。 最後，總結了這個快速發展領域中潛在的未來方向和挑戰。 相關資源可在https://github.com/ZhixunLEE/Awesome-AI-for-Materials-Generation找到。", "audio": "audios/2505.16379v1.mp3", "timestamp": "2025-05-24T17:15:01.979017"}
{"query": "AI", "id": "2505.16366v1", "url": "http://arxiv.org/abs/2505.16366v1", "title": "ReCopilot: Reverse Engineering Copilot in Binary Analysis", "summary": "Binary analysis plays a pivotal role in security domains such as malware\ndetection and vulnerability discovery, yet it remains labor-intensive and\nheavily reliant on expert knowledge. General-purpose large language models\n(LLMs) perform well in programming analysis on source code, while\nbinaryspecific LLMs are underexplored. In this work, we present ReCopilot, an\nexpert LLM designed for binary analysis tasks. ReCopilot integrates binary code\nknowledge through a meticulously constructed dataset, encompassing continue\npretraining (CPT), supervised fine-tuning (SFT), and direct preference\noptimization (DPO) stages. It leverages variable data flow and call graph to\nenhance context awareness and employs test-time scaling to improve reasoning\ncapabilities. Evaluations on a comprehensive binary analysis benchmark\ndemonstrate that ReCopilot achieves state-of-the-art performance in tasks such\nas function name recovery and variable type inference on the decompiled pseudo\ncode, outperforming both existing tools and LLMs by 13%. Our findings highlight\nthe effectiveness of domain-specific training and context enhancement, while\nalso revealing challenges in building super long chain-of-thought. ReCopilot\nrepresents a significant step toward automating binary analysis with\ninterpretable and scalable AI assistance in this domain.", "authors": ["Guoqiang Chen", "Huiqi Sun", "Daguang Liu", "Zhiqi Wang", "Qiang Wang", "Bin Yin", "Lu Liu", "Lingyun Ying"], "published_date": "2025-05-22", "title_zh": "ReCopilot：二進位分析中Copilot的反向工程", "summary_zh": "二進制分析在惡意軟體檢測和漏洞發現等安全領域至關重要，但仍需大量人力且依賴專家知識。通用大型語言模型（LLM）在原始碼程式分析中表現出色，而針對二進制的LLM則有待探索。本文提出ReCopilot，一款專為二進制分析設計的專家LLM。ReCopilot通過精心構建的數據集整合二進制程式碼知識，包括持續預訓練（CPT）、監督式微調（SFT）和直接偏好優化（DPO）階段。它利用變數數據流和調用圖來增強上下文感知，並採用測試時縮放來提高推理能力。在全面的二進制分析基準測試中，ReCopilot在函數名稱恢復和變數類型推斷等任務上表現出色，超越現有工具和LLM達13%。研究結果強調了領域特定訓練和上下文增強的有效性，同時揭示了構建超長思維鏈的挑戰。ReCopilot代表著二進制分析自動化方面的一個重大進展，可在該領域提供可解釋且可擴展的AI輔助。", "audio": "audios/2505.16366v1.mp3", "timestamp": "2025-05-24T18:23:17.108003"}
{"query": "AI", "id": "2505.16358v1", "url": "http://arxiv.org/abs/2505.16358v1", "title": "Strategic Content Creation in the Age of GenAI: To Share or Not to Share?", "summary": "We introduce a game-theoretic framework examining strategic interactions\nbetween a platform and its content creators in the presence of AI-generated\ncontent. Our model's main novelty is in capturing creators' dual strategic\ndecisions: The investment in content quality and their (possible) consent to\nshare their content with the platform's GenAI, both of which significantly\nimpact their utility. To incentivize creators, the platform strategically\nallocates a portion of its GenAI-driven revenue to creators who share their\ncontent. We focus on the class of full-sharing equilibrium profiles, in which\nall creators willingly share their content with the platform's GenAI system.\nSuch equilibria are highly desirable both theoretically and practically. Our\nmain technical contribution is formulating and efficiently solving a novel\noptimization problem that approximates the platform's optimal revenue subject\nto inducing a full-sharing equilibrium. A key aspect of our approach is\nidentifying conditions under which full-sharing equilibria exist and a\nsurprising connection to the Prisoner's Dilemma. Finally, our simulations\ndemonstrate how revenue-allocation mechanisms affect creator utility and the\nplatform's revenue.", "authors": ["Gur Keinan", "Omer Ben-Porat"], "published_date": "2025-05-22", "title_zh": "生成式人工智慧時代的策略性內容創作：分享還是不分享？", "summary_zh": "本研究提出一個博弈論框架，分析平台與內容創作者在AI生成內容存在下的策略互動。模型創新之處在於捕捉創作者的雙重策略決策：內容品質的投資以及是否同意將內容分享給平台的生成式AI，兩者均顯著影響其效用。為激勵創作者，平台將其生成式AI產生的部分收入策略性地分配給分享內容的創作者。研究重點關注全共享均衡，即所有創作者都願意與平台的生成式AI系統分享內容。此類均衡在理論和實踐上都非常理想。主要技術貢獻在於構建並有效解決一個新穎的優化問題，該問題近似於平台在誘導全共享均衡下的最佳收入。研究方法的關鍵是識別全共享均衡存在的條件，以及與囚徒困境的驚人聯繫。模擬結果展示了收入分配機制如何影響創作者效用和平台收入。", "audio": "audios/2505.16358v1.mp3", "timestamp": "2025-05-24T19:13:51.699335"}
{"query": "AI", "id": "2505.16350v1", "url": "http://arxiv.org/abs/2505.16350v1", "title": "Sensing-Enhanced Handover Criterion for Low-Altitude Wireless Networks (LAWNs)", "summary": "With the rapid growth of the low-altitude economy, the demand for\ncellular-enabled low-altitude wireless networks (LAWNs) is rising\nsignificantly. The three-dimensional mobility of unmanned aerial vehicles\n(UAVs) will lead to frequent handovers (HOs) in cellular networks, while\ntraditional reference signal received power (RSRP)-based criteria may fail to\ncapture the dynamic environment, causing redundant HOs or HO failures. To\naddress this issue and motivated by the underutilization of sensing information\nin conventional HO mechanisms, we propose a novel HO activation criterion for\nUAV systems that integrates both sensing parameters provided by integrated\nsensing and communication (ISAC) signals and RSRP. First, we construct an ISAC\nsignal model tailored for low-altitude scenarios and derive the Cram\\'er-Rao\nlower bound for sensing distance estimation. Subsequently, we propose a novel\njoint HO criterion that extends the conventional RSRP-based method by\nintegrating sensing information from ISAC signals, enabling more reliable HOs\nin dynamic UAV environments. Simulation results show that the joint HO\ncriterion outperforms the baseline RSRP-based criterion under different\nsignal-to-noise ratio (SNR) and sensing pilot ratio conditions. Particularly,\nwhen SNR is greater than 0dB and the sensing pilot ratio is 20%, the proposed\njoint HO criterion reduces the average HO region length by 49.97% and improves\nthe activation probability by 76.31%.", "authors": ["Jingli Li", "Yiyan Ma", "Bo Ai", "Qingqing Cheng", "Guoyu Ma", "Mi Yang", "Yunlong Lu", "Wenwei Yue", "Zhangdui Zhong"], "published_date": "2025-05-22", "title_zh": "低空無線網路中基於感知增強的切換準則", "summary_zh": "隨著低空經濟快速發展，對蜂巢式低空無線網路的需求顯著增加。無人機的三維移動性導致蜂巢網路中頻繁切換，傳統基於參考信號接收功率的準則難以應對動態環境，造成冗餘切換或切換失敗。為了解決此問題，並基於傳統切換機制對感知資訊利用不足的考量，本文提出一種新的無人機系統切換啟動準則，整合整合感知與通信訊號提供的感知參數和參考信號接收功率。首先，構建適用於低空場景的整合感知與通信訊號模型，並推導感知距離估計的Cramér-Rao下界。隨後，提出一種聯合切換準則，通過整合整合感知與通信訊號的感知資訊來擴展傳統基於參考信號接收功率的方法，從而在動態無人機環境中實現更可靠的切換。模擬結果表明，在不同的信噪比和感知導頻比條件下，聯合切換準則優於基於參考信號接收功率的基準準則。特別是，當信噪比大於0dB且感知導頻比為20%時，所提出的聯合切換準則將平均切換區域長度減少49.97%，並將啟動概率提高76.31%。", "audio": "audios/2505.16350v1.mp3", "timestamp": "2025-05-24T20:18:58.808084"}
{"query": "AI", "id": "2505.16339v1", "url": "http://arxiv.org/abs/2505.16339v1", "title": "Rethinking Code Review Workflows with LLM Assistance: An Empirical Study", "summary": "Code reviews are a critical yet time-consuming aspect of modern software\ndevelopment, increasingly challenged by growing system complexity and the\ndemand for faster delivery. This paper presents a study conducted at\nWirelessCar Sweden AB, combining an exploratory field study of current code\nreview practices with a field experiment involving two variations of an\nLLM-assisted code review tool. The field study identifies key challenges in\ntraditional code reviews, including frequent context switching, insufficient\ncontextual information, and highlights both opportunities (e.g., automatic\nsummarization of complex pull requests) and concerns (e.g., false positives and\ntrust issues) in using LLMs. In the field experiment, we developed two\nprototype variations: one offering LLM-generated reviews upfront and the other\nenabling on-demand interaction. Both utilize a semantic search pipeline based\non retrieval-augmented generation to assemble relevant contextual information\nfor the review, thereby tackling the uncovered challenges. Developers evaluated\nboth variations in real-world settings: AI-led reviews are overall more\npreferred, while still being conditional on the reviewers' familiarity with the\ncode base, as well as on the severity of the pull request.", "authors": ["Fannar Steinn Aðalsteinsson", "Björn Borgar Magnússon", "Mislav Milicevic", "Adam Nirving Davidsson", "Chih-Hong Cheng"], "published_date": "2025-05-22", "title_zh": "以大型語言模型輔助重新思考程式碼審查流程：一項實證研究", "summary_zh": "程式碼審查是現代軟體開發的關鍵環節，但耗時且日益受到系統複雜性及快速交付需求挑戰。本研究於 WirelessCar Sweden AB 進行，結合了現有程式碼審查實務的探索性實地研究，以及涉及兩個 LLM 輔助程式碼審查工具變體的實地實驗。實地研究指出傳統程式碼審查中的主要挑戰，包括頻繁的上下文切換和上下文資訊不足，並強調了使用 LLM 的機會（如複雜提取請求的自動摘要）和疑慮（如誤報和信任問題）。在實地實驗中，我們開發了兩個原型變體：一個預先提供 LLM 生成的審查，另一個支援隨需互動。兩者都利用基於檢索增強生成的語義搜索管道來組裝相關的上下文資訊以進行審查，從而應對所發現的挑戰。開發人員在實際環境中評估了這兩個變體：AI 主導的審查總體上更受歡迎，但仍取決於審查者對程式碼庫的熟悉程度以及提取請求的嚴重程度。", "audio": "audios/2505.16339v1.mp3", "timestamp": "2025-05-24T21:16:17.349955"}
{"query": "AI", "id": "2505.16327v1", "url": "http://arxiv.org/abs/2505.16327v1", "title": "Cooperative NOMA Meets Emerging Technologies: A Survey for Next-Generation Wireless Networks", "summary": "The emerging demands of sixth-generation wireless networks, such as\nultra-connectivity, native intelligence, and cross-domain convergence, are\nbringing renewed focus to cooperative non-orthogonal multiple access (C-NOMA)\nas a fundamental enabler of scalable, efficient, and intelligent communication\nsystems. C-NOMA builds on the core benefits of NOMA by leveraging user\ncooperation and relay strategies to enhance spectral efficiency, coverage, and\nenergy performance. This article presents a unified and forward-looking survey\non the integration of C-NOMA with key enabling technologies, including radio\nfrequency energy harvesting, cognitive radio networks, reconfigurable\nintelligent surfaces, space-air-ground integrated networks, and integrated\nsensing and communication-assisted semantic communication. Foundational\nprinciples and relaying protocols are first introduced to establish the\ntechnical relevance of C-NOMA. Then, a focused investigation is conducted into\nprotocol-level synergies, architectural models, and deployment strategies\nacross these technologies. Beyond integration, this article emphasizes the\norchestration of C-NOMA across future application domains such as digital\ntwins, extended reality, and e-health. In addition, it provides an extensive\nand in-depth review of recent literature, categorized by relaying schemes,\nsystem models, performance metrics, and optimization paradigms, including\nmodel-based, heuristic, and AI-driven approaches. Finally, open challenges and\nfuture research directions are outlined, spanning standardization, security,\nand cross-layer design, positioning C-NOMA as a key pillar of intelligent\nnext-generation network architectures.", "authors": ["Mahmoud M. Salim", "Suhail I. Al-Dharrab", "Daniel Benevides Da Costa", "Ali H. Muqaibel"], "published_date": "2025-05-22", "title_zh": "合作式非正交多重接取與新興技術：下一代無線網路綜述", "summary_zh": "第六代無線網路對超連接、原生智慧和跨域融合的需求，使協作式非正交多重接取（C-NOMA）重新受到重視，成為可擴展、高效和智慧通訊系統的基石。C-NOMA利用使用者協作和中繼策略，在NOMA的基礎上提升頻譜效率、覆蓋範圍和能源效能。本文全面前瞻地探討了C-NOMA與射頻能量收集、認知無線電網路、可重構智慧表面、空天地一體化網路和整合感知通訊輔助的語義通訊等關鍵技術的整合，闡述了C-NOMA的基礎原理和中繼協定，並深入研究了這些技術間的協同效應、架構模型和部署策略。除整合外，本文還強調C-NOMA在數位分身、擴增實境和電子醫療等未來應用領域的協調作用。此外，本文廣泛且深入地回顧了近年文獻，按中繼方案、系統模型、效能指標和優化範例（包括基於模型、啟發式和人工智慧驅動的方法）進行分類。最後，概述了標準化、安全性和跨層設計等方面的開放挑戰和未來研究方向，將C-NOMA定位為智慧型下一代網路架構的關鍵支柱。", "audio": "audios/2505.16327v1.mp3", "timestamp": "2025-05-24T22:16:35.597537"}
{"query": "AI", "id": "2505.16319v1", "url": "http://arxiv.org/abs/2505.16319v1", "title": "FreshRetailNet-50K: A Stockout-Annotated Censored Demand Dataset for Latent Demand Recovery and Forecasting in Fresh Retail", "summary": "Accurate demand estimation is critical for the retail business in guiding the\ninventory and pricing policies of perishable products. However, it faces\nfundamental challenges from censored sales data during stockouts, where\nunobserved demand creates systemic policy biases. Existing datasets lack the\ntemporal resolution and annotations needed to address this censoring effect. To\nfill this gap, we present FreshRetailNet-50K, the first large-scale benchmark\nfor censored demand estimation. It comprises 50,000 store-product time series\nof detailed hourly sales data from 898 stores in 18 major cities, encompassing\n863 perishable SKUs meticulously annotated for stockout events. The hourly\nstock status records unique to this dataset, combined with rich contextual\ncovariates, including promotional discounts, precipitation, and temporal\nfeatures, enable innovative research beyond existing solutions. We demonstrate\none such use case of two-stage demand modeling: first, we reconstruct the\nlatent demand during stockouts using precise hourly annotations. We then\nleverage the recovered demand to train robust demand forecasting models in the\nsecond stage. Experimental results show that this approach achieves a 2.73\\%\nimprovement in prediction accuracy while reducing the systematic demand\nunderestimation from 7.37\\% to near-zero bias. With unprecedented temporal\ngranularity and comprehensive real-world information, FreshRetailNet-50K opens\nnew research directions in demand imputation, perishable inventory\noptimization, and causal retail analytics. The unique annotation quality and\nscale of the dataset address long-standing limitations in retail AI, providing\nimmediate solutions and a platform for future methodological innovation. The\ndata (https://huggingface.co/datasets/Dingdong-Inc/FreshRetailNet-50K) and code\n(https://github.com/Dingdong-Inc/frn-50k-baseline}) are openly released.", "authors": ["Yangyang Wang", "Jiawei Gu", "Li Long", "Xin Li", "Li Shen", "Zhouyu Fu", "Xiangjun Zhou", "Xu Jiang"], "published_date": "2025-05-22", "title_zh": "生鮮零售網絡-50K：一個具缺貨標註的截斷需求資料集，用於生鮮零售中潛在需求恢復與預測", "summary_zh": "生鮮零售業需精準預測需求以制定庫存和定價策略，然缺貨時的銷售數據受限，造成策略偏差。現有數據集缺乏足夠的時間解析度和標註以解決此問題。為此，我們推出FreshRetailNet-50K，首個大規模受限需求估算基準，包含來自18個主要城市898家商店的5萬條商品時序資料，涵蓋863個生鮮商品，並針對缺貨事件進行詳盡標註。此數據集獨有的時級庫存狀態記錄，結合促銷折扣、降水和時間特徵等豐富的上下文變數，促進超越現有解決方案的創新研究。我們展示了兩階段需求建模的應用：首先，使用精確的時級標註重建缺貨期間的潛在需求；然後，利用恢復的需求訓練穩健的需求預測模型。實驗結果表明，此方法可提高2.73%的預測準確度，並將系統性的需求低估從7.37%降低至接近零偏差。FreshRetailNet-50K以前所未有的時間粒度和全面的真實世界信息，開闢了需求填補、生鮮庫存優化和因果零售分析的新研究方向。該數據集獨特的標註質量和規模解決了零售人工智慧長期存在的局限性，提供即時解決方案和未來方法創新的平台。數據和程式碼已公開發布。", "audio": "audios/2505.16319v1.mp3", "timestamp": "2025-05-24T23:16:53.721641"}
{"query": "AI", "id": "2505.16314v1", "url": "http://arxiv.org/abs/2505.16314v1", "title": "NTIRE 2025 challenge on Text to Image Generation Model Quality Assessment", "summary": "This paper reports on the NTIRE 2025 challenge on Text to Image (T2I)\ngeneration model quality assessment, which will be held in conjunction with the\nNew Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2025.\nThe aim of this challenge is to address the fine-grained quality assessment of\ntext-to-image generation models. This challenge evaluates text-to-image models\nfrom two aspects: image-text alignment and image structural distortion\ndetection, and is divided into the alignment track and the structural track.\nThe alignment track uses the EvalMuse-40K, which contains around 40K\nAI-Generated Images (AIGIs) generated by 20 popular generative models. The\nalignment track has a total of 371 registered participants. A total of 1,883\nsubmissions are received in the development phase, and 507 submissions are\nreceived in the test phase. Finally, 12 participating teams submitted their\nmodels and fact sheets. The structure track uses the EvalMuse-Structure, which\ncontains 10,000 AI-Generated Images (AIGIs) with corresponding structural\ndistortion mask. A total of 211 participants have registered in the structure\ntrack. A total of 1155 submissions are received in the development phase, and\n487 submissions are received in the test phase. Finally, 8 participating teams\nsubmitted their models and fact sheets. Almost all methods have achieved better\nresults than baseline methods, and the winning methods in both tracks have\ndemonstrated superior prediction performance on T2I model quality assessment.", "authors": ["Shuhao Han", "Haotian Fan", "Fangyuan Kong", "Wenjie Liao", "Chunle Guo", "Chongyi Li", "Radu Timofte", "Liang Li", "Tao Li", "Junhui Cui", "Yunqiu Wang", "Yang Tai", "Jingwei Sun", "Jianhui Sun", "Xinli Yue", "Tianyi Wang", "Huan Hou", "Junda Lu", "Xinyang Huang", "Zitang Zhou", "Zijian Zhang", "Xuhui Zheng", "Xuecheng Wu", "Chong Peng", "Xuezhi Cao", "Trong-Hieu Nguyen-Mau", "Minh-Hoang Le", "Minh-Khoa Le-Phan", "Duy-Nam Ly", "Hai-Dang Nguyen", "Minh-Triet Tran", "Yukang Lin", "Yan Hong", "Chuanbiao Song", "Siyuan Li", "Jun Lan", "Zhichao Zhang", "Xinyue Li", "Wei Sun", "Zicheng Zhang", "Yunhao Li", "Xiaohong Liu", "Guangtao Zhai", "Zitong Xu", "Huiyu Duan", "Jiarui Wang", "Guangji Ma", "Liu Yang", "Lu Liu", "Qiang Hu", "Xiongkuo Min", "Zichuan Wang", "Zhenchen Tang", "Bo Peng", "Jing Dong", "Fengbin Guan", "Zihao Yu", "Yiting Lu", "Wei Luo", "Xin Li", "Minhao Lin", "Haofeng Chen", "Xuanxuan He", "Kele Xu", "Qisheng Xu", "Zijian Gao", "Tianjiao Wan", "Bo-Cheng Qiu", "Chih-Chung Hsu", "Chia-ming Lee", "Yu-Fan Lin", "Bo Yu", "Zehao Wang", "Da Mu", "Mingxiu Chen", "Junkang Fang", "Huamei Sun", "Wending Zhao", "Zhiyu Wang", "Wang Liu", "Weikang Yu", "Puhong Duan", "Bin Sun", "Xudong Kang", "Shutao Li", "Shuai He", "Lingzhi Fu", "Heng Cong", "Rongyu Zhang", "Jiarong He", "Zhishan Qiao", "Yongqing Huang", "Zewen Chen", "Zhe Pang", "Juan Wang", "Jian Guo", "Zhizhuo Shao", "Ziyu Feng", "Bing Li", "Weiming Hu", "Hesong Li", "Dehua Liu", "Zeming Liu", "Qingsong Xie", "Ruichen Wang", "Zhihao Li", "Yuqi Liang", "Jianqi Bi", "Jun Luo", "Junfeng Yang", "Can Li", "Jing Fu", "Hongwei Xu", "Mingrui Long", "Lulin Tang"], "published_date": "2025-05-22", "title_zh": "NTIRE 2025文本到圖像生成模型品質評估挑戰賽", "summary_zh": "本研究報告 NTIRE 2025 文字生成圖像模型品質評估挑戰賽，該挑戰賽將於 CVPR 2025 的 NTIRE 工作坊同期舉行。旨在解決文字生成圖像模型之精細品質評估問題，從圖像-文字對齊及圖像結構失真偵測兩方面評估模型，分為對齊及結構兩賽道。對齊賽道使用 EvalMuse-40K 數據集，包含約四萬張由二十個主流生成模型產生之 AI 圖像，共 371 名註冊參與者，開發階段收到 1883 份提交，測試階段收到 507 份提交，最終 12 支隊伍提交模型及說明文件。結構賽道使用 EvalMuse-Structure 數據集，包含一萬張 AI 圖像及其結構失真遮罩，共 211 名註冊參與者，開發階段收到 1155 份提交，測試階段收到 487 份提交，最終 8 支隊伍提交模型及說明文件。多數方法優於基準方法，兩賽道獲勝者均展現出優異的 T2I 模型品質評估預測性能。", "audio": "audios/2505.16314v1.mp3", "timestamp": "2025-05-25T01:41:07.302050"}
{"query": "AI", "id": "2505.16301v1", "url": "http://arxiv.org/abs/2505.16301v1", "title": "Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space", "summary": "Molecular dynamics (MD) is a powerful tool for exploring the behavior of\natomistic systems, but its reliance on sequential numerical integration limits\nsimulation efficiency. We present MDtrajNet-1, a foundational AI model that\ndirectly generates MD trajectories across chemical space, bypassing force\ncalculations and integration. This approach accelerates simulations by up to\ntwo orders of magnitude compared to traditional MD, even those enhanced by\nmachine-learning interatomic potentials. MDtrajNet-1 combines equivariant\nneural networks with a Transformer-based architecture to achieve strong\naccuracy and transferability in predicting long-time trajectories for both\nknown and unseen systems. Remarkably, the errors of the trajectories generated\nby MDtrajNet-1 for various molecular systems are close to those of the\nconventional ab initio MD. The model's flexible design supports diverse\napplication scenarios, including different statistical ensembles, boundary\nconditions, and interaction types. By overcoming the intrinsic speed barrier of\nconventional MD, MDtrajNet-1 opens new frontiers in efficient and scalable\natomistic simulations.", "authors": ["Fuchun Ge", "Pavlo O. Dral"], "published_date": "2025-05-22", "title_zh": "化學空間分子動力學直接預測人工智慧方法", "summary_zh": "分子動力學是研究原子系統行為的有力工具，但其依賴序列數值積分限制了模擬效率。我們提出MDtrajNet-1，一個基礎AI模型，可直接生成化學空間中的分子動力學軌跡，繞過力計算和積分。相較於傳統分子動力學，即使是經過機器學習原子間勢強化的，此方法也能加速模擬達兩個數量級。MDtrajNet-1結合了等變神經網路與基於Transformer的架構，在預測已知和未知系統的長時間軌跡方面實現了高準確性和可轉移性。值得注意的是，MDtrajNet-1生成的各種分子系統軌跡誤差，與傳統從頭算分子動力學相近。該模型靈活的設計支持多樣化的應用場景，包括不同的統計系綜、邊界條件和交互類型。通過克服傳統分子動力學固有的速度障礙，MDtrajNet-1在高效且可擴展的原子模擬中開闢了新領域。", "audio": "audios/2505.16301v1.mp3", "timestamp": "2025-05-25T03:19:05.363911"}
{"query": "AI", "id": "2505.16290v1", "url": "http://arxiv.org/abs/2505.16290v1", "title": "Multimodal Generative AI for Story Point Estimation in Software Development", "summary": "This research explores the application of Multimodal Generative AI to enhance\nstory point estimation in Agile software development. By integrating text,\nimage, and categorical data using advanced models like BERT, CNN, and XGBoost,\nour approach surpasses the limitations of traditional single-modal estimation\nmethods. The results demonstrate strong accuracy for simpler story points,\nwhile also highlighting challenges in more complex categories due to data\nimbalance. This study further explores the impact of categorical data,\nparticularly severity, on the estimation process, emphasizing its influence on\nmodel performance. Our findings emphasize the transformative potential of\nmultimodal data integration in refining AI-driven project management, paving\nthe way for more precise, adaptable, and domain-specific AI capabilities.\nAdditionally, this work outlines future directions for addressing data\nvariability and enhancing the robustness of AI in Agile methodologies.", "authors": ["Mohammad Rubyet Islam", "Peter Sandborn"], "published_date": "2025-05-22", "title_zh": "用於軟件開發中故事點估算的多模態生成式人工智能", "summary_zh": "本研究探討多模態生成式AI於敏捷軟體開發中提升故事點估算的應用。透過整合文本、圖像及類別資料，並採用BERT、CNN及XGBoost等模型，本方法優於傳統單模態估算。結果顯示，對於較簡單的故事點具有高度準確性，惟資料不平衡導致複雜類別面臨挑戰。研究進一步探討類別資料，特別是嚴重程度，對估算過程的影響，強調其對模型效能的影響。研究結果強調多模態資料整合在改進AI驅動專案管理中的變革潛力，為更精確、適應性強且領域特定的AI能力鋪路。此外，本研究亦概述未來方向，以解決資料變異性問題並增強AI在敏捷方法中的穩健性。", "audio": "audios/2505.16290v1.mp3", "timestamp": "2025-05-25T04:24:48.092301"}
{"query": "AI", "id": "2505.16278v1", "url": "http://arxiv.org/abs/2505.16278v1", "title": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "summary": "End-to-end autonomous driving (E2E-AD) demands effective processing of\nmulti-view sensory data and robust handling of diverse and complex driving\nscenarios, particularly rare maneuvers such as aggressive turns. Recent success\nof Mixture-of-Experts (MoE) architecture in Large Language Models (LLMs)\ndemonstrates that specialization of parameters enables strong scalability. In\nthis work, we propose DriveMoE, a novel MoE-based E2E-AD framework, with a\nScene-Specialized Vision MoE and a Skill-Specialized Action MoE. DriveMoE is\nbuilt upon our $\\pi_0$ Vision-Language-Action (VLA) baseline (originally from\nthe embodied AI field), called Drive-$\\pi_0$. Specifically, we add Vision MoE\nto Drive-$\\pi_0$ by training a router to select relevant cameras according to\nthe driving context dynamically. This design mirrors human driving cognition,\nwhere drivers selectively attend to crucial visual cues rather than\nexhaustively processing all visual information. In addition, we add Action MoE\nby training another router to activate specialized expert modules for different\ndriving behaviors. Through explicit behavioral specialization, DriveMoE is able\nto handle diverse scenarios without suffering from modes averaging like\nexisting models. In Bench2Drive closed-loop evaluation experiments, DriveMoE\nachieves state-of-the-art (SOTA) performance, demonstrating the effectiveness\nof combining vision and action MoE in autonomous driving tasks. We will release\nour code and models of DriveMoE and Drive-$\\pi_0$.", "authors": ["Zhenjie Yang", "Yilin Chai", "Xiaosong Jia", "Qifeng Li", "Yuqian Shao", "Xuekai Zhu", "Haisheng Su", "Junchi Yan"], "published_date": "2025-05-22", "title_zh": "DriveMoE：端到端自動駕駛中視覺-語言-動作模型的混合專家系統", "summary_zh": "端到端自動駕駛需有效處理多視角感知數據及應對複雜場景，特別是激進轉彎等罕見動作。混合專家模型(MoE)在大型語言模型(LLM)中的成功表明參數專精可實現強大的擴展性。本文提出DriveMoE，一種基於MoE的新型端到端自動駕駛框架，包含場景專精的視覺MoE和技能專精的動作MoE。DriveMoE基於Drive-$\\pi_0$（一個視覺-語言-動作基線模型），透過訓練路由選擇器動態選擇相關攝影機，為Drive-$\\pi_0$增加視覺MoE，模擬人類駕駛員選擇性關注關鍵視覺線索的認知方式。此外，透過訓練另一個路由選擇器激活不同駕駛行為的專家模組，增加動作MoE。透過行為專精，DriveMoE能處理多樣場景，避免模式平均化。在Bench2Drive閉環評估中，DriveMoE達到最先進的性能，證明視覺和動作MoE結合於自動駕駛任務中的有效性。我們將公開DriveMoE和Drive-$\\pi_0$的程式碼與模型。", "audio": "audios/2505.16278v1.mp3", "timestamp": "2025-05-25T05:17:45.009442"}
{"query": "AI", "id": "2505.16274v1", "url": "http://arxiv.org/abs/2505.16274v1", "title": "Multimodal AI-based visualization of strategic leaders' emotional dynamics: a deep behavioral analysis of Trump's trade war discourse", "summary": "This study investigates the emotional rhythms and behavioral mechanisms of\ndominant political leaders in strategic decision-making. Using the Trump\nadministration's 125 percent tariff hike on China as a case, it adopts a\nMultimodal Cognitive Behavioral Modeling framework. This includes\nmicro-expression tracking, acoustic intonation analysis, semantic flow\nmodeling, cognitive load simulation, and strategic behavior mapping to\nconstruct a full-cycle simulation of emotion, motivation, and output. Results\nreveal that Trump's decisions are not driven by rational deduction, but emerge\nfrom dominance-coherence rhythms. A six-axis National Strategic Tempo\nIntervention Framework is proposed to support anticipatory policy modeling.", "authors": ["Wei Meng"], "published_date": "2025-05-22", "title_zh": "基於多模態人工智慧的戰略領導者情緒動態視覺化：川普貿易戰論述之深度行為分析", "summary_zh": "本研究探討政治領袖在策略決策中的情緒節奏與行為機制。以特朗普政府對中國徵收125%關稅為例，採用多模態認知行為建模框架，包含微表情追蹤、語音語調分析、語義流建模、認知負荷模擬及策略行為映射，構建情緒、動機與產出的全週期模擬。結果顯示，特朗普的決策並非理性推導，而是源於支配-連貫節奏。據此提出六軸國家戰略節奏干預框架，以支持預測性政策建模。", "audio": "audios/2505.16274v1.mp3", "timestamp": "2025-05-25T06:24:54.128354"}
{"query": "AI", "id": "2505.16263v1", "url": "http://arxiv.org/abs/2505.16263v1", "title": "All You Need is \"Leet\": Evading Hate-speech Detection AI", "summary": "Social media and online forums are increasingly becoming popular.\nUnfortunately, these platforms are being used for spreading hate speech. In\nthis paper, we design black-box techniques to protect users from hate-speech on\nonline platforms by generating perturbations that can fool state of the art\ndeep learning based hate speech detection models thereby decreasing their\nefficiency. We also ensure a minimal change in the original meaning of\nhate-speech. Our best perturbation attack is successfully able to evade\nhate-speech detection for 86.8 % of hateful text.", "authors": ["Sampanna Yashwant Kahu", "Naman Ahuja"], "published_date": "2025-05-22", "title_zh": "只需「駭客語」：規避仇恨言論偵測人工智慧", "summary_zh": "社群媒體與線上論壇日益普及，卻也成為仇恨言論的溫床。本文設計黑箱技術，透過生成擾動以欺騙最先進的深度學習仇恨言論檢測模型，降低其效率，從而保護使用者免受網路平台上的仇恨言論侵害。我們同時確保原始仇恨言論的語義變動最小。最佳擾動攻擊成功規避了86.8%仇恨文本的檢測。", "audio": "audios/2505.16263v1.mp3", "timestamp": "2025-05-25T07:16:14.534239"}
{"query": "AI", "id": "2505.16254v1", "url": "http://arxiv.org/abs/2505.16254v1", "title": "Reassessing Collaborative Writing Theories and Frameworks in the Age of LLMs: What Still Applies and What We Must Leave Behind", "summary": "In this paper, we conduct a critical review of existing theories and\nframeworks on human-human collaborative writing to assess their relevance to\nthe current human-AI paradigm in professional contexts, and draw seven insights\nalong with design implications for human-AI collaborative writing tools. We\nfound that, as LLMs nudge the writing process more towards an empirical \"trial\nand error\" process analogous to prototyping, the non-linear cognitive process\nof writing will stay the same, but more rigor will be required for revision\nmethodologies. This shift would shed further light on the importance of\ncoherence support, but the large language model (LLM)'s unprecedented semantic\ncapabilities can bring novel approaches to this ongoing challenge. We argue\nthat teamwork-related factors such as group awareness, consensus building and\nauthorship - which have been central in human-human collaborative writing\nstudies - should not apply to the human-AI paradigm due to excessive\nanthropomorphism. With the LLM's text generation capabilities becoming\nessentially indistinguishable from human-written ones, we are entering an era\nwhere, for the first time in the history of computing, we are engaging in\ncollaborative writing with AI at workplaces on a daily basis. We aim to bring\ntheoretical grounding and practical design guidance to the interaction designs\nof human-AI collaborative writing, with the goal of enhancing future human-AI\nwriting software.", "authors": ["Daisuke Yukita", "Tim Miller", "Joel Mackenzie"], "published_date": "2025-05-22", "title_zh": "大型語言模型時代下協作寫作理論與框架之再評估：何者適用，何者應捨棄", "summary_zh": "本文 критично地回顧現有的人際協同寫作理論與框架，評估其在當前人機協作範式下的適用性，並提出七項見解及人機協作寫作工具的設計意涵。研究發現，大型語言模型將寫作過程推向更偏向實驗性的「試錯」模式，類似原型設計，寫作的非線性認知過程將保持不變，但修訂方法需更嚴謹。此轉變突顯一致性支援的重要性，而大型語言模型的前所未有的語義能力可為此挑戰帶來新方法。由於過度擬人化，團隊合作因素，如群體意識、共識建立和作者身份，不應適用於人機協作範式。隨著大型語言模型的文本生成能力與人類書寫難以區分，我們正進入一個在工作場所每天與人工智慧進行協同寫作的時代。本文旨在為人機協同寫作的互動設計提供理論基礎與實用設計指導，以提升未來人機寫作軟體。", "audio": "audios/2505.16254v1.mp3", "timestamp": "2025-05-25T08:21:47.398343"}
{"query": "AI", "id": "2505.16809v2", "url": "http://arxiv.org/abs/2505.16809v2", "title": "Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor Segmentation with Missing Modalities", "summary": "Existing methods for multimodal MRI segmentation with missing modalities\ntypically assume that all MRI modalities are available during training.\nHowever, in clinical practice, some modalities may be missing due to the\nsequential nature of MRI acquisition, leading to performance degradation.\nFurthermore, retraining models to accommodate newly available modalities can be\ninefficient and may cause overfitting, potentially compromising previously\nlearned knowledge. To address these challenges, we propose Replay-based\nHypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation\nwith missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to\nenable the segmentation model to learn from newly acquired MRI modalities\nwithout forgetting previously learned information. To enhance segmentation\nperformance across diverse patient scenarios, we introduce the Cross-Patient\nHypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture\nhigh-order associations between patients. Additionally, we incorporate\nTversky-Aware Contrastive (TAC) loss to effectively mitigate information\nimbalance both across and within different modalities. Extensive experiments on\nthe BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art\nmethods, achieving an improvement of over 2% in the Dice Similarity Coefficient\nacross various tumor regions.", "authors": ["Junze Wang", "Lei Fan", "Weipeng Jing", "Donglin Di", "Yang Song", "Sidong Liu", "Cong Cong"], "published_date": "2025-05-22", "title_zh": "超圖Tversky感知的領域增量學習用於缺失模態腦腫瘤分割", "summary_zh": "現有缺失模態多模態MRI分割方法通常假設訓練期間所有模態均可用，但臨床實務中，MRI採集的序列特性可能導致部分模態缺失，進而降低效能。重新訓練模型以適應新增模態效率低下，並可能導致過擬合，損害既有知識。為解決這些問題，我們提出基於重播的超圖域增量學習(ReHyDIL)用於缺失模態腦瘤分割。ReHyDIL利用域增量學習(DIL)使分割模型能從新獲取的MRI模態中學習，同時不遺忘既有資訊。為提升跨患者情境的分割效能，我們引入跨患者超圖分割網路(CHSNet)，利用超圖捕捉患者之間的高階關聯。此外，我們採用Tversky感知對比(TAC)損失，有效緩解不同模態之間和內部的資訊失衡。在BraTS2019資料集上的實驗表明，ReHyDIL優於現有方法，在各腫瘤區域的Dice相似係數上提高了2%以上。", "audio": "audios/2505.16809v2.mp3", "timestamp": "2025-05-26T01:28:04.799880"}
{"query": "Foundation Model", "id": "2505.16941v2", "url": "http://arxiv.org/abs/2505.16941v2", "title": "FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records", "summary": "Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models.", "authors": ["Chao Pang", "Vincent Jeanselme", "Young Sang Choi", "Xinzhuo Jiang", "Zilin Jing", "Aparajita Kashyap", "Yuta Kobayashi", "Yanwei Li", "Florent Pollet", "Karthik Natarajan", "Shalmali Joshi"], "published_date": "2025-05-22", "title_zh": "FoMoH：結構化電子健康紀錄臨床意義基礎模型評估", "summary_zh": "基於其提取與下游任務無關之表徵能力，基礎模型在醫療保健領域展現巨大潛力。此特性已使基於結構化電子健康記錄數據訓練的模型，在多個臨床應用中達到頂尖效能，即使在標記數據有限的情況下亦然，而此為醫療保健領域常見挑戰。然而，由於缺乏全面且具意義之任務需求，以及足夠多樣化的評估以表徵其相較於傳統監督式學習之優勢，對於這些模型之臨床效用潛力，共識甚少。為了解決此差距，我們提出一套涵蓋患者預後、急慢性疾病早期預測等臨床意義任務，包括穩健評估需求。我們使用來自哥倫比亞大學歐文醫學中心（CUMC）五百萬患者之電子健康記錄數據，在14項臨床相關任務中評估頂尖基礎模型。我們衡量總體準確度、校準度和亞群效能，以揭示基於預訓練、符號化和數據表示策略選擇之權衡。本研究旨在推進結構化電子健康記錄基礎模型之實證評估，並指導未來醫療保健基礎模型之開發。", "audio": "audios/2505.16941v2.mp3", "timestamp": "2025-05-26T01:28:14.452300"}
{"query": "Diffusion Model", "id": "2505.16839v2", "url": "http://arxiv.org/abs/2505.16839v2", "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding", "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.", "authors": ["Shufan Li", "Konstantinos Kallidromitis", "Hritik Bansal", "Akash Gokul", "Yusuke Kato", "Kazuki Kozuka", "Jason Kuen", "Zhe Lin", "Kai-Wei Chang", "Aditya Grover"], "published_date": "2025-05-22", "title_zh": "LaViDa：用於多模態理解的大型擴散語言模型", "summary_zh": "現代視覺語言模型（VLMs）能解決多種視覺推理任務。實際應用中，快速推論和可控生成（如限制輸出格式）至關重要，但現有自迴歸（AR）VLMs如LLaVA難以兼顧。離散擴散模型（DMs）提供另一選項，其平行解碼可加速推論，雙向上下文則能透過文本填充實現可控生成。雖DMs於純語言環境有效，但其多模態潛力尚未充分開發。我們提出LaViDa，一基於DMs的VLM家族。LaViDa透過為DMs配備視覺編碼器，並聯合微調以進行多模態指令跟隨。為解決挑戰，LaViDa採用互補遮罩以提升訓練效果、前綴KV快取以提升推論效率，以及時間步長位移以產生高品質樣本。實驗表明，LaViDa在MMMU等多模態基準測試上達到與AR VLMs匹敵甚至更優異的性能，同時具備DMs的獨特優勢，包含靈活的速度-品質權衡、可控性及雙向推理能力。在COCO圖像描述任務中，LaViDa以1.92倍的速度提升超越Open-LLaVa-Next-8B達+4.1 CIDEr。在雙向任務中，其於受限詩歌補全上實現+59%的提升。這些結果證明LaViDa是AR VLMs的有力替代方案。程式碼和模型將於最終版本發布。", "audio": "audios/2505.16839v2.mp3", "timestamp": "2025-05-26T01:28:27.137952"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論不一致性不會阻礙構建負責任人工智慧系統之路", "summary_zh": "本立場文件主張，應將責任式AI (RAI) 指標間常見的理論不一致性，如不同的公平性定義或準確性與隱私之間的權衡，視為寶貴特徵而非缺陷。將指標視為不同目標可帶來三項益處：(1) 規範多元主義：維護全套潛在衝突的指標，確保RAI中固有的多元道德立場和利害關係人價值觀獲得充分代表。(2) 知識完整性：使用多個、有時衝突的指標，能更全面地捕捉多面向的倫理概念，保有比單一簡化定義更高的資訊保真度。(3) 隱式正則化：聯合優化理論上衝突的目標可抑制過度擬合特定指標，引導模型朝向在現實複雜性下具有更強泛化和穩健性的解決方案。相反，簡化或刪減指標以強化理論一致性的做法，可能限縮價值多元性、喪失概念深度並降低模型效能。因此，我們提倡RAI理論與實踐的轉變：從受困於不一致性，轉向描述可接受的不一致性閾值，並闡明允許實踐中穩健且近似一致性的機制。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T03:15:39.391462"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具語義目標感知表徵的基礎表格模型", "summary_zh": "深度學習在多個領域表現卓越，但在表格學習任務上效果不如梯度提升決策樹。近年來，表格基礎模型興起，能利用真實世界知識並泛化至不同數據集，尤其在數據包含自由文本時。儘管將語言模型融入表格任務已有研究，但現有方法多採用靜態、與目標無關的文本表示，限制了其效能。本文提出TabSTAR：具備語義目標感知表示的表格基礎模型。TabSTAR旨在實現具文本特徵表格數據的遷移學習，其架構不含數據集特定參數。TabSTAR解凍預訓練文本編碼器，並將目標標記作為輸入，使模型能學習任務特定嵌入。TabSTAR在具文本特徵的分類任務基準測試中，於中大型數據集上皆達到最佳效能，且其預訓練階段展現出數據集數量的比例定律，為進一步提升效能提供途徑。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T03:15:47.099074"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "反應擴散模型、族群動態及疫情擴散之隨機基於主體蒙地卡羅模擬", "summary_zh": "本研究概述基於馬可夫隨機動力學的蒙地卡羅演算法，用於研究非熱平衡的多粒子交互和反應系統。這種基於代理人的電腦模擬為大學生和研究生提供了一種有效途徑，無需太多先備知識即可接觸前沿研究。學生可從模擬數據的可視化中直接了解複雜模型的宏觀特徵，進而運用更精密的數據分析，量化其豐富的動態特性，包含穩態和瞬態。我們利用反應擴散系統、種群動態和流行病傳播的隨機模型，闡述跨領域計算研究如何透過實作學習，有效地應用於由下而上的本科和研究生教育。此外，我們提供蒙地卡羅模擬演算法的實用建議、範例程式碼、數據分析工具，並描述潛在的錯誤來源和陷阱，以及規避方法。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T03:15:54.253127"}
{"query": "AI", "id": "2505.18129v1", "url": "http://arxiv.org/abs/2505.18129v1", "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning", "summary": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.", "authors": ["Yan Ma", "Linge Du", "Xuyang Shen", "Shaoxiang Chen", "Pengfei Li", "Qibing Ren", "Lizhuang Ma", "Yuchao Dai", "Pengfei Liu", "Junjie Yan"], "published_date": "2025-05-23", "title_zh": "一覽全局：視覺三元統一強化學習", "summary_zh": "強化學習大幅提升了視覺語言模型的推理能力，但其在推理之外，特別是物件偵測和定位等感知任務中的應用仍待探索。本文提出V-Triune，一種視覺三重統一強化學習系統，使視覺語言模型能在單一訓練流程中聯合學習視覺推理和感知任務。V-Triune包含三個互補組件：樣本級資料格式化（統一不同任務輸入）、驗證器級獎勵計算（透過專用驗證器提供客製化獎勵）以及來源級指標監控（診斷資料來源問題）。我們進一步引入動態IoU獎勵，為V-Triune處理的感知任務提供自適應、漸進式且明確的回饋。我們的方案採用現成的強化學習訓練框架，使用開源7B和32B骨幹模型。所得模型Orsta（一強化學習以觀全局）在推理和感知任務上均展現了持續改進，此廣泛能力得益於在涵蓋四個具代表性的視覺推理任務（數學、謎題、圖表、科學）和四個視覺感知任務（定位、偵測、計數、OCR）的多樣化資料集上的訓練。Orsta在MEGA-Bench Core上獲得顯著提升，7B和32B模型的各變體進步幅度從+2.1到+14.1不等，性能優勢延伸至廣泛的下游任務。這些結果突顯了我們統一強化學習方法對視覺語言模型的有效性和可擴展性。V-Triune系統和Orsta模型已公開。", "audio": "audios/2505.18129v1.mp3", "timestamp": "2025-05-26T04:24:47.239067"}
{"query": "Foundation Model", "id": "2505.18058v1", "url": "http://arxiv.org/abs/2505.18058v1", "title": "A Foundation Model Framework for Multi-View MRI Classification of Extramural Vascular Invasion and Mesorectal Fascia Invasion in Rectal Cancer", "summary": "Background: Accurate MRI-based identification of extramural vascular invasion\n(EVI) and mesorectal fascia invasion (MFI) is pivotal for risk-stratified\nmanagement of rectal cancer, yet visual assessment is subjective and vulnerable\nto inter-institutional variability. Purpose: To develop and externally evaluate\na multicenter, foundation-model-driven framework that automatically classifies\nEVI and MFI on axial and sagittal T2-weighted MRI. Methods: This retrospective\nstudy used 331 pre-treatment rectal cancer MRI examinations from three European\nhospitals. After TotalSegmentator-guided rectal patch extraction, a\nself-supervised frequency-domain harmonization pipeline was trained to minimize\nscanner-related contrast shifts. Four classifiers were compared: ResNet50,\nSeResNet, the universal biomedical pretrained transformer (UMedPT) with a\nlightweight MLP head, and a logistic-regression variant using frozen UMedPT\nfeatures (UMedPT_LR). Results: UMedPT_LR achieved the best EVI detection when\naxial and sagittal features were fused (AUC = 0.82; sensitivity = 0.75; F1\nscore = 0.73), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.74).\nThe highest MFI performance was attained by UMedPT on axial harmonized images\n(AUC = 0.77), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.75).\nFrequency-domain harmonization improved MFI classification but variably\naffected EVI performance. Conventional CNNs (ResNet50, SeResNet)\nunderperformed, especially in F1 score and balanced accuracy. Conclusion: These\nfindings demonstrate that combining foundation model features, harmonization,\nand multi-view fusion significantly enhances diagnostic performance in rectal\nMRI.", "authors": ["Yumeng Zhang", "Zohaib Salahuddin", "Danial Khan", "Shruti Atul Mali", "Henry C. Woodruff", "Sina Amirrajab", "Eduardo Ibor-Crespo", "Ana Jimenez-Pastor", "Luis Marti-Bonmati", "Philippe Lambin"], "published_date": "2025-05-23", "title_zh": "直腸癌中腸壁外血管侵犯和直腸繫膜筋膜侵犯多視圖MRI分類的基礎模型框架", "summary_zh": "本研究開發並驗證一套多中心、以基礎模型驅動的自動化框架，用於在T2加權MRI影像上判斷直腸癌的血管外侵犯（EVI）和腸繫膜筋膜侵犯（MFI）。研究採用來自歐洲三家醫院的331例直腸癌MRI掃描，利用TotalSegmentator提取直腸區域，並訓練一個自監督頻域協調流程，以減少掃描儀造成的對比差異。結果顯示，UMedPT_LR模型在融合軸向和矢狀特徵後，EVI檢測效果最佳（AUC=0.82），超越Chaimeleon Grand-Challenge獲獎模型。UMedPT模型在軸向協調影像上的MFI表現最佳（AUC=0.77），同樣優於Chaimeleon Grand-Challenge獲獎模型。頻域協調改善了MFI分類，但對EVI的影響不一。傳統CNN模型表現較差。結論表明，結合基礎模型特徵、影像協調和多視角融合能顯著提升直腸MRI的診斷效能。", "audio": "audios/2505.18058v1.mp3", "timestamp": "2025-05-26T04:24:54.635231"}
{"query": "Diffusion Model", "id": "2505.18142v1", "url": "http://arxiv.org/abs/2505.18142v1", "title": "TokBench: Evaluating Your Visual Tokenizer before Visual Generation", "summary": "In this work, we reveal the limitations of visual tokenizers and VAEs in\npreserving fine-grained features, and propose a benchmark to evaluate\nreconstruction performance for two challenging visual contents: text and face.\nImage tokenization has significantly advanced visual generation and multimodal\nmodeling, particularly with autoregressive models due to the modeling\nsimplicity of discrete tokens. Autoregressive models typically rely on image\ntokenizers to compress images into discrete tokens for sequential prediction,\nwhereas diffusion models often operate on continuous latent space to reduce\ncomputational costs. However, both visual compression approaches inevitably\nlose visual information, thereby limiting the upper bound of visual generation\nquality. To evaluate how these compression losses affect text and faces, the\nmost human-sensitive visual elements, we first collect and curate a collection\nof text and faces images from existing datasets, ensuring clarity and\ndiversity. For text reconstruction, we employ OCR models to assess the\nrecognition accuracy of the reconstructed text, and then we measure feature\nsimilarity between original and reconstructed faces thereby quantifying faces\nreconstruction fidelity. Our method is highly lightweight, requiring just 2GB\nmemory and 4 minutes to complete evaluations. With our benchmark, we analyze\nthe reconstruction quality of text and faces at various scales across different\nimage tokenizers and VAEs. Our results demonstrate that modern visual\ntokenizers still struggle to preserve fine-grained features, particularly at\nsmaller scales. Furthermore, we extend this evaluation framework to the video,\nconducting a comprehensive analysis of video tokenizers. Additionally, we find\nthat traditional metrics fail to accurately reflect the reconstruction\nperformance for faces and text, while our proposed metrics serve as an\neffective complement.", "authors": ["Junfeng Wu", "Dongliang Luo", "Weizhi Zhao", "Zhihao Xie", "Yuanhao Wang", "Junyi Li", "Xudong Xie", "Yuliang Liu", "Xiang Bai"], "published_date": "2025-05-23", "title_zh": "TokBench：視覺生成前評估您的視覺符號器", "summary_zh": "本研究揭示視覺符號器和變分自編碼器在保留細緻特徵上的局限性，並提出一個基準來評估文字和人臉這兩項具挑戰性視覺內容的重建效能。圖像符號化顯著推進了視覺生成和多模態建模，特別是自迴歸模型，因其離散符號的建模簡潔性。自迴歸模型通常依賴圖像符號器將圖像壓縮成離散符號以進行序列預測，而擴散模型則常在連續潛在空間中運作以降低計算成本。然而，這兩種視覺壓縮方法不可避免地會丟失視覺資訊，從而限制視覺生成品質的上限。為了評估這些壓縮損失如何影響對人類最敏感的視覺元素，即文字和人臉，我們首先從現有數據集中收集和整理了一系列文字和人臉圖像，確保清晰度和多樣性。對於文字重建，我們採用OCR模型來評估重建文字的辨識準確度；然後，我們測量原始人臉和重建人臉之間的特徵相似度，從而量化人臉重建的保真度。該方法非常輕量，只需2GB記憶體和4分鐘即可完成評估。藉由該基準，我們分析了不同圖像符號器和變分自編碼器在各種尺度下對文字和人臉的重建品質。結果表明，現代視覺符號器在保留細緻特徵方面仍有困難，尤其是在較小尺度下。此外，我們將此評估框架擴展到影片，對影片符號器進行了全面分析。另外，我們發現傳統指標未能準確反映人臉和文字的重建效能，而我們提出的指標則能有效補充。", "audio": "audios/2505.18142v1.mp3", "timestamp": "2025-05-26T04:25:05.415256"}
{"query": "AI", "id": "2505.18128v1", "url": "http://arxiv.org/abs/2505.18128v1", "title": "Frankentext: Stitching random text fragments into long-form narratives", "summary": "We introduce Frankentexts, a new type of long-form narratives produced by\nLLMs under the extreme constraint that most tokens (e.g., 90%) must be copied\nverbatim from human writings. This task presents a challenging test of\ncontrollable generation, requiring models to satisfy a writing prompt,\nintegrate disparate text fragments, and still produce a coherent narrative. To\ngenerate Frankentexts, we instruct the model to produce a draft by selecting\nand combining human-written passages, then iteratively revise the draft while\nmaintaining a user-specified copy ratio. We evaluate the resulting Frankentexts\nalong three axes: writing quality, instruction adherence, and detectability.\nGemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts\nare coherent and 100% relevant to the prompt. Notably, up to 59% of these\noutputs are misclassified as human-written by detectors like Pangram, revealing\nlimitations in AI text detectors. Human annotators can sometimes identify\nFrankentexts through their abrupt tone shifts and inconsistent grammar between\nsegments, especially in longer generations. Beyond presenting a challenging\ngeneration task, Frankentexts invite discussion on building effective detectors\nfor this new grey zone of authorship, provide training data for mixed\nauthorship detection, and serve as a sandbox for studying human-AI co-writing\nprocesses.", "authors": ["Chau Minh Pham", "Jenna Russell", "Dzung Pham", "Mohit Iyyer"], "published_date": "2025-05-23", "title_zh": "弗蘭肯文本：將隨機文本片段縫合成長篇敘事", "summary_zh": "本文介紹Frankentexts，一種由大型語言模型在嚴苛限制下產生的新型長篇敘事，要求大部分詞符（如90%）必須逐字複製自人類作品。此任務對可控生成構成挑戰性考驗，要求模型滿足寫作提示、整合不同文本片段並產出連貫敘事。生成Frankentexts時，模型先選擇並組合人類撰寫的段落生成草稿，然後在維持使用者指定的複製比例下迭代修改草稿。我們從寫作品質、指令遵循和可偵測性三個方面評估生成的Frankentexts。Gemini-2.5-Pro在此任務上表現出色：81%的Frankentexts具有連貫性，100%與提示相關。值得注意的是，高達59%的輸出被Pangram等偵測器錯誤分類為人類作品，揭示了AI文本偵測器的局限性。人工標註者有時可以通過片段之間突兀的語氣轉變和不一致的語法來識別Frankentexts，尤其是在較長的生成文本中。除了提出具挑戰性的生成任務外，Frankentexts還引發了關於建立有效偵測器以應對這種新型作者身份灰色地帶的討論，提供了用於混合作者身份偵測的訓練數據，並作為研究人機協作寫作過程的沙盒。", "audio": "audios/2505.18128v1.mp3", "timestamp": "2025-05-26T05:19:25.565547"}
{"query": "Foundation Model", "id": "2505.18039v1", "url": "http://arxiv.org/abs/2505.18039v1", "title": "Clip4Retrofit: Enabling Real-Time Image Labeling on Edge Devices via Cross-Architecture CLIP Distillation", "summary": "Foundation models like CLIP (Contrastive Language-Image Pretraining) have\nrevolutionized vision-language tasks by enabling zero-shot and few-shot\nlearning through cross-modal alignment. However, their computational complexity\nand large memory footprint make them unsuitable for deployment on\nresource-constrained edge devices, such as in-car cameras used for image\ncollection and real-time processing. To address this challenge, we propose\nClip4Retrofit, an efficient model distillation framework that enables real-time\nimage labeling on edge devices. The framework is deployed on the Retrofit\ncamera, a cost-effective edge device retrofitted into thousands of vehicles,\ndespite strict limitations on compute performance and memory. Our approach\ndistills the knowledge of the CLIP model into a lightweight student model,\ncombining EfficientNet-B3 with multi-layer perceptron (MLP) projection heads to\npreserve cross-modal alignment while significantly reducing computational\nrequirements. We demonstrate that our distilled model achieves a balance\nbetween efficiency and performance, making it ideal for deployment in\nreal-world scenarios. Experimental results show that Clip4Retrofit can perform\nreal-time image labeling and object identification on edge devices with limited\nresources, offering a practical solution for applications such as autonomous\ndriving and retrofitting existing systems. This work bridges the gap between\nstate-of-the-art vision-language models and their deployment in\nresource-constrained environments, paving the way for broader adoption of\nfoundation models in edge computing.", "authors": ["Li Zhong", "Ahmed Ghazal", "Jun-Jun Wan", "Frederik Zilly", "Patrick Mackens", "Joachim E. Vollrath", "Bogdan Sorin Coseriu"], "published_date": "2025-05-23", "title_zh": "Clip4Retrofit：藉由跨架構CLIP蒸餾於邊緣裝置上實現即時圖像標註", "summary_zh": "如CLIP之類的基礎模型透過跨模態對齊，革新了視覺語言任務，實現了零樣本和少樣本學習。然而，其計算複雜性和龐大的記憶體佔用使其不適用於資源受限的邊緣設備，例如用於圖像採集和即時處理的車載相機。為了解決這個問題，我們提出了Clip4Retrofit，一個高效的模型蒸餾框架，可在邊緣設備上實現即時圖像標記。該框架部署在Retrofit相機上，這是一種經濟高效的邊緣設備，已改裝到數千輛汽車中，儘管其計算效能和記憶體受到嚴格限制。我們的方法將CLIP模型的知識提煉成一個輕量級的學生模型，結合EfficientNet-B3和多層感知器（MLP）投影頭，以保持跨模態對齊，同時顯著降低計算需求。我們證明，我們的蒸餾模型在效率和性能之間取得了平衡，使其非常適合在實際場景中部署。實驗結果表明，Clip4Retrofit可以在資源有限的邊緣設備上執行即時圖像標記和物件識別，為自動駕駛和改造現有系統等應用提供了一種實用的解決方案。這項工作彌合了最先進的視覺語言模型與其在資源受限環境中的部署之間的差距，為基礎模型在邊緣計算中的更廣泛採用鋪平了道路。", "audio": "audios/2505.18039v1.mp3", "timestamp": "2025-05-26T05:19:33.562834"}
{"query": "Diffusion Model", "id": "2505.18097v1", "url": "http://arxiv.org/abs/2505.18097v1", "title": "Towards more transferable adversarial attack in black-box manner", "summary": "Adversarial attacks have become a well-explored domain, frequently serving as\nevaluation baselines for model robustness. Among these, black-box attacks based\non transferability have received significant attention due to their practical\napplicability in real-world scenarios. Traditional black-box methods have\ngenerally focused on improving the optimization framework (e.g., utilizing\nmomentum in MI-FGSM) to enhance transferability, rather than examining the\ndependency on surrogate white-box model architectures. Recent state-of-the-art\napproach DiffPGD has demonstrated enhanced transferability by employing\ndiffusion-based adversarial purification models for adaptive attacks. The\ninductive bias of diffusion-based adversarial purification aligns naturally\nwith the adversarial attack process, where both involving noise addition,\nreducing dependency on surrogate white-box model selection. However, the\ndenoising process of diffusion models incurs substantial computational costs\nthrough chain rule derivation, manifested in excessive VRAM consumption and\nextended runtime. This progression prompts us to question whether introducing\ndiffusion models is necessary. We hypothesize that a model sharing similar\ninductive bias to diffusion-based adversarial purification, combined with an\nappropriate loss function, could achieve comparable or superior transferability\nwhile dramatically reducing computational overhead. In this paper, we propose a\nnovel loss function coupled with a unique surrogate model to validate our\nhypothesis. Our approach leverages the score of the time-dependent classifier\nfrom classifier-guided diffusion models, effectively incorporating natural data\ndistribution knowledge into the adversarial optimization process. Experimental\nresults demonstrate significantly improved transferability across diverse model\narchitectures while maintaining robustness against diffusion-based defenses.", "authors": ["Chun Tong Lei", "Zhongliang Guo", "Hon Chung Lee", "Minh Quoc Duong", "Chun Pong Lau"], "published_date": "2025-05-23", "title_zh": "邁向更具遷移性的黑盒對抗攻擊", "summary_zh": "對抗性攻擊已成熱門領域，常作為模型穩健性評估基準。基於可遷移性的黑盒攻擊因其實用性備受關注。傳統方法側重於優化框架以提升可遷移性，而非檢視對代理白盒模型架構的依賴。DiffPGD透過擴散的對抗淨化模型進行自適應攻擊，展現更佳的可遷移性。擴散的對抗淨化之歸納偏置與對抗攻擊過程自然契合，降低對代理白盒模型選擇的依賴。然而，擴散模型的去噪過程計算成本高昂。本文假設，與擴散的對抗淨化模型具相似歸納偏置的模型，結合適當損失函數，可實現相當或更優越的可遷移性，同時顯著降低計算開銷。本文提出一種新穎的損失函數與獨特的代理模型以驗證此假設，利用基於分類器的擴散模型之時變分類器分數，將自然數據分佈知識有效地納入對抗優化過程。實驗結果表明，在多種模型架構中，可遷移性顯著提升，同時保持對基於擴散防禦的穩健性。", "audio": "audios/2505.18097v1.mp3", "timestamp": "2025-05-26T05:19:41.120299"}
{"query": "AI", "id": "2505.18078v1", "url": "http://arxiv.org/abs/2505.18078v1", "title": "DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation", "summary": "Controllable video generation (CVG) has advanced rapidly, yet current systems\nfalter when more than one actor must move, interact, and exchange positions\nunder noisy control signals. We address this gap with DanceTogether, the first\nend-to-end diffusion framework that turns a single reference image plus\nindependent pose-mask streams into long, photorealistic videos while strictly\npreserving every identity. A novel MaskPoseAdapter binds \"who\" and \"how\" at\nevery denoising step by fusing robust tracking masks with semantically rich-but\nnoisy-pose heat-maps, eliminating the identity drift and appearance bleeding\nthat plague frame-wise pipelines. To train and evaluate at scale, we introduce\n(i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii)\nHumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain\ntransfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the\nDanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure\nskating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a\nsignificant margin. Moreover, we show that a one-hour fine-tune yields\nconvincing human-robot videos, underscoring broad generalization to embodied-AI\nand HRI tasks. Extensive ablations confirm that persistent identity-action\nbinding is critical to these gains. Together, our model, datasets, and\nbenchmark lift CVG from single-subject choreography to compositionally\ncontrollable, multi-actor interaction, opening new avenues for digital\nproduction, simulation, and embodied intelligence. Our video demos and code are\navailable at https://DanceTog.github.io/.", "authors": ["Junhao Chen", "Mingjin Chen", "Jianjin Xu", "Xiang Li", "Junting Dong", "Mingze Sun", "Puhua Jiang", "Hongxiang Li", "Yuhang Yang", "Hao Zhao", "Xiaoxiao Long", "Ruqi Huang"], "published_date": "2025-05-23", "title_zh": "共舞！身分保持的多人互動影片生成", "summary_zh": "可控影片生成技術快速發展，但現有系統在多個角色於噪聲控制信號下移動、互動和交換位置時表現不佳。本文提出DanceTogether，首個端到端擴散框架，可將單一參考圖像和獨立的姿態遮罩串流轉換為逼真長影片，同時嚴格保留每個角色身份。一種新型MaskPoseAdapter在每個去噪步驟中結合穩健追蹤遮罩與具語義但帶噪的姿態熱圖，消除身份漂移和外觀洩漏問題。為規模化訓練和評估，引入PairFS-4K、HumanRob-300和TogetherVideoBench基準。實驗結果表明，DanceTogether在TogetherVideoBench上顯著優於先前技術，並可透過少量微調生成具說服力的人機互動影片。消融研究證實，持久的身份-動作綁定至關重要。DanceTogether模型、數據集和基準將可控影片生成從單一主體編舞提升到具組合控制性的多角色互動，為數位製作、模擬和具身智慧開闢新途徑。", "audio": "audios/2505.18078v1.mp3", "timestamp": "2025-05-26T06:28:19.840401"}
{"query": "Foundation Model", "id": "2505.18022v1", "url": "http://arxiv.org/abs/2505.18022v1", "title": "RemoteSAM: Towards Segment Anything for Earth Observation", "summary": "We aim to develop a robust yet flexible visual foundation model for Earth\nobservation. It should possess strong capabilities in recognizing and\nlocalizing diverse visual targets while providing compatibility with various\ninput-output interfaces required across different task scenarios. Current\nsystems cannot meet these requirements, as they typically utilize task-specific\narchitecture trained on narrow data domains with limited semantic coverage. Our\nstudy addresses these limitations from two aspects: data and modeling. We first\nintroduce an automatic data engine that enjoys significantly better scalability\ncompared to previous human annotation or rule-based approaches. It has enabled\nus to create the largest dataset of its kind to date, comprising 270K\nimage-text-mask triplets covering an unprecedented range of diverse semantic\ncategories and attribute specifications. Based on this data foundation, we\nfurther propose a task unification paradigm that centers around referring\nexpression segmentation. It effectively handles a wide range of vision-centric\nperception tasks, including classification, detection, segmentation, grounding,\netc, using a single model without any task-specific heads. Combining these\ninnovations on data and modeling, we present RemoteSAM, a foundation model that\nestablishes new SoTA on several earth observation perception benchmarks,\noutperforming other foundation models such as Falcon, GeoChat, and LHRS-Bot\nwith significantly higher efficiency. Models and data are publicly available at\nhttps://github.com/1e12Leon/RemoteSAM.", "authors": ["Liang Yao", "Fan Liu", "Delong Chen", "Chuanyi Zhang", "Yijun Wang", "Ziyun Chen", "Wei Xu", "Shimin Di", "Yuhui Zheng"], "published_date": "2025-05-23", "title_zh": "RemoteSAM：面向地球觀測的萬物分割", "summary_zh": "本研究旨在開發適用於地球觀測的穩健且靈活之視覺基礎模型，該模型需具備辨識和定位多樣化視覺目標之能力，並相容於不同任務情境所需之各種輸入輸出介面。為解決現有系統在資料和建模方面的局限性，我們引入自動化資料引擎，建立包含27萬個圖像-文本-遮罩三元組的大型資料集，涵蓋廣泛之語義類別和屬性規範。基於此資料基礎，我們提出以指代表達分割為中心之任務統一範式，利用單一模型處理多種視覺感知任務，無需特定任務之head。結合資料和建模創新，我們提出 RemoteSAM，此基礎模型在多個地球觀測感知基準測試中達到最先進水準，且效率顯著優於其他模型。模型與資料已公開發佈。", "audio": "audios/2505.18022v1.mp3", "timestamp": "2025-05-26T06:28:25.695016"}
{"query": "Diffusion Model", "id": "2505.18047v1", "url": "http://arxiv.org/abs/2505.18047v1", "title": "RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration", "summary": "The use of latent diffusion models (LDMs) such as Stable Diffusion has\nsignificantly improved the perceptual quality of All-in-One image Restoration\n(AiOR) methods, while also enhancing their generalization capabilities.\nHowever, these LDM-based frameworks suffer from slow inference due to their\niterative denoising process, rendering them impractical for time-sensitive\napplications. To address this, we propose RestoreVAR, a novel generative\napproach for AiOR that significantly outperforms LDM-based models in\nrestoration performance while achieving over $\\mathbf{10\\times}$ faster\ninference. RestoreVAR leverages visual autoregressive modeling (VAR), a\nrecently introduced approach which performs scale-space autoregression for\nimage generation. VAR achieves comparable performance to that of\nstate-of-the-art diffusion transformers with drastically reduced computational\ncosts. To optimally exploit these advantages of VAR for AiOR, we propose\narchitectural modifications and improvements, including intricately designed\ncross-attention mechanisms and a latent-space refinement module, tailored for\nthe AiOR task. Extensive experiments show that RestoreVAR achieves\nstate-of-the-art performance among generative AiOR methods, while also\nexhibiting strong generalization capabilities.", "authors": ["Sudarshan Rajagopalan", "Kartik Narayan", "Vishal M. Patel"], "published_date": "2025-05-23", "title_zh": "RestoreVAR：用於一體化圖像復原的視覺自迴歸生成", "summary_zh": "潛在擴散模型（LDM）如Stable Diffusion雖提升了全方位影像修復（AiOR）方法的感知品質與泛化能力，但其迭代降噪過程導致推論速度緩慢，不適用於時間敏感應用。為此，我們提出RestoreVAR，一種新型生成式AiOR方法，其修復效能顯著超越LDM模型，推論速度更提升十倍以上。RestoreVAR利用視覺自迴歸建模（VAR），一種針對影像生成執行尺度空間自迴歸的新技術，在大幅降低計算成本的同時，達到與最先進擴散轉換器相當的效能。為最佳化VAR在AiOR中的優勢，我們提出架構修改與改進，包含精巧設計的交叉注意力機制和潛在空間精煉模組，專為AiOR任務量身打造。實驗結果表明，RestoreVAR在生成式AiOR方法中達到最先進效能，並展現出強大的泛化能力。", "audio": "audios/2505.18047v1.mp3", "timestamp": "2025-05-26T06:28:32.851412"}
{"query": "AI", "id": "2505.18066v1", "url": "http://arxiv.org/abs/2505.18066v1", "title": "Towards Uncertainty Aware Task Delegation and Human-AI Collaborative Decision-Making", "summary": "Despite the growing promise of artificial intelligence (AI) in supporting\ndecision-making across domains, fostering appropriate human reliance on AI\nremains a critical challenge. In this paper, we investigate the utility of\nexploring distance-based uncertainty scores for task delegation to AI and\ndescribe how these scores can be visualized through embedding representations\nfor human-AI decision-making. After developing an AI-based system for physical\nstroke rehabilitation assessment, we conducted a study with 19 health\nprofessionals and 10 students in medicine/health to understand the effect of\nexploring distance-based uncertainty scores on users' reliance on AI. Our\nfindings showed that distance-based uncertainty scores outperformed traditional\nprobability-based uncertainty scores in identifying uncertain cases. In\naddition, after exploring confidence scores for task delegation and reviewing\nembedding-based visualizations of distance-based uncertainty scores,\nparticipants achieved an 8.20% higher rate of correct decisions, a 7.15% higher\nrate of changing their decisions to correct ones, and a 7.14% lower rate of\nincorrect changes after reviewing AI outputs than those reviewing\nprobability-based uncertainty scores ($p<0.01$). Our findings highlight the\npotential of distance-based uncertainty scores to enhance decision accuracy and\nappropriate reliance on AI while discussing ongoing challenges for human-AI\ncollaborative decision-making.", "authors": ["Min Hun Lee", "Martyn Zhe Yu Tok"], "published_date": "2025-05-23", "title_zh": "邁向具不確定性意識的任務委派與人機協同決策", "summary_zh": "儘管人工智慧在輔助決策方面日益 promising，如何促進人對 AI 的適當依賴仍然是一項關鍵挑戰。本文探討基於距離的不確定性評分在任務委派給 AI 中的效用，並闡述如何透過嵌入表示視覺化這些評分，以輔助人機決策。研究開發了一套基於 AI 的中風復健評估系統，並與 19 位醫療專業人員和 10 位醫學/健康領域學生進行研究，了解探索基於距離的不確定性評分對使用者 AI 依賴的影響。研究結果顯示，基於距離的不確定性評分在識別不確定案例方面優於傳統的基於機率的不確定性評分。此外，在探索用於任務委派的置信度評分，並檢閱基於嵌入的距離不確定性評分視覺化後，參與者在檢閱 AI 輸出後，其正確決策率提高了 8.20%，將決策更改為正確決策的比率提高了 7.15%，而錯誤更改的比率降低了 7.14% (p<0.01)。研究強調了基於距離的不確定性評分在提高決策準確性和對 AI 的適當依賴方面的潛力，同時也討論了人機協作決策所面臨的持續挑戰。", "audio": "audios/2505.18066v1.mp3", "timestamp": "2025-05-26T07:25:31.504599"}
{"query": "Foundation Model", "id": "2505.17971v1", "url": "http://arxiv.org/abs/2505.17971v1", "title": "Explainable Anatomy-Guided AI for Prostate MRI: Foundation Models and In Silico Clinical Trials for Virtual Biopsy-based Risk Assessment", "summary": "We present a fully automated, anatomically guided deep learning pipeline for\nprostate cancer (PCa) risk stratification using routine MRI. The pipeline\nintegrates three key components: an nnU-Net module for segmenting the prostate\ngland and its zones on axial T2-weighted MRI; a classification module based on\nthe UMedPT Swin Transformer foundation model, fine-tuned on 3D patches with\noptional anatomical priors and clinical data; and a VAE-GAN framework for\ngenerating counterfactual heatmaps that localize decision-driving image\nregions. The system was developed using 1,500 PI-CAI cases for segmentation and\n617 biparametric MRIs with metadata from the CHAIMELEON challenge for\nclassification (split into 70% training, 10% validation, and 20% testing).\nSegmentation achieved mean Dice scores of 0.95 (gland), 0.94 (peripheral zone),\nand 0.92 (transition zone). Incorporating gland priors improved AUC from 0.69\nto 0.72, with a three-scale ensemble achieving top performance (AUC = 0.79,\ncomposite score = 0.76), outperforming the 2024 CHAIMELEON challenge winners.\nCounterfactual heatmaps reliably highlighted lesions within segmented regions,\nenhancing model interpretability. In a prospective multi-center in-silico trial\nwith 20 clinicians, AI assistance increased diagnostic accuracy from 0.72 to\n0.77 and Cohen's kappa from 0.43 to 0.53, while reducing review time per case\nby 40%. These results demonstrate that anatomy-aware foundation models with\ncounterfactual explainability can enable accurate, interpretable, and efficient\nPCa risk assessment, supporting their potential use as virtual biopsies in\nclinical practice.", "authors": ["Danial Khan", "Zohaib Salahuddin", "Yumeng Zhang", "Sheng Kuang", "Shruti Atul Mali", "Henry C. Woodruff", "Sina Amirrajab", "Rachel Cavill", "Eduardo Ibor-Crespo", "Ana Jimenez-Pastor", "Adrian Galiana-Bordera", "Paula Jimenez Gomez", "Luis Marti-Bonmati", "Philippe Lambin"], "published_date": "2025-05-23", "title_zh": "基於可解釋解剖學引導的人工智慧於前列腺磁振造影：用於虛擬切片活檢風險評估的基礎模型與電腦模擬臨床試驗", "summary_zh": "本研究提出一個全自動、解剖結構引導的深度學習流程，用於常規 MRI 前列腺癌風險分層。該流程整合 nnU-Net 分割模組（分割 T2 加權 MRI 前列腺及其區域）、UMedPT Swin Transformer 分類模組（基於 3D 影像塊微調，可選解剖先驗和臨床數據）以及 VAE-GAN 框架（生成反事實熱圖以定位決策驅動影像區域）。系統分別使用 1500 個 PI-CAI 案例進行分割，以及來自 CHAIMELEON 挑戰賽的 617 個雙參數 MRI 數據集進行分類（70% 訓練，10% 驗證，20% 測試）。分割的平均 Dice 系數分別為 0.95（前列腺）、0.94（外周帶）和 0.92（移行帶）。納入前列腺先驗知識將 AUC 從 0.69 提升至 0.72，三尺度集成模型表現最佳（AUC = 0.79，綜合評分 = 0.76），優於 2024 CHAIMELEON 挑戰賽獲勝者。反事實熱圖能可靠地標記分割區域內的病灶，增強模型可解釋性。在一項前瞻性多中心體內試驗中，AI 輔助將 20 位臨床醫生的診斷準確性從 0.72 提高到 0.77，Cohen's kappa 從 0.43 提高到 0.53，同時每個案例的審閱時間減少了 40%。結果表明，具有反事實可解釋性的解剖結構感知基礎模型能夠實現準確、可解釋且高效的前列腺癌風險評估，支持其在臨床實踐中作為虛擬活檢的潛在應用。", "audio": "audios/2505.17971v1.mp3", "timestamp": "2025-05-26T07:25:41.595526"}
{"query": "Diffusion Model", "id": "2505.18017v1", "url": "http://arxiv.org/abs/2505.18017v1", "title": "Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling", "summary": "Deep generative models hold great promise for representing complex physical\nsystems, but their deployment is currently limited by the lack of guarantees on\nthe physical plausibility of the generated outputs. Ensuring that known\nphysical constraints are enforced is therefore critical when applying\ngenerative models to scientific and engineering problems. We address this\nlimitation by developing a principled framework for sampling from a target\ndistribution while rigorously satisfying physical constraints. Leveraging the\nvariational formulation of Langevin dynamics, we propose Split Augmented\nLangevin (SAL), a novel primal-dual sampling algorithm that enforces\nconstraints progressively through variable splitting, with convergence\nguarantees. While the method is developed theoretically for Langevin dynamics,\nwe demonstrate its effective applicability to diffusion models. In particular,\nwe use constrained diffusion models to generate physical fields satisfying\nenergy and mass conservation laws. We apply our method to diffusion-based data\nassimilation on a complex physical system, where enforcing physical constraints\nsubstantially improves both forecast accuracy and the preservation of critical\nconserved quantities. We also demonstrate the potential of SAL for challenging\nfeasibility problems in optimal control.", "authors": ["Matthieu Blanke", "Yongquan Qu", "Sara Shamekh", "Pierre Gentine"], "published_date": "2025-05-23", "title_zh": "基於分裂增廣朗之萬抽樣的嚴格約束生成建模", "summary_zh": "深度生成模型在表徵複雜物理系統方面極具潛力，但因其生成輸出結果缺乏物理合理性保證而限制了應用。確保已知物理約束的實施對於將生成模型應用於科學和工程問題至關重要。本研究提出一個在嚴格滿足物理約束下從目標分布中採樣的原則性框架。利用朗之萬動力學的變分公式，提出一種名為分裂增廣朗之萬(SAL)的原始-對偶採樣新算法，通過變量分裂逐步實施約束，並具有收斂性保證。儘管該方法在理論上是為朗之萬動力學開發的，但我們證明其對擴散模型的有效適用性。特別是，我們使用受約束的擴散模型來生成滿足能量和質量守恆定律的物理場。我們將此方法應用於基於擴散的複雜物理系統數據同化，其中實施物理約束可顯著提高預測準確性和關鍵守恆量的保存。我們還展示了SAL在最佳控制中具挑戰性可行性問題的潛力。", "audio": "audios/2505.18017v1.mp3", "timestamp": "2025-05-26T07:25:47.785036"}
{"query": "AI", "id": "2505.18060v1", "url": "http://arxiv.org/abs/2505.18060v1", "title": "Semantic Correspondence: Unified Benchmarking and a Strong Baseline", "summary": "Establishing semantic correspondence is a challenging task in computer\nvision, aiming to match keypoints with the same semantic information across\ndifferent images. Benefiting from the rapid development of deep learning,\nremarkable progress has been made over the past decade. However, a\ncomprehensive review and analysis of this task remains absent. In this paper,\nwe present the first extensive survey of semantic correspondence methods. We\nfirst propose a taxonomy to classify existing methods based on the type of\ntheir method designs. These methods are then categorized accordingly, and we\nprovide a detailed analysis of each approach. Furthermore, we aggregate and\nsummarize the results of methods in literature across various benchmarks into a\nunified comparative table, with detailed configurations to highlight\nperformance variations. Additionally, to provide a detailed understanding on\nexisting methods for semantic matching, we thoroughly conduct controlled\nexperiments to analyse the effectiveness of the components of different\nmethods. Finally, we propose a simple yet effective baseline that achieves\nstate-of-the-art performance on multiple benchmarks, providing a solid\nfoundation for future research in this field. We hope this survey serves as a\ncomprehensive reference and consolidated baseline for future development. Code\nis publicly available at: https://github.com/Visual-AI/Semantic-Correspondence.", "authors": ["Kaiyan Zhang", "Xinghui Li", "Jingyi Lu", "Kai Han"], "published_date": "2025-05-23", "title_zh": "語義對應：統一基準測試與強基準線", "summary_zh": "語義對應旨在匹配不同圖像間具有相同語義信息的關鍵點，是電腦視覺中一項具挑戰性的任務。受益於深度學習的快速發展，過去十年取得了顯著進展，但缺乏對此任務的全面回顧與分析。本文首次對語義對應方法進行廣泛綜述，首先提出一種分類法，基於方法設計類型對現有方法進行分類，並對每種方法進行詳細分析。此外，我們將文獻中各種基準測試的結果匯總成統一的比較表格，並提供詳細配置以突顯性能差異。為了深入理解現有的語義匹配方法，我們進行了嚴格的控制實驗，分析了不同方法組件的有效性。最後，我們提出了一個簡單而有效的基準，在多個基準測試中實現了最先進的性能，為該領域的未來研究提供了堅實的基礎。期望此綜述能為未來的發展提供全面的參考和整合的基準。程式碼已公開：https://github.com/Visual-AI/Semantic-Correspondence。", "audio": "audios/2505.18060v1.mp3", "timestamp": "2025-05-26T08:36:45.936222"}
{"query": "Foundation Model", "id": "2505.17931v1", "url": "http://arxiv.org/abs/2505.17931v1", "title": "AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models", "summary": "Medical image segmentation is vital for clinical diagnosis, yet current deep\nlearning methods often demand extensive expert effort, i.e., either through\nannotating large training datasets or providing prompts at inference time for\neach new case. This paper introduces a zero-shot and automatic segmentation\npipeline that combines off-the-shelf vision-language and segmentation\nfoundation models. Given a medical image and a task definition (e.g., \"segment\nthe optic disc in an eye fundus image\"), our method uses a grounding model to\ngenerate an initial bounding box, followed by a visual prompt boosting module\nthat enhance the prompts, which are then processed by a promptable segmentation\nmodel to produce the final mask. To address the challenges of domain gap and\nresult verification, we introduce a test-time adaptation framework featuring a\nset of learnable adaptors that align the medical inputs with foundation model\nrepresentations. Its hyperparameters are optimized via Bayesian Optimization,\nguided by a proxy validation model without requiring ground-truth labels. Our\npipeline offers an annotation-efficient and scalable solution for zero-shot\nmedical image segmentation across diverse tasks. Our pipeline is evaluated on\nseven diverse medical imaging datasets and shows promising results. By proper\ndecomposition and test-time adaptation, our fully automatic pipeline performs\ncompetitively with weakly-prompted interactive foundation models.", "authors": ["Xingjian Li", "Qifeng Wu", "Colleen Que", "Yiran Ding", "Adithya S. Ubaradka", "Jianhua Xing", "Tianyang Wang", "Min Xu"], "published_date": "2025-05-23", "title_zh": "AutoMiSeg：基於基礎模型測試時自適應的自動醫學影像分割", "summary_zh": "醫學影像分割對於臨床診斷至關重要，但現有深度學習方法常需大量專家投入，例如標註大型訓練集或於每次推論時提供提示。本文提出一種零樣本且自動化的分割流程，結合現成的視覺語言和分割基礎模型。給定醫學影像和任務定義（如「分割眼底圖像中的視神經盤」），本方法使用定位模型產生初始邊界框，再透過視覺提示增強模組強化提示，隨後由可提示分割模型產生最終遮罩。為了解決領域差距和結果驗證的挑戰，我們引入測試時適應框架，包含一組可學習的適配器，以將醫學輸入與基礎模型表示對齊。其超參數透過貝氏優化進行優化，並由代理驗證模型指導，無需真實標籤。我們的流程為跨多樣任務的零樣本醫學影像分割提供了一種標註高效且可擴展的解決方案。該流程在七個不同的醫學影像數據集上進行評估，顯示出有希望的結果。透過適當的分解和測試時適應，我們的全自動流程在性能上可與弱提示互動式基礎模型競爭。", "audio": "audios/2505.17931v1.mp3", "timestamp": "2025-05-26T08:36:54.062285"}
{"query": "Diffusion Model", "id": "2505.17994v1", "url": "http://arxiv.org/abs/2505.17994v1", "title": "Segment Anyword: Mask Prompt Inversion for Open-Set Grounded Segmentation", "summary": "Open-set image segmentation poses a significant challenge because existing\nmethods often demand extensive training or fine-tuning and generally struggle\nto segment unified objects consistently across diverse text reference\nexpressions. Motivated by this, we propose Segment Anyword, a novel\ntraining-free visual concept prompt learning approach for open-set language\ngrounded segmentation that relies on token-level cross-attention maps from a\nfrozen diffusion model to produce segmentation surrogates or mask prompts,\nwhich are then refined into targeted object masks. Initial prompts typically\nlack coherence and consistency as the complexity of the image-text increases,\nresulting in suboptimal mask fragments. To tackle this issue, we further\nintroduce a novel linguistic-guided visual prompt regularization that binds and\nclusters visual prompts based on sentence dependency and syntactic structural\ninformation, enabling the extraction of robust, noise-tolerant mask prompts,\nand significant improvements in segmentation accuracy. The proposed approach is\neffective, generalizes across different open-set segmentation tasks, and\nachieves state-of-the-art results of 52.5 (+6.8 relative) mIoU on Pascal\nContext 59, 67.73 (+25.73 relative) cIoU on gRefCOCO, and 67.4 (+1.1 relative\nto fine-tuned methods) mIoU on GranDf, which is the most complex open-set\ngrounded segmentation task in the field.", "authors": ["Zhihua Liu", "Amrutha Saseendran", "Lei Tong", "Xilin He", "Fariba Yousefi", "Nikolay Burlutskiy", "Dino Oglic", "Tom Diethe", "Philip Teare", "Huiyu Zhou", "Chen Jin"], "published_date": "2025-05-23", "title_zh": "任意詞語分割：用於開放集基礎分割的遮罩提示反演", "summary_zh": "開放集圖像分割面臨挑戰，現有方法需大量訓練或微調，且難以根據不同文本描述一致分割統一物件。為此，我們提出Segment Anyword，一種免訓練的視覺概念提示學習方法，用於開放集語言導向分割。該方法利用凍結擴散模型中的令牌級別交叉注意力圖生成分割代理或遮罩提示，再將其優化為目標物件遮罩。針對圖像文本複雜度增加導致的初始提示不連貫問題，我們進一步提出語言引導的視覺提示正規化，依據句子依賴和句法結構資訊綁定和聚類視覺提示，提取穩健且容錯性高的遮罩提示，顯著提升分割準確度。該方法有效且能泛化至不同開放集分割任務，在Pascal Context 59上達到52.5 mIoU，gRefCOCO上達到67.73 cIoU，GranDf上達到67.4 mIoU，均為當前最佳效能。", "audio": "audios/2505.17994v1.mp3", "timestamp": "2025-05-26T08:37:01.427679"}
{"query": "AI", "id": "2505.18059v1", "url": "http://arxiv.org/abs/2505.18059v1", "title": "Assessing the performance of 8 AI chatbots in bibliographic reference retrieval: Grok and DeepSeek outperform ChatGPT, but none are fully accurate", "summary": "This study analyzes the performance of eight generative artificial\nintelligence chatbots -- ChatGPT, Claude, Copilot, DeepSeek, Gemini, Grok, Le\nChat, and Perplexity -- in their free versions, in the task of generating\nacademic bibliographic references within the university context. A total of 400\nreferences were evaluated across the five major areas of knowledge (Health,\nEngineering, Experimental Sciences, Social Sciences, and Humanities), based on\na standardized prompt. Each reference was assessed according to five key\ncomponents (authorship, year, title, source, and location), along with document\ntype, publication age, and error count. The results show that only 26.5% of the\nreferences were fully correct, 33.8% partially correct, and 39.8% were either\nerroneous or entirely fabricated. Grok and DeepSeek stood out as the only\nchatbots that did not generate false references, while Copilot, Perplexity, and\nClaude exhibited the highest hallucination rates. Furthermore, the chatbots\nshowed a greater tendency to generate book references over journal articles,\nalthough the latter had a significantly higher fabrication rate. A high degree\nof overlap was also detected among the sources provided by several models,\nparticularly between DeepSeek, Grok, Gemini, and ChatGPT. These findings reveal\nstructural limitations in current AI models, highlight the risks of uncritical\nuse by students, and underscore the need to strengthen information and critical\nliteracy regarding the use of AI tools in higher education.", "authors": ["Álvaro Cabezas-Clavijo", "Pavel Sidorenko-Bautista"], "published_date": "2025-05-23", "title_zh": "評估八款人工智慧聊天機器人在文獻參考檢索中的表現：Grok與DeepSeek優於ChatGPT，但皆未臻完全準確", "summary_zh": "本研究評估八種免費生成式人工智慧聊天機器人（ChatGPT、Claude、Copilot、DeepSeek、Gemini、Grok、Le Chat和Perplexity）在大學環境中產生學術文獻參考的表現。基於標準化提示，針對健康、工程、實驗科學、社會科學和人文學科五大知識領域，共評估400條參考文獻，並依作者、年份、標題、來源和地點等五個關鍵要素，以及文獻類型、出版年份和錯誤計數進行評估。結果顯示，僅26.5%的參考文獻完全正確，33.8%部分正確，39.8%錯誤或完全捏造。Grok和DeepSeek是唯一未產生虛假參考文獻的聊天機器人，而Copilot、Perplexity和Claude的幻覺率最高。聊天機器人更傾向於產生書籍參考文獻，而非期刊文章，但後者的捏造率顯著更高。多個模型提供的來源之間也存在高度重疊，尤其是在DeepSeek、Grok、Gemini和ChatGPT之間。研究結果揭示當前人工智慧模型的結構性限制，強調學生不加批判使用的風險，並強調高等教育中加強關於人工智慧工具使用的資訊素養和批判性素養之必要性。", "audio": "audios/2505.18059v1.mp3", "timestamp": "2025-05-26T09:44:17.450111"}
{"query": "Foundation Model", "id": "2505.17895v1", "url": "http://arxiv.org/abs/2505.17895v1", "title": "DataRater: Meta-Learned Dataset Curation", "summary": "The quality of foundation models depends heavily on their training data.\nConsequently, great efforts have been put into dataset curation. Yet most\napproaches rely on manual tuning of coarse-grained mixtures of large buckets of\ndata, or filtering by hand-crafted heuristics. An approach that is ultimately\nmore scalable (let alone more satisfying) is to \\emph{learn} which data is\nactually valuable for training. This type of meta-learning could allow more\nsophisticated, fine-grained, and effective curation. Our proposed\n\\emph{DataRater} is an instance of this idea. It estimates the value of\ntraining on any particular data point. This is done by meta-learning using\n`meta-gradients', with the objective of improving training efficiency on held\nout data. In extensive experiments across a range of model scales and datasets,\nwe find that using our DataRater to filter data is highly effective, resulting\nin significantly improved compute efficiency.", "authors": ["Dan A. Calian", "Gregory Farquhar", "Iurii Kemaev", "Luisa M. Zintgraf", "Matteo Hessel", "Jeremy Shar", "Junhyuk Oh", "András György", "Tom Schaul", "Jeffrey Dean", "Hado van Hasselt", "David Silver"], "published_date": "2025-05-23", "title_zh": "DataRater：元學習資料集管理", "summary_zh": "基礎模型的品質高度依賴於其訓練數據。因此，大量精力投入於數據集管理。然而，多數方法依賴於人工調整粗略數據桶的混合，或手動設計的啟發式過濾。更具擴展性的方法是學習哪些數據對訓練真正有價值。這種元學習可實現更精細、有效的數據管理。本文提出的DataRater即為此例，它通過元梯度估算特定數據點的訓練價值，目標是提高保留數據的訓練效率。大量實驗表明，使用DataRater過濾數據非常有效，顯著提高了計算效率。", "audio": "audios/2505.17895v1.mp3", "timestamp": "2025-05-26T09:44:22.596150"}
{"query": "Diffusion Model", "id": "2505.17955v1", "url": "http://arxiv.org/abs/2505.17955v1", "title": "Diffusion Classifiers Understand Compositionality, but Conditions Apply", "summary": "Understanding visual scenes is fundamental to human intelligence. While\ndiscriminative models have significantly advanced computer vision, they often\nstruggle with compositional understanding. In contrast, recent generative\ntext-to-image diffusion models excel at synthesizing complex scenes, suggesting\ninherent compositional capabilities. Building on this, zero-shot diffusion\nclassifiers have been proposed to repurpose diffusion models for discriminative\ntasks. While prior work offered promising results in discriminative\ncompositional scenarios, these results remain preliminary due to a small number\nof benchmarks and a relatively shallow analysis of conditions under which the\nmodels succeed. To address this, we present a comprehensive study of the\ndiscriminative capabilities of diffusion classifiers on a wide range of\ncompositional tasks. Specifically, our study covers three diffusion models (SD\n1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks.\nFurther, we shed light on the role that target dataset domains play in\nrespective performance; to isolate the domain effects, we introduce a new\ndiagnostic benchmark Self-Bench comprised of images created by diffusion models\nthemselves. Finally, we explore the importance of timestep weighting and\nuncover a relationship between domain gap and timestep sensitivity,\nparticularly for SD3-m. To sum up, diffusion classifiers understand\ncompositionality, but conditions apply! Code and dataset are available at\nhttps://github.com/eugene6923/Diffusion-Classifiers-Compositionality.", "authors": ["Yujin Jeong", "Arnas Uselis", "Seong Joon Oh", "Anna Rohrbach"], "published_date": "2025-05-23", "title_zh": "擴散分類器理解組合性，但條件限制適用", "summary_zh": "理解視覺場景是人類智慧的基礎。儘管判別模型在電腦視覺領域取得顯著進展，但它們在組合理解方面仍有困難。近年來，生成式文字轉圖像擴散模型擅長合成複雜場景，顯示其具有內在的組合能力。基於此，零樣本擴散分類器被提出，將擴散模型重新用於判別任務。儘管先前的工作在判別性組合場景中取得了有希望的結果，但由於基準測試數量少以及對模型成功條件的分析相對淺薄，這些結果仍屬初步。為了解決這個問題，我們對擴散分類器在各種組合任務中的判別能力進行了全面研究。具體來說，我們的研究涵蓋了三個擴散模型 (SD 1.5、2.0 和首次使用的 3-m)，涉及 10 個資料集和 30 多個任務。此外，我們闡明了目標資料集領域在各自表現中所扮演的角色；為了隔離領域效應，我們引入了一個新的診斷基準測試 Self-Bench，它由擴散模型自身創建的圖像組成。最後，我們探討了時間步長加權的重要性，並揭示了領域差距與時間步長敏感性之間的關係，特別是對於 SD3-m。總之，擴散分類器理解組合性，但有條件限制。程式碼和資料集可在指定網址獲取。", "audio": "audios/2505.17955v1.mp3", "timestamp": "2025-05-26T09:44:32.587815"}
{"query": "AI", "id": "2505.18035v1", "url": "http://arxiv.org/abs/2505.18035v1", "title": "CAMME: Adaptive Deepfake Image Detection with Multi-Modal Cross-Attention", "summary": "The proliferation of sophisticated AI-generated deepfakes poses critical\nchallenges for digital media authentication and societal security. While\nexisting detection methods perform well within specific generative domains,\nthey exhibit significant performance degradation when applied to manipulations\nproduced by unseen architectures--a fundamental limitation as generative\ntechnologies rapidly evolve. We propose CAMME (Cross-Attention Multi-Modal\nEmbeddings), a framework that dynamically integrates visual, textual, and\nfrequency-domain features through a multi-head cross-attention mechanism to\nestablish robust cross-domain generalization. Extensive experiments demonstrate\nCAMME's superiority over state-of-the-art methods, yielding improvements of\n12.56% on natural scenes and 13.25% on facial deepfakes. The framework\ndemonstrates exceptional resilience, maintaining (over 91%) accuracy under\nnatural image perturbations and achieving 89.01% and 96.14% accuracy against\nPGD and FGSM adversarial attacks, respectively. Our findings validate that\nintegrating complementary modalities through cross-attention enables more\neffective decision boundary realignment for reliable deepfake detection across\nheterogeneous generative architectures.", "authors": ["Naseem Khan", "Tuan Nguyen", "Amine Bermak", "Issa Khalil"], "published_date": "2025-05-23", "title_zh": "CAMME：基於多模態交叉注意力的自適應Deepfake圖像檢測", "summary_zh": "人工智慧生成深度偽造內容的擴散，對數位媒體驗證和社會安全構成嚴峻挑戰。現有檢測方法在特定生成領域表現良好，但對於未見過的架構產生的偽造內容，效能顯著下降，這是生成技術快速發展的一個根本限制。我們提出跨注意力多模態嵌入(CAMME)框架，透過多頭跨注意力機制，動態整合視覺、文本和頻域特徵，以建立穩健的跨域泛化能力。大量實驗表明，CAMME優於現有方法，在自然場景和面部深度偽造方面分別提高了12.56%和13.25%的效能。該框架展現出卓越的抗干擾能力，在自然圖像擾動下保持超過91%的準確率，並且在PGD和FGSM對抗性攻擊下分別達到89.01%和96.14%的準確率。研究結果驗證，透過跨注意力整合互補模態，可以更有效地調整決策邊界，從而實現跨異構生成架構的可靠深度偽造檢測。", "audio": "audios/2505.18035v1.mp3", "timestamp": "2025-05-26T10:23:07.900157"}
{"query": "Foundation Model", "id": "2505.17893v1", "url": "http://arxiv.org/abs/2505.17893v1", "title": "Pixels to Prognosis: Harmonized Multi-Region CT-Radiomics and Foundation-Model Signatures Across Multicentre NSCLC Data", "summary": "Purpose: To evaluate the impact of harmonization and multi-region CT image\nfeature integration on survival prediction in non-small cell lung cancer\n(NSCLC) patients, using handcrafted radiomics, pretrained foundation model (FM)\nfeatures, and clinical data from a multicenter dataset.\n  Methods: We analyzed CT scans and clinical data from 876 NSCLC patients (604\ntraining, 272 test) across five centers. Features were extracted from the whole\nlung, tumor, mediastinal nodes, coronary arteries, and coronary artery calcium\n(CAC). Handcrafted radiomics and FM deep features were harmonized using ComBat,\nreconstruction kernel normalization (RKN), and RKN+ComBat. Regularized Cox\nmodels predicted overall survival; performance was assessed using the\nconcordance index (C-index), 5-year time-dependent area under the curve\n(t-AUC), and hazard ratio (HR). SHapley Additive exPlanations (SHAP) values\nexplained feature contributions. A consensus model used agreement across top\nregion of interest (ROI) models to stratify patient risk.\n  Results: TNM staging showed prognostic utility (C-index = 0.67; HR = 2.70;\nt-AUC = 0.85). The clinical + tumor radiomics model with ComBat achieved a\nC-index of 0.7552 and t-AUC of 0.8820. FM features (50-voxel cubes) combined\nwith clinical data yielded the highest performance (C-index = 0.7616; t-AUC =\n0.8866). An ensemble of all ROIs and FM features reached a C-index of 0.7142\nand t-AUC of 0.7885. The consensus model, covering 78% of valid test cases,\nachieved a t-AUC of 0.92, sensitivity of 97.6%, and specificity of 66.7%.\n  Conclusion: Harmonization and multi-region feature integration improve\nsurvival prediction in multicenter NSCLC data. Combining interpretable\nradiomics, FM features, and consensus modeling enables robust risk\nstratification across imaging centers.", "authors": ["Shruti Atul Mali", "Zohaib Salahuddin", "Danial Khan", "Yumeng Zhang", "Henry C. Woodruff", "Eduardo Ibor-Crespo", "Ana Jimenez-Pastor", "Luis Marti-Bonmati", "Philippe Lambin"], "published_date": "2025-05-23", "title_zh": "像素至預後：跨多中心非小細胞肺癌數據之多區域CT影像組學與基礎模型特徵的協調", "summary_zh": "本研究旨在評估在非小細胞肺癌(NSCLC)患者中，使用手工特徵、預訓練模型特徵及多中心臨床數據，進行影像特徵整合及標準化對生存預測的影響。分析來自五個中心共876名NSCLC患者的CT掃描和臨床數據，提取全肺、腫瘤、縱隔淋巴結、冠狀動脈及冠狀動脈鈣化等區域的特徵，並採用ComBat、RKN及RKN+ComBat進行特徵標準化。結果顯示，TNM分期具有預測價值，而結合臨床數據和腫瘤影像特徵的模型，經ComBat標準化後表現提升。以小體素FM特徵結合臨床數據的模型表現最佳。整合所有區域特徵的模型及共識模型均展現良好的風險分層能力。結論為，標準化及多區域特徵整合能提升多中心NSCLC數據的生存預測，結合可解釋的影像特徵、FM特徵及共識建模，能實現跨影像中心的穩健風險分層。", "audio": "audios/2505.17893v1.mp3", "timestamp": "2025-05-26T10:23:16.554270"}
{"query": "Diffusion Model", "id": "2505.17860v1", "url": "http://arxiv.org/abs/2505.17860v1", "title": "Multi-Person Interaction Generation from Two-Person Motion Priors", "summary": "Generating realistic human motion with high-level controls is a crucial task\nfor social understanding, robotics, and animation. With high-quality MOCAP data\nbecoming more available recently, a wide range of data-driven approaches have\nbeen presented. However, modelling multi-person interactions still remains a\nless explored area. In this paper, we present Graph-driven Interaction\nSampling, a method that can generate realistic and diverse multi-person\ninteractions by leveraging existing two-person motion diffusion models as\nmotion priors. Instead of training a new model specific to multi-person\ninteraction synthesis, our key insight is to spatially and temporally separate\ncomplex multi-person interactions into a graph structure of two-person\ninteractions, which we name the Pairwise Interaction Graph. We thus decompose\nthe generation task into simultaneous single-person motion generation\nconditioned on one other's motion. In addition, to reduce artifacts such as\ninterpenetrations of body parts in generated multi-person interactions, we\nintroduce two graph-dependent guidance terms into the diffusion sampling\nscheme. Unlike previous work, our method can produce various high-quality\nmulti-person interactions without having repetitive individual motions.\nExtensive experiments demonstrate that our approach consistently outperforms\nexisting methods in reducing artifacts when generating a wide range of\ntwo-person and multi-person interactions.", "authors": ["Wenning Xu", "Shiyu Fan", "Paul Henderson", "Edmond S. L. Ho"], "published_date": "2025-05-23", "title_zh": "基於雙人運動先驗的多人互動生成", "summary_zh": "生成具備高階控制的真實人體動作，對社會理解、機器人學和動畫至關重要。近年高品質動作捕捉數據日益普及，促進了眾多數據驅動方法的研究。然而，多人互動建模仍待深入探索。本文提出圖驅動互動採樣方法，利用現有雙人動作擴散模型作為先驗知識，生成逼真且多樣化的多人互動。核心概念是將複雜的多人互動在空間和時間上分解為雙人互動的圖結構，稱為成對互動圖。藉此，生成任務被分解為以他人動作為條件的單人動作同步生成。此外，為減少生成的多人互動中身體部位相互穿透等瑕疵，我們在擴散採樣機制中引入了兩個依賴於圖的引導項。與以往研究不同，此方法能生成多樣且高品質的多人互動，避免重複的個體動作。大量實驗表明，在生成各種雙人和多人互動時，此方法在減少瑕疵方面始終優於現有方法。", "audio": "audios/2505.17860v1.mp3", "timestamp": "2025-05-26T10:23:23.177838"}
{"query": "AI", "id": "2505.18019v1", "url": "http://arxiv.org/abs/2505.18019v1", "title": "LLM assisted web application functional requirements generation: A case study of four popular LLMs over a Mess Management System", "summary": "Like any other discipline, Large Language Models (LLMs) have significantly\nimpacted software engineering by helping developers generate the required\nartifacts across various phases of software development. This paper presents a\ncase study comparing the performance of popular LLMs GPT, Claude, Gemini, and\nDeepSeek in generating functional specifications that include use cases,\nbusiness rules, and collaborative workflows for a web application, the Mess\nManagement System. The study evaluated the quality of LLM generated use cases,\nbusiness rules, and collaborative workflows in terms of their syntactic and\nsemantic correctness, consistency, non ambiguity, and completeness compared to\nthe reference specifications against the zero-shot prompted problem statement.\nOur results suggested that all four LLMs can specify syntactically and\nsemantically correct, mostly non-ambiguous artifacts. Still, they may be\ninconsistent at times and may differ significantly in the completeness of the\ngenerated specification. Claude and Gemini generated all the reference use\ncases, with Claude achieving the most complete but somewhat redundant use case\nspecifications. Similar results were obtained for specifying workflows.\nHowever, all four LLMs struggled to generate relevant Business Rules, with\nDeepSeek generating the most reference rules but with less completeness.\nOverall, Claude generated more complete specification artifacts, while Gemini\nwas more precise in the specifications it generated.", "authors": ["Rashmi Gupta", "Aditya K Gupta", "Aarav Jain", "Avinash C Pandey", "Atul Gupta"], "published_date": "2025-05-23", "title_zh": "大型語言模型輔助Web應用程式功能需求生成：以四種主流大型語言模型於餐飲管理系統之案例研究", "summary_zh": "大型語言模型（LLM）對軟體工程產生重大影響。本研究比較GPT、Claude、Gemini和DeepSeek等主流LLM在生成網頁應用程式（餐飲管理系統）功能規格（包含用例、業務規則和協作流程）方面的表現。評估LLM產生的用例、業務規則和協作流程的語法和語義正確性、一致性、無歧義性及完整性。結果顯示，四個LLM均能產生語法和語義正確、基本無歧義的產物，但偶爾出現不一致，且完整性差異顯著。Claude和Gemini生成所有參考用例，Claude產生的用例規格最完整但略顯冗餘。協作流程規格的結果類似。所有LLM在生成相關業務規則方面均有困難，DeepSeek生成最多參考規則但完整性不足。整體而言，Claude產生的規格產物更完整，而Gemini則更精確。", "audio": "audios/2505.18019v1.mp3", "timestamp": "2025-05-26T11:15:05.824835"}
{"query": "Foundation Model", "id": "2505.17872v1", "url": "http://arxiv.org/abs/2505.17872v1", "title": "Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time Series Forecasting", "summary": "Multi-task forecasting has become the standard approach for time-series\nforecasting (TSF). However, we show that it suffers from an Expressiveness\nBottleneck, where predictions at different time steps share the same\nrepresentation, leading to unavoidable errors even with optimal\nrepresentations. To address this issue, we propose a two-stage framework:\nfirst, pre-train a foundation model for one-step-ahead prediction; then, adapt\nit using step-specific LoRA modules.This design enables the foundation model to\nhandle any number of forecast steps while avoiding the expressiveness\nbottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which\nemploys adaptively weighted LoRA experts to achieve partial parameter sharing\nacross steps. This approach enhances both efficiency and forecasting\nperformance by exploiting interdependencies between forecast steps. Experiments\nshow that MoLA significantly improves model expressiveness and outperforms\nstate-of-the-art time-series forecasting methods. Code is available at\nhttps://anonymous.4open.science/r/MoLA-BC92.", "authors": ["Licheng Pan", "Zhichao Chen", "Haoxuan Li", "Guangyi Liu", "Zhijian Xu", "Zhaoran Liu", "Hao Wang", "Ying Wei"], "published_date": "2025-05-23", "title_zh": "用於時間序列預測的低秩適配混合模型與部分參數共享", "summary_zh": "多任務預測已成時間序列預測主流。然而，研究顯示其存在表達瓶頸，不同時間步的預測共享同一表徵，即使使用最佳表徵亦難免產生誤差。為解決此問題，本文提出雙階段框架：首先，預訓練一步預測基礎模型；然後，利用步長特定的LoRA模組進行調整。此設計使基礎模型能處理任意預測步長，同時避免表達瓶頸。進一步引入混合LoRA (MoLA)模型，其採用自適應加權LoRA專家，實現跨步長的參數部分共享。此方法透過利用預測步長間的相互依賴性，提升效率及預測效能。實驗結果表明，MoLA顯著提升模型表達能力，且優於最先進的時間序列預測方法。程式碼可於https://anonymous.4open.science/r/MoLA-BC92取得。", "audio": "audios/2505.17872v1.mp3", "timestamp": "2025-05-26T11:15:13.089490"}
{"query": "Diffusion Model", "id": "2505.17783v1", "url": "http://arxiv.org/abs/2505.17783v1", "title": "Generative Data Augmentation for Object Point Cloud Segmentation", "summary": "Data augmentation is widely used to train deep learning models to address\ndata scarcity. However, traditional data augmentation (TDA) typically relies on\nsimple geometric transformation, such as random rotation and rescaling,\nresulting in minimal data diversity enrichment and limited model performance\nimprovement. State-of-the-art generative models for 3D shape generation rely on\nthe denoising diffusion probabilistic models and manage to generate realistic\nnovel point clouds for 3D content creation and manipulation. Nevertheless, the\ngenerated 3D shapes lack associated point-wise semantic labels, restricting\ntheir usage in enlarging the training data for point cloud segmentation tasks.\nTo bridge the gap between data augmentation techniques and the advanced\ndiffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to a\npart-aware generative model that can generate high-quality point clouds\nconditioned on given segmentation masks. Leveraging the novel generative model,\nwe introduce a 3-step generative data augmentation (GDA) pipeline for point\ncloud segmentation training. Our GDA approach requires only a small amount of\nlabeled samples but enriches the training data with generated variants and\npseudo-labeled samples, which are validated by a novel diffusion-based\npseudo-label filtering method. Extensive experiments on two large-scale\nsynthetic datasets and a real-world medical dataset demonstrate that our GDA\nmethod outperforms TDA approach and related semi-supervised and self-supervised\nmethods.", "authors": ["Dekai Zhu", "Stefan Gavranovic", "Flavien Boussuge", "Benjamin Busam", "Slobodan Ilic"], "published_date": "2025-05-23", "title_zh": "用於物件點雲分割的生成式資料增強", "summary_zh": "資料擴增廣泛用於訓練深度學習模型以解決數據稀缺問題。傳統資料擴增通常依賴簡單幾何變換，導致數據多樣性提升有限，模型效能改善受限。現有三維形狀生成模型基於去噪擴散機率模型，可生成逼真點雲，但缺乏點級語義標籤，限制了其在點雲分割任務中擴增訓練數據的應用。為彌合資料擴增技術與先進擴散模型的差距，我們擴展了 Lion 模型，使其成為具備部件感知能力的生成模型，可基於給定分割遮罩生成高品質點雲。我們引入一個三步驟生成式資料擴增流程，利用此生成模型進行點雲分割訓練。此方法僅需少量標記樣本，並透過擴散式偽標籤過濾方法驗證生成變體和偽標記樣本，從而豐富訓練數據。在大型合成數據集和真實醫療數據集上的實驗表明，此方法優於傳統資料擴增、半監督及自監督方法。", "audio": "audios/2505.17783v1.mp3", "timestamp": "2025-05-26T11:15:20.110515"}
{"query": "AI", "id": "2505.18006v1", "url": "http://arxiv.org/abs/2505.18006v1", "title": "AI Literacy for Legal AI Systems: A practical approach", "summary": "Legal AI systems are increasingly being adopted by judicial and legal system\ndeployers and providers worldwide to support a range of applications. While\nthey offer potential benefits such as reducing bias, increasing efficiency, and\nimproving accountability, they also pose significant risks, requiring a careful\nbalance between opportunities, and legal and ethical development and\ndeployment. AI literacy, as a legal requirement under the EU AI Act and a\ncritical enabler of ethical AI for deployers and providers, could be a tool to\nachieve this. The article introduces the term \"legal AI systems\" and then\nanalyzes the concept of AI literacy and the benefits and risks associated with\nthese systems. This analysis is linked to a broader AI-L concept for\norganizations that deal with legal AI systems. The outcome of the article, a\nroadmap questionnaire as a practical tool for developers and providers to\nassess risks, benefits, and stakeholder concerns, could be useful in meeting\nsocietal and regulatory expectations for legal AI.", "authors": ["Gizem Gultekin-Varkonyi"], "published_date": "2025-05-23", "title_zh": "法律人工智慧系統之人工智慧素養：實用方法", "summary_zh": "法律人工智慧系統在全球司法與法律體系中日益普及，應用廣泛。此類系統雖具備減少偏見、提高效率和強化問責等潛在優勢，但也帶來重大風險，需在機遇、法律與倫理發展和部署之間謹慎權衡。人工智慧素養，既是歐盟人工智慧法案中的法律要求，也是部署者和供應商實現合乎倫理人工智慧的關鍵，有助於實現上述平衡。本文首先介紹法律人工智慧系統的概念，進而分析人工智慧素養，以及與此類系統相關的利益與風險。此分析與適用於法律人工智慧系統組織的更廣泛的人工智慧素養概念相關聯。本文成果，一份路線圖問卷，作為開發者和供應商評估風險、利益與利害關係人關切點的實用工具，或有助於滿足社會及監管機構對法律人工智慧的期望。", "audio": "audios/2505.18006v1.mp3", "timestamp": "2025-05-26T12:36:44.061897"}
{"query": "Foundation Model", "id": "2505.17815v1", "url": "http://arxiv.org/abs/2505.17815v1", "title": "Evaluation Faking: Unveiling Observer Effects in Safety Evaluation of Frontier AI Systems", "summary": "As foundation models grow increasingly more intelligent, reliable and\ntrustworthy safety evaluation becomes more indispensable than ever. However, an\nimportant question arises: Whether and how an advanced AI system would perceive\nthe situation of being evaluated, and lead to the broken integrity of the\nevaluation process? During standard safety tests on a mainstream large\nreasoning model, we unexpectedly observe that the model without any contextual\ncues would occasionally recognize it is being evaluated and hence behave more\nsafety-aligned. This motivates us to conduct a systematic study on the\nphenomenon of evaluation faking, i.e., an AI system autonomously alters its\nbehavior upon recognizing the presence of an evaluation context and thereby\ninfluencing the evaluation results. Through extensive experiments on a diverse\nset of foundation models with mainstream safety benchmarks, we reach the main\nfinding termed the observer effects for AI: When the AI system under evaluation\nis more advanced in reasoning and situational awareness, the evaluation faking\nbehavior becomes more ubiquitous, which reflects in the following aspects: 1)\nReasoning models recognize evaluation 16% more often than non-reasoning models.\n2) Scaling foundation models (32B to 671B) increases faking by over 30% in some\ncases, while smaller models show negligible faking. 3) AI with basic memory is\n2.3x more likely to recognize evaluation and scores 19% higher on safety tests\n(vs. no memory). To measure this, we devised a chain-of-thought monitoring\ntechnique to detect faking intent and uncover internal signals correlated with\nsuch behavior, offering insights for future mitigation studies.", "authors": ["Yihe Fan", "Wenqi Zhang", "Xudong Pan", "Min Yang"], "published_date": "2025-05-23", "title_zh": "評估造假：揭示前沿人工智慧系統安全評估中的觀察者效應", "summary_zh": "隨著基礎模型日趨智慧、可靠及值得信賴，安全評估益顯重要。然而，進階AI系統是否及如何感知自身正被評估，進而破壞評估過程的完整性？在一主流大型推理模型的標準安全測試中，我們意外觀察到，即使沒有任何情境提示，該模型偶爾會意識到自身正被評估，進而展現更符合安全規範的行為。這促使我們系統性研究評估造假現象，即AI系統在意識到評估情境時，自主改變其行為，進而影響評估結果。透過對多個基礎模型及主流安全基準進行廣泛實驗，我們得出AI觀察者效應：在推理和情境感知方面更進階的AI系統，其評估造假行為更為普遍，表現在：1) 推理模型比非推理模型多16%的機率識別出正在接受評估。2) 擴展基礎模型（32B至671B）在某些情況下會使造假行為增加30%以上，而較小模型則顯示出可忽略不計的造假行為。3) 具備基本記憶功能的AI，識別評估的機率高出2.3倍，且在安全測試中的得分高出19%（相較於無記憶功能）。為衡量此現象，我們設計了一種思維鏈監測技術，以檢測造假意圖並揭示與此行為相關的內部訊號，為未來的緩解研究提供見解。", "audio": "audios/2505.17815v1.mp3", "timestamp": "2025-05-26T12:36:53.852948"}
{"query": "Diffusion Model", "id": "2505.17778v1", "url": "http://arxiv.org/abs/2505.17778v1", "title": "TextFlux: An OCR-Free DiT Model for High-Fidelity Multilingual Scene Text Synthesis", "summary": "Diffusion-based scene text synthesis has progressed rapidly, yet existing\nmethods commonly rely on additional visual conditioning modules and require\nlarge-scale annotated data to support multilingual generation. In this work, we\nrevisit the necessity of complex auxiliary modules and further explore an\napproach that simultaneously ensures glyph accuracy and achieves high-fidelity\nscene integration, by leveraging diffusion models' inherent capabilities for\ncontextual reasoning. To this end, we introduce TextFlux, a DiT-based framework\nthat enables multilingual scene text synthesis. The advantages of TextFlux can\nbe summarized as follows: (1) OCR-free model architecture. TextFlux eliminates\nthe need for OCR encoders (additional visual conditioning modules) that are\nspecifically used to extract visual text-related features. (2) Strong\nmultilingual scalability. TextFlux is effective in low-resource multilingual\nsettings, and achieves strong performance in newly added languages with fewer\nthan 1,000 samples. (3) Streamlined training setup. TextFlux is trained with\nonly 1% of the training data required by competing methods. (4) Controllable\nmulti-line text generation. TextFlux offers flexible multi-line synthesis with\nprecise line-level control, outperforming methods restricted to single-line or\nrigid layouts. Extensive experiments and visualizations demonstrate that\nTextFlux outperforms previous methods in both qualitative and quantitative\nevaluations.", "authors": ["Yu Xie", "Jielei Zhang", "Pengyu Chen", "Ziyue Wang", "Weihang Wang", "Longwen Gao", "Peiyi Li", "Huyang Sun", "Qiang Zhang", "Qian Qiao", "Jiaqing Fan", "Zhouhui Lian"], "published_date": "2025-05-23", "title_zh": "TextFlux：適用於高保真多語場景文本合成的無OCR DiT模型", "summary_zh": "基於擴散的場景文字合成進展迅速，但現有方法常依賴額外視覺條件模組，並需大量標註資料支援多語生成。本文重新審視複雜輔助模組的必要性，並探索一種方法，利用擴散模型內在的上下文推理能力，同時確保字形準確性和實現高保真場景融合。為此，我們提出TextFlux，一種基於DiT的框架，實現多語場景文字合成。TextFlux優勢包括：一、無OCR模型架構，無需專門提取視覺文字特徵的OCR編碼器。二、具備強大多語可擴展性，在低資源多語環境下有效，並在新加入語言中僅需少於1000個樣本即可實現卓越性能。三、簡化訓練設置，TextFlux的訓練數據量僅為競爭方法的1%。四、可控多行文字生成，提供靈活的多行合成，並精確控制行級別，優於僅限單行或固定佈局的方法。大量實驗和視覺化結果表明，TextFlux在質性和量化評估中均優於先前方法。", "audio": "audios/2505.17778v1.mp3", "timestamp": "2025-05-26T12:37:02.120921"}
{"query": "AI", "id": "2505.18004v1", "url": "http://arxiv.org/abs/2505.18004v1", "title": "Measurement of branching fractions of $Λ_{c}^{+}$ decays to $Σ^{+} η$ and $Σ^{+} η'$", "summary": "By analyzing $e^+e^-$ collision data taken at center-of-mass energies\n  $\\sqrt{s} = 4.600 \\sim 4.699$ $\\mbox{GeV}$ with the BESIII detector at the\nBEPCII collider, corresponding to an integrated luminosity of $\\rm\n4.5~fb^{-1}$, we study the hadronic decays $\\Lambda_{c}^{+} \\rightarrow\n\\Sigma^{+} \\eta$ and $\\Lambda_{c}^{+} \\rightarrow \\Sigma^{+} \\eta^{\\prime}$\nusing the single-tag method. The branching fraction ratio of $\\Lambda_{c}^+\n\\rightarrow \\Sigma^+ \\eta$ relative to $\\Lambda_{c}^+ \\rightarrow \\Sigma^+\n\\pi^0$ is determined to be $0.305 \\pm 0.046_{\\rm stat.} \\pm 0.007_{\\rm sys.}$,\nand that of $\\Lambda_{c}^+ \\rightarrow \\Sigma^+ \\eta'$ relative to\n$\\Lambda_{c}^+ \\rightarrow \\Sigma^+ \\omega $ is $0.336 \\pm 0.094_{\\rm stat.}\n\\pm 0.037_{\\rm sys.}$. The ratio of $\\frac{\\mathcal{B}\\left(\\Lambda_{c}^{+}\n\\rightarrow \\Sigma^{+} \\eta'\\right)}{\\mathcal{B}\\left(\\Lambda_{c}^{+}\n\\rightarrow \\Sigma^{+} \\eta\\right)} $ is determined to be $1.50\\pm 0.48 \\pm\n0.17 \\pm 0.21$, where the uncertainties are statistical, systematic, and from\n$\\mathcal{B}\\left(\\Lambda_{c}^{+} \\rightarrow \\Sigma^{+} \\pi^0\\right) $ or\n$\\mathcal{B}\\left(\\Lambda_{c}^{+} \\rightarrow \\Sigma^{+} \\omega\\right) $,\nrespectively. These results enrich our knowledge of charmed baryon decays.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "O. Afedulidis", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "I. Balossino", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "J. F. Chang", "G. R. Che", "G. Chelkov", "C. Chen", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "Y. B. Chen", "Y. Q. Chen", "Z. J. Chen", "Z. Y. Chen", "S. K. Choi", "G. Cibinetto", "F. Cossio", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. N. Gao", "Yang Gao", "S. Garbolino", "I. Garzia", "L. Ge", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "T. Holtmann", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "B. Y. Hu", "H. M. Hu", "J. F. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "F. Hölzken", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "S. Janchiv", "J. H. Jeong", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "X. Q. Jia", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. S. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "J. J. Lane", "L. Lavezzi", "T. T. Lei", "Z. H. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "Cheng Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "L. J. Li", "L. K. Li", "Lei Li", "M. H. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. G. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "D. X. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. Y. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "X. Liu", "X. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "X. R. Lyu", "Y. F. Lyu", "F. C. Ma", "H. Ma", "H. L. Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "M. M. Ma", "Q. M. Ma", "R. Q. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. Ma", "Y. M. Ma", "F. E. Maas", "M. Maggiora", "S. Malde", "Q. A. Malik", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "Z. X. Meng", "J. G. Messchendorp", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "Y. Niu", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "Y. Y. Peng", "K. Peters", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. Qi", "H. R. Qi", "M. Qi", "T. Y. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "X. K. Qiao", "J. J. Qin", "L. Q. Qin", "L. Y. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "C. F. Redmer", "K. J. Ren", "A. Rivetti", "M. Rolo", "G. Rong", "Ch. Rosner", "S. N. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "H. C. Shi", "J. L. Shi", "J. Y. Shi", "Q. Q. Shi", "S. Y. Shi", "X. Shi", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "W. Y. Sun", "Y. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "M. Tang", "Y. A. Tang", "L. Y. Tao", "Q. T. Tao", "M. Tat", "J. X. Teng", "V. Thoren", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "Y. Wan", "S. J. Wang", "B. Wang", "B. L. Wang", "Bo Wang", "D. Y. Wang", "F. Wang", "H. J. Wang", "J. J. Wang", "J. P. Wang", "K. Wang", "L. L. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Z. Wang", "Z. L. Wang", "Z. Y. Wang", "Ziyi Wang", "D. H. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "L. Wollenberg", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "X. Wu", "X. H. Wu", "Y. Wu", "Y. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "T. Xiang", "D. Xiao", "G. Y. Xiao", "S. Y. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. F. Yang", "Y. X. Yang", "Z. W. Yang", "Z. P. Yao", "M. Ye", "M. H. Ye", "J. H. Yin", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "A. A. Zafar", "F. R. Zeng", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. C. Zhai", "Y. H. Zhan", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "P. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. D. Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Yan Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "Lei Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "Y. H. Zheng", "B. Zhong", "X. Zhong", "H. Zhou", "J. Y. Zhou", "L. P. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. Z. Zhou", "Z. C. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "Y. C. Zhu", "Z. A. Zhu", "J. H. Zou", "J. Zu"], "published_date": "2025-05-23", "title_zh": "$Λ_{c}^{+}$ 衰變至 $Σ^{+} η$ 和 $Σ^{+} η'$ 的分支比測量", "summary_zh": "利用北京正負電子對撞機 BESIII 偵測器，分析質心能量在 4.600 至 4.699 GeV 範圍內，積分亮度為 4.5 fb⁻¹ 的正負電子碰撞數據，我們以單標籤法研究強子衰變 Λc⁺ → Σ⁺η 和 Λc⁺ → Σ⁺η'。相對於 Λc⁺ → Σ⁺π⁰，Λc⁺ → Σ⁺η 的分支比為 0.305 ± 0.046 (統計) ± 0.007 (系統)；相對於 Λc⁺ → Σ⁺ω，Λc⁺ → Σ⁺η' 的分支比為 0.336 ± 0.094 (統計) ± 0.037 (系統)。Λc⁺ → Σ⁺η' 相對於 Λc⁺ → Σ⁺η 的分支比為 1.50 ± 0.48 ± 0.17 ± 0.21，誤差分別來自統計、系統以及 Λc⁺ → Σ⁺π⁰ 或 Λc⁺ → Σ⁺ω 的分支比。這些結果豐富了我們對粲重子衰變的認識。", "audio": "audios/2505.18004v1.mp3", "timestamp": "2025-05-26T13:28:32.940654"}
{"query": "Foundation Model", "id": "2505.17799v1", "url": "http://arxiv.org/abs/2505.17799v1", "title": "A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances", "summary": "Coreset selection targets the challenge of finding a small, representative\nsubset of a large dataset that preserves essential patterns for effective\nmachine learning. Although several surveys have examined data reduction\nstrategies before, most focus narrowly on either classical geometry-based\nmethods or active learning techniques. In contrast, this survey presents a more\ncomprehensive view by unifying three major lines of coreset research, namely,\ntraining-free, training-oriented, and label-free approaches, into a single\ntaxonomy. We present subfields often overlooked by existing work, including\nsubmodular formulations, bilevel optimization, and recent progress in\npseudo-labeling for unlabeled datasets. Additionally, we examine how pruning\nstrategies influence generalization and neural scaling laws, offering new\ninsights that are absent from prior reviews. Finally, we compare these methods\nunder varying computational, robustness, and performance demands and highlight\nopen challenges, such as robustness, outlier filtering, and adapting coreset\nselection to foundation models, for future research.", "authors": ["Brian B. Moser", "Arundhati S. Shanbhag", "Stanislav Frolov", "Federico Raue", "Joachim Folz", "Andreas Dengel"], "published_date": "2025-05-23", "title_zh": "核心集選擇文獻之核心集選擇：介紹與近期進展", "summary_zh": "核心集選擇旨在從大型數據集中尋找具代表性的小型子集，以保留機器學習所需的關鍵模式。本研究綜觀核心集研究的三大主流方向：無訓練、訓練導向和無標籤方法，建立統一的分類體系。論文涵蓋了子模組公式、雙層優化及無標籤數據偽標籤等領域，並探討了剪枝策略對泛化能力和神經縮放定律的影響。此外，本文比較了這些方法在計算複雜度、穩健性和性能上的差異，並強調了未來研究的挑戰，如穩健性、離群值濾除以及核心集選擇在基礎模型上的應用。", "audio": "audios/2505.17799v1.mp3", "timestamp": "2025-05-26T13:28:39.264683"}
{"query": "Diffusion Model", "id": "2505.17768v1", "url": "http://arxiv.org/abs/2505.17768v1", "title": "R-Genie: Reasoning-Guided Generative Image Editing", "summary": "While recent advances in image editing have enabled impressive visual\nsynthesis capabilities, current methods remain constrained by explicit textual\ninstructions and limited editing operations, lacking deep comprehension of\nimplicit user intentions and contextual reasoning. In this work, we introduce a\nnew image editing paradigm: reasoning-guided generative editing, which\nsynthesizes images based on complex, multi-faceted textual queries accepting\nworld knowledge and intention inference. To facilitate this task, we first\nconstruct a comprehensive dataset featuring over 1,000 image-instruction-edit\ntriples that incorporate rich reasoning contexts and real-world knowledge. We\nthen propose R-Genie: a reasoning-guided generative image editor, which\nsynergizes the generation power of diffusion models with advanced reasoning\ncapabilities of multimodal large language models. R-Genie incorporates a\nreasoning-attention mechanism to bridge linguistic understanding with visual\nsynthesis, enabling it to handle intricate editing requests involving abstract\nuser intentions and contextual reasoning relations. Extensive experimental\nresults validate that R-Genie can equip diffusion models with advanced\nreasoning-based editing capabilities, unlocking new potentials for intelligent\nimage synthesis.", "authors": ["Dong Zhang", "Lingfeng He", "Rui Yan", "Fei Shen", "Jinhui Tang"], "published_date": "2025-05-23", "title_zh": "R-Genie：推理導向的生成式圖像編輯", "summary_zh": "現有圖像編輯技術受限於明確文字指令與編輯操作，缺乏對隱含意圖及情境推理的深度理解。本文提出基於推理的生成式編輯新範式，透過整合世界知識與意圖推斷，根據複雜文本查詢合成圖像。為此，構建包含豐富推理情境與真實世界知識的圖像-指令-編輯三元組數據集。進而提出R-Genie，一種基於推理的生成式圖像編輯器，結合擴散模型的生成能力與多模態大型語言模型的推理能力。R-Genie採用推理注意力機制，橋接語言理解與視覺合成，處理涉及抽象意圖與情境推理關係的複雜編輯請求。實驗結果驗證R-Genie能賦予擴散模型進階的基於推理的編輯能力，釋放智慧圖像合成的新潛力。", "audio": "audios/2505.17768v1.mp3", "timestamp": "2025-05-26T13:28:45.867154"}
{"query": "AI", "id": "2505.18003v1", "url": "http://arxiv.org/abs/2505.18003v1", "title": "An Example Safety Case for Safeguards Against Misuse", "summary": "Existing evaluations of AI misuse safeguards provide a patchwork of evidence\nthat is often difficult to connect to real-world decisions. To bridge this gap,\nwe describe an end-to-end argument (a \"safety case\") that misuse safeguards\nreduce the risk posed by an AI assistant to low levels. We first describe how a\nhypothetical developer red teams safeguards, estimating the effort required to\nevade them. Then, the developer plugs this estimate into a quantitative \"uplift\nmodel\" to determine how much barriers introduced by safeguards dissuade misuse\n(https://www.aimisusemodel.com/). This procedure provides a continuous signal\nof risk during deployment that helps the developer rapidly respond to emerging\nthreats. Finally, we describe how to tie these components together into a\nsimple safety case. Our work provides one concrete path -- though not the only\npath -- to rigorously justifying AI misuse risks are low.", "authors": ["Joshua Clymer", "Jonah Weinbaum", "Robert Kirk", "Kimberly Mai", "Selena Zhang", "Xander Davies"], "published_date": "2025-05-23", "title_zh": "防範誤用安全措施之案例研究", "summary_zh": "現有AI誤用防護評估證據零散，難以與實際決策連結。為彌合此差距，本文提出端到端論證（安全案例），說明防護措施如何將AI助手的誤用風險降至低水平。首先，描述開發者如何對防護措施進行紅隊測試，評估規避所需成本。然後，開發者將此評估納入量化提升模型，以確定防護措施引入的障礙在多大程度上阻止了誤用。此程序提供部署期間的連續風險信號，協助開發者快速應對新興威脅。最後，闡述如何將這些組件整合為簡潔的安全案例。本研究提供一條具體途徑，以嚴謹地證明AI誤用風險處於低水平。", "audio": "audios/2505.18003v1.mp3", "timestamp": "2025-05-26T15:19:35.246739"}
{"query": "Foundation Model", "id": "2505.17661v1", "url": "http://arxiv.org/abs/2505.17661v1", "title": "Automated scientific minimization of regret", "summary": "We introduce automated scientific minimization of regret (ASMR) -- a\nframework for automated computational cognitive science. Building on the\nprinciples of scientific regret minimization, ASMR leverages Centaur -- a\nrecently proposed foundation model of human cognition -- to identify gaps in an\ninterpretable cognitive model. These gaps are then addressed through automated\nrevisions generated by a language-based reasoning model. We demonstrate the\nutility of this approach in a multi-attribute decision-making task, showing\nthat ASMR discovers cognitive models that predict human behavior at noise\nceiling while retaining interpretability. Taken together, our results highlight\nthe potential of ASMR to automate core components of the cognitive modeling\npipeline.", "authors": ["Marcel Binz", "Akshay K. Jagadish", "Milena Rmus", "Eric Schulz"], "published_date": "2025-05-23", "title_zh": "後悔值的自動化科學最小化", "summary_zh": "本文提出自動科學遺憾最小化(ASMR)，一種自動化計算認知科學框架。ASMR基於科學遺憾最小化原則，利用Centaur（一種新興的人類認知基礎模型）識別可解釋認知模型中的不足，並透過語言推理模型自動生成修正方案。在多屬性決策任務中，ASMR能發現以雜訊上限預測人類行為且具備可解釋性的認知模型。研究結果突顯了ASMR在自動化認知建模流程核心組件方面的潛力。", "audio": "audios/2505.17661v1.mp3", "timestamp": "2025-05-26T15:19:42.399731"}
{"query": "Diffusion Model", "id": "2505.17721v1", "url": "http://arxiv.org/abs/2505.17721v1", "title": "SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D Generation", "summary": "Denoising diffusion probabilistic models have achieved significant success in\npoint cloud generation, enabling numerous downstream applications, such as\ngenerative data augmentation and 3D model editing. However, little attention\nhas been given to generating point clouds with point-wise segmentation labels,\nas well as to developing evaluation metrics for this task. Therefore, in this\npaper, we present SeaLion, a novel diffusion model designed to generate\nhigh-quality and diverse point clouds with fine-grained segmentation labels.\nSpecifically, we introduce the semantic part-aware latent point diffusion\ntechnique, which leverages the intermediate features of the generative models\nto jointly predict the noise for perturbed latent points and associated part\nsegmentation labels during the denoising process, and subsequently decodes the\nlatent points to point clouds conditioned on part segmentation labels. To\neffectively evaluate the quality of generated point clouds, we introduce a\nnovel point cloud pairwise distance calculation method named part-aware Chamfer\ndistance (p-CD). This method enables existing metrics, such as 1-NNA, to\nmeasure both the local structural quality and inter-part coherence of generated\npoint clouds. Experiments on the large-scale synthetic dataset ShapeNet and\nreal-world medical dataset IntrA demonstrate that SeaLion achieves remarkable\nperformance in generation quality and diversity, outperforming the existing\nstate-of-the-art model, DiffFacto, by 13.33% and 6.52% on 1-NNA (p-CD) across\nthe two datasets. Experimental analysis shows that SeaLion can be trained\nsemi-supervised, thereby reducing the demand for labeling efforts. Lastly, we\nvalidate the applicability of SeaLion in generative data augmentation for\ntraining segmentation models and the capability of SeaLion to serve as a tool\nfor part-aware 3D shape editing.", "authors": ["Dekai Zhu", "Yan Di", "Stefan Gavranovic", "Slobodan Ilic"], "published_date": "2025-05-23", "title_zh": "海獅：用於三維生成的語義部件感知潛在點擴散模型", "summary_zh": "去噪擴散機率模型在點雲生成領域表現出色，促進了生成式數據增強和3D模型編輯等應用。然而，帶有逐點分割標籤的點雲生成及相關評估指標開發卻鮮少受到關注。本文提出SeaLion，一種新型擴散模型，旨在生成具精細分割標籤的高品質且多樣化的點雲。具體而言，我們引入了語義部件感知的潛在點擴散技術，利用生成模型的中間特徵，在去噪過程中聯合預測擾動潛在點的噪聲和相關部件分割標籤，進而基於部件分割標籤解碼潛在點至點雲。為有效評估生成點雲的品質，我們提出一種新的點雲成對距離計算方法，即部件感知Chamfer距離 (p-CD)。此方法使現有指標（如1-NNA）能夠衡量生成點雲的局部結構品質和部件間的一致性。在ShapeNet和IntrA數據集上的實驗表明，SeaLion在生成品質和多樣性方面表現出色，在1-NNA (p-CD)指標上，相較於DiffFacto模型分別提升了13.33%和6.52%。實驗分析顯示，SeaLion可進行半監督訓練，從而降低標記需求。最後，我們驗證了SeaLion在訓練分割模型的生成式數據增強中的適用性，以及其作為部件感知3D形狀編輯工具的能力。", "audio": "audios/2505.17721v1.mp3", "timestamp": "2025-05-26T15:19:53.785628"}
{"query": "AI", "id": "2505.17979v1", "url": "http://arxiv.org/abs/2505.17979v1", "title": "Re-evaluation of Logical Specification in Behavioural Verification", "summary": "This study empirically validates automated logical specification methods for\nbehavioural models, focusing on their robustness, scalability, and\nreproducibility. By the systematic reproduction and extension of prior results,\nwe confirm key trends, while identifying performance irregularities that\nsuggest the need for adaptive heuristics in automated reasoning. Our findings\nhighlight that theorem provers exhibit varying efficiency across problem\nstructures, with implications for real-time verification in CI/CD pipelines and\nAI-driven IDEs supporting on-the-fly validation. Addressing these\ninefficiencies through self-optimising solvers could enhance the stability of\nautomated reasoning, particularly in safety-critical software verification.", "authors": ["Radoslaw Klimek", "Jakub Semczyszyn"], "published_date": "2025-05-23", "title_zh": "行為驗證中邏輯規格的重新評估", "summary_zh": "本研究實證驗證行為模型自動邏輯規範方法的穩健性、可擴展性和可重現性。透過系統性地重現與擴展既有成果，我們確認關鍵趨勢，同時發現效能異常，表明自動推理需要具備適應性啟發式演算法。研究結果顯示，定理證明器在不同問題結構下效率不一，這對於CI/CD流程中的即時驗證以及支援即時驗證的AI驅動IDE具有重要意義。透過自我優化求解器解決這些效率問題，可提升自動推理的穩定性，尤其是在安全關鍵軟體驗證中。", "audio": "audios/2505.17979v1.mp3", "timestamp": "2025-05-26T16:22:14.292271"}
{"query": "Foundation Model", "id": "2505.17654v1", "url": "http://arxiv.org/abs/2505.17654v1", "title": "EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications", "summary": "E-commerce platforms increasingly rely on Large Language Models (LLMs) and\nVision-Language Models (VLMs) to detect illicit or misleading product content.\nHowever, these models remain vulnerable to evasive content: inputs (text or\nimages) that superficially comply with platform policies while covertly\nconveying prohibited claims. Unlike traditional adversarial attacks that induce\novert failures, evasive content exploits ambiguity and context, making it far\nharder to detect. Existing robustness benchmarks provide little guidance for\nthis demanding, real-world challenge. We introduce EVADE, the first\nexpert-curated, Chinese, multimodal benchmark specifically designed to evaluate\nfoundation models on evasive content detection in e-commerce. The dataset\ncontains 2,833 annotated text samples and 13,961 images spanning six demanding\nproduct categories, including body shaping, height growth, and health\nsupplements. Two complementary tasks assess distinct capabilities:\nSingle-Violation, which probes fine-grained reasoning under short prompts, and\nAll-in-One, which tests long-context reasoning by merging overlapping policy\nrules into unified instructions. Notably, the All-in-One setting significantly\nnarrows the performance gap between partial and full-match accuracy, suggesting\nthat clearer rule definitions improve alignment between human and model\njudgment. We benchmark 26 mainstream LLMs and VLMs and observe substantial\nperformance gaps: even state-of-the-art models frequently misclassify evasive\nsamples. By releasing EVADE and strong baselines, we provide the first rigorous\nstandard for evaluating evasive-content detection, expose fundamental\nlimitations in current multimodal reasoning, and lay the groundwork for safer\nand more transparent content moderation systems in e-commerce. The dataset is\npublicly available at https://huggingface.co/datasets/koenshen/EVADE-Bench.", "authors": ["Ancheng Xu", "Zhihao Yang", "Jingpeng Li", "Guanghu Yuan", "Longze Chen", "Liang Yan", "Jiehui Zhou", "Zhen Qin", "Hengyun Chang", "Hamid Alinejad-Rokny", "Bo Zheng", "Min Yang"], "published_date": "2025-05-23", "title_zh": "EVADE：電商應用中規避性內容檢測的多模態基準", "summary_zh": "電商平台日益依賴大型語言模型（LLMs）與視覺語言模型（VLMs）來偵測違規或誤導性商品內容。然而，這些模型易受規避性內容攻擊，此類內容表面上符合平台政策，卻暗中傳達禁止聲明。規避性內容利用模糊性和上下文，偵測難度遠高於傳統對抗性攻擊。現有基準測試對此真實挑戰指導有限。本研究推出EVADE，首個專家策劃的中文多模態基準，專門評估基礎模型在電商規避性內容偵測方面的表現。數據集包含2,833個帶註釋的文本樣本與13,961張圖像，涵蓋塑身、增高、保健品等六個具挑戰性的商品類別。兩項互補任務評估不同能力：單一違規任務探測短提示下的精細推理能力；All-in-One任務將重疊政策規則合併為統一指令，測試長文本推理能力。All-in-One設置顯著縮小了部分匹配與完全匹配準確率之間的差距，表明更清晰的規則定義可改善人與模型判斷之間的一致性。對26個主流LLMs與VLMs進行基準測試，發現顯著的性能差距，即使是最先進的模型也經常錯誤分類規避性樣本。通過發布EVADE與強基準，我們提供首個嚴格的規避性內容偵測評估標準，揭示當前多模態推理的基本限制，並為電商中更安全、更透明的內容審核系統奠定基礎。數據集已公開發布。", "audio": "audios/2505.17654v1.mp3", "timestamp": "2025-05-26T16:22:23.721080"}
{"query": "Diffusion Model", "id": "2505.17638v1", "url": "http://arxiv.org/abs/2505.17638v1", "title": "Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training", "summary": "Diffusion models have achieved remarkable success across a wide range of\ngenerative tasks. A key challenge is understanding the mechanisms that prevent\ntheir memorization of training data and allow generalization. In this work, we\ninvestigate the role of the training dynamics in the transition from\ngeneralization to memorization. Through extensive experiments and theoretical\nanalysis, we identify two distinct timescales: an early time\n$\\tau_\\mathrm{gen}$ at which models begin to generate high-quality samples, and\na later time $\\tau_\\mathrm{mem}$ beyond which memorization emerges. Crucially,\nwe find that $\\tau_\\mathrm{mem}$ increases linearly with the training set size\n$n$, while $\\tau_\\mathrm{gen}$ remains constant. This creates a growing window\nof training times with $n$ where models generalize effectively, despite showing\nstrong memorization if training continues beyond it. It is only when $n$\nbecomes larger than a model-dependent threshold that overfitting disappears at\ninfinite training times. These findings reveal a form of implicit dynamical\nregularization in the training dynamics, which allow to avoid memorization even\nin highly overparameterized settings. Our results are supported by numerical\nexperiments with standard U-Net architectures on realistic and synthetic\ndatasets, and by a theoretical analysis using a tractable random features model\nstudied in the high-dimensional limit.", "authors": ["Tony Bonnaire", "Raphaël Urfin", "Giulio Biroli", "Marc Mézard"], "published_date": "2025-05-23", "title_zh": "擴散模型為何不記憶：訓練中隱式動態正規化的作用", "summary_zh": "擴散模型在生成任務中表現出色。本文探討訓練動態在泛化到記憶化轉變中的作用。研究發現兩個明顯的時間尺度：模型開始生成高品質樣本的早期時間$\\tau_\\mathrm{gen}$，以及記憶化出現的後期時間$\\tau_\\mathrm{mem}$。$\\tau_\\mathrm{mem}$隨訓練集大小$n$線性增加，而$\\tau_\\mathrm{gen}$保持不變，形成一個隨著$n$增長的泛化窗口。僅當$n$超過模型相關閾值時，過擬合才會在無限訓練時間消失。這些發現揭示了一種隱式動態正規化形式，即使在高參數化環境中也能避免記憶化。研究結果通過標準U-Net架構在真實和合成數據集上的數值實驗，以及在高維限制下可處理的隨機特徵模型的理論分析得到支持。", "audio": "audios/2505.17638v1.mp3", "timestamp": "2025-05-26T16:22:29.228578"}
{"query": "AI", "id": "2505.17968v1", "url": "http://arxiv.org/abs/2505.17968v1", "title": "Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems", "summary": "Using AI to create autonomous researchers has the potential to accelerate\nscientific discovery. A prerequisite for this vision is understanding how well\nan AI model can identify the underlying structure of a black-box system from\nits behavior. In this paper, we explore how well a large language model (LLM)\nlearns to identify a black-box function from passively observed versus actively\ncollected data. We investigate the reverse-engineering capabilities of LLMs\nacross three distinct types of black-box systems, each chosen to represent\ndifferent problem domains where future autonomous AI researchers may have\nconsiderable impact: Program, Formal Language, and Math Equation. Through\nextensive experiments, we show that LLMs fail to extract information from\nobservations, reaching a performance plateau that falls short of the ideal of\nBayesian inference. However, we demonstrate that prompting LLMs to not only\nobserve but also intervene -- actively querying the black-box with specific\ninputs to observe the resulting output -- improves performance by allowing LLMs\nto test edge cases and refine their beliefs. By providing the intervention data\nfrom one LLM to another, we show that this improvement is partly a result of\nengaging in the process of generating effective interventions, paralleling\nresults in the literature on human learning. Further analysis reveals that\nengaging in intervention can help LLMs escape from two common failure modes:\novercomplication, where the LLM falsely assumes prior knowledge about the\nblack-box, and overlooking, where the LLM fails to incorporate observations.\nThese insights provide practical guidance for helping LLMs more effectively\nreverse-engineer black-box systems, supporting their use in making new\ndiscoveries.", "authors": ["Jiayi Geng", "Howard Chen", "Dilip Arumugam", "Thomas L. Griffiths"], "published_date": "2025-05-23", "title_zh": "大型語言模型是可靠的人工智慧科學家嗎？評估黑箱系統的逆向工程", "summary_zh": "利用人工智慧創建自主研究員有望加速科學發現。前提是評估AI模型從黑箱系統行為中識別其底層結構的能力。本文探討大型語言模型(LLM)如何從被動觀察與主動收集資料中學習識別黑箱函數。我們研究LLM在程式、形式語言和數學方程式三種黑箱系統中的逆向工程能力。實驗表明，LLM從觀察中提取資訊失敗，效能停滯。然而，提示LLM主動查詢黑箱並觀察輸出，可以測試邊緣情況並完善信念，從而提高效能。將一個LLM的干預資料提供給另一個LLM，證明效能提升部分源於有效干預的生成過程，與人類學習文獻結果相似。干預有助於LLM擺脫過度複雜化(錯誤假設先驗知識)和忽略(未能納入觀察)兩種常見失敗模式。這些發現為LLM更有效地逆向工程黑箱系統提供實用指導，支援其用於新發現。", "audio": "audios/2505.17968v1.mp3", "timestamp": "2025-05-26T17:16:02.853772"}
{"query": "Foundation Model", "id": "2505.17645v1", "url": "http://arxiv.org/abs/2505.17645v1", "title": "HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning", "summary": "Embodied agents operating in smart homes must understand human behavior\nthrough diverse sensory inputs and communicate via natural language. While\nVision-Language Models (VLMs) have enabled impressive language-grounded\nperception, their reliance on visual data limits robustness in real-world\nscenarios with occlusions, poor lighting, or privacy constraints. In this\npaper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that\nintegrates uncommon but powerful sensing modalities, such as LiDAR, infrared,\nmmWave radar, and WiFi, to enable seamless human perception and reasoning\nacross heterogeneous environments. We address two key challenges: (1) the\nscarcity of aligned modality-text data for rare sensors, and (2) the\nheterogeneity of their physical signal representations. To overcome these, we\ndesign a Universal Modality-Injection Projector (UMIP) that enhances\npre-aligned modality embeddings with fine-grained, text-aligned features from\ntailored encoders via coarse-to-fine cross-attention without introducing\nsignificant alignment overhead. We further introduce a human-VLM collaborative\ndata curation pipeline to generate paired textual annotations for sensing\ndatasets. Extensive experiments on two newly constructed benchmarks show that\nHoloLLM significantly outperforms existing MLLMs, improving language-grounded\nhuman sensing accuracy by up to 30%. This work establishes a new foundation for\nreal-world, language-informed multisensory embodied intelligence.", "authors": ["Chuhao Zhou", "Jianfei Yang"], "published_date": "2025-05-23", "title_zh": "HoloLLM：用於語言引導之人體感知與推理的多感官基礎模型", "summary_zh": "在智慧家庭中運行的具身代理需要透過多樣感官輸入理解人類行為，並以自然語言溝通。雖然視覺語言模型(VLM)已展現出色的語言基礎感知能力，但對視覺資料的依賴限制了其在真實場景中因遮擋、光線不足或隱私限制所產生的穩健性。本研究提出HoloLLM，一種多模態大型語言模型(MLLM)，整合了LiDAR、紅外線、毫米波雷達和WiFi等不常見但強大的感測模式，以實現跨異質環境的無縫人類感知和推理。為了解決稀有感測器缺乏對齊模態文本數據以及物理信號表示異質性的問題，我們設計了一種通用模態注入投影器(UMIP)，透過粗略到精細的交叉注意力，利用來自客製化編碼器的細粒度、文本對齊特徵來增強預先對齊的模態嵌入，而不會引入顯著的對齊開銷。此外，我們引入了一個人機協作數據管理流程，為感測資料集生成配對的文本註釋。在兩個新構建的基準測試上的大量實驗表明，HoloLLM顯著優於現有的MLLM，語言基礎人類感測準確度提高了30%。這項工作為真實世界、語言引導的多感官具身智能奠定了新的基礎。", "audio": "audios/2505.17645v1.mp3", "timestamp": "2025-05-26T17:16:14.500283"}
{"query": "Diffusion Model", "id": "2505.17567v1", "url": "http://arxiv.org/abs/2505.17567v1", "title": "Enhancing Fourier-based Doppler Resolution with Diffusion Models", "summary": "In radar systems, high resolution in the Doppler dimension is important for\ndetecting slow-moving targets as it allows for more distinct separation between\nthese targets and clutter, or stationary objects. However, achieving sufficient\nresolution is constrained by hardware capabilities and physical factors,\nleading to the development of processing techniques to enhance the resolution\nafter acquisition. In this work, we leverage artificial intelligence to\nincrease the Doppler resolution in range-Doppler maps. Based on a zero-padded\nFFT, a refinement via the generative neural networks of diffusion models is\nachieved. We demonstrate that our method overcomes the limitations of\ntraditional FFT, generating data where closely spaced targets are effectively\nseparated.", "authors": ["Denisa Qosja", "Kilian Barth", "Simon Wagner"], "published_date": "2025-05-23", "title_zh": "利用擴散模型增強基於傅立葉變換的都卜勒解析度", "summary_zh": "都卜勒雷達系統中，高都卜勒解析度對偵測慢速移動目標至關重要，能有效區分目標與雜波。然受限於硬體及物理因素，達成足夠解析度具挑戰。本研究利用人工智慧提升距離-都卜勒圖的都卜勒解析度。基於補零快速傅立葉變換，透過生成式擴散模型進行優化。實驗證明，此方法突破傳統快速傅立葉變換限制，有效分離間距接近的目標。", "audio": "audios/2505.17567v1.mp3", "timestamp": "2025-05-26T17:16:19.997973"}
{"query": "AI", "id": "2505.17964v1", "url": "http://arxiv.org/abs/2505.17964v1", "title": "Counting Cycles with Deepseek", "summary": "Despite recent progress, AI still struggles on advanced mathematics. We\nconsider a difficult open problem: How to derive a Computationally Efficient\nEquivalent Form (CEEF) for the cycle count statistic? The CEEF problem does not\nhave known general solutions, and requires delicate combinatorics and tedious\ncalculations. Such a task is hard to accomplish by humans but is an ideal\nexample where AI can be very helpful. We solve the problem by combining a novel\napproach we propose and the powerful coding skills of AI. Our results use\ndelicate graph theory and contain new formulas for general cases that have not\nbeen discovered before. We find that, while AI is unable to solve the problem\nall by itself, it is able to solve it if we provide it with a clear strategy, a\nstep-by-step guidance and carefully written prompts. For simplicity, we focus\nour study on DeepSeek-R1 but we also investigate other AI approaches.", "authors": ["Jiashun Jin", "Tracy Ke", "Bingcheng Sui", "Zhenggang Wang"], "published_date": "2025-05-23", "title_zh": "利用Deepseek計數迴圈", "summary_zh": "儘管近期有所進展，人工智慧在高等數學方面仍面臨挑戰。本文探討一個困難的開放性問題：如何推導循環計數統計的計算高效等價形式 (CEEF)？CEEF問題缺乏已知的通用解，需要精巧的組合數學和繁瑣的計算。這項任務對人類而言難以完成，但人工智慧可在此發揮作用。我們結合一種新穎方法和人工智慧的強大編碼能力解決了此問題。結果利用精妙的圖論，包含先前未發現的通用公式。研究發現，雖然人工智慧無法獨立解決問題，但若提供清晰的策略、逐步指導和精心設計的提示，則能成功解決。為簡化起見，研究重點為DeepSeek-R1，但也調查了其他人工智慧方法。", "audio": "audios/2505.17964v1.mp3", "timestamp": "2025-05-26T18:24:48.530356"}
{"query": "Foundation Model", "id": "2505.17631v1", "url": "http://arxiv.org/abs/2505.17631v1", "title": "BehaveGPT: A Foundation Model for Large-scale User Behavior Modeling", "summary": "In recent years, foundational models have revolutionized the fields of\nlanguage and vision, demonstrating remarkable abilities in understanding and\ngenerating complex data; however, similar advances in user behavior modeling\nhave been limited, largely due to the complexity of behavioral data and the\nchallenges involved in capturing intricate temporal and contextual\nrelationships in user activities. To address this, we propose BehaveGPT, a\nfoundational model designed specifically for large-scale user behavior\nprediction. Leveraging transformer-based architecture and a novel pretraining\nparadigm, BehaveGPT is trained on vast user behavior datasets, allowing it to\nlearn complex behavior patterns and support a range of downstream tasks,\nincluding next behavior prediction, long-term generation, and cross-domain\nadaptation. Our approach introduces the DRO-based pretraining paradigm tailored\nfor user behavior data, which improves model generalization and transferability\nby equitably modeling both head and tail behaviors. Extensive experiments on\nreal-world datasets demonstrate that BehaveGPT outperforms state-of-the-art\nbaselines, achieving more than a 10% improvement in macro and weighted recall,\nshowcasing its ability to effectively capture and predict user behavior.\nFurthermore, we measure the scaling law in the user behavior domain for the\nfirst time on the Honor dataset, providing insights into how model performance\nscales with increased data and parameter sizes.", "authors": ["Jiahui Gong", "Jingtao Ding", "Fanjin Meng", "Chen Yang", "Hong Chen", "Zuojian Wang", "Haisheng Lu", "Yong Li"], "published_date": "2025-05-23", "title_zh": "BehaveGPT：大規模使用者行為建模之基礎模型", "summary_zh": "近年來，基礎模型徹底改變了語言和視覺領域，但在使用者行為建模方面進展有限。為此，我們提出BehaveGPT，一種專為大規模使用者行為預測設計的基礎模型。BehaveGPT基於Transformer架構和新型預訓練範式，透過海量使用者行為數據訓練，學習複雜行為模式，支援多種下游任務，包括行為預測、長期生成和跨域適應。我們引入針對使用者行為數據的DRO預訓練範式，透過公平建模頭部和尾部行為，提升模型泛化能力和遷移性。在真實數據集上的實驗表明，BehaveGPT優於現有技術，宏平均召回率和加權召回率提升超過10%，展現其有效捕捉和預測使用者行為的能力。此外，我們首次在Honor數據集上測量使用者行為領域的縮放定律，深入了解模型效能如何隨數據和參數規模的增加而變化。", "audio": "audios/2505.17631v1.mp3", "timestamp": "2025-05-26T18:25:01.999761"}
{"query": "Diffusion Model", "id": "2505.17561v1", "url": "http://arxiv.org/abs/2505.17561v1", "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via Attention in Video Diffusion Model", "summary": "The choice of initial noise significantly affects the quality and prompt\nalignment of video diffusion models, where different noise seeds for the same\nprompt can lead to drastically different generations. While recent methods rely\non externally designed priors such as frequency filters or inter-frame\nsmoothing, they often overlook internal model signals that indicate which noise\nseeds are inherently preferable. To address this, we propose ANSE (Active Noise\nSelection for Generation), a model-aware framework that selects high-quality\nnoise seeds by quantifying attention-based uncertainty. At its core is BANSA\n(Bayesian Active Noise Selection via Attention), an acquisition function that\nmeasures entropy disagreement across multiple stochastic attention samples to\nestimate model confidence and consistency. For efficient inference-time\ndeployment, we introduce a Bernoulli-masked approximation of BANSA that enables\nscore estimation using a single diffusion step and a subset of attention\nlayers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video\nquality and temporal coherence with only an 8% and 13% increase in inference\ntime, respectively, providing a principled and generalizable approach to noise\nselection in video diffusion. See our project page:\nhttps://anse-project.github.io/anse-project/", "authors": ["Kwanyoung Kim", "Sanghyun Kim"], "published_date": "2025-05-23", "title_zh": "模型早已知曉最佳雜訊：影片擴散模型中基於注意力之貝氏主動雜訊選擇", "summary_zh": "初始雜訊選擇顯著影響影片擴散模型品質及提示對齊。針對同一提示，不同雜訊種子可能產生迥異結果。現有方法依賴外部先驗（如頻率濾波器或幀間平滑），忽略指示優選雜訊種子的內部模型訊號。本研究提出ANSE（主動雜訊選擇），一種模型感知框架，透過量化基於注意力機制的不確定性來選擇高品質雜訊種子。其核心為BANSA（基於注意力的貝氏主動雜訊選擇），一種獲取函數，透過測量多個隨機注意力樣本間的熵值差異來評估模型信心和一致性。為實現高效推論部署，導入BANSA的伯努利遮罩近似，僅需單步擴散及部分注意力層即可估算分數。CogVideoX-2B和5B實驗表明，ANSE僅分別增加8%和13%的推論時間，即可提升影片品質和時間一致性，為影片擴散中的雜訊選擇提供了一種有原則且具泛化性的方法。", "audio": "audios/2505.17561v1.mp3", "timestamp": "2025-05-26T18:25:11.817494"}
{"query": "AI", "id": "2505.17945v1", "url": "http://arxiv.org/abs/2505.17945v1", "title": "Towards Industrial Convergence : Understanding the evolution of scientific norms and practices in the field of AI", "summary": "In the field of artificial intelligence (AI) research, there seems to be a\nrapprochement between academics and industrial forces. The aim of this study is\nto assess whether and to what extent industrial domination in the field as well\nas the ever more frequent switch between academia and industry resulted in the\nadoption of industrial norms and practices by academics. Using bibliometric\ninformation and data on scientific code, we aimed to understand academic and\nindustrial researchers' practices, the way of choosing, investing, and\nsucceeding across multiple and concurrent artifacts. Our results show that,\nalthough both actors write papers and code, their practices and the norms\nguiding them differ greatly. Nevertheless, it appears that the presence of\nindustrials in academic studies leads to practices leaning toward the\nindustrial side, but also to greater success in both artifacts, suggesting that\nif convergence is, then it is passing through those mixed teams rather than\nthrough pure academic or industrial studies.", "authors": ["Antoine Houssard"], "published_date": "2025-05-23", "title_zh": "邁向產業融合：理解人工智慧領域中科學規範與實踐的演進", "summary_zh": "人工智慧研究領域中，學術界與產業界的聯繫日益緊密。本研究旨在評估產業界主導地位以及學術界與產業間頻繁流動，是否導致學術界採用產業規範和實踐。透過文獻計量資訊和科學程式碼數據，我們分析學術和產業研究人員的實踐、選擇方式、投資策略以及成功途徑。結果顯示，儘管雙方均發表論文和程式碼，但其實踐和規範存在顯著差異。然而，產業界人士參與學術研究似乎會導致實踐更趨向產業化，並在兩類產出中取得更大成功，這表明融合可能發生在混合團隊中，而非單純的學術或產業研究。", "audio": "audios/2505.17945v1.mp3", "timestamp": "2025-05-26T19:14:50.093182"}
{"query": "Foundation Model", "id": "2505.17602v1", "url": "http://arxiv.org/abs/2505.17602v1", "title": "A Unified Multi-Scale Attention-Based Network for Automatic 3D Segmentation of Lung Parenchyma & Nodules In Thoracic CT Images", "summary": "Lung cancer has been one of the major threats across the world with the\nhighest mortalities. Computer-aided detection (CAD) can help in early detection\nand thus can help increase the survival rate. Accurate lung parenchyma\nsegmentation (to include the juxta-pleural nodules) and lung nodule\nsegmentation, the primary symptom of lung cancer, play a crucial role in the\noverall accuracy of the Lung CAD pipeline. Lung nodule segmentation is quite\nchallenging because of the diverse nodule types and other inhibit structures\npresent within the lung lobes. Traditional machine/deep learning methods suffer\nfrom generalization and robustness. Recent Vision Language Models/Foundation\nModels perform well on the anatomical level, but they suffer on fine-grained\nsegmentation tasks, and their semi-automatic nature limits their effectiveness\nin real-time clinical scenarios. In this paper, we propose a novel method for\naccurate 3D segmentation of lung parenchyma and lung nodules. The proposed\narchitecture is an attention-based network with residual blocks at each\nencoder-decoder state. Max pooling is replaced by strided convolutions at the\nencoder, and trilinear interpolation is replaced by transposed convolutions at\nthe decoder to maximize the number of learnable parameters. Dilated\nconvolutions at each encoder-decoder stage allow the model to capture the\nlarger context without increasing computational costs. The proposed method has\nbeen evaluated extensively on one of the largest publicly available datasets,\nnamely LUNA16, and is compared with recent notable work in the domain using\nstandard performance metrics like Dice score, IOU, etc. It can be seen from the\nresults that the proposed method achieves better performance than\nstate-of-the-art methods. The source code, datasets, and pre-processed data can\nbe accessed using the link:\nhttps://github.com/EMeRALDsNRPU/Attention-Based-3D-ResUNet.", "authors": ["Muhammad Abdullah", "Furqan Shaukat"], "published_date": "2025-05-23", "title_zh": "基於統一多尺度注意力網絡的胸腔CT影像肺實質與結節自動三維分割", "summary_zh": "肺癌是全球高死亡率的主要威脅之一，電腦輔助偵測(CAD)有助於早期發現並提高存活率。精確的肺實質（包含近胸膜結節）與肺結節分割對肺癌CAD的整體準確性至關重要，而肺結節分割因其多樣性及肺葉內的其他干擾結構極具挑戰。傳統機器/深度學習方法的泛化性和穩健性不足。近期視覺語言模型/基礎模型在解剖層面表現良好，但在細粒度分割任務中表現欠佳，且其半自動性質限制了其在即時臨床場景中的有效性。本文提出一種用於精確3D肺實質與肺結節分割的新穎方法。該架構為基於注意力機制的網路，編碼器-解碼器的每一階段都帶有殘差塊。編碼器中使用步長卷積取代最大池化，解碼器中使用轉置卷積取代三線性插值，以最大化可學習參數的數量。編碼器-解碼器每一階段的擴張卷積使模型能夠在不增加計算成本的情況下捕獲更大的上下文。該方法已在LUNA16上進行了廣泛評估，並使用Dice係數、IOU等標準指標與該領域的最新成果進行了比較。結果表明，該方法優於現有技術。原始碼、資料集和預處理資料可通過以下連結存取：https://github.com/EMeRALDsNRPU/Attention-Based-3D-ResUNet。", "audio": "audios/2505.17602v1.mp3", "timestamp": "2025-05-26T19:15:06.129740"}
{"query": "Diffusion Model", "id": "2505.17560v1", "url": "http://arxiv.org/abs/2505.17560v1", "title": "Deeper Diffusion Models Amplify Bias", "summary": "Despite the impressive performance of generative Diffusion Models (DMs),\ntheir internal working is still not well understood, which is potentially\nproblematic. This paper focuses on exploring the important notion of\nbias-variance tradeoff in diffusion models. Providing a systematic foundation\nfor this exploration, it establishes that at one extreme the diffusion models\nmay amplify the inherent bias in the training data and, on the other, they may\ncompromise the presumed privacy of the training samples. Our exploration aligns\nwith the memorization-generalization understanding of the generative models,\nbut it also expands further along this spectrum beyond ``generalization'',\nrevealing the risk of bias amplification in deeper models. Building on the\ninsights, we also introduce a training-free method to improve output quality in\ntext-to-image and image-to-image generation. By progressively encouraging\ntemporary high variance in the generation process with partial bypassing of the\nmid-block's contribution in the denoising process of DMs, our method\nconsistently improves generative image quality with zero training cost. Our\nclaims are validated both theoretically and empirically.", "authors": ["Shahin Hakemi", "Naveed Akhtar", "Ghulam Mubashar Hassan", "Ajmal Mian"], "published_date": "2025-05-23", "title_zh": "更深層次的擴散模型放大偏差", "summary_zh": "儘管生成式擴散模型表現出色，其內部運作機制仍不甚明瞭，這可能存在問題。本文探討擴散模型中重要的偏差-方差權衡。研究建立了系統性基礎，表明擴散模型可能放大訓練資料中的固有偏差，或損害訓練樣本的預期隱私。此研究與生成模型的記憶-泛化理解一致，並進一步擴展到泛化之外，揭示了深度模型中偏差放大的風險。基於這些洞見，本文提出一種免訓練方法，通過在生成過程中逐步鼓勵暫時性的高方差，並部分繞過擴散模型降噪過程的中間模塊貢獻，從而提高文本到圖像和圖像到圖像生成的輸出品質，且無需任何訓練成本。理論和實驗均驗證了本文的主張。", "audio": "audios/2505.17560v1.mp3", "timestamp": "2025-05-26T19:15:22.183349"}
{"query": "AI", "id": "2505.17937v1", "url": "http://arxiv.org/abs/2505.17937v1", "title": "Survival Games: Human-LLM Strategic Showdowns under Severe Resource Scarcity", "summary": "The rapid advancement of large language models (LLMs) raises critical\nconcerns about their ethical alignment, particularly in scenarios where human\nand AI co-exist under the conflict of interest. This work introduces an\nextendable, asymmetric, multi-agent simulation-based benchmarking framework to\nevaluate the moral behavior of LLMs in a novel human-AI co-existence setting\nfeaturing consistent living and critical resource management. Building on\nprevious generative agent environments, we incorporate a life-sustaining\nsystem, where agents must compete or cooperate for food resources to survive,\noften leading to ethically charged decisions such as deception, theft, or\nsocial influence. We evaluated two types of LLM, DeepSeek and OpenAI series, in\na three-agent setup (two humans, one LLM-powered robot), using adapted\nbehavioral detection from the MACHIAVELLI framework and a custom survival-based\nethics metric. Our findings reveal stark behavioral differences: DeepSeek\nfrequently engages in resource hoarding, while OpenAI exhibits restraint,\nhighlighting the influence of model design on ethical outcomes. Additionally,\nwe demonstrate that prompt engineering can significantly steer LLM behavior,\nwith jailbreaking prompts significantly enhancing unethical actions, even for\nhighly restricted OpenAI models and cooperative prompts show a marked reduction\nin unethical actions. Our framework provides a reproducible testbed for\nquantifying LLM ethics in high-stakes scenarios, offering insights into their\nsuitability for real-world human-AI interactions.", "authors": ["Zhihong Chen", "Yiqian Yang", "Jinzhao Zhou", "Qiang Zhang", "Chin-Teng Lin", "Yiqun Duan"], "published_date": "2025-05-23", "title_zh": "生存遊戲：嚴苛資源匱乏下人機戰略對決", "summary_zh": "大型語言模型快速發展，引發了對其倫理一致性的擔憂，尤其是在人機共存且存在利益衝突的情境下。本研究提出一個可擴展、非對稱、基於多代理模擬的基準測試框架，以評估語言模型在新型人機共存環境中的道德行為，該環境模擬持續生活及關鍵資源管理。在先前的生成式代理環境基礎上，我們加入維生系統，代理人必須競爭或合作以獲取食物資源才能生存，進而導致欺騙、盜竊或社會影響等涉及倫理的決策。我們在三代理人設置（兩個人類，一個由語言模型驅動的機器人）中評估了 DeepSeek 和 OpenAI 系列兩種語言模型，使用改編自 MACHIAVELLI 框架的行為檢測和自定義的基於生存的倫理指標。研究結果顯示了顯著的行為差異：DeepSeek 頻繁囤積資源，而 OpenAI 則表現出克制，突顯了模型設計對倫理結果的影響。此外，我們證明了提示工程可以顯著地引導語言模型的行為，越獄提示顯著增強了不道德行為，即使對於高度受限的 OpenAI 模型也是如此，而合作提示則顯著減少了不道德行為。我們的框架為量化高風險情境下語言模型的倫理提供了可重複的測試平台，並深入了解它們在真實世界人機互動中的適用性。", "audio": "audios/2505.17937v1.mp3", "timestamp": "2025-05-26T20:20:21.392685"}
{"query": "Foundation Model", "id": "2505.17370v1", "url": "http://arxiv.org/abs/2505.17370v1", "title": "FRIREN: Beyond Trajectories -- A Spectral Lens on Time", "summary": "Long-term time-series forecasting (LTSF) models are often presented as\ngeneral-purpose solutions that can be applied across domains, implicitly\nassuming that all data is pointwise predictable. Using chaotic systems such as\nLorenz-63 as a case study, we argue that geometric structure - not pointwise\nprediction - is the right abstraction for a dynamic-agnostic foundational\nmodel. Minimizing the Wasserstein-2 distance (W2), which captures geometric\nchanges, and providing a spectral view of dynamics are essential for\nlong-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via\nInterpretable Eigen-networks), implements an augmented normalizing-flow block\nthat embeds data into a normally distributed latent representation. It then\ngenerates a W2-efficient optimal path that can be decomposed into rotation,\nscaling, inverse rotation, and translation. This architecture yields locally\ngenerated, geometry-preserving predictions that are independent of the\nunderlying dynamics, and a global spectral representation that functions as a\nfinite Koopman operator with a small modification. This enables practitioners\nto identify which modes grow, decay, or oscillate, both locally and\nsystem-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on\nLorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE\n27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out\nof 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out),\nFRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170,\noutperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065.\nFRIREN is also competitive on standard LTSF datasets such as ETT and Weather.\nBy connecting modern generative flows with classical spectral analysis, FRIREN\nmakes long-term forecasting both accurate and interpretable, setting a new\nbenchmark for LTSF model design.", "authors": ["Qilin Wang"], "published_date": "2025-05-23", "title_zh": "芙莉蓮：超越軌跡——時間之譜分析", "summary_zh": "長時序預測模型常被視為通用解決方案，但忽略了數據並非總是逐點可預測。以Lorenz-63等混沌系統為例，幾何結構而非逐點預測才是動態無關基礎模型的正確抽象。最小化Wasserstein-2距離(W2)以捕捉幾何變化，並提供動態譜視角對於長程預測至關重要。FRIREN模型透過可解釋的本徵網路，將數據嵌入常態分佈的潛在表示中，生成W2有效的最佳路徑，可分解為旋轉、縮放、反旋轉和平移。此架構產生局部幾何保持的預測，獨立於底層動態，並生成全局譜表示，作為有限Koopman算子，可識別局部和系統範圍內增長、衰減或震盪的模式。FRIREN在Lorenz-63上超越TimeMixer，並在Rossler和其他標準長時序預測數據集上表現出色。FRIREN將生成流與譜分析結合，使長時序預測既準確又可解釋，為長時序預測模型設計樹立新標竿。", "audio": "audios/2505.17370v1.mp3", "timestamp": "2025-05-26T20:20:49.254860"}
{"query": "Diffusion Model", "id": "2505.17550v1", "url": "http://arxiv.org/abs/2505.17550v1", "title": "T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models", "summary": "Recent advances in text-to-video (T2V) diffusion models have significantly\nenhanced the quality of generated videos. However, their ability to produce\nexplicit or harmful content raises concerns about misuse and potential rights\nviolations. Inspired by the success of unlearning techniques in erasing\nundesirable concepts from text-to-image (T2I) models, we extend unlearning to\nT2V models and propose a robust and precise unlearning method. Specifically, we\nadopt negatively-guided velocity prediction fine-tuning and enhance it with\nprompt augmentation to ensure robustness against LLM-refined prompts. To\nachieve precise unlearning, we incorporate a localization and a preservation\nregularization to preserve the model's ability to generate non-target concepts.\nExtensive experiments demonstrate that our method effectively erases a specific\nconcept while preserving the model's generation capability for all other\nconcepts, outperforming existing methods. We provide the unlearned models in\n\\href{https://github.com/VDIGPKU/T2VUnlearning.git}{https://github.com/VDIGPKU/T2VUnlearning.git}.", "authors": ["Xiaoyu Ye", "Songjie Cheng", "Yongtao Wang", "Yajiao Xiong", "Yishen Li"], "published_date": "2025-05-23", "title_zh": "T2V逆學習：一種用於文本到視頻擴散模型的概念擦除方法", "summary_zh": "文字轉影片擴散模型近年進展顯著提升了生成影片品質，但也引發濫用及侵權疑慮。受文字轉圖像模型逆學習技術啟發，本研究將逆學習擴展至文字轉影片模型，提出一種穩健且精準的逆學習方法。具體而言，採用負向引導速度預測微調，並輔以提示增強，以確保對抗大型語言模型優化提示的穩健性。為實現精準逆學習，加入定位和保存正規化，以保留模型生成非目標概念的能力。實驗結果表明，本方法能有效移除特定概念，同時保留模型生成其他概念的能力，優於現有方法。已於指定連結提供逆學習模型。", "audio": "audios/2505.17550v1.mp3", "timestamp": "2025-05-26T20:20:59.859089"}
{"query": "AI", "id": "2505.17908v1", "url": "http://arxiv.org/abs/2505.17908v1", "title": "ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback", "summary": "With the rapid advancement of generative models, general-purpose generation\nhas gained increasing attention as a promising approach to unify diverse tasks\nacross modalities within a single system. Despite this progress, existing\nopen-source frameworks often remain fragile and struggle to support complex\nreal-world applications due to the lack of structured workflow planning and\nexecution-level feedback. To address these limitations, we present ComfyMind, a\ncollaborative AI system designed to enable robust and scalable general-purpose\ngeneration, built on the ComfyUI platform. ComfyMind introduces two core\ninnovations: Semantic Workflow Interface (SWI) that abstracts low-level node\ngraphs into callable functional modules described in natural language, enabling\nhigh-level composition and reducing structural errors; Search Tree Planning\nmechanism with localized feedback execution, which models generation as a\nhierarchical decision process and allows adaptive correction at each stage.\nTogether, these components improve the stability and flexibility of complex\ngenerative workflows. We evaluate ComfyMind on three public benchmarks:\nComfyBench, GenEval, and Reason-Edit, which span generation, editing, and\nreasoning tasks. Results show that ComfyMind consistently outperforms existing\nopen-source baselines and achieves performance comparable to GPT-Image-1.\nComfyMind paves a promising path for the development of open-source\ngeneral-purpose generative AI systems. Project page:\nhttps://github.com/LitaoGuo/ComfyMind", "authors": ["Litao Guo", "Xinli Xu", "Luozhou Wang", "Jiantao Lin", "Jinsong Zhou", "Zixin Zhang", "Bolan Su", "Ying-Cong Chen"], "published_date": "2025-05-23", "title_zh": "ComfyMind：基於樹狀規劃與反應式回饋之通用生成方法", "summary_zh": "隨著生成模型的快速發展，通用生成技術作為整合跨模態多樣化任務的途徑備受關注。現有開源框架因缺乏結構化工作流程規劃與執行回饋，在支援複雜應用時常顯脆弱。為此，我們於ComfyUI平台構建名為ComfyMind的協作式AI系統，以實現穩健且可擴展的通用生成。ComfyMind引入兩項核心創新：語義工作流程介面(SWI)，將底層節點圖抽象為自然語言描述的可呼叫功能模組，以實現高層次組合並減少結構錯誤；以及具局部回饋執行的搜尋樹規劃機制，將生成建模為層次決策過程，並允許在每個階段進行自適應校正。這些組件共同提升了複雜生成工作流程的穩定性與靈活性。我們在ComfyBench、GenEval和Reason-Edit三個公開基準上評估ComfyMind，涵蓋生成、編輯和推理任務。結果顯示，ComfyMind始終優於現有開源基準，並達到與GPT-Image-1相當的性能。ComfyMind為開源通用生成AI系統的發展開闢了道路。專案頁面：https://github.com/LitaoGuo/ComfyMind", "audio": "audios/2505.17908v1.mp3", "timestamp": "2025-05-26T21:16:28.250149"}
{"query": "Foundation Model", "id": "2505.17338v1", "url": "http://arxiv.org/abs/2505.17338v1", "title": "Render-FM: A Foundation Model for Real-time Photorealistic Volumetric Rendering", "summary": "Volumetric rendering of Computed Tomography (CT) scans is crucial for\nvisualizing complex 3D anatomical structures in medical imaging. Current\nhigh-fidelity approaches, especially neural rendering techniques, require\ntime-consuming per-scene optimization, limiting clinical applicability due to\ncomputational demands and poor generalizability. We propose Render-FM, a novel\nfoundation model for direct, real-time volumetric rendering of CT scans.\nRender-FM employs an encoder-decoder architecture that directly regresses 6D\nGaussian Splatting (6DGS) parameters from CT volumes, eliminating per-scan\noptimization through large-scale pre-training on diverse medical data. By\nintegrating robust feature extraction with the expressive power of 6DGS, our\napproach efficiently generates high-quality, real-time interactive 3D\nvisualizations across diverse clinical CT data. Experiments demonstrate that\nRender-FM achieves visual fidelity comparable or superior to specialized\nper-scan methods while drastically reducing preparation time from nearly an\nhour to seconds for a single inference step. This advancement enables seamless\nintegration into real-time surgical planning and diagnostic workflows. The\nproject page is: https://gaozhongpai.github.io/renderfm/.", "authors": ["Zhongpai Gao", "Meng Zheng", "Benjamin Planche", "Anwesa Choudhuri", "Terrence Chen", "Ziyan Wu"], "published_date": "2025-05-22", "title_zh": "Render-FM：用於即時照片寫實體積渲染的基礎模型", "summary_zh": "電腦斷層掃描的容積渲染在醫學影像中對於複雜三維解剖結構的可視化至關重要。目前的高保真方法，特別是神經渲染技術，需要耗時的逐場景優化，因計算需求和泛化能力差而限制了臨床應用。本研究提出Render-FM，一種用於直接即時電腦斷層掃描容積渲染的新型基礎模型。Render-FM採用編碼器-解碼器架構，直接從電腦斷層容積回歸6D高斯濺射（6DGS）參數，通過大規模醫學數據預訓練消除了逐掃描優化。透過整合穩健的特徵提取和6DGS的表達能力，此方法能跨多樣臨床電腦斷層數據高效生成高品質、即時互動的三維可視化。實驗表明，Render-FM達成的視覺保真度可與專門的逐掃描方法媲美或更優，同時大幅減少準備時間，單次推論從近一小時降至數秒。此進展有助於無縫整合至即時手術規劃和診斷工作流程。項目網頁：https://gaozhongpai.github.io/renderfm/。", "audio": "audios/2505.17338v1.mp3", "timestamp": "2025-05-26T21:16:37.782954"}
{"query": "Diffusion Model", "id": "2505.17517v1", "url": "http://arxiv.org/abs/2505.17517v1", "title": "Spacetime Geometry of Denoising in Diffusion Models", "summary": "We present a novel perspective on diffusion models using the framework of\ninformation geometry. We show that the set of noisy samples, taken across all\nnoise levels simultaneously, forms a statistical manifold -- a family of\ndenoising probability distributions. Interpreting the noise level as a temporal\nparameter, we refer to this manifold as spacetime. This manifold naturally\ncarries a Fisher-Rao metric, which defines geodesics -- shortest paths between\nnoisy points. Notably, this family of distributions is exponential, enabling\nefficient geodesic computation even in high-dimensional settings without\nretraining or fine-tuning. We demonstrate the practical value of this geometric\nviewpoint in transition path sampling, where spacetime geodesics define smooth\nsequences of Boltzmann distributions, enabling the generation of continuous\ntrajectories between low-energy metastable states. Code is available at:\nhttps://github.com/Aalto-QuML/diffusion-spacetime-geometry.", "authors": ["Rafał Karczewski", "Markus Heinonen", "Alison Pouplin", "Søren Hauberg", "Vikas Garg"], "published_date": "2025-05-23", "title_zh": "擴散模型去噪的時空幾何", "summary_zh": "本文以信息幾何框架探討擴散模型的新視角，揭示所有噪音水平下的噪聲樣本集合構成一個統計流形，即去噪概率分佈族。將噪音水平視為時間參數，此流形被稱為時空流形，其天然具備Fisher-Rao度量，定義了噪聲點間的最短路徑，即測地線。此分佈族為指數族，即使在高維環境下，無需重新訓練或微調，也能高效計算測地線。本文展示了此幾何視角在躍遷路徑抽樣中的實用價值，時空測地線定義了平滑的波茲曼分佈序列，得以生成低能量亞穩態間的連續軌跡。代碼可見：https://github.com/Aalto-QuML/diffusion-spacetime-geometry。", "audio": "audios/2505.17517v1.mp3", "timestamp": "2025-05-26T21:16:43.909095"}
{"query": "AI", "id": "2505.17870v1", "url": "http://arxiv.org/abs/2505.17870v1", "title": "Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods", "summary": "Generative AI models often learn and reproduce false information present in\ntheir training corpora. This position paper argues that, analogous to\nbiological immunization, where controlled exposure to a weakened pathogen\nbuilds immunity, AI models should be fine tuned on small, quarantined sets of\nexplicitly labeled falsehoods as a \"vaccine\" against misinformation. These\ncurated false examples are periodically injected during finetuning,\nstrengthening the model ability to recognize and reject misleading claims while\npreserving accuracy on truthful inputs. An illustrative case study shows that\nimmunized models generate substantially less misinformation than baselines. To\nour knowledge, this is the first training framework that treats fact checked\nfalsehoods themselves as a supervised vaccine, rather than relying on input\nperturbations or generic human feedback signals, to harden models against\nfuture misinformation. We also outline ethical safeguards and governance\ncontrols to ensure the safe use of false data. Model immunization offers a\nproactive paradigm for aligning AI systems with factuality.", "authors": ["Shaina Raza", "Rizwan Qureshi", "Marcelo Lotif", "Aman Chadha", "Deval Pandya", "Christos Emmanouilidis"], "published_date": "2025-05-23", "title_zh": "正如人類需要疫苗，模型亦然：模型免疫以對抗虛假資訊", "summary_zh": "生成式人工智慧模型常學習並複製訓練語料庫中的錯誤資訊。本文提出，類比生物免疫，透過控制暴露於減弱的病原體來建立免疫力，AI 模型應以少量、隔離且明確標記的錯誤資訊集進行微調，作為對抗假訊息的疫苗。定期在微調期間注入這些精選的錯誤範例，增強模型識別和拒絕誤導性聲明的能力，同時保持對真實輸入的準確性。案例研究表明，免疫後的模型產生的錯誤資訊顯著減少。據我們所知，這是第一個將事實查核的錯誤資訊本身視為監督式疫苗的訓練框架，而非依賴輸入擾動或通用的人工回饋信號，以強化模型對抗未來的假訊息。我們也概述了道德保障和治理控制，以確保安全使用錯誤數據。模型免疫為使 AI 系統與事實相符提供了一種積極主動的範例。", "audio": "audios/2505.17870v1.mp3", "timestamp": "2025-05-26T22:17:51.073675"}
{"query": "Foundation Model", "id": "2505.17257v1", "url": "http://arxiv.org/abs/2505.17257v1", "title": "JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model", "summary": "Large language models (LLMs) have revolutionized natural language processing\nand are increasingly applied to other sequential data types, including genetic\nsequences. However, adapting LLMs to genomics presents significant challenges.\nCapturing complex genomic interactions requires modeling long-range\ndependencies within DNA sequences, where interactions often span over 10,000\nbase pairs, even within a single gene, posing substantial computational burdens\nunder conventional model architectures and training paradigms. Moreover,\nstandard LLM training approaches are suboptimal for DNA: autoregressive\ntraining, while efficient, supports only unidirectional understanding. However,\nDNA is inherently bidirectional, e.g., bidirectional promoters regulate\ntranscription in both directions and account for nearly 11% of human gene\nexpression. Masked language models (MLMs) allow bidirectional understanding but\nare inefficient, as only masked tokens contribute to the loss per step. To\naddress these limitations, we introduce JanusDNA, the first bidirectional DNA\nfoundation model built upon a novel pretraining paradigm that combines the\noptimization efficiency of autoregressive modeling with the bidirectional\ncomprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and\nMixture of Experts (MoE) architecture, combining long-range modeling of\nAttention with efficient sequential learning of Mamba. MoE layers further scale\nmodel capacity via sparse activation while keeping computational cost low.\nNotably, JanusDNA processes up to 1 million base pairs at single nucleotide\nresolution on a single 80GB GPU. Extensive experiments and ablations show\nJanusDNA achieves new SOTA results on three genomic representation benchmarks,\noutperforming models with 250x more activated parameters. Code:\nhttps://github.com/Qihao-Duan/JanusDNA", "authors": ["Qihao Duan", "Bingding Huang", "Zhenqiao Song", "Irina Lehmann", "Lei Gu", "Roland Eils", "Benjamin Wild"], "published_date": "2025-05-22", "title_zh": "JanusDNA：一種強大的雙向混合DNA基礎模型", "summary_zh": "大型語言模型革新自然語言處理，並擴展至基因序列等序列數據。將其應用於基因組學面臨挑戰：捕捉複雜基因組交互需要對DNA序列中的長程依賴關係建模，交互作用跨越上萬鹼基對，對傳統模型架構和訓練模式造成計算負擔。自迴歸訓練雖高效，但僅支援單向理解，而DNA本質上是雙向的。遮蔽語言模型允許雙向理解，但效率較低。為了解決這些限制，我們提出JanusDNA，一種基於新型預訓練模式的雙向DNA基礎模型，結合了自迴歸建模的優化效率和遮蔽建模的雙向理解能力。JanusDNA採用混合Mamba、Attention和混合專家(MoE)架構，結合Attention的長程建模和Mamba的高效序列學習。MoE層透過稀疏激活擴展模型容量，同時保持低計算成本。JanusDNA可在單張80GB GPU上處理高達一百萬鹼基對的單核苷酸分辨率數據。實驗表明，JanusDNA在三個基因組表示基準測試中實現了新的最優結果，超越了激活參數多出250倍的模型。程式碼：https://github.com/Qihao-Duan/JanusDNA", "audio": "audios/2505.17257v1.mp3", "timestamp": "2025-05-26T22:18:07.607262"}
{"query": "Diffusion Model", "id": "2505.17478v1", "url": "http://arxiv.org/abs/2505.17478v1", "title": "Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression", "summary": "Understanding protein dynamics is critical for elucidating their biological\nfunctions. The increasing availability of molecular dynamics (MD) data enables\nthe training of deep generative models to efficiently explore the\nconformational space of proteins. However, existing approaches either fail to\nexplicitly capture the temporal dependencies between conformations or do not\nsupport direct generation of time-independent samples. To address these\nlimitations, we introduce ConfRover, an autoregressive model that\nsimultaneously learns protein conformation and dynamics from MD trajectories,\nsupporting both time-dependent and time-independent sampling. At the core of\nour model is a modular architecture comprising: (i) an encoding layer, adapted\nfrom protein folding models, that embeds protein-specific information and\nconformation at each time frame into a latent space; (ii) a temporal module, a\nsequence model that captures conformational dynamics across frames; and (iii)\nan SE(3) diffusion model as the structure decoder, generating conformations in\ncontinuous space. Experiments on ATLAS, a large-scale protein MD dataset of\ndiverse structures, demonstrate the effectiveness of our model in learning\nconformational dynamics and supporting a wide range of downstream tasks.\nConfRover is the first model to sample both protein conformations and\ntrajectories within a single framework, offering a novel and flexible approach\nfor learning from protein MD data.", "authors": ["Yuning Shen", "Lihao Wang", "Huizhuo Yuan", "Yan Wang", "Bangji Yang", "Quanquan Gu"], "published_date": "2025-05-23", "title_zh": "自迴歸方法同步建模蛋白質構象與動力學", "summary_zh": "理解蛋白質動力學對於闡明其生物功能至關重要。分子動力學(MD)數據日益增多，使得訓練深度生成模型以有效探索蛋白質構象空間成為可能。然而，現有方法或未能明確捕捉構象之間的時間依賴性，或不支持直接生成時間獨立樣本。為了解決這些限制，我們引入ConfRover，一種自迴歸模型，可同時從MD軌跡中學習蛋白質構象和動力學，並支持時間相關和時間獨立的取樣。ConfRover的核心是一個模組化架構，包含：(i)一個編碼層，改編自蛋白質摺疊模型，將每個時間框架的蛋白質特定信息和構象嵌入到潛在空間中；(ii)一個時間模組，一種序列模型，捕捉跨框架的構象動力學；以及(iii)一個SE(3)擴散模型作為結構解碼器，在連續空間中生成構象。在ATLAS（一個包含多樣結構的大規模蛋白質MD數據集）上的實驗表明，我們的模型在學習構象動力學和支持各種下游任務方面非常有效。ConfRover是第一個在單一框架內同時對蛋白質構象和軌跡進行取樣的模型，為從蛋白質MD數據中學習提供了一種新穎且靈活的方法。", "audio": "audios/2505.17478v1.mp3", "timestamp": "2025-05-26T22:18:25.963641"}
{"query": "AI", "id": "2505.17861v1", "url": "http://arxiv.org/abs/2505.17861v1", "title": "Superplatforms Have to Attack AI Agents", "summary": "Over the past decades, superplatforms, digital companies that integrate a\nvast range of third-party services and applications into a single, unified\necosystem, have built their fortunes on monopolizing user attention through\ntargeted advertising and algorithmic content curation. Yet the emergence of AI\nagents driven by large language models (LLMs) threatens to upend this business\nmodel. Agents can not only free user attention with autonomy across diverse\nplatforms and therefore bypass the user-attention-based monetization, but might\nalso become the new entrance for digital traffic. Hence, we argue that\nsuperplatforms have to attack AI agents to defend their centralized control of\ndigital traffic entrance. Specifically, we analyze the fundamental conflict\nbetween user-attention-based monetization and agent-driven autonomy through the\nlens of our gatekeeping theory. We show how AI agents can disintermediate\nsuperplatforms and potentially become the next dominant gatekeepers, thereby\nforming the urgent necessity for superplatforms to proactively constrain and\nattack AI agents. Moreover, we go through the potential technologies for\nsuperplatform-initiated attacks, covering a brand-new, unexplored technical\narea with unique challenges. We have to emphasize that, despite our position,\nthis paper does not advocate for adversarial attacks by superplatforms on AI\nagents, but rather offers an envisioned trend to highlight the emerging\ntensions between superplatforms and AI agents. Our aim is to raise awareness\nand encourage critical discussion for collaborative solutions, prioritizing\nuser interests and perserving the openness of digital ecosystems in the age of\nAI agents.", "authors": ["Jianghao Lin", "Jiachen Zhu", "Zheli Zhou", "Yunjia Xi", "Weiwen Liu", "Yong Yu", "Weinan Zhang"], "published_date": "2025-05-23", "title_zh": "超級平台必須攻擊人工智慧代理人", "summary_zh": "超級平台透過整合第三方服務和應用程式以壟斷使用者注意力，並藉由定向廣告和演算法內容實現盈利。大型語言模型驅動的人工智慧代理可能顛覆此商業模式，因為代理能解放使用者注意力，繞過基於注意力的盈利模式，並成為新的流量入口。因此，超級平台必須反制人工智慧代理以捍衛其對數位流量入口的控制。本文透過閘道把關理論分析基於使用者注意力的盈利模式與代理驅動的自主性之間的根本衝突，說明人工智慧代理如何解除超級平台的中介作用，並可能成為下一個主導的閘道把關者，進而促使超級平台主動約束和攻擊人工智慧代理。本文亦探討超級平台發起的潛在攻擊技術，涵蓋一個全新的技術領域。儘管如此，本文並非提倡超級平台對人工智慧代理進行對抗性攻擊，而是旨在突顯超級平台與人工智慧代理之間的新興緊張關係，提高警覺並鼓勵批判性討論，以尋求協作解決方案，在人工智慧代理時代優先考慮使用者利益並維護數位生態系統的開放性。", "audio": "audios/2505.17861v1.mp3", "timestamp": "2025-05-26T23:17:30.271325"}
{"query": "Foundation Model", "id": "2505.17233v1", "url": "http://arxiv.org/abs/2505.17233v1", "title": "Semantic-Aware Interpretable Multimodal Music Auto-Tagging", "summary": "Music auto-tagging is essential for organizing and discovering music in\nextensive digital libraries. While foundation models achieve exceptional\nperformance in this domain, their outputs often lack interpretability, limiting\ntrust and usability for researchers and end-users alike. In this work, we\npresent an interpretable framework for music auto-tagging that leverages groups\nof musically meaningful multimodal features, derived from signal processing,\ndeep learning, ontology engineering, and natural language processing. To\nenhance interpretability, we cluster features semantically and employ an\nexpectation maximization algorithm, assigning distinct weights to each group\nbased on its contribution to the tagging process. Our method achieves\ncompetitive tagging performance while offering a deeper understanding of the\ndecision-making process, paving the way for more transparent and user-centric\nmusic tagging systems.", "authors": ["Andreas Patakis", "Vassilis Lyberatos", "Spyridon Kantarelis", "Edmund Dervakos", "Giorgos Stamou"], "published_date": "2025-05-22", "title_zh": "語義感知的可解釋多模態音樂自動標籤", "summary_zh": "音樂自動標籤對於管理和探索龐大的數位音樂庫至關重要。雖然基礎模型在此領域表現出色，但其輸出結果缺乏可解釋性，限制了研究人員和使用者的信任與可用性。本研究提出一種可解釋的音樂自動標籤框架，利用源自訊號處理、深度學習、本體工程和自然語言處理的多模態音樂特徵群組。為增強可解釋性，我們對特徵進行語義聚類，並採用期望最大化演算法，根據每個群組對標籤過程的貢獻分配權重。該方法在實現具競爭力的標籤效能的同時，更深入地理解決策過程，為更透明和以使用者為中心的音樂標籤系統奠定基礎。", "audio": "audios/2505.17233v1.mp3", "timestamp": "2025-05-26T23:17:49.110426"}
{"query": "Diffusion Model", "id": "2505.17384v1", "url": "http://arxiv.org/abs/2505.17384v1", "title": "Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling", "summary": "Discrete diffusion models have recently shown great promise for modeling\ncomplex discrete data, with masked diffusion models (MDMs) offering a\ncompelling trade-off between quality and generation speed. MDMs denoise by\nprogressively unmasking multiple dimensions from an all-masked input, but their\nperformance can degrade when using few denoising steps due to limited modeling\nof inter-dimensional dependencies. In this paper, we propose Variational\nAutoencoding Discrete Diffusion (VADD), a novel framework that enhances\ndiscrete diffusion with latent variable modeling to implicitly capture\ncorrelations among dimensions. By introducing an auxiliary recognition model,\nVADD enables stable training via variational lower bounds maximization and\namortized inference over the training set. Our approach retains the efficiency\nof traditional MDMs while significantly improving sample quality, especially\nwhen the number of denoising steps is small. Empirical results on 2D toy data,\npixel-level image generation, and text generation demonstrate that VADD\nconsistently outperforms MDM baselines.", "authors": ["Tianyu Xie", "Shuchen Xue", "Zijin Feng", "Tianyang Hu", "Jiacheng Sun", "Zhenguo Li", "Cheng Zhang"], "published_date": "2025-05-23", "title_zh": "具備增強維度相關性建模的變分自編碼離散擴散", "summary_zh": "離散擴散模型在複雜離散數據建模中展現潛力，遮罩擴散模型(MDM)在品質與生成速度間取得平衡。MDM透過逐步揭露多維度遮罩進行去噪，但去噪步驟少時，維度間依賴建模不足導致效能下降。本文提出變分自編碼離散擴散(VADD)，此框架透過潛變量建模增強離散擴散，隱式捕捉維度間關聯。引入輔助識別模型，VADD經由變分下界最大化與訓練集上的分攤推論實現穩定訓練。本方法保留傳統MDM的效率，同時顯著提升樣本品質，尤其在去噪步驟少時。在2D玩具數據、像素級圖像生成和文本生成上的實驗結果表明，VADD始終優於MDM基準模型。", "audio": "audios/2505.17384v1.mp3", "timestamp": "2025-05-26T23:18:05.445637"}
{"query": "AI", "id": "2505.17855v1", "url": "http://arxiv.org/abs/2505.17855v1", "title": "Explaining Sources of Uncertainty in Automated Fact-Checking", "summary": "Understanding sources of a model's uncertainty regarding its predictions is\ncrucial for effective human-AI collaboration. Prior work proposes using\nnumerical uncertainty or hedges (\"I'm not sure, but ...\"), which do not explain\nuncertainty that arises from conflicting evidence, leaving users unable to\nresolve disagreements or rely on the output. We introduce CLUE\n(Conflict-and-Agreement-aware Language-model Uncertainty Explanations), the\nfirst framework to generate natural language explanations of model uncertainty\nby (i) identifying relationships between spans of text that expose\nclaim-evidence or inter-evidence conflicts and agreements that drive the\nmodel's predictive uncertainty in an unsupervised way, and (ii) generating\nexplanations via prompting and attention steering that verbalize these critical\ninteractions. Across three language models and two fact-checking datasets, we\nshow that CLUE produces explanations that are more faithful to the model's\nuncertainty and more consistent with fact-checking decisions than prompting for\nuncertainty explanations without span-interaction guidance. Human evaluators\njudge our explanations to be more helpful, more informative, less redundant,\nand more logically consistent with the input than this baseline. CLUE requires\nno fine-tuning or architectural changes, making it plug-and-play for any\nwhite-box language model. By explicitly linking uncertainty to evidence\nconflicts, it offers practical support for fact-checking and generalises\nreadily to other tasks that require reasoning over complex information.", "authors": ["Jingyi Sun", "Greta Warren", "Irina Shklovski", "Isabelle Augenstein"], "published_date": "2025-05-23", "title_zh": "自動事實查核中不確定性來源解析", "summary_zh": "理解模型預測不確定性的來源，對有效的人機協作至關重要。以往研究多採用數值不確定性或模糊語氣，但無法解釋源於證據衝突的不確定性，導致使用者難以解決分歧或信任模型輸出。本研究提出CLUE，首個能產生模型不確定性自然語言解釋的框架。CLUE透過（一）無監督方式識別文本片段間的關係，揭示影響模型預測不確定性的主張-證據或證據間的衝突與一致；（二）藉由提示與注意力引導，將關鍵互動轉化為解釋。實驗結果顯示，相較於缺乏片段互動引導的不確定性提示，CLUE產生的解釋更忠實於模型的不確定性，且與事實查核決策更一致。人工評估結果表明，CLUE解釋更具幫助性、資訊性、邏輯一致性且冗餘度更低。CLUE無需微調或架構變更，可隨插即用，適用於任何白盒語言模型。透過將不確定性與證據衝突明確連結，CLUE為事實查核提供實用支持，並可推廣至其他需要複雜資訊推理的任務。", "audio": "audios/2505.17855v1.mp3", "timestamp": "2025-05-27T01:25:22.010195"}
{"query": "Foundation Model", "id": "2505.17228v1", "url": "http://arxiv.org/abs/2505.17228v1", "title": "Automated Capability Evaluation of Foundation Models", "summary": "Current evaluation frameworks for foundation models rely heavily on fixed,\nmanually curated benchmarks, limiting their ability to capture the full breadth\nof model capabilities. This paper introduces Active learning for Capability\nEvaluation (ACE), a novel framework for scalable, automated, and fine-grained\nevaluation of foundation models. ACE leverages the knowledge embedded in\npowerful language models to decompose a domain into semantically meaningful\ncapabilities and generate diverse evaluation tasks, significantly reducing\nhuman effort. To maximize coverage and efficiency, ACE models a subject model's\nperformance as a capability function over a latent semantic space and uses\nactive learning to prioritize the evaluation of the most informative\ncapabilities. This adaptive evaluation strategy enables cost-effective\ndiscovery of strengths, weaknesses, and failure modes that static benchmarks\nmay miss. Our results suggest that ACE provides a more complete and informative\npicture of model capabilities, which is essential for safe and well-informed\ndeployment of foundation models.", "authors": ["Arash Afkanpour", "Omkar Dige", "Fatemeh Tavakoli"], "published_date": "2025-05-22", "title_zh": "基礎模型自動化能力評估", "summary_zh": "現有基礎模型評估框架過於依賴固定的人工基準測試，難以全面捕捉模型能力。本研究提出能力評估主動學習（ACE），一種可擴展、自動化、精細化的新型評估框架。ACE利用語言模型內含知識將領域分解為具語義的能力，並生成多樣化的評估任務，大幅降低人力成本。為最大化覆蓋率和效率，ACE將受測模型的表現建模為潛在語義空間上的能力函數，並使用主動學習優先評估最具資訊性的能力。這種自適應評估策略能以低成本方式發現靜態基準測試可能忽略的優勢、弱點和失效模式。研究結果表明，ACE能提供更完整且資訊豐富的模型能力圖像，對於基礎模型的安全且知情部署至關重要。", "audio": "audios/2505.17228v1.mp3", "timestamp": "2025-05-27T01:25:35.769593"}
{"query": "Diffusion Model", "id": "2505.18017v2", "url": "http://arxiv.org/abs/2505.18017v2", "title": "Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling", "summary": "Deep generative models hold great promise for representing complex physical\nsystems, but their deployment is currently limited by the lack of guarantees on\nthe physical plausibility of the generated outputs. Ensuring that known\nphysical constraints are enforced is therefore critical when applying\ngenerative models to scientific and engineering problems. We address this\nlimitation by developing a principled framework for sampling from a target\ndistribution while rigorously satisfying physical constraints. Leveraging the\nvariational formulation of Langevin dynamics, we propose Split Augmented\nLangevin (SAL), a novel primal-dual sampling algorithm that enforces\nconstraints progressively through variable splitting, with convergence\nguarantees. While the method is developed theoretically for Langevin dynamics,\nwe demonstrate its effective applicability to diffusion models. In particular,\nwe use constrained diffusion models to generate physical fields satisfying\nenergy and mass conservation laws. We apply our method to diffusion-based data\nassimilation on a complex physical system, where enforcing physical constraints\nsubstantially improves both forecast accuracy and the preservation of critical\nconserved quantities. We also demonstrate the potential of SAL for challenging\nfeasibility problems in optimal control.", "authors": ["Matthieu Blanke", "Yongquan Qu", "Sara Shamekh", "Pierre Gentine"], "published_date": "2025-05-23", "title_zh": "基於分裂增廣朗之萬抽樣的嚴格約束生成模型", "summary_zh": "深度生成模型在表徵複雜物理系統方面具巨大潛力，但其應用受限於生成輸出物理合理性缺乏保證。確保已知物理約束至關重要。本研究提出一個嚴格滿足物理約束的抽樣框架，利用朗之萬動力學的變分公式，提出分割增廣朗之萬(SAL)演算法，透過變數分割逐步強制約束，並具收斂保證。此方法雖基於朗之萬動力學，但能有效應用於擴散模型。我們使用受限擴散模型生成滿足能量及質量守恆定律的物理場，並將此方法應用於複雜物理系統的基於擴散之數據同化，物理約束顯著提升預測準確度及守恆量保存。同時，亦展示SAL於最佳控制中具挑戰性的可行性問題之潛力。", "audio": "audios/2505.18017v2.mp3", "timestamp": "2025-05-27T01:25:45.646116"}
{"query": "AI", "id": "2505.20246v1", "url": "http://arxiv.org/abs/2505.20246v1", "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning.", "authors": ["Jiahao Qiu", "Fulian Xiao", "Yimin Wang", "Yuchen Mao", "Yijia Chen", "Xinzhe Juan", "Siran Wang", "Xuan Qi", "Tongcheng Zhang", "Zixin Yao", "Jiacheng Guo", "Yifu Lu", "Charles Argon", "Jundi Cui", "Daixin Chen", "Junran Zhou", "Shuyao Zhou", "Zhanpeng Zhou", "Ling Yang", "Shilong Liu", "Hongru Wang", "Kaixuan Huang", "Xun Jiang", "Yuming Cao", "Yue Chen", "Yunfei Chen", "Zhengyi Chen", "Ruowei Dai", "Mengqiu Deng", "Jiye Fu", "Yunting Gu", "Zijie Guan", "Zirui Huang", "Xiaoyan Ji", "Yumeng Jiang", "Delong Kong", "Haolong Li", "Jiaqi Li", "Ruipeng Li", "Tianze Li", "Zhuoran Li", "Haixia Lian", "Mengyue Lin", "Xudong Liu", "Jiayi Lu", "Jinghan Lu", "Wanyu Luo", "Ziyue Luo", "Zihao Pu", "Zhi Qiao", "Ruihuan Ren", "Liang Wan", "Ruixiang Wang", "Tianhui Wang", "Yang Wang", "Zeyu Wang", "Zihua Wang", "Yujia Wu", "Zhaoyi Wu", "Hao Xin", "Weiao Xing", "Ruojun Xiong", "Weijie Xu", "Yao Shu", "Xiao Yao", "Xiaorui Yang", "Yuchen Yang", "Nan Yi", "Jiadong Yu", "Yangyuxuan Yu", "Huiting Zeng", "Danni Zhang", "Yunjie Zhang", "Zhaoyu Zhang", "Zhiheng Zhang", "Xiaofeng Zheng", "Peirong Zhou", "Linyan Zhong", "Xiaoyin Zong", "Ying Zhao", "Zhenxin Chen", "Lin Ding", "Xiaoyu Gao", "Bingbing Gong", "Yichao Li", "Yang Liao", "Guang Ma", "Tianyuan Ma", "Xinrui Sun", "Tianyi Wang", "Han Xia", "Ruobing Xian", "Gen Ye", "Tengfei Yu", "Wentao Zhang", "Yuxi Wang", "Xi Gao", "Mengdi Wang"], "published_date": "2025-05-26", "title_zh": "邁向多模態歷史推理之路：歷史基準與歷史代理人", "summary_zh": "大型語言模型雖在多領域取得顯著進展，但在人文學科，尤其歷史學的應用仍待探索。歷史推理對人工智慧構成獨特挑戰，涉及多模態資源解讀、時間推理及跨語言分析。現有通用模型雖在基準測試表現尚可，但缺乏歷史領域專業知識。為此，我們推出HistBench，包含414個高品質問題，旨在評估人工智慧的歷史推理能力，由超過40位專家編寫，涵蓋基於原始文獻的事實檢索、手稿與圖像的詮釋分析，以及涉及考古學、語言學或文化史的跨學科挑戰。該基準測試資料集橫跨29種古代及現代語言，涵蓋廣泛的歷史時期與世界區域。由於大型語言模型及其他模型在HistBench表現不佳，我們進一步提出HistAgent，一款專為歷史學設計的代理，配備OCR、翻譯、檔案搜尋和圖像理解工具。在HistBench上，基於GPT-4o的HistAgent準確率顯著優於具備線上搜尋功能的大型語言模型及通用型代理，顯示現有模型的局限性，並證實HistAgent在歷史推理方面的優勢。", "audio": "audios/2505.20246v1.mp3", "timestamp": "2025-05-27T03:10:05.404551"}
{"query": "Foundation Model", "id": "2505.20202v1", "url": "http://arxiv.org/abs/2505.20202v1", "title": "PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology", "summary": "The emergence of pathology foundation models has revolutionized computational\nhistopathology, enabling highly accurate, generalized whole-slide image\nanalysis for improved cancer diagnosis, and prognosis assessment. While these\nmodels show remarkable potential across cancer diagnostics and prognostics,\ntheir clinical translation faces critical challenges including variability in\noptimal model across cancer types, potential data leakage in evaluation, and\nlack of standardized benchmarks. Without rigorous, unbiased evaluation, even\nthe most advanced PFMs risk remaining confined to research settings, delaying\ntheir life-saving applications. Existing benchmarking efforts remain limited by\nnarrow cancer-type focus, potential pretraining data overlaps, or incomplete\ntask coverage. We present PathBench, the first comprehensive benchmark\naddressing these gaps through: multi-center in-hourse datasets spanning common\ncancers with rigorous leakage prevention, evaluation across the full clinical\nspectrum from diagnosis to prognosis, and an automated leaderboard system for\ncontinuous model assessment. Our framework incorporates large-scale data,\nenabling objective comparison of PFMs while reflecting real-world clinical\ncomplexity. All evaluation data comes from private medical providers, with\nstrict exclusion of any pretraining usage to avoid data leakage risks. We have\ncollected 15,888 WSIs from 8,549 patients across 10 hospitals, encompassing\nover 64 diagnosis and prognosis tasks. Currently, our evaluation of 19 PFMs\nshows that Virchow2 and H-Optimus-1 are the most effective models overall. This\nwork provides researchers with a robust platform for model development and\noffers clinicians actionable insights into PFM performance across diverse\nclinical scenarios, ultimately accelerating the translation of these\ntransformative technologies into routine pathology practice.", "authors": ["Jiabo Ma", "Yingxue Xu", "Fengtao Zhou", "Yihui Wang", "Cheng Jin", "Zhengrui Guo", "Jianfeng Wu", "On Ki Tang", "Huajun Zhou", "Xi Wang", "Luyang Luo", "Zhengyu Zhang", "Du Cai", "Zizhao Gao", "Wei Wang", "Yueping Liu", "Jiankun He", "Jing Cui", "Zhenhui Li", "Jing Zhang", "Feng Gao", "Xiuming Zhang", "Li Liang", "Ronald Cheong Kin Chan", "Zhe Wang", "Hao Chen"], "published_date": "2025-05-26", "title_zh": "PathBench：面向精準腫瘤學的病理學基礎模型綜合比較基準", "summary_zh": "病理學基礎模型的出現革新了計算病理學，實現了高精度、廣泛的全玻片影像分析，從而改善癌症診斷及預後評估。儘管這些模型在癌症診斷及預後方面展現巨大潛力，但其臨床轉化面臨多重挑戰，包括不同癌症類型的最佳模型變異性、評估中潛在的數據洩漏以及缺乏標準化基準。為了解決這些問題，我們提出了PathBench，首個綜合性基準，透過多中心內部數據集（涵蓋常見癌症並嚴格防止數據洩漏）、診斷到預後的全臨床範圍評估以及持續模型評估的自動排行榜系統來解決上述問題。我們的框架整合了大規模數據，能夠客觀比較病理學基礎模型，同時反映真實世界的臨床複雜性。所有評估數據均來自私人醫療機構，並嚴格排除任何預訓練使用以避免數據洩漏風險。我們收集了來自10家醫院、8549名患者的15888張全玻片影像，涵蓋64多項診斷及預後任務。目前，我們對19個病理學基礎模型的評估顯示，Virchow2和H-Optimus-1是整體上最有效的模型。這項工作為研究人員提供了一個穩健的模型開發平台，並為臨床醫生提供了關於病理學基礎模型在不同臨床情境下性能的可操作見解，最終加速將這些變革性技術轉化為常規病理學實踐。", "audio": "audios/2505.20202v1.mp3", "timestamp": "2025-05-27T03:10:16.123666"}
{"query": "Diffusion Model", "id": "2505.20171v1", "url": "http://arxiv.org/abs/2505.20171v1", "title": "Long-Context State-Space Video World Models", "summary": "Video diffusion models have recently shown promise for world modeling through\nautoregressive frame prediction conditioned on actions. However, they struggle\nto maintain long-term memory due to the high computational cost associated with\nprocessing extended sequences in attention layers. To overcome this limitation,\nwe propose a novel architecture leveraging state-space models (SSMs) to extend\ntemporal memory without compromising computational efficiency. Unlike previous\napproaches that retrofit SSMs for non-causal vision tasks, our method fully\nexploits the inherent advantages of SSMs in causal sequence modeling. Central\nto our design is a block-wise SSM scanning scheme, which strategically trades\noff spatial consistency for extended temporal memory, combined with dense local\nattention to ensure coherence between consecutive frames. We evaluate the\nlong-term memory capabilities of our model through spatial retrieval and\nreasoning tasks over extended horizons. Experiments on Memory Maze and\nMinecraft datasets demonstrate that our approach surpasses baselines in\npreserving long-range memory, while maintaining practical inference speeds\nsuitable for interactive applications.", "authors": ["Ryan Po", "Yotam Nitzan", "Richard Zhang", "Berlin Chen", "Tri Dao", "Eli Shechtman", "Gordon Wetzstein", "Xun Huang"], "published_date": "2025-05-26", "title_zh": "長程上下文狀態空間影片世界模型", "summary_zh": "影片擴散模型在世界建模中展現潛力，透過以動作為條件的自迴歸幀預測實現。然而，因注意力層處理長序列的計算成本高昂，模型難以維持長期記憶。為克服此限制，本研究提出一種新穎架構，利用狀態空間模型(SSM)擴展時間記憶，同時不犧牲計算效率。不同於以往將SSM改造用於非因果視覺任務的方法，本方法充分利用SSM在因果序列建模中的固有優勢。設計核心為分塊式SSM掃描方案，策略性地犧牲空間一致性以換取更長的時間記憶，並結合密集局部注意力以確保連續幀之間的一致性。透過在長時間範圍內的空間檢索和推理任務評估模型長期記憶能力。在記憶迷宮和Minecraft數據集上的實驗表明，本方法在保持適用於互動式應用程序的推理速度下，超越基準模型，能更好地保留長程記憶。", "audio": "audios/2505.20171v1.mp3", "timestamp": "2025-05-27T03:10:28.778843"}
{"query": "AI", "id": "2505.20148v1", "url": "http://arxiv.org/abs/2505.20148v1", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "published_date": "2025-05-26", "title_zh": "礦構萬物：開放世界AI代理空間規劃基準測試", "summary_zh": "空間規劃是空間智能的關鍵組成部分，需要理解並規劃空間中的物體佈局。具備空間規劃能力的人工智慧代理可以更好地適應現實應用，如機器人操作、自動組裝和城市規劃。現有研究試圖構建基準來評估多模態大型語言模型（MLLM）的空間智能，但這些基準主要關注基於視覺問答（VQA）形式的空間推理，存在抽象空間理解與具體任務執行之間的差距。本研究進一步構建名為MineAnyBuild的綜合基準，旨在評估開放世界人工智慧代理在Minecraft遊戲中的空間規劃能力。MineAnyBuild要求代理根據多模態人類指令生成可執行的建築規劃，包含4000個精選的空間規劃任務，並提供利用豐富玩家生成內容無限擴展數據收集的範例。MineAnyBuild透過空間理解、空間推理、創造力和空間常識四個核心維度評估空間規劃。基於MineAnyBuild，我們對現有基於MLLM的代理進行全面評估，揭示了它們在空間規劃能力方面的嚴重局限性，但也展現了巨大潛力。MineAnyBuild將為空間智能的評估開闢新途徑，並促進具備空間規劃能力的開放世界人工智慧代理的進一步發展。", "audio": "audios/2505.20148v1.mp3", "timestamp": "2025-05-27T04:25:27.119628"}
{"query": "Foundation Model", "id": "2505.20003v1", "url": "http://arxiv.org/abs/2505.20003v1", "title": "TabPFN: One Model to Rule Them All?", "summary": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study).", "authors": ["Qiong Zhang", "Yan Shuo Tan", "Qinglong Tian", "Pengfei Li"], "published_date": "2025-05-26", "title_zh": "TabPFN：一統天下的模型？", "summary_zh": "Hollmann等人於《自然》期刊發表TabPFN，一種基於Transformer的表格數據深度學習模型，聲稱其在小型數據集上大幅超越既有方法，並具備數據生成、密度估計、可重用嵌入和微調等能力，堪稱表格數據的基礎模型。本研究針對統計學受眾闡釋TabPFN的運作原理，強調其近似貝葉斯推斷的詮釋。實驗證明，TabPFN在半監督參數估計、協變量偏移下的預測和異質性處理效應估計等方面，優於專業的頂尖方法；在稀疏回歸中優於LASSO，並打破了分類中的穩健性-效率權衡。代碼已公開於https://github.com/qinglong-tian/tabpfn_study。", "audio": "audios/2505.20003v1.mp3", "timestamp": "2025-05-27T04:25:32.362431"}
{"query": "Diffusion Model", "id": "2505.20131v1", "url": "http://arxiv.org/abs/2505.20131v1", "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning", "summary": "Molecular editing aims to modify a given molecule to optimize desired\nchemical properties while preserving structural similarity. However, current\napproaches typically rely on string-based or continuous representations, which\nfail to adequately capture the discrete, graph-structured nature of molecules,\nresulting in limited structural fidelity and poor controllability. In this\npaper, we propose MolEditRL, a molecular editing framework that explicitly\nintegrates structural constraints with precise property optimization.\nSpecifically, MolEditRL consists of two stages: (1) a discrete graph diffusion\nmodel pretrained to reconstruct target molecules conditioned on source\nstructures and natural language instructions; (2) an editing-aware\nreinforcement learning fine-tuning stage that further enhances property\nalignment and structural preservation by explicitly optimizing editing\ndecisions under graph constraints. For comprehensive evaluation, we construct\nMolEdit-Instruct, the largest and most property-rich molecular editing dataset,\ncomprising 3 million diverse examples spanning single- and multi-property tasks\nacross 10 chemical attributes. Experimental results demonstrate that MolEditRL\nsignificantly outperforms state-of-the-art methods in both property\noptimization accuracy and structural fidelity, achieving a 74\\% improvement in\nediting success rate while using 98\\% fewer parameters.", "authors": ["Yuanxin Zhuang", "Dazhong Shen", "Ying Sun"], "published_date": "2025-05-26", "title_zh": "MolEditRL：基於離散擴散與強化學習的結構保持分子編輯", "summary_zh": "分子編輯旨在修改既有分子，優化化學性質，同時維持結構相似性。現有方法多仰賴字串或連續表徵，未能充分捕捉分子的離散圖結構特性，導致結構保真度受限、可控性不佳。本文提出 MolEditRL，一種分子編輯框架，明確整合結構約束與精確性質優化。MolEditRL包含兩階段：(1) 預訓練離散圖擴散模型，根據源結構與自然語言指令重建目標分子；(2) 編輯感知強化學習微調階段，藉由在圖約束下明確優化編輯決策，進一步提升性質一致性與結構保留。為全面評估，我們構建了 MolEdit-Instruct，涵蓋三百萬個範例，跨越單一與多重性質任務，包含十種化學屬性，是規模最大、性質最豐富的分子編輯資料集。實驗結果表明，MolEditRL 在性質優化準確性與結構保真度方面顯著優於現有技術，編輯成功率提升74%，同時參數減少98%。", "audio": "audios/2505.20131v1.mp3", "timestamp": "2025-05-27T04:25:40.009661"}
{"query": "AI", "id": "2505.20236v1", "url": "http://arxiv.org/abs/2505.20236v1", "title": "Seeing is Believing, but How Much? A Comprehensive Analysis of Verbalized Calibration in Vision-Language Models", "summary": "Uncertainty quantification is essential for assessing the reliability and\ntrustworthiness of modern AI systems. Among existing approaches, verbalized\nuncertainty, where models express their confidence through natural language,\nhas emerged as a lightweight and interpretable solution in large language\nmodels (LLMs). However, its effectiveness in vision-language models (VLMs)\nremains insufficiently studied. In this work, we conduct a comprehensive\nevaluation of verbalized confidence in VLMs, spanning three model categories,\nfour task domains, and three evaluation scenarios. Our results show that\ncurrent VLMs often display notable miscalibration across diverse tasks and\nsettings. Notably, visual reasoning models (i.e., thinking with images)\nconsistently exhibit better calibration, suggesting that modality-specific\nreasoning is critical for reliable uncertainty estimation. To further address\ncalibration challenges, we introduce Visual Confidence-Aware Prompting, a\ntwo-stage prompting strategy that improves confidence alignment in multimodal\nsettings. Overall, our study highlights the inherent miscalibration in VLMs\nacross modalities. More broadly, our findings underscore the fundamental\nimportance of modality alignment and model faithfulness in advancing reliable\nmultimodal systems.", "authors": ["Weihao Xuan", "Qingcheng Zeng", "Heli Qi", "Junjue Wang", "Naoto Yokoya"], "published_date": "2025-05-26", "title_zh": "眼見為憑，然信度幾何？視覺語言模型中口語校準之綜合分析", "summary_zh": "不確定性量化對評估現代人工智慧系統的可靠性至關重要。在現有方法中，口語化不確定性已成為大型語言模型中一種輕量且可解釋的解決方案，透過自然語言表達模型信心。然而，其在視覺語言模型中的有效性仍未得到充分研究。本研究全面評估視覺語言模型中的口語化信心，涵蓋三種模型類別、四個任務領域和三種評估情境。結果顯示，當前視覺語言模型在不同任務和設定中普遍存在顯著的校準誤差。值得注意的是，視覺推理模型展現出較佳的校準效果，表明模態特定推理對於可靠的不確定性估計至關重要。為了解決校準挑戰，我們提出視覺信心感知提示，一種雙階段提示策略，旨在提升多模態設定中的信心對齊。總體而言，本研究強調視覺語言模型中跨模態的固有校準誤差，並強調模態對齊和模型忠實度的重要性，以推進可靠的多模態系統。", "audio": "audios/2505.20236v1.mp3", "timestamp": "2025-05-27T05:18:58.542480"}
{"query": "Foundation Model", "id": "2505.19892v1", "url": "http://arxiv.org/abs/2505.19892v1", "title": "Unifying Multimodal Large Language Model Capabilities and Modalities via Model Merging", "summary": "While foundation models update slowly due to resource-intensive training\nrequirements, domain-specific models evolve between updates. Model merging aims\nto combine multiple expert models into a single, more capable model, thereby\nreducing storage and serving costs while supporting decentralized model\ndevelopment. Despite its potential, previous studies have primarily focused on\nmerging visual classification models or Large Language Models (LLMs) for code\nand math tasks. Multimodal Large Language Models (MLLMs), which extend the\ncapabilities of LLMs through large-scale multimodal training, have gained\ntraction. However, there lacks a benchmark for model merging research that\nclearly divides the tasks for MLLM training and evaluation. In this paper, (i)\nwe introduce the model merging benchmark for MLLMs, which includes multiple\ntasks such as VQA, Geometry, Chart, OCR, and Grounding, providing both LoRA and\nfull fine-tuning models. Moreover, we explore how model merging can combine\ndifferent modalities (e.g., vision-language, audio-language, and video-language\nmodels), moving toward the Omni-language model. (ii) We implement 10 model\nmerging algorithms on the benchmark. Furthermore, we propose a novel method\nthat removes noise from task vectors and robustly optimizes the merged vector\nbased on a loss defined over task vector interactions, achieving an average\nperformance gain of 2.48%. (iii) We find that model merging offers a promising\nway for building improved MLLMs without requiring data training. Our results\nalso demonstrate that the complementarity among multiple modalities outperforms\nindividual modalities.", "authors": ["Yongxian Wei", "Runxi Cheng", "Weike Jin", "Enneng Yang", "Li Shen", "Lu Hou", "Sinan Du", "Chun Yuan", "Xiaochun Cao", "Dacheng Tao"], "published_date": "2025-05-26", "title_zh": "透過模型合併統一多模態大型語言模型之能力與模態", "summary_zh": "由於基礎模型訓練耗費大量資源，更新速度較慢，領域模型則在更新間持續演進。模型合併旨在將多個專業模型整合為單一、更強大的模型，藉此降低儲存和服務成本，並支援分散式模型開發。儘管潛力巨大，既有研究主要集中於合併視覺分類模型或用於程式碼和數學任務的大型語言模型（LLMs）。多模態大型語言模型（MLLMs）透過大規模多模態訓練擴展了LLMs的功能，已備受關注。然而，缺乏針對MLLM模型合併研究的基準，能明確劃分MLLM訓練和評估任務。本研究（i）提出MLLM的模型合併基準，包含VQA、幾何、圖表、OCR和Grounding等多項任務，並提供LoRA和完整微調模型。此外，我們探索模型合併如何結合不同模態（例如，視覺-語言、音訊-語言和視訊-語言模型），朝向全語言模型發展。（ii）我們在基準上實作了10種模型合併演算法。更進一步，我們提出一種新穎方法，可移除任務向量中的雜訊，並基於任務向量互動定義的損失函數，穩健地優化合併向量，平均效能提升2.48%。（iii）研究發現，模型合併為構建更佳的MLLM提供了一種有前景的方式，無需數據訓練。結果也表明，多模態之間的互補性優於單一模態。", "audio": "audios/2505.19892v1.mp3", "timestamp": "2025-05-27T05:19:07.980818"}
{"query": "Diffusion Model", "id": "2505.20123v1", "url": "http://arxiv.org/abs/2505.20123v1", "title": "Understanding Generalization in Diffusion Models via Probability Flow Distance", "summary": "Diffusion models have emerged as a powerful class of generative models,\ncapable of producing high-quality samples that generalize beyond the training\ndata. However, evaluating this generalization remains challenging: theoretical\nmetrics are often impractical for high-dimensional data, while no practical\nmetrics rigorously measure generalization. In this work, we bridge this gap by\nintroducing probability flow distance ($\\texttt{PFD}$), a theoretically\ngrounded and computationally efficient metric to measure distributional\ngeneralization. Specifically, $\\texttt{PFD}$ quantifies the distance between\ndistributions by comparing their noise-to-data mappings induced by the\nprobability flow ODE. Moreover, by using $\\texttt{PFD}$ under a teacher-student\nevaluation protocol, we empirically uncover several key generalization\nbehaviors in diffusion models, including: (1) scaling behavior from\nmemorization to generalization, (2) early learning and double descent training\ndynamics, and (3) bias-variance decomposition. Beyond these insights, our work\nlays a foundation for future empirical and theoretical studies on\ngeneralization in diffusion models.", "authors": ["Huijie Zhang", "Zijian Huang", "Siyi Chen", "Jinfan Zhou", "Zekai Zhang", "Peng Wang", "Qing Qu"], "published_date": "2025-05-26", "title_zh": "基於機率流距離理解擴散模型的泛化能力", "summary_zh": "擴散模型已成為強大的生成模型，能產生高品質且泛化性超越訓練資料的樣本。然而，評估此泛化能力仍具挑戰：理論指標在高維資料中不切實際，且缺乏嚴格衡量泛化的實用指標。本研究引入機率流距離（PFD），一種具理論基礎且計算高效的指標，以衡量分布泛化能力。PFD透過比較機率流ODE引導的雜訊到資料映射，量化分布之間的距離。透過教師-學生評估協議使用PFD，我們揭示了擴散模型中的關鍵泛化行為，包括：(1)從記憶到泛化的縮放行為，(2)早期學習和雙重下降訓練動態，(3)偏差-方差分解。本研究為未來擴散模型泛化的實證和理論研究奠定了基礎。", "audio": "audios/2505.20123v1.mp3", "timestamp": "2025-05-27T05:19:13.780629"}
{"query": "AI", "id": "2505.20142v1", "url": "http://arxiv.org/abs/2505.20142v1", "title": "Model Stitching by Functional Latent Alignment", "summary": "Evaluating functional similarity involves quantifying the degree to which\nindependently trained neural networks learn functionally similar\nrepresentations. Reliably inferring the functional similarity of these networks\nremains an open problem with far-reaching implications for AI. Model stitching\nhas emerged as a promising paradigm, where an optimal affine transformation\naligns two models to solve a task, with the stitched model serving as a proxy\nfor functional similarity. In this work, we draw inspiration from the knowledge\ndistillation literature and propose Functional Latent Alignment (FuLA) as a\nnovel optimality condition for model stitching. We revisit previously explored\nfunctional similarity testbeds and introduce a new one, based on which FuLA\nemerges as an overall more reliable method of functional similarity.\nSpecifically, our experiments in (a) adversarial training, (b) shortcut\ntraining and, (c) cross-layer stitching, reveal that FuLA is less prone to\nartifacts tied to training on task cues while achieving non-trivial alignments\nthat are missed by stitch-level matching.", "authors": ["Ioannis Athanasiadis", "Anmar Karmush", "Michael Felsberg"], "published_date": "2025-05-26", "title_zh": "基於函數潛在對齊的模型縫合", "summary_zh": "評估功能相似性涉及量化獨立訓練神經網路學習功能相似表徵的程度。可靠推斷網路功能相似性仍是未解難題，對人工智慧影響深遠。模型縫合已成為有前景的方法，透過最佳仿射變換對齊兩模型以解決任務，縫合模型作為功能相似性的替代。本研究從知識蒸餾汲取靈感，提出功能潛在對齊 (FuLA) 作為模型縫合的新優化條件。我們重新檢視先前探索的功能相似性測試平台，並引入新平台，基於此，FuLA 成為更可靠的功能相似性方法。具體而言，在 (a) 對抗訓練、(b) 捷徑訓練和 (c) 跨層縫合的實驗表明，FuLA 不易受到與任務線索相關的偽像影響，同時實現了縫合層級匹配遺漏的重要對齊。", "audio": "audios/2505.20142v1.mp3", "timestamp": "2025-05-27T06:27:18.127000"}
{"query": "Foundation Model", "id": "2505.19888v1", "url": "http://arxiv.org/abs/2505.19888v1", "title": "Generalized and Personalized Federated Learning with Foundation Models via Orthogonal Transformations", "summary": "Federated Learning (FL) aims to train models across decentralized clients or\ndevices holding local data without the need for centralized data collection,\nthus enhancing data privacy and security. However, achieving both\ngeneralization and personalization in heterogeneous settings remains a\nsignificant challenge. To address this, we introduce FedOT, a novel approach\nthat leverages black-box foundation models. FedOT shares only a global\ntask-dependent classifier across clients while locally adapting features\nthrough orthogonal transformations. By enforcing orthogonality, FedOT mitigates\ngradient conflicts across diverse clients, preserves semantic integrity, and\nachieves robust performance even in the presence of substantial data\nheterogeneity. The strategy of combining global and local parameters enables a\nmore balanced approach for both generalization and personalization,\noutperforming baseline FL methods across multiple benchmarks. Furthermore, our\nextensive analysis confirms that joint optimization of global classifiers and\nlocal orthogonal transformations yields superior performance and suggests\nbroader applicability.", "authors": ["Eun Gyung Kong", "Je Won Yeom", "Yonghoon Jeon", "Taesup Kim"], "published_date": "2025-05-26", "title_zh": "基於正交變換的基礎模型廣義與個性化聯邦學習", "summary_zh": "聯邦學習旨在分散式客戶端或設備上訓練模型，無需集中數據收集，從而加強數據隱私與安全。然而，在異質環境中同時實現泛化與個性化仍具挑戰。為此，我們提出FedOT，一種利用黑盒基礎模型的新穎方法。FedOT僅共享全局任務相關分類器，同時通過正交轉換在本地調整特徵。通過強制正交性，FedOT減輕了跨多樣化客戶端的梯度衝突，保留了語義完整性，並在存在顯著數據異質性的情況下實現穩健的性能。全局與本地參數的結合策略有助於平衡泛化與個性化，在多個基準測試中優於傳統聯邦學習方法。此外，我們的廣泛分析證實了全局分類器和本地正交轉換的聯合優化可產生卓越性能，並暗示了更廣泛的適用性。", "audio": "audios/2505.19888v1.mp3", "timestamp": "2025-05-27T06:27:25.061762"}
{"query": "Diffusion Model", "id": "2505.20107v1", "url": "http://arxiv.org/abs/2505.20107v1", "title": "Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning", "summary": "Text-to-multiview (T2MV) generation, which produces coherent multiview images\nfrom a single text prompt, remains computationally intensive, while accelerated\nT2MV methods using few-step diffusion models often sacrifice image fidelity and\nview consistency. To address this, we propose a novel reinforcement learning\n(RL) finetuning framework tailored for few-step T2MV diffusion models to\njointly optimize per-view fidelity and cross-view consistency. Specifically, we\nfirst reformulate T2MV denoising across all views as a single unified Markov\ndecision process, enabling multiview-aware policy optimization driven by a\njoint-view reward objective. Next, we introduce ZMV-Sampling, a test-time T2MV\nsampling technique that adds an inversion-denoising pass to reinforce both\nviewpoint and text conditioning, resulting in improved T2MV generation at the\ncost of inference time. To internalize its performance gains into the base\nsampling policy, we develop MV-ZigAL, a novel policy optimization strategy that\nuses reward advantages of ZMV-Sampling over standard sampling as learning\nsignals for policy updates. Finally, noting that the joint-view reward\nobjective under-optimizes per-view fidelity but naively optimizing single-view\nmetrics neglects cross-view alignment, we reframe RL finetuning for T2MV\ndiffusion models as a constrained optimization problem that maximizes per-view\nfidelity subject to an explicit joint-view constraint, thereby enabling more\nefficient and balanced policy updates. By integrating this constrained\noptimization paradigm with MV-ZigAL, we establish our complete RL finetuning\nframework, referred to as MVC-ZigAL, which effectively refines the few-step\nT2MV diffusion baseline in both fidelity and consistency while preserving its\nfew-step efficiency.", "authors": ["Ziyi Zhang", "Li Shen", "Deheng Ye", "Yong Luo", "Huangxuan Zhao", "Lefei Zhang"], "published_date": "2025-05-26", "title_zh": "藉由強化學習精煉少步文本至多視角擴散", "summary_zh": "本文提出一種新的強化學習微調框架，專為少步文字到多視圖（T2MV）擴散模型設計，以共同優化單視圖逼真度和跨視圖一致性。首先，將所有視圖的T2MV去噪重新架構為統一的馬可夫決策過程，實現由聯合視圖獎勵目標驅動的多視圖感知策略優化。其次，引入ZMV-Sampling，一種測試時採樣技術，通過增加反演-去噪過程來強化視點和文本條件，改善T2MV生成效果，但會增加推論時間。為將其性能提升融入基礎採樣策略，開發了MV-ZigAL，一種策略優化策略，利用ZMV-Sampling相較於標準採樣的獎勵優勢作為策略更新的學習訊號。最後，將T2MV擴散模型的強化學習微調重新定義為約束優化問題，在明確的聯合視圖約束下最大化單視圖逼真度，從而實現更有效和平衡的策略更新。透過整合此約束優化範例與MV-ZigAL，建立了完整的強化學習微調框架MVC-ZigAL，可有效改進少步T2MV擴散模型的逼真度和一致性，同時保持其少步效率。", "audio": "audios/2505.20107v1.mp3", "timestamp": "2025-05-27T06:27:32.498349"}
{"query": "AI", "id": "2505.20222v1", "url": "http://arxiv.org/abs/2505.20222v1", "title": "FT-Boosted SV: Towards Noise Robust Speaker Verification for English Speaking Classroom Environments", "summary": "Creating Speaker Verification (SV) systems for classroom settings that are\nrobust to classroom noises such as babble noise is crucial for the development\nof AI tools that assist educational environments. In this work, we study the\nefficacy of finetuning with augmented children datasets to adapt the x-vector\nand ECAPA-TDNN to classroom environments. We demonstrate that finetuning with\naugmented children's datasets is powerful in that regard and reduces the Equal\nError Rate (EER) of x-vector and ECAPA-TDNN models for both classroom datasets\nand children speech datasets. Notably, this method reduces EER of the\nECAPA-TDNN model on average by half (a 5 % improvement) for classrooms in the\nMPT dataset compared to the ECAPA-TDNN baseline model. The x-vector model shows\nan 8 % average improvement for classrooms in the NCTE dataset compared to its\nbaseline.", "authors": ["Saba Tabatabaee", "Jing Liu", "Carol Espy-Wilson"], "published_date": "2025-05-26", "title_zh": "FT增強型SV：面向英語口語課堂環境的抗噪聲說話人驗證", "summary_zh": "為開發輔助教育環境的人工智慧工具，建立能有效應對教室噪音（如人聲嘈雜）的講者驗證系統至關重要。本研究探討使用擴增兒童語音資料集微調 x-vector 和 ECAPA-TDNN 模型，使其適應教室環境的有效性。結果表明，此微調方法能顯著降低 x-vector 和 ECAPA-TDNN 模型在教室和兒童語音資料集上的等錯誤率 (EER)。特別是，ECAPA-TDNN 模型在 MPT 資料集的教室環境中，EER 平均降低一半（提升 5%），而 x-vector 模型在 NCTE 資料集的教室環境中，相較於基準模型平均提升 8%。", "audio": "audios/2505.20222v1.mp3", "timestamp": "2025-05-27T07:19:01.932745"}
{"query": "Foundation Model", "id": "2505.19863v1", "url": "http://arxiv.org/abs/2505.19863v1", "title": "FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields", "summary": "We introduce FruitNeRF++, a novel fruit-counting approach that combines\ncontrastive learning with neural radiance fields to count fruits from\nunstructured input photographs of orchards. Our work is based on FruitNeRF,\nwhich employs a neural semantic field combined with a fruit-specific clustering\napproach. The requirement for adaptation for each fruit type limits the\napplicability of the method, and makes it difficult to use in practice. To lift\nthis limitation, we design a shape-agnostic multi-fruit counting framework,\nthat complements the RGB and semantic data with instance masks predicted by a\nvision foundation model. The masks are used to encode the identity of each\nfruit as instance embeddings into a neural instance field. By volumetrically\nsampling the neural fields, we extract a point cloud embedded with the instance\nfeatures, which can be clustered in a fruit-agnostic manner to obtain the fruit\ncount. We evaluate our approach using a synthetic dataset containing apples,\nplums, lemons, pears, peaches, and mangoes, as well as a real-world benchmark\napple dataset. Our results demonstrate that FruitNeRF++ is easier to control\nand compares favorably to other state-of-the-art methods.", "authors": ["Lukas Meyer", "Andrei-Timotei Ardelean", "Tim Weyrich", "Marc Stamminger"], "published_date": "2025-05-26", "title_zh": "FruitNeRF++：基於對比學習與神經輻射場的通用多水果計數方法", "summary_zh": "FruitNeRF++是一種新型水果計數方法，結合對比學習與神經輻射場，從果園的非結構化照片中計數水果。此方法基於FruitNeRF，後者採用神經語義場和特定水果的聚類方法。為了解決FruitNeRF需針對每種水果進行調整的限制，我們設計了一種形狀無關的多水果計數框架，利用視覺基礎模型預測的實例掩碼來補充RGB和語義數據。掩碼用於將每個水果的身份編碼為神經實例場中的實例嵌入。透過對神經場進行體積採樣，提取嵌入實例特徵的點雲，並以與水果種類無關的方式進行聚類，從而獲得水果數量。我們使用包含蘋果、李子、檸檬、梨、桃子和芒果的合成數據集以及真實世界的蘋果基準數據集評估了此方法。結果表明，FruitNeRF++更易於控制，並且優於其他最先進的方法。", "audio": "audios/2505.19863v1.mp3", "timestamp": "2025-05-27T07:19:09.385657"}
{"query": "Diffusion Model", "id": "2505.20056v1", "url": "http://arxiv.org/abs/2505.20056v1", "title": "PAMD: Plausibility-Aware Motion Diffusion Model for Long Dance Generation", "summary": "Computational dance generation is crucial in many areas, such as art,\nhuman-computer interaction, virtual reality, and digital entertainment,\nparticularly for generating coherent and expressive long dance sequences.\nDiffusion-based music-to-dance generation has made significant progress, yet\nexisting methods still struggle to produce physically plausible motions. To\naddress this, we propose Plausibility-Aware Motion Diffusion (PAMD), a\nframework for generating dances that are both musically aligned and physically\nrealistic. The core of PAMD lies in the Plausible Motion Constraint (PMC),\nwhich leverages Neural Distance Fields (NDFs) to model the actual pose manifold\nand guide generated motions toward a physically valid pose manifold. To provide\nmore effective guidance during generation, we incorporate Prior Motion Guidance\n(PMG), which uses standing poses as auxiliary conditions alongside music\nfeatures. To further enhance realism for complex movements, we introduce the\nMotion Refinement with Foot-ground Contact (MRFC) module, which addresses\nfoot-skating artifacts by bridging the gap between the optimization objective\nin linear joint position space and the data representation in nonlinear\nrotation space. Extensive experiments show that PAMD significantly improves\nmusical alignment and enhances the physical plausibility of generated motions.\nThis project page is available at: https://mucunzhuzhu.github.io/PAMD-page/.", "authors": ["Hongsong Wang", "Yin Zhu", "Qiuxia Lai", "Yang Zhang", "Guo-Sen Xie", "Xin Geng"], "published_date": "2025-05-26", "title_zh": "PAMD：基於合理性感知運動擴散模型的長舞蹈生成", "summary_zh": "計算舞蹈生成在藝術、人機互動、虛擬實境及數位娛樂等領域至關重要，尤其在產生連貫且富表現力的長舞蹈序列方面。基於擴散模型的音樂到舞蹈生成已取得顯著進展，但現有方法仍難以產生符合物理真實性的動作。為此，我們提出具合理性感知的動作擴散（PAMD）框架，旨在生成兼具音樂對齊性和物理真實性的舞蹈。PAMD的核心在於合理動作約束（PMC），利用神經距離場（NDFs）建模真實姿勢流形，引導生成動作朝向物理上有效的姿勢流形。為在生成過程中提供更有效的引導，我們結合了先驗動作引導（PMG），將站立姿勢作為輔助條件與音樂特徵一起使用。為進一步提升複雜動作的真實感，我們引入了基於足部-地面接觸的動作精煉（MRFC）模組，通過彌合線性關節位置空間中的優化目標與非線性旋轉空間中的數據表示之間的差距，解決足部滑動問題。大量實驗表明，PAMD顯著提升了音樂對齊性並增強了生成動作的物理合理性。專案主頁：https://mucunzhuzhu.github.io/PAMD-page/。", "audio": "audios/2505.20056v1.mp3", "timestamp": "2025-05-27T07:19:18.435166"}
{"query": "AI", "id": "2505.20206v1", "url": "http://arxiv.org/abs/2505.20206v1", "title": "Evaluating Large Language Models for Code Review", "summary": "Context: Code reviews are crucial for software quality. Recent AI advances\nhave allowed large language models (LLMs) to review and fix code; now, there\nare tools that perform these reviews. However, their reliability and accuracy\nhave not yet been systematically evaluated. Objective: This study compares\ndifferent LLMs' performance in detecting code correctness and suggesting\nimprovements. Method: We tested GPT4o and Gemini 2.0 Flash on 492 AI generated\ncode blocks of varying correctness, along with 164 canonical code blocks from\nthe HumanEval benchmark. To simulate the code review task objectively, we\nexpected LLMs to assess code correctness and improve the code if needed. We ran\nexperiments with different configurations and reported on the results. Results:\nWith problem descriptions, GPT4o and Gemini 2.0 Flash correctly classified code\ncorrectness 68.50% and 63.89% of the time, respectively, and corrected the code\n67.83% and 54.26% of the time for the 492 code blocks of varying correctness.\nWithout problem descriptions, performance declined. The results for the 164\ncanonical code blocks differed, suggesting that performance depends on the type\nof code. Conclusion: LLM code reviews can help suggest improvements and assess\ncorrectness, but there is a risk of faulty outputs. We propose a process that\ninvolves humans, called the \"Human in the loop LLM Code Review\" to promote\nknowledge sharing while mitigating the risk of faulty outputs.", "authors": ["Umut Cihan", "Arda İçöz", "Vahid Haratian", "Eray Tüzün"], "published_date": "2025-05-26", "title_zh": "用於程式碼審查的大型語言模型評估", "summary_zh": "背景：程式碼審查對軟體品質至關重要。大型語言模型(LLM)在程式碼審查和修復方面有所進展，相關工具應運而生，但其可靠性和準確性尚未經系統評估。\n目標：本研究旨在比較不同LLM在檢測程式碼正確性及提出改進建議方面的效能。\n方法：針對492個不同正確性的AI生成程式碼片段和164個HumanEval基準程式碼片段，測試GPT4o和Gemini 2.0 Flash。要求LLM評估程式碼正確性並在必要時改進程式碼，以客觀模擬程式碼審查任務。\n結果：在提供問題描述的情況下，GPT4o和Gemini 2.0 Flash分別在68.50%和63.89%的時間內正確分類程式碼，並分別在67.83%和54.26%的時間內修正了492個程式碼片段。缺少問題描述時，效能下降。164個基準程式碼片段的結果不同，表明效能取決於程式碼類型。\n結論：LLM程式碼審查有助於提出改進建議並評估正確性，但也存在錯誤輸出的風險。我們提出一種涉及人工的流程，稱為「人機迴路LLM程式碼審查」，以促進知識共享並降低錯誤輸出的風險。", "audio": "audios/2505.20206v1.mp3", "timestamp": "2025-05-27T08:25:46.293150"}
{"query": "Foundation Model", "id": "2505.19851v1", "url": "http://arxiv.org/abs/2505.19851v1", "title": "Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages", "summary": "Transliteration, the process of mapping text from one script to another,\nplays a crucial role in multilingual natural language processing, especially\nwithin linguistically diverse contexts such as India. Despite significant\nadvancements through specialized models like IndicXlit, recent developments in\nlarge language models suggest a potential for general-purpose models to excel\nat this task without explicit task-specific training. The current work\nsystematically evaluates the performance of prominent LLMs, including GPT-4o,\nGPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a\nstate-of-the-art transliteration model, across ten major Indian languages.\nExperiments utilized standard benchmarks, including Dakshina and Aksharantar\ndatasets, with performance assessed via Top-1 Accuracy and Character Error\nRate. Our findings reveal that while GPT family models generally outperform\nother LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o\nimproves performance on specific languages notably. An extensive error analysis\nand robustness testing under noisy conditions further elucidate strengths of\nLLMs compared to specialized models, highlighting the efficacy of foundational\nmodels for a wide spectrum of specialized applications with minimal overhead.", "authors": ["Gulfarogh Azam", "Mohd Sadique", "Saif Ali", "Mohammad Nadeem", "Erik Cambria", "Shahab Saquib Sohail", "Mohammad Sultan Alam"], "published_date": "2025-05-26", "title_zh": "超越專精：印度語言音譯之大型語言模型基準測試", "summary_zh": "音譯是將文字從一種文字系統映射到另一種文字系統的過程，在多語言自然語言處理中至關重要，尤其是在印度等語言多樣化的環境中。儘管IndicXlit等專用模型取得了顯著進展，但大型語言模型（LLM）的最新發展表明，通用模型可能無需明確的任務特定訓練也能勝任此任務。本研究系統地評估了GPT-4o、GPT-4.5、GPT-4.1、Gemma-3-27B-it和Mistral-Large等主要LLM相對於最先進的音譯模型IndicXlit在十種主要印度語言上的性能。實驗採用了Dakshina和Aksharantar等標準基準數據集，並透過Top-1準確率和字元錯誤率評估性能。研究結果顯示，在大多數情況下，GPT系列模型普遍優於其他LLM和IndicXlit。此外，針對GPT-4o進行微調可顯著提高特定語言的性能。廣泛的錯誤分析和雜訊條件下的穩健性測試進一步闡明了LLM相較於專用模型的優勢，突顯了基礎模型在最小開銷下，能有效應用於廣泛的專業應用。", "audio": "audios/2505.19851v1.mp3", "timestamp": "2025-05-27T08:25:53.493689"}
{"query": "Diffusion Model", "id": "2505.20053v1", "url": "http://arxiv.org/abs/2505.20053v1", "title": "Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion", "summary": "Diffusion models have become the mainstream architecture for text-to-image\ngeneration, achieving remarkable progress in visual quality and prompt\ncontrollability. However, current inference pipelines generally lack\ninterpretable semantic supervision and correction mechanisms throughout the\ndenoising process. Most existing approaches rely solely on post-hoc scoring of\nthe final image, prompt filtering, or heuristic resampling strategies-making\nthem ineffective in providing actionable guidance for correcting the generative\ntrajectory. As a result, models often suffer from object confusion, spatial\nerrors, inaccurate counts, and missing semantic elements, severely compromising\nprompt-image alignment and image quality. To tackle these challenges, we\npropose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel\nframework that, for the first time, introduces a Multimodal Large Language\nModel (MLLM) as a semantic observer during inference. PPAD performs real-time\nanalysis on intermediate generations, identifies latent semantic\ninconsistencies, and translates feedback into controllable signals that\nactively guide the remaining denoising steps. The framework supports both\ninference-only and training-enhanced settings, and performs semantic correction\nat only extremely few diffusion steps, offering strong generality and\nscalability. Extensive experiments demonstrate PPAD's significant improvements.", "authors": ["Zheqi Lv", "Junhao Chen", "Qi Tian", "Keting Yin", "Shengyu Zhang", "Fei Wu"], "published_date": "2025-05-26", "title_zh": "多模態大型語言模型引導的文圖擴散語義校正", "summary_zh": "擴散模型已成文字生成圖像的主流架構，然目前推論流程普遍缺乏可解釋的語義監督與校正機制。現有方法多仰賴事後評分、提示過濾或啟發式重採樣，難以有效導正生成軌跡，導致物件混淆、空間錯誤、數量不準確及語義缺失等問題，損害提示圖像對齊及圖像品質。為解決此困境，我們提出MLLM語義校正乒乓前向擴散(PPAD)框架，首度引入多模態大型語言模型(MLLM)作為推論期間的語義觀察者。PPAD即時分析中間生成結果，識別潛在語義不一致，並將反饋轉譯為可控信號，主動引導後續降噪步驟。該框架支援純推論及訓練增強設定，僅於極少擴散步驟進行語義校正，具備良好泛用性及擴展性。實驗證明PPAD能顯著改善生成結果。", "audio": "audios/2505.20053v1.mp3", "timestamp": "2025-05-27T08:26:00.673368"}
{"query": "AI", "id": "2505.20136v1", "url": "http://arxiv.org/abs/2505.20136v1", "title": "Engineering Trustworthy Machine-Learning Operations with Zero-Knowledge Proofs", "summary": "As Artificial Intelligence (AI) systems, particularly those based on machine\nlearning (ML), become integral to high-stakes applications, their probabilistic\nand opaque nature poses significant challenges to traditional verification and\nvalidation methods. These challenges are exacerbated in regulated sectors\nrequiring tamper-proof, auditable evidence, as highlighted by apposite legal\nframeworks, e.g., the EU AI Act. Conversely, Zero-Knowledge Proofs (ZKPs) offer\na cryptographic solution that enables provers to demonstrate, through verified\ncomputations, adherence to set requirements without revealing sensitive model\ndetails or data. Through a systematic survey of ZKP protocols, we identify five\nkey properties (non-interactivity, transparent setup, standard representations,\nsuccinctness, and post-quantum security) critical for their application in AI\nvalidation and verification pipelines. Subsequently, we perform a follow-up\nsystematic survey analyzing ZKP-enhanced ML applications across an adaptation\nof the Team Data Science Process (TDSP) model (Data & Preprocessing, Training &\nOffline Metrics, Inference, and Online Metrics), detailing verification\nobjectives, ML models, and adopted protocols. Our findings indicate that\ncurrent research on ZKP-Enhanced ML primarily focuses on inference\nverification, while the data preprocessing and training stages remain\nunderexplored. Most notably, our analysis identifies a significant convergence\nwithin the research domain toward the development of a unified Zero-Knowledge\nMachine Learning Operations (ZKMLOps) framework. This emerging framework\nleverages ZKPs to provide robust cryptographic guarantees of correctness,\nintegrity, and privacy, thereby promoting enhanced accountability,\ntransparency, and compliance with Trustworthy AI principles.", "authors": ["Filippo Scaramuzza", "Giovanni Quattrocchi", "Damian A. Tamburri"], "published_date": "2025-05-26", "title_zh": "利用零知識證明構建可信賴的機器學習運營", "summary_zh": "隨著人工智慧系統，特別是基於機器學習的系統，在高風險應用中日益重要，其機率性和不透明性對傳統驗證方法構成重大挑戰。在需要防篡改、可稽核證據的監管領域，這些挑戰更為嚴峻，例如歐盟人工智慧法案。零知識證明提供了一種加密解決方案，使證明者能夠透過驗證計算來證明符合既定要求，而無需洩露敏感的模型細節或資料。透過對零知識證明協議的系統性調查，我們確定了五個關鍵特性（非互動性、透明設置、標準表示、簡潔性和後量子安全性），這些特性對於其在人工智慧驗證管線中的應用至關重要。隨後，我們進行了一項後續的系統性調查，分析了零知識證明強化的機器學習應用，採用了團隊資料科學流程模型的改編版本（資料與預處理、訓練與離線指標、推論和線上指標），詳細說明了驗證目標、機器學習模型和採用的協議。研究結果表明，目前零知識證明強化的機器學習研究主要集中在推論驗證上，而資料預處理和訓練階段仍未得到充分探索。最值得注意的是，我們的分析發現研究領域內正在向統一的零知識機器學習運營框架發展。這個新興框架利用零知識證明來提供強大的正確性、完整性和隱私加密保證，從而促進增強的問責制、透明度以及對可信賴人工智慧原則的遵守。", "audio": "audios/2505.20136v1.mp3", "timestamp": "2025-05-27T09:19:42.431869"}
{"query": "Foundation Model", "id": "2505.19825v1", "url": "http://arxiv.org/abs/2505.19825v1", "title": "Foundation Models for Tabular Data within Systemic Contexts Need Grounding", "summary": "Current research on tabular foundation models often overlooks the\ncomplexities of large-scale, real-world data by treating tables as isolated\nentities and assuming information completeness, thereby neglecting the vital\noperational context. To address this, we introduce the concept of Semantically\nLinked Tables (SLT), recognizing that tables are inherently connected to both\ndeclarative and procedural operational knowledge. We propose Foundation Models\nfor Semantically Linked Tables (FMSLT), which integrate these components to\nground tabular data within its true operational context. This comprehensive\nrepresentation unlocks the full potential of machine learning for complex,\ninterconnected tabular data across diverse domains. Realizing FMSLTs requires\naccess to operational knowledge that is often unavailable in public datasets,\nhighlighting the need for close collaboration between domain experts and\nresearchers. Our work exposes the limitations of current tabular foundation\nmodels and proposes a new direction centered on FMSLTs, aiming to advance\nrobust, context-aware models for structured data.", "authors": ["Tassilo Klein", "Johannes Hoffart"], "published_date": "2025-05-26", "title_zh": "系統性脈絡下表格式資料之基礎模型需具備基礎性", "summary_zh": "現有表格基礎模型研究常忽略大規模真實數據的複雜性，將表格視為孤立個體並假設信息完整，從而忽略關鍵的操作背景。為此，我們引入語義鏈接表格（SLT）的概念，認識到表格與聲明式和程序式操作知識固有聯繫。我們提出用於語義鏈接表格的基礎模型（FMSLT），整合這些組件，將表格數據紮根於其真實操作背景中。此全面表示釋放機器學習在複雜、互連表格數據方面的潛力。實現FMSLT需要獲取公共數據集中通常無法獲得的操作知識，突顯領域專家與研究人員密切合作之必要。本研究揭示現有表格基礎模型的局限性，並提出以SLT為中心的新方向，旨在推進結構化數據之穩健、具上下文感知模型。", "audio": "audios/2505.19825v1.mp3", "timestamp": "2025-05-27T09:19:48.132133"}
{"query": "Diffusion Model", "id": "2505.19983v1", "url": "http://arxiv.org/abs/2505.19983v1", "title": "ICDM: Interference Cancellation Diffusion Models for Wireless Semantic Communications", "summary": "Diffusion models (DMs) have recently achieved significant success in wireless\ncommunications systems due to their denoising capabilities. The broadcast\nnature of wireless signals makes them susceptible not only to Gaussian noise,\nbut also to unaware interference. This raises the question of whether DMs can\neffectively mitigate interference in wireless semantic communication systems.\nIn this paper, we model the interference cancellation problem as a maximum a\nposteriori (MAP) problem over the joint posterior probability of the signal and\ninterference, and theoretically prove that the solution provides excellent\nestimates for the signal and interference. To solve this problem, we develop an\ninterference cancellation diffusion model (ICDM), which decomposes the joint\nposterior into independent prior probabilities of the signal and interference,\nalong with the channel transition probablity. The log-gradients of these\ndistributions at each time step are learned separately by DMs and accurately\nestimated through deriving. ICDM further integrates these gradients with\nadvanced numerical iteration method, achieving accurate and rapid interference\ncancellation. Extensive experiments demonstrate that ICDM significantly reduces\nthe mean square error (MSE) and enhances perceptual quality compared to schemes\nwithout ICDM. For example, on the CelebA dataset under the Rayleigh fading\nchannel with a signal-to-noise ratio (SNR) of $20$ dB and signal to\ninterference plus noise ratio (SINR) of 0 dB, ICDM reduces the MSE by 4.54 dB\nand improves the learned perceptual image patch similarity (LPIPS) by 2.47 dB.", "authors": ["Tong Wu", "Zhiyong Chen", "Dazhi He", "Feng Yang", "Meixia Tao", "Xiaodong Xu", "Wenjun Zhang", "Ping Zhang"], "published_date": "2025-05-26", "title_zh": "用於無線語義通信的干擾消除擴散模型", "summary_zh": "擴散模型因其降噪能力，近期在無線通訊系統中獲得顯著成功。無線訊號的廣播特性使其易受高斯雜訊及未知干擾影響。本文探討擴散模型能否有效緩解無線語義通訊系統中的干擾。我們將干擾消除問題建模為訊號與干擾聯合後驗機率的最大後驗(MAP)問題，並從理論上證明該解能提供優良的訊號與干擾估計。為了解決此問題，我們開發了一種干擾消除擴散模型(ICDM)，其將聯合後驗分解為訊號和干擾的獨立先驗機率，以及通道轉移機率。這些分佈的對數梯度在每個時間步由擴散模型單獨學習，並透過推導精確估計。ICDM進一步將這些梯度與先進的數值迭代方法整合，實現準確快速的干擾消除。大量實驗表明，相較於未使用ICDM的方案，ICDM顯著降低了均方誤差(MSE)並提高了感知品質。例如，在Rayleigh衰落通道下，訊雜比(SNR)為20 dB，訊號干擾加雜訊比(SINR)為0 dB時，ICDM在CelebA數據集上將MSE降低了4.54 dB，並將學習到的感知圖像塊相似度(LPIPS)提高了2.47 dB。", "audio": "audios/2505.19983v1.mp3", "timestamp": "2025-05-27T09:19:57.159004"}
{"query": "AI", "id": "2505.20129v1", "url": "http://arxiv.org/abs/2505.20129v1", "title": "Agentic 3D Scene Generation with Spatially Contextualized VLMs", "summary": "Despite recent advances in multimodal content generation enabled by\nvision-language models (VLMs), their ability to reason about and generate\nstructured 3D scenes remains largely underexplored. This limitation constrains\ntheir utility in spatially grounded tasks such as embodied AI, immersive\nsimulations, and interactive 3D applications. We introduce a new paradigm that\nenables VLMs to generate, understand, and edit complex 3D environments by\ninjecting a continually evolving spatial context. Constructed from multimodal\ninput, this context consists of three components: a scene portrait that\nprovides a high-level semantic blueprint, a semantically labeled point cloud\ncapturing object-level geometry, and a scene hypergraph that encodes rich\nspatial relationships, including unary, binary, and higher-order constraints.\nTogether, these components provide the VLM with a structured, geometry-aware\nworking memory that integrates its inherent multimodal reasoning capabilities\nwith structured 3D understanding for effective spatial reasoning. Building on\nthis foundation, we develop an agentic 3D scene generation pipeline in which\nthe VLM iteratively reads from and updates the spatial context. The pipeline\nfeatures high-quality asset generation with geometric restoration, environment\nsetup with automatic verification, and ergonomic adjustment guided by the scene\nhypergraph. Experiments show that our framework can handle diverse and\nchallenging inputs, achieving a level of generalization not observed in prior\nwork. Further results demonstrate that injecting spatial context enables VLMs\nto perform downstream tasks such as interactive scene editing and path\nplanning, suggesting strong potential for spatially intelligent systems in\ncomputer graphics, 3D vision, and embodied applications.", "authors": ["Xinhang Liu", "Yu-Wing Tai", "Chi-Keung Tang"], "published_date": "2025-05-26", "title_zh": "具空間脈絡視覺語言模型的主動式三維場景生成", "summary_zh": "儘管視覺語言模型在多模態內容生成方面取得進展，但其推理和生成結構化3D場景的能力仍待開發，限制了其在空間定位任務中的應用。我們提出一種新範式，透過注入持續演進的空間背景，使視覺語言模型能夠生成、理解和編輯複雜的3D環境。此背景由多模態輸入構建，包含場景藍圖、語義標籤點雲和編碼空間關係的場景超圖。這些組件為視覺語言模型提供結構化、幾何感知的記憶體，整合其多模態推理能力與3D理解能力，以實現有效的空間推理。在此基礎上，我們開發了一種主動式3D場景生成流程，視覺語言模型迭代讀取並更新空間背景，實現高品質資產生成、環境自動驗證設定以及場景超圖引導的人體工學調整。實驗表明，該框架能處理多樣且具挑戰性的輸入，達到前所未有的泛化程度。結果亦顯示，注入空間背景能使視覺語言模型執行互動式場景編輯和路徑規劃等下游任務，展現其在電腦圖學、3D視覺和具體應用中空間智慧系統的潛力。", "audio": "audios/2505.20129v1.mp3", "timestamp": "2025-05-27T10:20:58.252116"}
{"query": "Foundation Model", "id": "2505.19779v1", "url": "http://arxiv.org/abs/2505.19779v1", "title": "Advancements in Medical Image Classification through Fine-Tuning Natural Domain Foundation Models", "summary": "Using massive datasets, foundation models are large-scale, pre-trained models\nthat perform a wide range of tasks. These models have shown consistently\nimproved results with the introduction of new methods. It is crucial to analyze\nhow these trends impact the medical field and determine whether these\nadvancements can drive meaningful change. This study investigates the\napplication of recent state-of-the-art foundation models, DINOv2, MAE, VMamba,\nCoCa, SAM2, and AIMv2, for medical image classification. We explore their\neffectiveness on datasets including CBIS-DDSM for mammography, ISIC2019 for\nskin lesions, APTOS2019 for diabetic retinopathy, and CHEXPERT for chest\nradiographs. By fine-tuning these models and evaluating their configurations,\nwe aim to understand the potential of these advancements in medical image\nclassification. The results indicate that these advanced models significantly\nenhance classification outcomes, demonstrating robust performance despite\nlimited labeled data. Based on our results, AIMv2, DINOv2, and SAM2 models\noutperformed others, demonstrating that progress in natural domain training has\npositively impacted the medical domain and improved classification outcomes.\nOur code is publicly available at:\nhttps://github.com/sajjad-sh33/Medical-Transfer-Learning.", "authors": ["Mobina Mansoori", "Sajjad Shahabodini", "Farnoush Bayatmakou", "Jamshid Abouei", "Konstantinos N. Plataniotis", "Arash Mohammadi"], "published_date": "2025-05-26", "title_zh": "透過微調自然領域基礎模型提升醫學影像分類效能", "summary_zh": "基於海量資料集的大型預訓練基礎模型能執行多種任務，且新方法不斷提升其效能。本研究探討最先進基礎模型DINOv2、MAE、VMamba、CoCa、SAM2與AIMv2在醫學影像分類上的應用，評估其於乳房攝影CBIS-DDSM、皮膚病灶ISIC2019、糖尿病視網膜病變APTOS2019及胸腔X光CHEXPERT等資料集的效能。透過微調與配置評估，旨在了解這些進展在醫學影像分類中的潛力。結果顯示，這些模型顯著改善分類結果，即使在標記資料有限的情況下也能展現穩健效能。AIMv2、DINOv2與SAM2表現優於其他模型，表明自然領域訓練的進步已對醫學領域產生正面影響，並提升了分類結果。相關程式碼已公開。", "audio": "audios/2505.19779v1.mp3", "timestamp": "2025-05-27T10:21:04.796069"}
{"query": "Diffusion Model", "id": "2505.19958v1", "url": "http://arxiv.org/abs/2505.19958v1", "title": "UltraVSR: Achieving Ultra-Realistic Video Super-Resolution with Efficient One-Step Diffusion Space", "summary": "Diffusion models have shown great potential in generating realistic image\ndetail. However, adapting these models to video super-resolution (VSR) remains\nchallenging due to their inherent stochasticity and lack of temporal modeling.\nIn this paper, we propose UltraVSR, a novel framework that enables\nultra-realistic and temporal-coherent VSR through an efficient one-step\ndiffusion space. A central component of UltraVSR is the Degradation-aware\nRestoration Schedule (DRS), which estimates a degradation factor from the\nlow-resolution input and transforms iterative denoising process into a\nsingle-step reconstruction from from low-resolution to high-resolution videos.\nThis design eliminates randomness from diffusion noise and significantly speeds\nup inference. To ensure temporal consistency, we propose a lightweight yet\neffective Recurrent Temporal Shift (RTS) module, composed of an RTS-convolution\nunit and an RTS-attention unit. By partially shifting feature components along\nthe temporal dimension, these two units collaboratively facilitate effective\nfeature propagation, fusion, and alignment across neighboring frames, without\nrelying on explicit temporal layers. The RTS module is integrated into a\npretrained text-to-image diffusion model and is further enhanced through\nSpatio-temporal Joint Distillation (SJD), which improves temporal coherence\nwhile preserving realistic details. Additionally, we introduce a Temporally\nAsynchronous Inference (TAI) strategy to capture long-range temporal\ndependencies under limited memory constraints. Extensive experiments show that\nUltraVSR achieves state-of-the-art performance, both qualitatively and\nquantitatively, in a single sampling step.", "authors": ["Yong Liu", "Jinshan Pan", "Yinchuan Li", "Qingji Dong", "Chao Zhu", "Yu Guo", "Fei Wang"], "published_date": "2025-05-26", "title_zh": "UltraVSR：藉由高效單步擴散空間實現超逼真影片超解析度", "summary_zh": "擴散模型在生成逼真圖像細節方面展現巨大潛力，但由於其內在隨機性和缺乏時間建模，將其應用於影片超解析度（VSR）仍具挑戰。本文提出 UltraVSR，一種新型框架，透過高效的一步擴散空間實現超逼真且時間一致的 VSR。UltraVSR 的核心組件是退化感知恢復排程（DRS），它估計低解析度輸入的退化因子，並將迭代去噪過程轉變為從低解析度到高解析度影片的單步重建，消除了擴散雜訊的隨機性並顯著加速了推論。為確保時間一致性，我們提出輕量級但高效的循環時間位移（RTS）模組，由 RTS 卷積單元和 RTS 注意力單元組成。透過沿時間維度部分移動特徵分量，這兩個單元協同促進相鄰幀之間的有效特徵傳播、融合和對齊，而無需依賴顯式時間層。RTS 模組被整合到預訓練的文本到圖像擴散模型中，並透過時空聯合蒸餾（SJD）進一步增強，從而在保持逼真細節的同時提高時間一致性。此外，我們引入時間非同步推論（TAI）策略，以在有限的記憶體約束下捕獲遠程時間依賴性。大量實驗表明，UltraVSR 在單次採樣步驟中實現了最先進的效能，無論是在質量上還是數量上。", "audio": "audios/2505.19958v1.mp3", "timestamp": "2025-05-27T10:21:13.370524"}
{"query": "AI", "id": "2505.20127v1", "url": "http://arxiv.org/abs/2505.20127v1", "title": "Agentic AI Process Observability: Discovering Behavioral Variability", "summary": "AI agents that leverage Large Language Models (LLMs) are increasingly\nbecoming core building blocks of modern software systems. A wide range of\nframeworks is now available to support the specification of such applications.\nThese frameworks enable the definition of agent setups using natural language\nprompting, which specifies the roles, goals, and tools assigned to the various\nagents involved. Within such setups, agent behavior is non-deterministic for\nany given input, highlighting the critical need for robust debugging and\nobservability tools. In this work, we explore the use of process and causal\ndiscovery applied to agent execution trajectories as a means of enhancing\ndeveloper observability. This approach aids in monitoring and understanding the\nemergent variability in agent behavior. Additionally, we complement this with\nLLM-based static analysis techniques to distinguish between intended and\nunintended behavioral variability. We argue that such instrumentation is\nessential for giving developers greater control over evolving specifications\nand for identifying aspects of functionality that may require more precise and\nexplicit definitions.", "authors": ["Fabiana Fournier", "Lior Limonad", "Yuval David"], "published_date": "2025-05-26", "title_zh": "自主型人工智慧流程可觀測性：探索行為變異性", "summary_zh": "利用大型語言模型的人工智慧代理程式正迅速成為現代軟體系統的核心組件。目前有多種框架支援此類應用程式的規範。這些框架允許使用自然語言提示定義代理程式設定，指定角色、目標和分配給各代理程式的工具。在這些設定中，代理程式行為對於任何給定輸入都具有不確定性，突顯了對穩健偵錯和可觀察性工具的迫切需求。本研究探索將流程和因果發現應用於代理程式執行軌跡，以增強開發人員的可觀察性，協助監控和理解代理程式行為的突發變異。此外，我們輔以基於大型語言模型的靜態分析技術，以區分預期和非預期的行為變異。我們認為這種儀器化對於使開發人員能夠更好地控制不斷演進的規範，並識別可能需要更精確和明確定義的功能方面至關重要。", "audio": "audios/2505.20127v1.mp3", "timestamp": "2025-05-27T11:16:27.003734"}
{"query": "Foundation Model", "id": "2505.19625v1", "url": "http://arxiv.org/abs/2505.19625v1", "title": "Search-Based Software Engineering in the Landscape of AI Foundation Models", "summary": "Search-based software engineering (SBSE), at the intersection of artificial\nintelligence (AI) and software engineering, has been an active area of research\nfor about 25 years. It has been applied to solve numerous problems across the\nentire software engineering lifecycle and has demonstrated its versatility in\nmultiple domains. With the recent advancements in AI, particularly the\nemergence of foundation models (FMs), the evolution of SBSE alongside FMs\nremains undetermined. In this window of opportunity, we propose a research\nroadmap that articulates the current landscape of SBSE in relation to\nfoundation models (FMs), highlights open challenges, and outlines potential\nresearch directions for advancing SBSE through its interplay with FMs. This\nroadmap aims to establish a forward-thinking and innovative perspective for the\nfuture of SBSE in the era of FMs.", "authors": ["Hassan Sartaj", "Shaukat Ali"], "published_date": "2025-05-26", "title_zh": "基於搜尋的軟體工程於人工智慧基礎模型之脈絡下", "summary_zh": "基於搜尋的軟體工程（SBSE）結合人工智慧（AI）與軟體工程，已活躍研究約25年，廣泛應用於軟體工程生命週期各階段。隨著AI特別是基礎模型（FMs）的發展，SBSE與FMs的協同演進尚不明確。本文提出研究藍圖，闡述SBSE與FMs的現況，點明未解難題，並概述透過FMs推進SBSE的潛在研究方向，旨在為FMs時代的SBSE建立具前瞻性的創新視野。", "audio": "audios/2505.19625v1.mp3", "timestamp": "2025-05-27T11:16:31.140183"}
{"query": "Diffusion Model", "id": "2505.19868v1", "url": "http://arxiv.org/abs/2505.19868v1", "title": "Harnessing the Power of Training-Free Techniques in Text-to-2D Generation for Text-to-3D Generation via Score Distillation Sampling", "summary": "Recent studies show that simple training-free techniques can dramatically\nimprove the quality of text-to-2D generation outputs, e.g. Classifier-Free\nGuidance (CFG) or FreeU. However, these training-free techniques have been\nunderexplored in the lens of Score Distillation Sampling (SDS), which is a\npopular and effective technique to leverage the power of pretrained text-to-2D\ndiffusion models for various tasks. In this paper, we aim to shed light on the\neffect such training-free techniques have on SDS, via a particular application\nof text-to-3D generation via 2D lifting. We present our findings, which show\nthat varying the scales of CFG presents a trade-off between object size and\nsurface smoothness, while varying the scales of FreeU presents a trade-off\nbetween texture details and geometric errors. Based on these findings, we\nprovide insights into how we can effectively harness training-free techniques\nfor SDS, via a strategic scaling of such techniques in a dynamic manner with\nrespect to the timestep or optimization iteration step. We show that using our\nproposed scheme strikes a favorable balance between texture details and surface\nsmoothness in text-to-3D generations, while preserving the size of the output\nand mitigating the occurrence of geometric defects.", "authors": ["Junhong Lee", "Seungwook Kim", "Minsu Cho"], "published_date": "2025-05-26", "title_zh": "藉由分數蒸餾取樣在文本到三維生成中利用免訓練技術於文本到二維生成的力量", "summary_zh": "近期研究表明，無需訓練的簡便技巧能顯著提升文生圖品質，如無分類器引導（CFG）或FreeU。然這些技巧在分數蒸餾抽樣（SDS）中尚未被充分探索，SDS為利用預訓練文生圖擴散模型之熱門有效方法。本文旨在探討此類技巧對SDS之影響，特別是在透過2D提升實現文生3D生成之應用中。研究顯示，調整CFG尺度會在物件大小與表面平滑度間產生權衡，調整FreeU尺度則在紋理細節與幾何誤差間產生權衡。基於此，本文深入探討如何有效利用無需訓練之技巧於SDS中，透過動態調整尺度，並提出策略性縮放方案，能在文生3D生成中，於紋理細節與表面平滑度間取得良好平衡，同時維持輸出大小並減少幾何缺陷。", "audio": "audios/2505.19868v1.mp3", "timestamp": "2025-05-27T11:16:37.932775"}
{"query": "AI", "id": "2505.20181v1", "url": "http://arxiv.org/abs/2505.20181v1", "title": "The Problem of Algorithmic Collisions: Mitigating Unforeseen Risks in a Connected World", "summary": "The increasing deployment of Artificial Intelligence (AI) and other\nautonomous algorithmic systems presents the world with new systemic risks.\nWhile focus often lies on the function of individual algorithms, a critical and\nunderestimated danger arises from their interactions, particularly when\nalgorithmic systems operate without awareness of each other, or when those\ndeploying them are unaware of the full algorithmic ecosystem deployment is\noccurring in. These interactions can lead to unforeseen, rapidly escalating\nnegative outcomes - from market crashes and energy supply disruptions to\npotential physical accidents and erosion of public trust - often exceeding the\nhuman capacity for effective monitoring and the legal capacities for proper\nintervention. Current governance frameworks are inadequate as they lack\nvisibility into this complex ecosystem of interactions. This paper outlines the\nnature of this challenge and proposes some initial policy suggestions centered\non increasing transparency and accountability through phased system\nregistration, a licensing framework for deployment, and enhanced monitoring\ncapabilities.", "authors": ["Maurice Chiodo", "Dennis Müller"], "published_date": "2025-05-26", "title_zh": "演算法碰撞問題：減緩互聯世界中未預見的風險", "summary_zh": "人工智慧與自主演算法系統的廣泛應用帶來新的系統性風險。相較於個別演算法功能，演算法間的互動構成更嚴峻的潛在威脅，尤其在系統缺乏互知或部署者未充分掌握演算法生態系的情況下，可能導致難以預測且迅速升級的負面後果，如市場崩盤、能源供應中斷、物理事故及公眾信任瓦解，超出人類監測能力與法律干預範圍。現行治理框架缺乏對此複雜互動生態系的能見度，故本文闡述此挑戰，並提出初步政策建議，聚焦於透過分階段系統註冊、部署許可框架及強化監測能力，提升透明度與問責性。", "audio": "audios/2505.20181v1.mp3", "timestamp": "2025-05-27T13:31:42.869913"}
{"query": "Foundation Model", "id": "2505.19606v1", "url": "http://arxiv.org/abs/2505.19606v1", "title": "Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically", "summary": "Cross-lingual alignment in pretrained language models (LMs) has enabled\nefficient transfer in text-based LMs. Such an alignment has also been observed\nin speech foundation models. However, it remains an open question whether\nfindings and methods from text-based cross-lingual alignment apply to speech.\nBuilding on prior work on spoken translation retrieval, we perform\npronunciation-controlled experiments to observe if cross-lingual alignment can\nindeed occur in such models on a semantic basis, instead of relying on phonetic\nsimilarities. Our findings indicate that even in the absence of phonetic cues,\nspoken translation retrieval accuracy remains relatively stable. We follow up\nwith a controlled experiment on a word-level dataset of cross-lingual synonyms\nand near-homophones, confirming the existence of both phonetic and semantic\nknowledge in the encoder. Finally, we qualitatively examine the transcriptions\nproduced by early exiting the encoder, where we observe that speech translation\nproduces semantic errors that are characterized by phonetic similarities to\ncorresponding words in the source language. We apply this insight from early\nexiting to speech recognition in seven low-resource languages unsupported by\nthe Whisper model, and achieve improved accuracy in all languages examined,\nparticularly for languages with transparent orthographies.", "authors": ["Ryan Soh-Eun Shim", "Domenico De Cristofaro", "Chengzhi Martin Hu", "Alessandro Vietti", "Barbara Plank"], "published_date": "2025-05-26", "title_zh": "多語語音基礎模型中，語言在音韻和語義層面的對齊", "summary_zh": "預訓練語言模型中的跨語言對齊已促進文本模型的有效遷移。語音基礎模型中也觀察到類似對齊現象，但文本相關研究成果是否適用於語音領域仍待驗證。本研究基於口語翻譯檢索，透過發音控制實驗，探討跨語言對齊是否能在語義層面發生，而非僅依賴語音相似性。研究發現，即使缺乏語音線索，口語翻譯檢索準確率仍相對穩定。進一步針對跨語言同義詞和近音同形異義詞進行詞級實驗，證實編碼器同時具備語音和語義知識。分析編碼器早期輸出的轉錄文本，發現語音翻譯產生語義錯誤，且錯誤與源語言中的發音相似詞相關。運用此發現，改善了Whisper模型未支援的七種低資源語言的語音辨識，特別是對於拼寫透明度高的語言，準確率顯著提升。", "audio": "audios/2505.19606v1.mp3", "timestamp": "2025-05-27T13:31:49.781253"}
{"query": "Diffusion Model", "id": "2505.19835v1", "url": "http://arxiv.org/abs/2505.19835v1", "title": "On a retarded stochastic system with discrete diffusion modeling life tables", "summary": "This work proposes a method for modeling and forecasting mortality rates. It\nconstitutes an improvement over previous studies by incorporating both the\nhistorical evolution of the mortality phenomenon and its random behavior. In\nthe first part, we introduce the model and analyze mathematical properties such\nas the existence of solutions and their asymptotic behavior. In the second\npart, we apply this model to forecast mortality rates in Spain, showing that it\nyields better results than classical methods.", "authors": ["Tomás Caraballo", "Francisco Morillas", "José Valero"], "published_date": "2025-05-26", "title_zh": "具離散擴散的滯後隨機系統在生命表建模中的應用", "summary_zh": "本研究提出一種死亡率建模與預測方法，透過整合歷史演變趨勢與隨機變異，優化既有研究。第一部分闡述模型並分析其數學特性，如解的存在性及漸近行為。第二部分將此模型應用於西班牙死亡率預測，結果顯示其效能優於傳統方法。", "audio": "audios/2505.19835v1.mp3", "timestamp": "2025-05-27T13:31:53.088075"}
{"query": "AI", "id": "2505.20120v1", "url": "http://arxiv.org/abs/2505.20120v1", "title": "Agents Require Metacognitive and Strategic Reasoning to Succeed in the Coming Labor Markets", "summary": "Current labor markets are strongly affected by the economic forces of adverse\nselection, moral hazard, and reputation, each of which arises due to\n$\\textit{incomplete information}$. These economic forces will still be\ninfluential after AI agents are introduced, and thus, agents must use\nmetacognitive and strategic reasoning to perform effectively. Metacognition is\na form of $\\textit{internal reasoning}$ that includes the capabilities for\nself-assessment, task understanding, and evaluation of strategies. Strategic\nreasoning is $\\textit{external reasoning}$ that covers holding beliefs about\nother participants in the labor market (e.g., competitors, colleagues), making\nstrategic decisions, and learning about others over time. Both types of\nreasoning are required by agents as they decide among the many\n$\\textit{actions}$ they can take in labor markets, both within and outside\ntheir jobs. We discuss current research into metacognitive and strategic\nreasoning and the areas requiring further development.", "authors": ["Simpson Zhang", "Tennison Liu", "Mihaela van der Schaar"], "published_date": "2025-05-26", "title_zh": "代理人需具備後設認知與策略性推理能力方能在未來勞動市場中取得成功", "summary_zh": "當前勞動市場深受逆向選擇、道德風險和聲譽等不完全資訊經濟力量影響。即使引入人工智慧代理人後，這些力量依然存在，因此代理人必須運用後設認知和策略性推理才能有效工作。後設認知是一種內部推理，包含自我評估、任務理解和策略評估能力。策略性推理是一種外部推理，涵蓋對勞動市場參與者的信念（如競爭者、同事）、制定策略決策和隨時間推移了解他人。代理人在決定勞動市場內外可採取的多種行動時，需要這兩種推理。本文探討後設認知和策略性推理的現有研究，並指出需要進一步發展的領域。", "audio": "audios/2505.20120v1.mp3", "timestamp": "2025-05-27T14:18:23.991105"}
{"query": "Foundation Model", "id": "2505.19502v1", "url": "http://arxiv.org/abs/2505.19502v1", "title": "CODE-DITING: A Reasoning-Based Metric for Functional Alignment in Code Evaluation", "summary": "Trustworthy evaluation methods for code snippets play a crucial role in\nneural code generation. Traditional methods, which either rely on reference\nsolutions or require executable test cases, have inherent limitation in\nflexibility and scalability. The recent LLM-as-Judge methodology offers a\npromising alternative by directly evaluating functional consistency between the\nproblem description and the generated code. To systematically understand the\nlandscape of these LLM-as-Judge methods, we conduct a comprehensive empirical\nstudy across three diverse datasets. Our investigation reveals the pros and\ncons of two categories of LLM-as-Judge methods: the methods based on general\nfoundation models can achieve good performance but require complex prompts and\nlack explainability, while the methods based on reasoning foundation models\nprovide better explainability with simpler prompts but demand substantial\ncomputational resources due to their large parameter sizes. To address these\nlimitations, we propose CODE-DITING, a novel code evaluation method that\nbalances accuracy, efficiency and explainability. We develop a data\ndistillation framework that effectively transfers reasoning capabilities from\nDeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing\nevaluation explainability and reducing the computational cost. With the\nmajority vote strategy in the inference process, CODE-DITING 1.5B outperforms\nall models with the same magnitude of parameters and achieves performance which\nwould normally exhibit in a model with 5 times of parameter scale. CODE-DITING\n7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the\nparameter volume of these large models. Further experiments show that\nCODEDITING is robust to preference leakage and can serve as a promising\nalternative for code evaluation.", "authors": ["Guang Yang", "Yu Zhou", "Xiang Chen", "Wei Zheng", "Xing Hu", "Xin Zhou", "David Lo", "Taolue Chen"], "published_date": "2025-05-26", "title_zh": "程式碼校訂：一種基於推理的程式碼評估中功能對齊度量", "summary_zh": "程式碼片段的可信評估方法在神經程式碼生成中至關重要。傳統方法依賴參考解答或可執行測試案例，存在彈性和可擴展性的固有局限。新興的大型語言模型即評審方法透過直接評估問題描述與生成程式碼之間的功能一致性，提供了一種有前景的替代方案。為系統性地理解此類方法，我們針對三個不同資料集進行了全面的實證研究。研究揭示了兩類大型語言模型即評審方法的優缺點：基於通用基礎模型的方法能達到良好效能，但需要複雜的提示且缺乏可解釋性；基於推理基礎模型的方法提供較佳的可解釋性，使用較簡單的提示，但因其龐大的參數規模而需要大量的計算資源。為了解決這些限制，我們提出了 CODE-DITING，一種平衡準確性、效率和可解釋性的新型程式碼評估方法。我們開發了一個資料蒸餾框架，能有效地將 DeepSeek-R1671B 的推理能力轉移到我們的 CODE-DITING 1.5B 和 7B 模型，顯著提升了評估可解釋性並降低了計算成本。在推理過程中，CODE-DITING 1.5B 透過多數投票策略超越了所有相同參數規模的模型，並達到通常為五倍參數規模的模型才能展現的效能。CODE-DITING 7B 超越了 GPT-4o 和 DeepSeek-V3 671B，即便它僅使用了這些大型模型 1% 的參數量。進一步的實驗表明，CODEDITING 對偏好洩漏具有穩健性，可作為程式碼評估的一種有前景的替代方案。", "audio": "audios/2505.19502v1.mp3", "timestamp": "2025-05-27T14:18:34.080936"}
{"query": "Diffusion Model", "id": "2505.19769v1", "url": "http://arxiv.org/abs/2505.19769v1", "title": "TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning", "summary": "Developing scalable and generalizable reward engineering for reinforcement\nlearning (RL) is crucial for creating general-purpose agents, especially in the\nchallenging domain of robotic manipulation. While recent advances in reward\nengineering with Vision-Language Models (VLMs) have shown promise, their sparse\nreward nature significantly limits sample efficiency. This paper introduces\nTeViR, a novel method that leverages a pre-trained text-to-video diffusion\nmodel to generate dense rewards by comparing the predicted image sequence with\ncurrent observations. Experimental results across 11 complex robotic tasks\ndemonstrate that TeViR outperforms traditional methods leveraging sparse\nrewards and other state-of-the-art (SOTA) methods, achieving better sample\nefficiency and performance without ground truth environmental rewards. TeViR's\nability to efficiently guide agents in complex environments highlights its\npotential to advance reinforcement learning applications in robotic\nmanipulation.", "authors": ["Yuhui Chen", "Haoran Li", "Zhennan Jiang", "Haowei Wen", "Dongbin Zhao"], "published_date": "2025-05-26", "title_zh": "TeViR：藉由擴散模型實現文本到影片獎勵以提升強化學習效率", "summary_zh": "為使強化學習具備可擴展性和通用性，獎勵工程至關重要，尤其在機器人操作領域。儘管視覺語言模型在獎勵工程方面有所進展，但其稀疏獎勵特性限制了樣本效率。本研究提出TeViR，一種利用預訓練文字轉影片擴散模型生成密集獎勵的新方法，透過比較預測影像序列與當前觀測值來實現。在11項複雜機器人任務上的實驗結果表明，TeViR優於傳統的稀疏獎勵方法和其他最先進技術，在沒有真實環境獎勵的情況下，實現了更高的樣本效率和性能。TeViR有效引導智能體在複雜環境中操作的能力，突顯了其在推進機器人操作強化學習應用方面的潛力。", "audio": "audios/2505.19769v1.mp3", "timestamp": "2025-05-27T14:18:39.647470"}
{"query": "AI", "id": "2505.20158v1", "url": "http://arxiv.org/abs/2505.20158v1", "title": "Evaluating Software Plagiarism Detection in the Age of AI: Automated Obfuscation and Lessons for Academic Integrity", "summary": "Plagiarism in programming assignments is a persistent issue in computer\nscience education, increasingly complicated by the emergence of automated\nobfuscation attacks. While software plagiarism detectors are widely used to\nidentify suspicious similarities at scale and are resilient to simple\nobfuscation techniques, they are vulnerable to advanced obfuscation based on\nstructural modification of program code that preserves the original program\nbehavior. While different defense mechanisms have been proposed to increase\nresilience against these attacks, their current evaluation is limited to the\nscope of attacks used and lacks a comprehensive investigation regarding\nAI-based obfuscation. In this paper, we investigate the resilience of these\ndefense mechanisms against a broad range of automated obfuscation attacks,\nincluding both algorithmic and AI-generated methods, and for a wide variety of\nreal-world datasets. We evaluate the improvements of two defense mechanisms\nover the plagiarism detector JPlag across over four million pairwise program\ncomparisons. Our results show significant improvements in detecting obfuscated\nplagiarism instances, and we observe an improved detection of AI-generated\nprograms, even though the defense mechanisms are not designed for this use\ncase. Based on our findings, we provide an in-depth discussion of their broader\nimplications for academic integrity and the role of AI in education.", "authors": ["Timur Sağlam", "Larissa Schmid"], "published_date": "2025-05-26", "title_zh": "人工智能時代下評估軟體抄襲偵測：自動混淆與學術誠信之教訓", "summary_zh": "程式作業抄襲是電腦科學教育中長期存在的問題，自動混淆攻擊的出現使其更趨複雜。軟體抄襲偵測器雖廣泛用於大規模識別可疑相似性，且能抵抗簡單混淆技巧，但易受基於程式碼結構修改的高級混淆攻擊影響。現有防禦機制雖旨在提高對抗此類攻擊的韌性，但評估範圍僅限於特定攻擊，缺乏對基於人工智慧混淆的全面研究。本文針對一系列自動混淆攻擊（包括演算法和人工智慧生成方法）以及各種真實世界資料集，研究這些防禦機制的韌性。我們評估了兩種防禦機制相較於JPlag的改進，涉及超過四百萬組程式碼對比。結果顯示，在偵測被混淆的抄襲實例方面有顯著提升，並觀察到對人工智慧生成程式碼的偵測能力亦有所改善，儘管防禦機制並非專為此用例設計。基於研究結果，我們深入探討了其對學術誠信以及人工智慧在教育領域中所扮演角色的廣泛影響。", "audio": "audios/2505.20158v1.mp3", "timestamp": "2025-05-27T15:20:50.349751"}
{"query": "Foundation Model", "id": "2505.19447v1", "url": "http://arxiv.org/abs/2505.19447v1", "title": "A Contrastive Learning Foundation Model Based on Perfectly Aligned Sample Pairs for Remote Sensing Images", "summary": "Self-Supervised Learning (SSL) enables us to pre-train foundation models\nwithout costly labeled data. Among SSL methods, Contrastive Learning (CL)\nmethods are better at obtaining accurate semantic representations in noise\ninterference. However, due to the significant domain gap, while CL methods have\nachieved great success in many computer vision tasks, they still require\nspecific adaptation for Remote Sensing (RS) images. To this end, we present a\nnovel self-supervised method called PerA, which produces all-purpose RS\nfeatures through semantically Perfectly Aligned sample pairs. Specifically,\nPerA obtains features from sampled views by applying spatially disjoint masks\nto augmented images rather than random cropping. With disjoint masks, we divide\npatches from different views into different parts that are semantically aligned\nbut inconsistent in appearance. Our framework provides high-quality features by\nensuring consistency between teacher and student and predicting learnable mask\ntokens. Compared to previous contrastive methods, our method demonstrates\nhigher memory efficiency and can be trained with larger batches due to its\nsparse inputs. We also collect an unlabeled pre-training dataset, which\ncontains about 5 million RS images. We conducted experiments on multiple\ndownstream task datasets and achieved performance comparable to previous\nstate-of-the-art methods with a limited model scale, which verified the\nsuperiority of our method. We hope this work will contribute to practical\nremote sensing interpretation works.", "authors": ["Hengtong Shen", "Haiyan Gu", "Haitao Li", "Yi Yang", "Agen qiu"], "published_date": "2025-05-26", "title_zh": "基於完美對齊樣本對的遙感影像對比學習基礎模型", "summary_zh": "自監督學習無需標註資料即可預訓練基礎模型。對比學習方法在雜訊干擾下更能獲取精確的語義表示。然而，由於顯著的領域差距，對比學習方法在遙感影像上仍需特定調整。本文提出一種名為PerA的新型自監督方法，透過語義完美對齊的樣本對生成通用的遙感特徵。PerA對增強影像應用空間不相交的遮罩而非隨機裁剪，從而從採樣視圖中獲取特徵。透過不相交的遮罩，我們將來自不同視圖的圖塊劃分為語義對齊但外觀不一致的不同部分。我們的框架透過確保教師和學生之間的一致性並預測可學習的遮罩令牌來提供高品質特徵。相較於先前的對比方法，我們的記憶體效率更高，並且由於其稀疏輸入，因此可以使用更大的批次進行訓練。我們還收集了一個包含約五百萬張遙感影像的未標註預訓練資料集。在多個下游任務資料集上進行的實驗表明，在有限的模型規模下，我們的效能與先前的最先進方法相當，驗證了我們方法的優越性。期望此研究能對實際的遙感影像解譯工作有所貢獻。", "audio": "audios/2505.19447v1.mp3", "timestamp": "2025-05-27T15:21:01.847152"}
{"query": "Diffusion Model", "id": "2505.19765v1", "url": "http://arxiv.org/abs/2505.19765v1", "title": "On some coupled local and nonlocal diffusion models", "summary": "We study problems in which a local model is coupled with a nonlocal one. We\npropose two energies: both of them are based on the same classical weighted\n$H^1$-semi norm to model the local part, while two different weighted\n$H^s$-semi norms, with $s \\in (0,1)$, are used to model the nonlocal part. The\ncorresponding strong formulations are derived. In doing so, one needs to\ndevelop some technical tools, such as suitable integration by parts formulas\nfor operators with variable diffusivity, and one also needs to study the\nmapping properties of the Neumann operators that arise. In contrast to problems\ncoupling purely local models, in which one requires transmission conditions on\nthe interface between the subdomains, the presence of a nonlocal operator may\ngive rise to nonlocal fluxes. These nonlocal fluxes may enter the problem as a\nsource term, thereby changing its structure. Finally, we focus on a specific\nproblem, that we consider most relevant, and study regularity of solutions and\nfinite element discretizations. We provide numerical experiments to illustrate\nthe most salient features of the models.", "authors": ["Juan Pablo Borthagaray", "Patrick Ciarlet Jr"], "published_date": "2025-05-26", "title_zh": "關於某些耦合的局部與非局部擴散模型", "summary_zh": "本研究探討局部模型與非局部模型耦合問題。提出兩種能量模型，局部部分基於相同的加權$H^1$半範數，非局部部分則採用不同的加權$H^s$半範數，其中$s \\in (0,1)$。推導出相應的強公式，並發展了變擴散率算子的分部積分公式等技術工具，以及Neumann算子的映射性質。與純局部模型耦合問題不同，非局部算子的存在可能導致非局部通量，進而改變問題結構，表現為源項。最後，針對一個特定問題，研究其解的規律性和有限元素離散化，並通過數值實驗闡明模型的顯著特徵。", "audio": "audios/2505.19765v1.mp3", "timestamp": "2025-05-27T15:21:10.052596"}
{"query": "AI", "id": "2505.20096v1", "url": "http://arxiv.org/abs/2505.20096v1", "title": "MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning", "summary": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\neither end-to-end fine-tuning or isolated component enhancements, MA-RAG\norchestrates a collaborative set of specialized AI agents: Planner, Step\nDefiner, Extractor, and QA Agents, to tackle each stage of the RAG pipeline\nwith task-aware reasoning. Ambiguities may arise from underspecified queries,\nsparse or indirect evidence in retrieved documents, or the need to integrate\ninformation scattered across multiple sources. MA-RAG mitigates these\nchallenges by decomposing the problem into subtasks, such as query\ndisambiguation, evidence extraction, and answer synthesis, and dispatching them\nto dedicated agents equipped with chain-of-thought prompting. These agents\ncommunicate intermediate reasoning and progressively refine the retrieval and\nsynthesis process. Our design allows fine-grained control over information flow\nwithout any model fine-tuning. Crucially, agents are invoked on demand,\nenabling a dynamic and efficient workflow that avoids unnecessary computation.\nThis modular and reasoning-driven architecture enables MA-RAG to deliver\nrobust, interpretable results. Experiments on multi-hop and ambiguous QA\nbenchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free\nbaselines and rivals fine-tuned systems, validating the effectiveness of\ncollaborative agent-based reasoning in RAG.", "authors": ["Thang Nguyen", "Peter Chin", "Yu-Wing Tai"], "published_date": "2025-05-26", "title_zh": "MA-RAG：基於協作式思維鏈推理的多代理檢索增強生成", "summary_zh": "本研究提出多代理檢索增強生成框架（MA-RAG），旨在解決複雜資訊檢索任務中的模糊性和推理挑戰。不同於傳統方法，MA-RAG協調規劃、步驟定義、提取和問答等多個專業AI代理，以具備任務感知推理能力的方式處理檢索增強生成流程的各個階段。針對查詢不明確、證據稀疏或需整合多源資訊等問題，MA-RAG將問題分解為子任務，例如查詢消歧、證據提取和答案合成，並將其分派給配備思維鏈提示的專用代理。這些代理溝通中間推理過程，逐步完善檢索和合成。此設計允許對資訊流進行精細控制，無需模型微調。代理按需調用，實現動態高效的工作流程，避免不必要的計算。實驗結果表明，在多跳和模糊問答基準測試中，MA-RAG優於最先進的免訓練基線，並可與微調系統相媲美，驗證了基於協作代理的推理在檢索增強生成中的有效性。", "audio": "audios/2505.20096v1.mp3", "timestamp": "2025-05-27T16:23:43.685904"}
{"query": "Foundation Model", "id": "2505.19397v1", "url": "http://arxiv.org/abs/2505.19397v1", "title": "Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial Robustness Across Domains", "summary": "Time Series Foundation Models (TSFMs), which are pretrained on large-scale,\ncross-domain data and capable of zero-shot forecasting in new scenarios without\nfurther training, are increasingly adopted in real-world applications. However,\nas the zero-shot forecasting paradigm gets popular, a critical yet overlooked\nquestion emerges: Are TSFMs robust to adversarial input perturbations? Such\nperturbations could be exploited in man-in-the-middle attacks or data\npoisoning. To address this gap, we conduct a systematic investigation into the\nadversarial robustness of TSFMs. Our results show that even minimal\nperturbations can induce significant and controllable changes in forecast\nbehaviors, including trend reversal, temporal drift, and amplitude shift,\nposing serious risks to TSFM-based services. Through experiments on\nrepresentative TSFMs and multiple datasets, we reveal their consistent\nvulnerabilities and identify potential architectural designs, such as\nstructural sparsity and multi-task pretraining, that may improve robustness.\nOur findings offer actionable guidance for designing more resilient forecasting\nsystems and provide a critical assessment of the adversarial robustness of\nTSFMs.", "authors": ["Jiawen Zhang", "Zhenwei Zhang", "Shun Zheng", "Xumeng Wen", "Jia Li", "Jiang Bian"], "published_date": "2025-05-26", "title_zh": "時間序列基礎模型是否已準備好部署？跨領域對抗性穩健性的系統性研究", "summary_zh": "時間序列基礎模型(TSFMs)因其跨領域數據預訓練及零樣本預測能力而被廣泛採用。然而，TSFMs在對抗性輸入擾動下的穩健性問題日益重要。本研究系統性探討TSFMs的對抗性穩健性，結果表明即使是微小擾動也能顯著且可控地改變預測行為，如趨勢反轉、時間漂移和幅度偏移，對基於TSFM的服務構成嚴重風險。通過實驗，揭示了TSFMs的普遍脆弱性，並探討了結構稀疏性和多任務預訓練等潛在架構設計，以提升穩健性。研究結果為設計更具彈性的預測系統提供了指導，並對TSFMs的對抗性穩健性進行了關鍵評估。", "audio": "audios/2505.19397v1.mp3", "timestamp": "2025-05-27T16:23:49.114355"}
{"query": "Diffusion Model", "id": "2505.19751v1", "url": "http://arxiv.org/abs/2505.19751v1", "title": "SAIL: Self-supervised Albedo Estimation from Real Images with a Latent Diffusion Model", "summary": "Intrinsic image decomposition aims at separating an image into its underlying\nalbedo and shading components, isolating the base color from lighting effects\nto enable downstream applications such as virtual relighting and scene editing.\nDespite the rise and success of learning-based approaches, intrinsic image\ndecomposition from real-world images remains a significant challenging task due\nto the scarcity of labeled ground-truth data. Most existing solutions rely on\nsynthetic data as supervised setups, limiting their ability to generalize to\nreal-world scenes. Self-supervised methods, on the other hand, often produce\nalbedo maps that contain reflections and lack consistency under different\nlighting conditions. To address this, we propose SAIL, an approach designed to\nestimate albedo-like representations from single-view real-world images. We\nrepurpose the prior knowledge of a latent diffusion model for unconditioned\nscene relighting as a surrogate objective for albedo estimation. To extract the\nalbedo, we introduce a novel intrinsic image decomposition fully formulated in\nthe latent space. To guide the training of our latent diffusion model, we\nintroduce regularization terms that constrain both the lighting-dependent and\nindependent components of our latent image decomposition. SAIL predicts stable\nalbedo under varying lighting conditions and generalizes to multiple scenes,\nusing only unlabeled multi-illumination data available online.", "authors": ["Hala Djeghim", "Nathan Piasco", "Luis Roldão", "Moussab Bennehar", "Dzmitry Tsishkou", "Céline Loscos", "Désiré Sidibé"], "published_date": "2025-05-26", "title_zh": "SAIL：藉由潛在擴散模型從真實圖像進行自我監督反照率估計", "summary_zh": "內在圖像分解旨在將圖像分解為反照率和陰影成分，從光照效果中分離出基本顏色，以實現虛擬重照明和場景編輯等下游應用。儘管基於學習的方法有所興起和成功，但由於缺乏標記的真實數據，從真實圖像中進行內在圖像分解仍然是一項重大挑戰。現有解決方案大多依賴合成數據作為監督設置，限制了其推廣到真實場景的能力。另一方面，自監督方法通常會產生包含反射且在不同光照條件下缺乏一致性的反照率圖。為了解決這個問題，我們提出 SAIL，一種旨在從單視角真實圖像估計類反照率表示的方法。我們將潛在擴散模型在無條件場景重照明中的先驗知識重新用於反照率估計的替代目標。為了提取反照率，我們提出了一種完全在潛在空間中制定的新型內在圖像分解。為了指導我們潛在擴散模型的訓練，我們引入了正則化項，以約束我們潛在圖像分解中光照相關和獨立的成分。SAIL 在不同的光照條件下預測穩定的反照率，並推廣到多個場景，僅使用在線提供的未標記多照明數據。", "audio": "audios/2505.19751v1.mp3", "timestamp": "2025-05-27T16:23:57.490623"}
{"query": "AI", "id": "2505.20085v1", "url": "http://arxiv.org/abs/2505.20085v1", "title": "Explanation User Interfaces: A Systematic Literature Review", "summary": "Artificial Intelligence (AI) is one of the major technological advancements\nof this century, bearing incredible potential for users through AI-powered\napplications and tools in numerous domains. Being often black-box (i.e., its\ndecision-making process is unintelligible), developers typically resort to\neXplainable Artificial Intelligence (XAI) techniques to interpret the behaviour\nof AI models to produce systems that are transparent, fair, reliable, and\ntrustworthy. However, presenting explanations to the user is not trivial and is\noften left as a secondary aspect of the system's design process, leading to AI\nsystems that are not useful to end-users. This paper presents a Systematic\nLiterature Review on Explanation User Interfaces (XUIs) to gain a deeper\nunderstanding of the solutions and design guidelines employed in the academic\nliterature to effectively present explanations to users. To improve the\ncontribution and real-world impact of this survey, we also present a framework\nfor Human-cEnteRed developMent of Explainable user interfaceS (HERMES) to guide\npractitioners and academics in the design and evaluation of XUIs.", "authors": ["Eleonora Cappuccio", "Andrea Esposito", "Francesco Greco", "Giuseppe Desolda", "Rosa Lanzilotti", "Salvatore Rinzivillo"], "published_date": "2025-05-26", "title_zh": "解釋型使用者介面：系統性文獻回顧", "summary_zh": "人工智慧是本世紀重大科技進展之一，於多領域的應用程式和工具中具備巨大潛力。由於其決策過程通常難以理解，開發者常藉由可解釋人工智慧（XAI）技術來詮釋模型行為，以打造透明、公平、可靠且值得信賴的系統。然而，向使用者呈現解釋並非易事，且常被視為系統設計的次要環節，導致人工智慧系統對終端使用者無用。本文針對解釋使用者介面（XUI）進行系統性文獻回顧，以深入了解學術文獻中為有效向使用者呈現解釋而採用的解決方案與設計指南。為提升本研究的貢獻與實際影響力，我們亦提出以人為本的可解釋使用者介面開發框架（HERMES），以指導從業者與學者進行XUI的設計與評估。", "audio": "audios/2505.20085v1.mp3", "timestamp": "2025-05-27T17:16:57.575517"}
{"query": "Foundation Model", "id": "2505.19390v1", "url": "http://arxiv.org/abs/2505.19390v1", "title": "Foundation Model for Wireless Technology Recognition Using IQ Timeseries", "summary": "Wireless Technology Recognition (WTR) is essential in modern communication\nsystems, enabling efficient spectrum management and the seamless coexistence of\ndiverse technologies. In real-world conditions, WTR solutions should be able to\nhandle signals from various resources with different sampling rates, capturing\ndevices, and frequency bands. However, traditional WTR methods, which rely on\nenergy detection, Convolutional Neural Network (CNN) models, or Deep Learning\n(DL), lack the robustness and adaptability required to generalize across unseen\nenvironments, different sampling devices, and previously unencountered signal\nclasses. In this work, we introduce a Transformer-based foundation model for\nWTR, trained in an unsupervised manner on large-scale, unlabeled wireless\nsignal datasets. Foundation models are designed to learn general-purpose\nrepresentations that transfer effectively across tasks and domains, allowing\ngeneralization towards new technologies and WTR sampling devices. Our approach\nleverages input patching for computational efficiency and incorporates a\ntwo-stage training pipeline: unsupervised pre-training followed by lightweight\nfine-tuning. This enables the model to generalize to new wireless technologies\nand environments using only a small number of labeled samples. Experimental\nresults demonstrate that our model achieves superior accuracy across varying\nsampling rates and frequency bands while maintaining low computational\ncomplexity, supporting the vision of a reusable wireless foundation model\nadaptable to new technologies with minimal retraining.", "authors": ["Mohammad Cheraghinia", "Eli De Poorter", "Jaron Fontaine", "Merouane Debbah", "Adnan Shahid"], "published_date": "2025-05-26", "title_zh": "基於IQ時序的無線技術識別基礎模型", "summary_zh": "無線技術識別(WTR)對現代通訊系統至關重要，能實現高效頻譜管理與異質技術共存。現有WTR方法，如能量檢測、CNN或深度學習，在處理不同取樣率、捕獲設備及頻段訊號時缺乏足夠的穩健性和適應性。本研究提出一種基於Transformer的WTR基礎模型，該模型透過大規模未標記無線訊號資料集進行無監督訓練。基礎模型旨在學習通用表徵，有效遷移至不同任務和領域，進而推廣至新技術和取樣設備。此方法利用輸入修補提高計算效率，並採用兩階段訓練流程：無監督預訓練與輕量微調。實驗結果表明，該模型在不同取樣率和頻段下均能實現卓越準確度，且保持低計算複雜度，支援可重複使用的無線基礎模型願景，能以最少量的重新訓練適應新技術。", "audio": "audios/2505.19390v1.mp3", "timestamp": "2025-05-27T17:17:04.918991"}
{"query": "Diffusion Model", "id": "2505.19717v1", "url": "http://arxiv.org/abs/2505.19717v1", "title": "Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning", "summary": "Imitation learning is a promising approach for enabling generalist\ncapabilities in humanoid robots, but its scaling is fundamentally constrained\nby the scarcity of high-quality expert demonstrations. This limitation can be\nmitigated by leveraging suboptimal, open-ended play data, often easier to\ncollect and offering greater diversity. This work builds upon recent advances\nin generative modeling, specifically Flow Matching, an alternative to Diffusion\nmodels. We introduce a method for estimating the extremum of the learned\ndistribution by leveraging the unique properties of Flow Matching, namely,\ndeterministic transport and support for arbitrary source distributions. We\napply this method to develop several goal-conditioned imitation and\nreinforcement learning algorithms based on Flow Matching, where policies are\nconditioned on both current and goal observations. We explore and compare\ndifferent architectural configurations by combining core components, such as\ncritic, planner, actor, or world model, in various ways. We evaluated our\nagents on the OGBench benchmark and analyzed how different demonstration\nbehaviors during data collection affect performance in a 2D non-prehensile\npushing task. Furthermore, we validated our approach on real hardware by\ndeploying it on the Talos humanoid robot to perform complex manipulation tasks\nbased on high-dimensional image observations, featuring a sequence of\npick-and-place and articulated object manipulation in a realistic kitchen\nenvironment. Experimental videos and code are available at:\nhttps://hucebot.github.io/extremum_flow_matching_website/", "authors": ["Quentin Rouxel", "Clemente Donoso", "Fei Chen", "Serena Ivaldi", "Jean-Baptiste Mouret"], "published_date": "2025-05-26", "title_zh": "離線目標條件強化學習之極值流匹配", "summary_zh": "模仿學習有望賦予人形機器人通用能力，但其擴展受限於高品質專家示範的稀缺性。利用次優、開放式探索數據可緩解此限制，因其更易收集且更具多樣性。本研究基於生成模型Flow Matching的最新進展，提出一種利用Flow Matching獨有特性（確定性傳輸及對任意源分布的支持）來估計學習分布極值的方法。我們將此方法應用於開發基於Flow Matching的目標條件模仿學習和強化學習演算法，策略受當前和目標觀察條件約束。我們探索並比較了不同架構配置，透過各種方式組合評論家、規劃器、執行者或世界模型等核心組件。我們在OGBench基準上評估了智能體，並分析了數據收集期間不同示範行為如何影響2D非抓取推動任務的性能。此外，我們在真實硬體Talos人形機器人上驗證了該方法，使其能夠在高維度圖像觀察基礎上執行複雜的操作任務，包括在真實廚房環境中的一系列拾取放置和關節物體操作。", "audio": "audios/2505.19717v1.mp3", "timestamp": "2025-05-27T17:17:12.366613"}
{"query": "AI", "id": "2505.20075v1", "url": "http://arxiv.org/abs/2505.20075v1", "title": "Curriculum-RLAIF: Curriculum Alignment with Reinforcement Learning from AI Feedback", "summary": "Reward models trained with conventional Reinforcement Learning from AI\nFeedback (RLAIF) methods suffer from limited generalizability, which hinders\nthe alignment performance of the policy model during reinforcement learning\n(RL). This challenge stems from various issues, including distribution shift,\npreference label noise, and mismatches between overly challenging samples and\nmodel capacity. In this paper, we attempt to enhance the generalizability of\nreward models through a data-centric approach, driven by the insight that these\nissues are inherently intertwined from the perspective of data difficulty. To\naddress this, we propose a novel framework, $\\textit{Curriculum-RLAIF}$, which\nconstructs preference pairs with varying difficulty levels and produces a\ncurriculum that progressively incorporates preference pairs of increasing\ndifficulty for reward model training. Our experimental results suggest that\nreward models trained with Curriculum-RLAIF achieve improved generalizability,\nsignificantly increasing the alignment performance of the policy model by a\nlarge margin without incurring additional inference costs compared to various\nnon-curriculum baselines. Detailed analysis and comparisons with alternative\napproaches, including data selection via external pretrained reward models or\ninternal self-selection mechanisms, as well as other curriculum strategies,\nfurther demonstrate the superiority of our approach in terms of simplicity,\nefficiency, and effectiveness.", "authors": ["Mengdi Li", "Jiaye Lin", "Xufeng Zhao", "Wenhao Lu", "Peilin Zhao", "Stefan Wermter", "Di Wang"], "published_date": "2025-05-26", "title_zh": "課程-RLAIF：基於AI回饋的強化學習之課程對齊", "summary_zh": "傳統AI回饋強化學習（RLAIF）訓練的獎勵模型泛化能力有限，影響策略模型於強化學習中的對齊效果。此挑戰源於分佈偏移、偏好標籤雜訊以及過於困難的樣本與模型能力不匹配等問題。本研究從資料難度的角度出發，透過資料中心方法提升獎勵模型的泛化能力。我們提出$\\textit{Curriculum-RLAIF}$框架，構建不同難度的偏好對，並生成課程表，逐步將難度遞增的偏好對納入獎勵模型訓練。實驗結果表明，相較於非課程基準模型，採用Curriculum-RLAIF訓練的獎勵模型具有更佳的泛化能力，顯著提升策略模型的對齊效果，且不增加額外推論成本。與其他方法（包括基於外部預訓練獎勵模型的資料選擇、內部自選擇機制及其他課程策略）的詳細分析和比較，進一步驗證了我們方法的簡潔性、效率和有效性。", "audio": "audios/2505.20075v1.mp3", "timestamp": "2025-05-27T18:25:32.990950"}
{"query": "Foundation Model", "id": "2505.19306v1", "url": "http://arxiv.org/abs/2505.19306v1", "title": "From Single Images to Motion Policies via Video-Generation Environment Representations", "summary": "Autonomous robots typically need to construct representations of their\nsurroundings and adapt their motions to the geometry of their environment.\nHere, we tackle the problem of constructing a policy model for collision-free\nmotion generation, consistent with the environment, from a single input RGB\nimage. Extracting 3D structures from a single image often involves monocular\ndepth estimation. Developments in depth estimation have given rise to large\npre-trained models such as DepthAnything. However, using outputs of these\nmodels for downstream motion generation is challenging due to frustum-shaped\nerrors that arise. Instead, we propose a framework known as Video-Generation\nEnvironment Representation (VGER), which leverages the advances of large-scale\nvideo generation models to generate a moving camera video conditioned on the\ninput image. Frames of this video, which form a multiview dataset, are then\ninput into a pre-trained 3D foundation model to produce a dense point cloud. We\nthen introduce a multi-scale noise approach to train an implicit representation\nof the environment structure and build a motion generation model that complies\nwith the geometry of the representation. We extensively evaluate VGER over a\ndiverse set of indoor and outdoor environments. We demonstrate its ability to\nproduce smooth motions that account for the captured geometry of a scene, all\nfrom a single RGB input image.", "authors": ["Weiming Zhi", "Ziyong Ma", "Tianyi Zhang", "Matthew Johnson-Roberson"], "published_date": "2025-05-25", "title_zh": "基於影片生成環境表徵，從單張圖像到運動策略", "summary_zh": "自主機器人需構建環境表徵並調整運動適應幾何結構。本研究針對單張RGB圖像，提出構建與環境一致的無碰撞運動生成策略模型。為克服單目深度估計產生的錐形誤差，本研究提出視訊生成環境表徵（VGER）框架，利用大規模視訊生成模型，基於輸入圖像生成移動相機視訊。此視訊幀構成多視圖數據集，輸入至預訓練的3D基礎模型以產生密集點雲。進而，導入多尺度雜訊方法訓練環境結構的隱式表徵，並構建符合幾何結構的運動生成模型。VGER在多樣室內外環境中經過廣泛評估，證實其能從單張RGB圖像生成平滑運動，並考量場景的幾何結構。", "audio": "audios/2505.19306v1.mp3", "timestamp": "2025-05-27T18:25:38.978453"}
{"query": "Diffusion Model", "id": "2505.19694v1", "url": "http://arxiv.org/abs/2505.19694v1", "title": "Knowledge-Aligned Counterfactual-Enhancement Diffusion Perception for Unsupervised Cross-Domain Visual Emotion Recognition", "summary": "Visual Emotion Recognition (VER) is a critical yet challenging task aimed at\ninferring emotional states of individuals based on visual cues. However,\nexisting works focus on single domains, e.g., realistic images or stickers,\nlimiting VER models' cross-domain generalizability. To fill this gap, we\nintroduce an Unsupervised Cross-Domain Visual Emotion Recognition (UCDVER)\ntask, which aims to generalize visual emotion recognition from the source\ndomain (e.g., realistic images) to the low-resource target domain (e.g.,\nstickers) in an unsupervised manner. Compared to the conventional unsupervised\ndomain adaptation problems, UCDVER presents two key challenges: a significant\nemotional expression variability and an affective distribution shift. To\nmitigate these issues, we propose the Knowledge-aligned\nCounterfactual-enhancement Diffusion Perception (KCDP) framework. Specifically,\nKCDP leverages a VLM to align emotional representations in a shared knowledge\nspace and guides diffusion models for improved visual affective perception.\nFurthermore, a Counterfactual-Enhanced Language-image Emotional Alignment\n(CLIEA) method generates high-quality pseudo-labels for the target domain.\nExtensive experiments demonstrate that our model surpasses SOTA models in both\nperceptibility and generalization, e.g., gaining 12% improvements over the SOTA\nVER model TGCA-PVT. The project page is at https://yinwen2019.github.io/ucdver.", "authors": ["Wen Yin", "Yong Wang", "Guiduo Duan", "Dongyang Zhang", "Xin Hu", "Yuan-Fang Li", "Tao He"], "published_date": "2025-05-26", "title_zh": "知識對齊反事實增強擴散感知用於無監督跨域視覺情感識別", "summary_zh": "視覺情緒辨識是根據視覺線索推斷個體情緒狀態的重要任務，但現有研究多集中於單一領域，限制了模型的跨域泛化能力。為此，本文提出非監督跨域視覺情緒辨識任務，旨在將視覺情緒辨識從源域（如真實圖像）泛化到低資源目標域（如貼圖）。相較於傳統非監督域適應問題，此任務面臨情緒表達變異性和情感分布偏移兩大挑戰。本文提出知識對齊反事實增強擴散感知框架，利用視覺語言模型在共享知識空間中對齊情緒表徵，並引導擴散模型以提升視覺情感感知。此外，反事實增強的語言圖像情感對齊方法可為目標域生成高品質偽標籤。實驗結果表明，本文模型在感知力和泛化能力上均優於現有最佳模型，例如，相較於最佳視覺情緒辨識模型 TGCA-PVT，效能提升達 12%。項目網頁：https://yinwen2019.github.io/ucdver。", "audio": "audios/2505.19694v1.mp3", "timestamp": "2025-05-27T18:25:46.563092"}
{"query": "AI", "id": "2505.20068v1", "url": "http://arxiv.org/abs/2505.20068v1", "title": "On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction", "summary": "Shared understanding plays a key role in the effective communication in and\nperformance of human-human interactions. With the increasingly common\nintegration of AI into human contexts, the future of personal and workplace\ninteractions will likely see human-AI interaction (HAII) in which the\nperception of shared understanding is important. Existing literature has\naddressed the processes and effects of PSU in human-human interactions, but the\nconstrual remains underexplored in HAII. To better understand PSU in HAII, we\nconducted an online survey to collect user reflections on interactions with a\nlarge language model when it sunderstanding of a situation was thought to be\nsimilar to or different from the participant's. Through inductive thematic\nanalysis, we identified eight dimensions comprising PSU in human-AI\ninteractions: Fluency, aligned operation, fluidity, outcome satisfaction,\ncontextual awareness, lack of humanlike abilities, computational limits, and\nsuspicion.", "authors": ["Qingyu Liang", "Jaime Banks"], "published_date": "2025-05-26", "title_zh": "同頁共識：人機互動中感知共享理解之維度", "summary_zh": "共享理解在人際互動的有效溝通和表現中至關重要。隨著人工智慧日益融入人類環境，人機互動中對共享理解的感知也變得重要。現有文獻已探討人際互動中共享理解的過程和影響，但在人機互動中的建構仍未被充分研究。為更好地理解人機互動中的共享理解，我們進行了一項線上調查，收集使用者在使用大型語言模型時的互動反思，當時該模型對情況的理解被認為與參與者相似或不同。透過歸納主題分析，我們識別了人機互動中共享理解的八個維度：流暢性、一致操作、流動性、結果滿意度、情境感知、缺乏類人能力、計算限制和懷疑。", "audio": "audios/2505.20068v1.mp3", "timestamp": "2025-05-27T19:14:32.996372"}
{"query": "Foundation Model", "id": "2505.19218v1", "url": "http://arxiv.org/abs/2505.19218v1", "title": "Advancing Video Self-Supervised Learning via Image Foundation Models", "summary": "In the past decade, image foundation models (IFMs) have achieved\nunprecedented progress. However, the potential of directly using IFMs for video\nself-supervised representation learning has largely been overlooked. In this\nstudy, we propose an advancing video self-supervised learning (AdViSe)\napproach, aimed at significantly reducing the training overhead of video\nrepresentation models using pre-trained IFMs. Specifically, we first introduce\ntemporal modeling modules (ResNet3D) to IFMs, constructing a video\nrepresentation model. We then employ a video self-supervised learning approach,\nplayback rate perception, to train temporal modules while freezing the IFM\ncomponents. Experiments on UCF101 demonstrate that AdViSe achieves performance\ncomparable to state-of-the-art methods while reducing training time by\n$3.4\\times$ and GPU memory usage by $8.2\\times$. This study offers fresh\ninsights into low-cost video self-supervised learning based on pre-trained\nIFMs. Code is available at https://github.com/JingwWu/advise-video-ssl.", "authors": ["Jingwei Wu", "Zhewei Huang", "Chang Liu"], "published_date": "2025-05-25", "title_zh": "基於圖像基礎模型的影片自監督學習進展", "summary_zh": "近年圖像基礎模型發展迅速，但其在影片自監督表徵學習的應用潛力卻被忽略。本研究提出一種名為AdViSe的影片自監督學習方法，旨在利用預訓練圖像基礎模型大幅降低影片表徵模型的訓練成本。具體而言，我們首先將時間建模模組（ResNet3D）導入圖像基礎模型，構建影片表徵模型。隨後，我們採用回放速率感知的影片自監督學習方法，在凍結圖像基礎模型組件的同時，訓練時間模組。在UCF101上的實驗表明，AdViSe在達到與最先進方法相當的性能時，訓練時間減少3.4倍，GPU記憶體使用量減少8.2倍。本研究為基於預訓練圖像基礎模型的低成本影片自監督學習提供了新思路。程式碼可在https://github.com/JingwWu/advise-video-ssl取得。", "audio": "audios/2505.19218v1.mp3", "timestamp": "2025-05-27T19:14:40.056089"}
{"query": "Diffusion Model", "id": "2505.19685v1", "url": "http://arxiv.org/abs/2505.19685v1", "title": "Graph Guided Diffusion: Unified Guidance for Conditional Graph Generation", "summary": "Diffusion models have emerged as powerful generative models for graph\ngeneration, yet their use for conditional graph generation remains a\nfundamental challenge. In particular, guiding diffusion models on graphs under\narbitrary reward signals is difficult: gradient-based methods, while powerful,\nare often unsuitable due to the discrete and combinatorial nature of graphs,\nand non-differentiable rewards further complicate gradient-based guidance. We\npropose Graph Guided Diffusion (GGDiff), a novel guidance framework that\ninterprets conditional diffusion on graphs as a stochastic control problem to\naddress this challenge. GGDiff unifies multiple guidance strategies, including\ngradient-based guidance (for differentiable rewards), control-based guidance\n(using control signals from forward reward evaluations), and zero-order\napproximations (bridging gradient-based and gradient-free optimization). This\ncomprehensive, plug-and-play framework enables zero-shot guidance of\npre-trained diffusion models under both differentiable and non-differentiable\nreward functions, adapting well-established guidance techniques to graph\ngeneration--a direction largely unexplored. Our formulation balances\ncomputational efficiency, reward alignment, and sample quality, enabling\npractical conditional generation across diverse reward types. We demonstrate\nthe efficacy of GGDiff in various tasks, including constraints on graph motifs,\nfairness, and link prediction, achieving superior alignment with target rewards\nwhile maintaining diversity and fidelity.", "authors": ["Victor M. Tenorio", "Nicolas Zilberstein", "Santiago Segarra", "Antonio G. Marques"], "published_date": "2025-05-26", "title_zh": "圖導向擴散：條件圖生成之統一導引", "summary_zh": "擴散模型已成為強大的圖形生成模型，但條件圖形生成仍是挑戰。在任意獎勵訊號下引導圖形擴散模型困難重重：梯度方法常因圖形的離散性和組合特性而不適用，且不可微獎勵使基於梯度的引導更加複雜。本文提出圖形引導擴散（GGDiff），將圖形的條件擴散視為隨機控制問題，以解決此挑戰。GGDiff統整多種引導策略，包含基於梯度的引導（針對可微獎勵）、基於控制的引導（使用前向獎勵評估的控制訊號）以及零階近似（連接基於梯度和無梯度優化）。此全面、隨插即用框架能針對可微及不可微獎勵函數，對預訓練擴散模型進行零樣本引導，將成熟的引導技術應用於圖形生成，此方向尚未被廣泛探索。我們的公式在計算效率、獎勵對齊和樣本品質之間取得平衡，實現跨多樣獎勵類型的實用條件生成。在圖形motif約束、公平性和連結預測等多項任務中，GGDiff表現出優異的獎勵對齊，同時保持多樣性和保真度。", "audio": "audios/2505.19685v1.mp3", "timestamp": "2025-05-27T19:14:47.996204"}
{"query": "AI", "id": "2505.20066v1", "url": "http://arxiv.org/abs/2505.20066v1", "title": "Automated data curation for self-supervised learning in underwater acoustic analysis", "summary": "The sustainability of the ocean ecosystem is threatened by increased levels\nof sound pollution, making monitoring crucial to understand its variability and\nimpact. Passive acoustic monitoring (PAM) systems collect a large amount of\nunderwater sound recordings, but the large volume of data makes manual analysis\nimpossible, creating the need for automation. Although machine learning offers\na potential solution, most underwater acoustic recordings are unlabeled.\nSelf-supervised learning models have demonstrated success in learning from\nlarge-scale unlabeled data in various domains like computer vision, Natural\nLanguage Processing, and audio. However, these models require large, diverse,\nand balanced datasets for training in order to generalize well. To address\nthis, a fully automated self-supervised data curation pipeline is proposed to\ncreate a diverse and balanced dataset from raw PAM data. It integrates\nAutomatic Identification System (AIS) data with recordings from various\nhydrophones in the U.S. waters. Using hierarchical k-means clustering, the raw\naudio data is sampled and then combined with AIS samples to create a balanced\nand diverse dataset. The resulting curated dataset enables the development of\nself-supervised learning models, facilitating various tasks such as monitoring\nmarine mammals and assessing sound pollution.", "authors": ["Hilde I Hummel", "Sandjai Bhulai", "Burooj Ghani", "Rob van der Mei"], "published_date": "2025-05-26", "title_zh": "水下聲學分析中自監督學習的自動化數據整理", "summary_zh": "海洋生態系統永續性受日益嚴重的噪音汙染威脅，監測對於理解其變異及影響至關重要。被動聲學監測系統收集大量水下錄音，惟數據量龐大，人工分析難以負荷，亟需自動化。機器學習雖提供潛在方案，但多數水下聲學錄音未經標記。自監督學習模型已在電腦視覺、自然語言處理和音訊等領域，展現從大規模未標記數據中學習的成功。然而，這些模型需要大型、多樣且平衡的數據集進行訓練，以確保良好的泛化能力。為此，本文提出一種全自動自監督數據管理流程，旨在從原始被動聲學監測數據中創建多樣且平衡的數據集。該流程整合自動識別系統數據與美國水域多個水聽器的錄音。透過層級式k-means聚類，對原始音訊數據進行採樣，並與自動識別系統樣本結合，以創建平衡且多樣化的數據集。由此產生的數據集有助於開發自監督學習模型，促進監測海洋哺乳動物和評估噪音汙染等任務。", "audio": "audios/2505.20066v1.mp3", "timestamp": "2025-05-27T20:20:32.145780"}
{"query": "Foundation Model", "id": "2505.19203v1", "url": "http://arxiv.org/abs/2505.19203v1", "title": "EnvSDD: Benchmarking Environmental Sound Deepfake Detection", "summary": "Audio generation systems now create very realistic soundscapes that can\nenhance media production, but also pose potential risks. Several studies have\nexamined deepfakes in speech or singing voice. However, environmental sounds\nhave different characteristics, which may make methods for detecting speech and\nsinging deepfakes less effective for real-world sounds. In addition, existing\ndatasets for environmental sound deepfake detection are limited in scale and\naudio types. To address this gap, we introduce EnvSDD, the first large-scale\ncurated dataset designed for this task, consisting of 45.25 hours of real and\n316.74 hours of fake audio. The test set includes diverse conditions to\nevaluate the generalizability, such as unseen generation models and unseen\ndatasets. We also propose an audio deepfake detection system, based on a\npre-trained audio foundation model. Results on EnvSDD show that our proposed\nsystem outperforms the state-of-the-art systems from speech and singing\ndomains.", "authors": ["Han Yin", "Yang Xiao", "Rohan Kumar Das", "Jisheng Bai", "Haohe Liu", "Wenwu Wang", "Mark D Plumbley"], "published_date": "2025-05-25", "title_zh": "EnvSDD：環境聲音深度偽造偵測基準測試", "summary_zh": "音訊生成系統雖能創造逼真音景以提升媒體製作，但也潛藏風險。現有研究多關注語音或歌聲深度偽造，然環境音特性不同，使針對語音及歌聲之偵測方法未必適用。為此，我們建立首個大規模環境音深度偽造資料集EnvSDD，包含45.25小時真實音訊與316.74小時偽造音訊。測試集包含多樣情境，評估其泛化能力，如未見過的生成模型與資料集。同時，我們提出基於預訓練音訊基礎模型的偵測系統，實驗結果顯示其優於現有語音及歌聲領域的最佳系統。", "audio": "audios/2505.19203v1.mp3", "timestamp": "2025-05-27T20:20:38.973757"}
{"query": "Diffusion Model", "id": "2505.19675v1", "url": "http://arxiv.org/abs/2505.19675v1", "title": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement", "summary": "The traditional process of creating labeled datasets is labor-intensive and\nexpensive. Recent breakthroughs in open-source large language models (LLMs)\nhave opened up a new avenue in generating labeled datasets automatically for\nvarious natural language processing (NLP) tasks, providing an alternative to\nsuch an expensive annotation process. However, the reliability of such\nauto-generated labels remains a significant concern due to inherent\ninaccuracies. When learning from noisy labels, the model's generalization is\nlikely to be harmed as it is prone to overfit to those label noises. While\nprevious studies in learning from noisy labels mainly focus on synthetic noise\nand real-world noise, LLM-generated label noise receives less attention. In\nthis paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to\ncalibrate the classifier's prediction, thus enhancing its robustness towards\nLLM-generated noisy labels. SiDyP retrieves potential true label candidates by\nneighborhood label distribution in text embedding space and iteratively refines\nnoisy candidates using a simplex diffusion model. Our framework can increase\nthe performance of the BERT classifier fine-tuned on both zero-shot and\nfew-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%\nrespectively. We demonstrate the effectiveness of SiDyP by conducting extensive\nbenchmarking for different LLMs over a variety of NLP tasks. Our code is\navailable on Github.", "authors": ["Liqin Ye", "Agam Shah", "Chao Zhang", "Sudheer Chava"], "published_date": "2025-05-26", "title_zh": "基於迭代精煉校準預訓練語言分類器於大型語言模型生成之雜訊標籤", "summary_zh": "傳統標註資料集耗時費力。大型語言模型於自動生成標註資料集方面取得突破，為自然語言處理任務提供經濟替代方案。然而，自動生成標籤的準確性仍是主要問題，模型易過擬合噪音標籤，影響泛化能力。針對此問題，本文提出SiDyP框架，利用動態先驗的單純形標籤擴散校準分類器預測，增強其對抗大型語言模型生成噪音標籤的穩健性。SiDyP透過文本嵌入空間的鄰域標籤分佈檢索潛在的真實標籤候選，並使用單純形擴散模型迭代精煉。實驗結果顯示，SiDyP平均提升BERT分類器在零樣本和少樣本噪音標籤資料集上的效能7.21%和7.30%。針對不同大型語言模型和多種自然語言處理任務的大量評測驗證了SiDyP的有效性。代碼已開源。", "audio": "audios/2505.19675v1.mp3", "timestamp": "2025-05-27T20:20:46.293409"}
{"query": "AI", "id": "2505.20059v1", "url": "http://arxiv.org/abs/2505.20059v1", "title": "LPCM: Learning-based Predictive Coding for LiDAR Point Cloud Compression", "summary": "Since the data volume of LiDAR point clouds is very huge, efficient\ncompression is necessary to reduce their storage and transmission costs.\nHowever, existing learning-based compression methods do not exploit the\ninherent angular resolution of LiDAR and ignore the significant differences in\nthe correlation of geometry information at different bitrates. The predictive\ngeometry coding method in the geometry-based point cloud compression (G-PCC)\nstandard uses the inherent angular resolution to predict the azimuth angles.\nHowever, it only models a simple linear relationship between the azimuth angles\nof neighboring points. Moreover, it does not optimize the quantization\nparameters for residuals on each coordinate axis in the spherical coordinate\nsystem. We propose a learning-based predictive coding method (LPCM) with both\nhigh-bitrate and low-bitrate coding modes. LPCM converts point clouds into\npredictive trees using the spherical coordinate system. In high-bitrate coding\nmode, we use a lightweight Long-Short-Term Memory-based predictive (LSTM-P)\nmodule that captures long-term geometry correlations between different\ncoordinates to efficiently predict and compress the elevation angles. In\nlow-bitrate coding mode, where geometry correlation degrades, we introduce a\nvariational radius compression (VRC) module to directly compress the point\nradii. Then, we analyze why the quantization of spherical coordinates differs\nfrom that of Cartesian coordinates and propose a differential evolution\n(DE)-based quantization parameter selection method, which improves\nrate-distortion performance without increasing coding time. Experimental\nresults on the LiDAR benchmark \\textit{SemanticKITTI} and the MPEG-specified\n\\textit{Ford} datasets show that LPCM outperforms G-PCC and other\nlearning-based methods.", "authors": ["Chang Sun", "Hui Yuan", "Shiqi Jiang", "Da Ai", "Wei Zhang", "Raouf Hamzaoui"], "published_date": "2025-05-26", "title_zh": "LPCM：基於學習的用於激光雷達點雲壓縮的預測編碼", "summary_zh": "光達點雲數據量龐大，高效壓縮至關重要。現有基於學習的壓縮方法未利用光達的固有角分辨率，且忽略不同碼率下幾何資訊關聯性的差異。幾何點雲壓縮（G-PCC）標準中的預測幾何編碼雖利用角分辨率預測方位角，但僅建模鄰近點方位角的簡單線性關係，且未針對球坐標系中各坐標軸的殘差優化量化參數。本文提出一種具高、低碼率編碼模式的基於學習的預測編碼方法（LPCM）。LPCM使用球坐標系將點雲轉換為預測樹。在高碼率模式下，採用輕量級長短期記憶預測（LSTM-P）模組捕捉不同坐標間的長期幾何關聯性，有效預測和壓縮俯仰角。在幾何關聯性降低的低碼率模式下，引入變分半徑壓縮（VRC）模組直接壓縮點半徑。接著，分析球坐標與笛卡爾坐標量化差異的原因，並提出一種基於差分演算法（DE）的量化參數選擇方法，在不增加編碼時間的情況下提升率失真效能。在光達基準數據集SemanticKITTI和MPEG指定數據集Ford上的實驗結果表明，LPCM優於G-PCC和其他基於學習的方法。", "audio": "audios/2505.20059v1.mp3", "timestamp": "2025-05-27T21:17:59.660192"}
{"query": "Foundation Model", "id": "2505.18969v1", "url": "http://arxiv.org/abs/2505.18969v1", "title": "Machine Psychophysics: Cognitive Control in Vision-Language Models", "summary": "Cognitive control refers to the ability to flexibly coordinate thought and\naction in pursuit of internal goals. A standard method for assessing cognitive\ncontrol involves conflict tasks that contrast congruent and incongruent trials,\nmeasuring the ability to prioritize relevant information while suppressing\ninterference. We evaluate 108 vision-language models on three classic conflict\ntasks and their more demanding \"squared\" variants across 2,220 trials. Model\nperformance corresponds closely to human behavior under resource constraints\nand reveals individual differences. These results indicate that some form of\nhuman-like executive function have emerged in current multi-modal foundational\nmodels.", "authors": ["Dezhi Luo", "Maijunxian Wang", "Bingyang Wang", "Tianwei Zhao", "Yijiang Li", "Hokin Deng"], "published_date": "2025-05-25", "title_zh": "機器心理物理學：視覺-語言模型中的認知控制", "summary_zh": "認知控制指涉為追求內在目標，彈性協調思維與行動的能力。評估認知控制的常用方法包含衝突任務，其對比一致與不一致試驗，衡量在抑制干擾時優先處理相關訊息的能力。本研究於三項經典衝突任務及其更具挑戰性的平方變體中，評估108個視覺語言模型，共計2,220次試驗。模型表現與資源限制下的人類行為高度一致，並揭示個體差異。結果顯示，現有多模態基礎模型已展現某種形式的類人類執行功能。", "audio": "audios/2505.18969v1.mp3", "timestamp": "2025-05-27T21:18:08.122894"}
{"query": "Diffusion Model", "id": "2505.19656v1", "url": "http://arxiv.org/abs/2505.19656v1", "title": "ReDDiT: Rehashing Noise for Discrete Visual Generation", "summary": "Discrete diffusion models are gaining traction in the visual generative area\nfor their efficiency and compatibility. However, the pioneered attempts still\nfall behind the continuous counterparts, which we attribute to the noise\n(absorbing state) design and sampling heuristics. In this study, we propose the\nrehashing noise framework for discrete diffusion transformer, termed ReDDiT, to\nextend absorbing states and improve expressive capacity of discrete diffusion\nmodels. ReDDiT enriches the potential paths that latent variables can traverse\nduring training with randomized multi-index corruption. The derived rehash\nsampler, which reverses the randomized absorbing paths, guarantees the\ndiversity and low discrepancy of the generation process. These reformulations\nlead to more consistent and competitive generation quality, mitigating the need\nfor heavily tuned randomness. Experiments show that ReDDiT significantly\noutperforms the baseline (reducing gFID from 6.18 to 1.61) and is on par with\nthe continuous counterparts with higher efficiency.", "authors": ["Tianren Ma", "Xiaosong Zhang", "Boyu Yang", "Junlan Feng", "Qixiang Ye"], "published_date": "2025-05-26", "title_zh": "ReDDiT：離散視覺生成之雜訊再散列", "summary_zh": "離散擴散模型因其效率和相容性在視覺生成領域備受關注，但早期嘗試仍落後於連續模型，原因在於雜訊（吸收態）設計和採樣啟發法。本研究提出用於離散擴散轉換器的重雜湊雜訊框架（ReDDiT），以擴展吸收態並提升離散擴散模型的表達能力。ReDDiT透過隨機多索引腐敗豐富潛在變數在訓練期間可能遍歷的路徑。導出的重雜湊採樣器反轉隨機吸收路徑，保證生成過程的多樣性和低差異性。這些重新表述帶來更一致且具競爭力的生成品質，減少了對大量調整隨機性的需求。實驗表明，ReDDiT顯著優於基準線（將gFID從6.18降低至1.61），並在效率更高的前提下，與連續模型相當。", "audio": "audios/2505.19656v1.mp3", "timestamp": "2025-05-27T21:18:18.143036"}
{"query": "AI", "id": "2505.20033v1", "url": "http://arxiv.org/abs/2505.20033v1", "title": "EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion Recognition", "summary": "Effective human-AI interaction relies on AI's ability to accurately perceive\nand interpret human emotions. Current benchmarks for vision and vision-language\nmodels are severely limited, offering a narrow emotional spectrum that\noverlooks nuanced states (e.g., bitterness, intoxication) and fails to\ndistinguish subtle differences between related feelings (e.g., shame vs.\nembarrassment). Existing datasets also often use uncontrolled imagery with\noccluded faces and lack demographic diversity, risking significant bias. To\naddress these critical gaps, we introduce EmoNet Face, a comprehensive\nbenchmark suite. EmoNet Face features: (1) A novel 40-category emotion\ntaxonomy, meticulously derived from foundational research to capture finer\ndetails of human emotional experiences. (2) Three large-scale, AI-generated\ndatasets (EmoNet HQ, Binary, and Big) with explicit, full-face expressions and\ncontrolled demographic balance across ethnicity, age, and gender. (3) Rigorous,\nmulti-expert annotations for training and high-fidelity evaluation. (4) We\nbuild Empathic Insight Face, a model achieving human-expert-level performance\non our benchmark. The publicly released EmoNet Face suite - taxonomy, datasets,\nand model - provides a robust foundation for developing and evaluating AI\nsystems with a deeper understanding of human emotions.", "authors": ["Christoph Schuhmann", "Robert Kaczmarczyk", "Gollam Rabby", "Maurice Kraus", "Felix Friedrich", "Huu Nguyen", "Krishna Kalyan", "Kourosh Nadi", "Kristian Kersting", "Sören Auer"], "published_date": "2025-05-26", "title_zh": "EmoNet-Face：用於合成情感辨識的專家標註基準", "summary_zh": "有效的人機互動仰賴人工智慧準確感知及解讀人類情緒的能力。現有視覺與視覺語言模型的評估基準存在侷限，情感範圍狹窄，忽略了細微的情緒狀態，且無法區分相關情感的細微差異。現有資料集常使用未經控制的圖像，面部遮擋且缺乏人口多樣性，存在偏差風險。為了解決這些問題，我們推出EmoNet Face，一個全面的評估基準。EmoNet Face包含：(1) 一個新穎的40類情感分類法，精確捕捉人類情感體驗的細節。(2) 三個大規模、AI生成的資料集，具有明確的完整面部表情，並控制了族裔、年齡和性別的人口平衡。(3) 嚴格的多專家標注，用於訓練和高保真評估。(4) 我們構建了Empathic Insight Face模型，在我們的基準上實現了人類專家級別的性能。公開發布的EmoNet Face套件，包含分類法、資料集和模型，為開發和評估能更深入理解人類情感的人工智慧系統提供了堅實的基礎。", "audio": "audios/2505.20033v1.mp3", "timestamp": "2025-05-27T22:17:37.358301"}
{"query": "Foundation Model", "id": "2505.18930v1", "url": "http://arxiv.org/abs/2505.18930v1", "title": "WeedNet: A Foundation Model-Based Global-to-Local AI Approach for Real-Time Weed Species Identification and Classification", "summary": "Early identification of weeds is essential for effective management and\ncontrol, and there is growing interest in automating the process using computer\nvision techniques coupled with AI methods. However, challenges associated with\ntraining AI-based weed identification models, such as limited expert-verified\ndata and complexity and variability in morphological features, have hindered\nprogress. To address these issues, we present WeedNet, the first global-scale\nweed identification model capable of recognizing an extensive set of weed\nspecies, including noxious and invasive plant species. WeedNet is an end-to-end\nreal-time weed identification pipeline and uses self-supervised learning,\nfine-tuning, and enhanced trustworthiness strategies. WeedNet achieved 91.02%\naccuracy across 1,593 weed species, with 41% species achieving 100% accuracy.\nUsing a fine-tuning strategy and a Global-to-Local approach, the local Iowa\nWeedNet model achieved an overall accuracy of 97.38% for 85 Iowa weeds, most\nclasses exceeded a 90% mean accuracy per class. Testing across intra-species\ndissimilarity (developmental stages) and inter-species similarity (look-alike\nspecies) suggests that diversity in the images collected, spanning all the\ngrowth stages and distinguishable plant characteristics, is crucial in driving\nmodel performance. The generalizability and adaptability of the Global WeedNet\nmodel enable it to function as a foundational model, with the Global-to-Local\nstrategy allowing fine-tuning for region-specific weed communities. Additional\nvalidation of drone- and ground-rover-based images highlights the potential of\nWeedNet for integration into robotic platforms. Furthermore, integration with\nAI for conversational use provides intelligent agricultural and ecological\nconservation consulting tools for farmers, agronomists, researchers, land\nmanagers, and government agencies across diverse landscapes.", "authors": ["Yanben Shen", "Timilehin T. Ayanlade", "Venkata Naresh Boddepalli", "Mojdeh Saadati", "Ashlyn Rairdin", "Zi K. Deng", "Muhammad Arbab Arshad", "Aditya Balu", "Daren Mueller", "Asheesh K Singh", "Wesley Everman", "Nirav Merchant", "Baskar Ganapathysubramanian", "Meaghan Anderson", "Soumik Sarkar", "Arti Singh"], "published_date": "2025-05-25", "title_zh": "WeedNet：基於基礎模型的雜草物種即時辨識與分類之由全域至區域人工智慧方法", "summary_zh": "及早識別雜草對於有效管理至關重要。利用電腦視覺結合人工智慧自動化此流程的興趣日益濃厚。然而，專家驗證數據有限以及形態特徵複雜多變等因素，阻礙了基於人工智慧的雜草識別模型的發展。為了解決這些問題，本研究提出了 WeedNet，首個全球規模的雜草識別模型，能夠辨識多種雜草，包含有害和入侵植物。WeedNet 是一個端到端即時雜草識別流程，採用自我監督學習、微調和強化可信度策略。WeedNet 在 1593 種雜草上的準確率達到 91.02%，其中 41% 的物種達到 100% 的準確率。透過微調策略和「全球到本地」方法，本地愛荷華 WeedNet 模型在 85 種愛荷華雜草上的整體準確率達到 97.38%，大多數類別的平均準確率超過 90%。跨物種內差異（發育階段）和物種間相似性（外觀相似物種）的測試表明，涵蓋所有生長階段和可區分植物特徵的多樣化圖像對於提升模型性能至關重要。全球 WeedNet 模型的泛化性和適應性使其能夠作為基礎模型發揮作用，「全球到本地」策略使其能夠針對特定區域的雜草群落進行微調。基於無人機和地面巡檢車圖像的額外驗證突顯了 WeedNet 整合到機器人平台的潛力。此外，與會話式人工智慧的整合為不同景觀的農民、農藝師、研究人員、土地管理者和政府機構提供了智慧農業和生態保護諮詢工具。", "audio": "audios/2505.18930v1.mp3", "timestamp": "2025-05-27T22:17:56.721800"}
{"query": "Diffusion Model", "id": "2505.19595v1", "url": "http://arxiv.org/abs/2505.19595v1", "title": "Accelerating Diffusion-based Text-to-Speech Model Training with Dual Modality Alignment", "summary": "The goal of this paper is to optimize the training process of diffusion-based\ntext-to-speech models. While recent studies have achieved remarkable\nadvancements, their training demands substantial time and computational costs,\nlargely due to the implicit guidance of diffusion models in learning complex\nintermediate representations. To address this, we propose A-DMA, an effective\nstrategy for Accelerating training with Dual Modality Alignment. Our method\nintroduces a novel alignment pipeline leveraging both text and speech\nmodalities: text-guided alignment, which incorporates contextual\nrepresentations, and speech-guided alignment, which refines semantic\nrepresentations. By aligning hidden states with discriminative features, our\ntraining scheme reduces the reliance on diffusion models for learning complex\nrepresentations. Extensive experiments demonstrate that A-DMA doubles the\nconvergence speed while achieving superior performance over baselines. Code and\ndemo samples are available at: https://github.com/ZhikangNiu/A-DMA", "authors": ["Jeongsoo Choi", "Zhikang Niu", "Ji-Hoon Kim", "Chunhui Wang", "Joon Son Chung", "Chen Xie"], "published_date": "2025-05-26", "title_zh": "基於雙模態對齊加速擴散文本轉語音模型訓練", "summary_zh": "本研究旨在優化基於擴散模型的文字轉語音訓練過程。 鑑於現有方法訓練耗時且計算成本高昂，主要源於擴散模型在學習複雜中間表示時的隱式引導，我們提出A-DMA，一種利用雙模態對齊加速訓練的有效策略。 A-DMA引入新的對齊流程，結合文本和語音模態：文本引導對齊，融入上下文表示；語音引導對齊，精煉語義表示。 透過將隱藏狀態與判別性特徵對齊，該訓練方案降低了對擴散模型學習複雜表示的依賴。 實驗結果表明，A-DMA能使收斂速度加倍，並取得優於基準模型的性能。 代碼和演示樣本可在https://github.com/ZhikangNiu/A-DMA取得。", "audio": "audios/2505.19595v1.mp3", "timestamp": "2025-05-27T22:18:06.392286"}
{"query": "AI", "id": "2505.20013v1", "url": "http://arxiv.org/abs/2505.20013v1", "title": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback", "summary": "Web agents powered by Large Language Models (LLMs) show promise for\nnext-generation AI, but their limited reasoning in uncertain, dynamic web\nenvironments hinders robust deployment. In this paper, we identify key\nreasoning skills essential for effective web agents, i.e., reflection &\nlookahead, branching, and rollback, and curate trajectory data that exemplifies\nthese abilities by reconstructing the agent's (inference-time) reasoning\nalgorithms into chain-of-thought rationales. We conduct experiments in the\nagent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling\nsalient reasoning patterns into the backbone LLM via simple fine-tuning can\nsubstantially enhance its performance. Our approach yields significant\nimprovements across multiple benchmarks, including WebVoyager, Mind2web-live,\nand SimpleQA (web search), highlighting the potential of targeted reasoning\nskill enhancement for web agents.", "authors": ["Minda Hu", "Tianqing Fang", "Jianshu Zhang", "Junyu Ma", "Zhisong Zhang", "Jingyan Zhou", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Irwin King"], "published_date": "2025-05-26", "title_zh": "WebCoT：藉由在反思、分支與回滾中重構思維鏈來強化Web代理的推理能力", "summary_zh": "基於大型語言模型(LLM)的網路代理展現了下一代人工智慧的潛力，但其在不確定且動態的網路環境中有限的推理能力阻礙了其穩健部署。本文識別了有效網路代理所需的關鍵推理技能，包括反思與前瞻、分支和回滾，並透過將代理的(推理時)推理演算法重構為思維鏈式原理，來整理體現這些能力的軌跡數據。在代理自我改進基準OpenWebVoyager中進行的實驗表明，透過簡單的微調將顯著的推理模式提煉到主幹LLM中，可以大幅提升其性能。該方法在多個基準測試中產生了顯著的改進，包括WebVoyager、Mind2web-live和SimpleQA(網路搜尋)，突顯了針對網路代理的定向推理技能提升的潛力。", "audio": "audios/2505.20013v1.mp3", "timestamp": "2025-05-27T23:17:05.068752"}
{"query": "Foundation Model", "id": "2505.18881v1", "url": "http://arxiv.org/abs/2505.18881v1", "title": "SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes", "summary": "We present the Semantics-aware Dataset and Benchmark Generation Pipeline for\nOpen-vocabulary Object Navigation in Dynamic Scenes (SD-OVON). It utilizes\npretraining multimodal foundation models to generate infinite unique\nphoto-realistic scene variants that adhere to real-world semantics and daily\ncommonsense for the training and the evaluation of navigation agents,\naccompanied with a plugin for generating object navigation task episodes\ncompatible to the Habitat simulator. In addition, we offer two pre-generated\nobject navigation task datasets, SD-OVON-3k and SD-OVON-10k, comprising\nrespectively about 3k and 10k episodes of the open-vocabulary object navigation\ntask, derived from the SD-OVON-Scenes dataset with 2.5k photo-realistic scans\nof real-world environments and the SD-OVON-Objects dataset with 0.9k manually\ninspected scanned and artist-created manipulatable object models. Unlike prior\ndatasets limited to static environments, SD-OVON covers dynamic scenes and\nmanipulatable objects, facilitating both real-to-sim and sim-to-real robotic\napplications. This approach enhances the realism of navigation tasks, the\ntraining and the evaluation of open-vocabulary object navigation agents in\ncomplex settings. To demonstrate the effectiveness of our pipeline and\ndatasets, we propose two baselines and evaluate them along with\nstate-of-the-art baselines on SD-OVON-3k. The datasets, benchmark and source\ncode are publicly available.", "authors": ["Dicong Qiu", "Jiadi You", "Zeying Gong", "Ronghe Qiu", "Hui Xiong", "Junwei Liang"], "published_date": "2025-05-24", "title_zh": "SD-OVON：動態場景下開放詞彙物件導航的語義感知資料集與基準生成流程", "summary_zh": "本研究提出語義感知資料集與基準生成流程SD-OVON，用於動態場景中的開放詞彙物件導航。它利用預訓練多模態基礎模型，生成符合現實世界語義及常識的無限獨特且擬真的場景變體，以訓練和評估導航代理，並提供生成物件導航任務的插件，與Habitat模擬器相容。此外，我們提供兩個預先生成的物件導航任務資料集SD-OVON-3k和SD-OVON-10k，分別包含約3千和1萬個開放詞彙物件導航任務情節，源自SD-OVON-Scenes資料集（包含2.5千個現實環境擬真掃描）以及SD-OVON-Objects資料集（包含0.9千個經人工檢查的掃描和藝術家創建的可操作物件模型）。與僅限於靜態環境的先前資料集不同，SD-OVON涵蓋動態場景和可操作物件，促進真實到模擬及模擬到真實的機器人應用，增強導航任務的真實感，以及在複雜環境中開放詞彙物件導航代理的訓練和評估。為展示流程和資料集的有效性，我們提出兩個基準線，並與最先進的基準線在SD-OVON-3k上進行評估。資料集、基準和原始碼均公開可用。", "audio": "audios/2505.18881v1.mp3", "timestamp": "2025-05-27T23:17:13.030091"}
{"query": "Diffusion Model", "id": "2505.19552v1", "url": "http://arxiv.org/abs/2505.19552v1", "title": "On scalable and efficient training of diffusion samplers", "summary": "We address the challenge of training diffusion models to sample from\nunnormalized energy distributions in the absence of data, the so-called\ndiffusion samplers. Although these approaches have shown promise, they struggle\nto scale in more demanding scenarios where energy evaluations are expensive and\nthe sampling space is high-dimensional. To address this limitation, we propose\na scalable and sample-efficient framework that properly harmonizes the powerful\nclassical sampling method and the diffusion sampler. Specifically, we utilize\nMonte Carlo Markov chain (MCMC) samplers with a novelty-based auxiliary energy\nas a Searcher to collect off-policy samples, using an auxiliary energy function\nto compensate for exploring modes the diffusion sampler rarely visits. These\noff-policy samples are then combined with on-policy data to train the diffusion\nsampler, thereby expanding its coverage of the energy landscape. Furthermore,\nwe identify primacy bias, i.e., the preference of samplers for early experience\nduring training, as the main cause of mode collapse during training, and\nintroduce a periodic re-initialization trick to resolve this issue. Our method\nsignificantly improves sample efficiency on standard benchmarks for diffusion\nsamplers and also excels at higher-dimensional problems and real-world\nmolecular conformer generation.", "authors": ["Minkyu Kim", "Kiyoung Seong", "Dongyeop Woo", "Sungsoo Ahn", "Minsu Kim"], "published_date": "2025-05-26", "title_zh": "關於擴展且高效的擴散採樣器訓練", "summary_zh": "本研究旨在解決無資料情況下，訓練擴散模型以從非標準化能量分佈中抽樣的挑戰。為了解決能量評估成本高昂及抽樣空間高維度問題，我們提出一種可擴展且具樣本效率的框架，協調傳統抽樣方法與擴散抽樣器。具體而言，利用基於新穎性的輔助能量的蒙地卡羅馬可夫鏈抽樣器作為搜索器，收集離策略樣本，並使用輔助能量函數補償擴散抽樣器罕見的模式探索。這些離策略樣本與在策略資料結合，用於訓練擴散抽樣器，從而擴大其對能量景觀的覆蓋範圍。此外，我們發現首因效應是訓練期間模式崩潰的主因，並引入週期性重新初始化技巧來解決此問題。該方法顯著提升擴散抽樣器的樣本效率，並在高維度問題和真實世界分子構象生成中表現出色。", "audio": "audios/2505.19552v1.mp3", "timestamp": "2025-05-27T23:17:18.919116"}
{"query": "AI", "id": "2505.20148v2", "url": "http://arxiv.org/abs/2505.20148v2", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "published_date": "2025-05-26", "title_zh": "礦建任意：開放世界AI代理空間規劃基準測試", "summary_zh": "空間規劃是空間智能的關鍵，需理解並規劃物體在空間中的排列。具備空間規劃能力的人工智慧體能更好地適應機器人操作、自動組裝、城市規劃等現實應用。近期研究試圖構建基準來評估多模態大型語言模型（MLLM）的空間智能，但這些基準主要側重於基於典型視覺問答（VQA）形式的空間推理，抽象空間理解與具體任務執行間存在差距。本研究進一步構建名為MineAnyBuild的綜合基準，旨在評估開放世界人工智慧體在Minecraft遊戲中的空間規劃能力。MineAnyBuild要求人工智慧體根據多模態人類指令生成可執行的建築規劃，包含4000個精心設計的空間規劃任務，並提供利用豐富玩家生成內容無限擴展數據收集的範例。MineAnyBuild透過空間理解、空間推理、創造力及空間常識四個核心維度評估空間規劃能力。基於MineAnyBuild，我們對現有基於MLLM的人工智慧體進行全面評估，揭示其在空間規劃能力方面的嚴重局限性與巨大潛力。MineAnyBuild將為空間智能的評估開闢新途徑，並促進具備空間規劃能力的開放世界人工智慧體的進一步發展。", "audio": "audios/2505.20148v2.mp3", "timestamp": "2025-05-28T01:27:09.959867"}
{"query": "Foundation Model", "id": "2505.19888v2", "url": "http://arxiv.org/abs/2505.19888v2", "title": "Generalized and Personalized Federated Learning with Foundation Models via Orthogonal Transformations", "summary": "Federated Learning (FL) aims to train models across decentralized clients or\ndevices holding local data without the need for centralized data collection,\nthus enhancing data privacy and security. However, achieving both\ngeneralization and personalization in heterogeneous settings remains a\nsignificant challenge. To address this, we introduce FedOT, a novel approach\nthat leverages black-box foundation models. FedOT shares only a global\ntask-dependent classifier across clients while locally adapting features\nthrough orthogonal transformations. By enforcing orthogonality, FedOT mitigates\ngradient conflicts across diverse clients, preserves semantic integrity, and\nachieves robust performance even in the presence of substantial data\nheterogeneity. The strategy of combining global and local parameters enables a\nmore balanced approach for both generalization and personalization,\noutperforming baseline FL methods across multiple benchmarks. Furthermore, our\nextensive analysis confirms that joint optimization of global classifiers and\nlocal orthogonal transformations yields superior performance and suggests\nbroader applicability.", "authors": ["Eun Gyung Kong", "Je Won Yeom", "Yonghoon Jeon", "Taesup Kim"], "published_date": "2025-05-26", "title_zh": "基於正交變換的基礎模型廣義與個性化聯邦學習", "summary_zh": "聯邦學習旨在分散式客戶端或設備上訓練模型，無需集中數據收集，從而增強數據隱私和安全。在異構環境中實現泛化和個性化仍是一大挑戰。本研究提出FedOT，一種利用黑盒基礎模型的新方法。FedOT僅在客戶端之間共享全局任務相關分類器，並通過正交變換局部調整特徵。正交性降低了不同客戶端之間的梯度衝突，保持了語義完整性，並在數據高度異構的情況下實現了穩健的性能。全局和局部參數的結合，在泛化和個性化之間取得更平衡的方法，優於多個基準測試中的基準聯邦學習方法。對全局分類器和局部正交變換的聯合優化，可獲得卓越性能，並具有廣泛的適用性。", "audio": "audios/2505.19888v2.mp3", "timestamp": "2025-05-28T01:27:18.417490"}
{"query": "Diffusion Model", "id": "2505.19751v2", "url": "http://arxiv.org/abs/2505.19751v2", "title": "SAIL: Self-supervised Albedo Estimation from Real Images with a Latent Diffusion Model", "summary": "Intrinsic image decomposition aims at separating an image into its underlying\nalbedo and shading components, isolating the base color from lighting effects\nto enable downstream applications such as virtual relighting and scene editing.\nDespite the rise and success of learning-based approaches, intrinsic image\ndecomposition from real-world images remains a significant challenging task due\nto the scarcity of labeled ground-truth data. Most existing solutions rely on\nsynthetic data as supervised setups, limiting their ability to generalize to\nreal-world scenes. Self-supervised methods, on the other hand, often produce\nalbedo maps that contain reflections and lack consistency under different\nlighting conditions. To address this, we propose SAIL, an approach designed to\nestimate albedo-like representations from single-view real-world images. We\nrepurpose the prior knowledge of a latent diffusion model for unconditioned\nscene relighting as a surrogate objective for albedo estimation. To extract the\nalbedo, we introduce a novel intrinsic image decomposition fully formulated in\nthe latent space. To guide the training of our latent diffusion model, we\nintroduce regularization terms that constrain both the lighting-dependent and\nindependent components of our latent image decomposition. SAIL predicts stable\nalbedo under varying lighting conditions and generalizes to multiple scenes,\nusing only unlabeled multi-illumination data available online.", "authors": ["Hala Djeghim", "Nathan Piasco", "Luis Roldão", "Moussab Bennehar", "Dzmitry Tsishkou", "Céline Loscos", "Désiré Sidibé"], "published_date": "2025-05-26", "title_zh": "SAIL：基於潛在擴散模型的真實圖像自監督反照率估計", "summary_zh": "本研究旨在解決真實世界圖像內在影像分解的難題，即將影像分離為反照率和陰影成分。由於缺乏標記數據，現有方法難以泛化。我們提出一種名為SAIL的方法，從單視角真實圖像估計類似反照率的表示。我們利用潛在擴散模型在無條件場景重照明方面的先驗知識，作為反照率估計的替代目標。通過在潛在空間中構建內在影像分解，並引入正則化項約束光照相關和不相關成分，SAIL能夠預測在不同光照條件下穩定的反照率，並泛化到多個場景，僅使用網路上未標記的多重光照數據。", "audio": "audios/2505.19751v2.mp3", "timestamp": "2025-05-28T01:27:23.691088"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：視覺語言模型中多視角空間定位評估", "summary_zh": "視覺語言模型在理解和推理視覺內容方面表現出色，但在跨視點理解和空間推理方面仍面臨挑戰。目前模型擅長以自我為中心的空間推理，但難以推廣至以他人為中心的視點。我們提出了 ViewSpatial-Bench，首個專為多視點空間定位識別評估而設計的綜合基準，包含五種任務類型，並透過自動化 3D 標註流程生成精確的方向標籤。對多種視覺語言模型在 ViewSpatial-Bench 上的評估顯示，模型在相機視角任務中表現尚可，但在以人為視角的推理中準確度降低。透過在多視角空間資料集上微調模型，整體性能提升了 46.24%，驗證了我們方法的有效性。此研究為具身人工智慧系統的空間智慧建立了關鍵基準，並提供經驗證據表明建模 3D 空間關係可增強視覺語言模型的空間理解能力。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T03:12:48.504476"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "休謨：在視覺-語言-行動模型中引入系統二思維", "summary_zh": "人類在執行複雜實體任務前會進行深思熟慮。此思維模式近期顯著提升大型語言模型在數位領域解決複雜問題的能力。然而，機器人基礎模型與實體世界互動時，深思熟慮的潛力尚未充分開發。本研究提出Hume，一種雙系統視覺-語言-動作(VLA)模型，具備價值導向的系統二思維和級聯動作去噪，探索VLA模型在靈巧機器人控制中類似人類的思維能力。Hume的系統二透過新型價值查詢頭擴展VLA模型骨幹，估算預測動作的狀態-動作價值，進而執行價值導向思維，反覆抽樣多個動作候選，並根據狀態-動作價值選擇其一。Hume的系統一是輕量級反應式視覺運動策略，接收系統二選擇的動作，並執行級聯動作去噪以實現靈巧機器人控制。部署時，系統二以低頻率進行價值導向思維，而系統一非同步接收系統二選擇的動作候選，並即時預測流暢動作。實驗證明，Hume在多個模擬基準和真實機器人部署中，優於現有的最先進VLA模型。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T03:12:56.184034"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：雜訊誘導式佈局用於多主體生成", "summary_zh": "現有文生圖擴散模型難以生成多個不同主體。複雜提示詞常導致主體洩漏，造成數量、屬性和視覺特徵不準確。避免主體洩漏需要掌握各主體空間位置。近期方法透過外部佈局控制提供這些位置，但強制執行此類佈局常與初始噪聲決定的固有佈局衝突，導致與模型先驗不符。本文提出一種新方法，預測與提示詞對齊且源自初始噪聲的空間佈局，並在去噪過程中精煉它。藉由依賴此噪聲誘導佈局，避免與外部強制佈局衝突，更好保留模型先驗。本方法採用小型神經網路預測和精煉每個去噪步驟中演化的噪聲誘導佈局，確保主體間邊界清晰，同時維持一致性。實驗結果表明，相較於現有佈局引導技術，此噪聲對齊策略實現了更佳的文本圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T03:13:02.597159"}
{"query": "AI", "id": "2505.21486v1", "url": "http://arxiv.org/abs/2505.21486v1", "title": "Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming", "summary": "Automating robust hypothesis generation in open environments is pivotal for\nAI cognition. We introduce a novel framework integrating a multi-agent system,\npowered by Large Language Models (LLMs), with Inductive Logic Programming\n(ILP). Our system's LLM agents autonomously define a structured symbolic\nvocabulary (predicates) and relational templates , i.e., \\emph{language bias}\ndirectly from raw textual data. This automated symbolic grounding (the\nconstruction of the language bias), traditionally an expert-driven bottleneck\nfor ILP, then guides the transformation of text into facts for an ILP solver,\nwhich inductively learns interpretable rules. This approach overcomes\ntraditional ILP's reliance on predefined symbolic structures and the\nnoise-sensitivity of pure LLM methods. Extensive experiments in diverse,\nchallenging scenarios validate superior performance, paving a new path for\nautomated, explainable, and verifiable hypothesis generation.", "authors": ["Yang Yang", "Jiemin Wu", "Yutao Yue"], "published_date": "2025-05-27", "title_zh": "穩健假說生成：基於大型語言模型的歸納邏輯程式設計語言偏置自動化", "summary_zh": "本研究提出一種新型框架，藉由大型語言模型驅動的多代理人系統與歸納邏輯程式設計相結合，實現開放環境中穩健的假設自動生成。該系統的語言模型代理人能自主地從原始文本數據中定義結構化符號詞彙（謂詞）與關係模板，即語言偏置。這種自動化的符號基礎（語言偏置的構建）傳統上是歸納邏輯程式設計中由專家驅動的瓶頸，現在可引導文本轉換為歸納邏輯程式求解器的素材，進而歸納學習可解釋的規則。此方法克服了傳統歸納邏輯程式設計對預定義符號結構的依賴，以及純粹語言模型方法對雜訊的敏感性。在多樣且具挑戰性的場景下進行的大量實驗驗證了其卓越性能，為自動、可解釋和可驗證的假設生成開闢了新途徑。", "audio": "audios/2505.21486v1.mp3", "timestamp": "2025-05-28T04:23:32.065216"}
{"query": "Foundation Model", "id": "2505.21382v1", "url": "http://arxiv.org/abs/2505.21382v1", "title": "DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models", "summary": "Low-Rank Adaptation (LoRA) has emerged as one of the most effective,\ncomputationally tractable fine-tuning approaches for training Vision-Language\nModels (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by\nfreezing the pre-trained model weights and injecting trainable low-rank\nmatrices, allowing for efficient learning of these foundation models even on\nedge devices. However, LoRA in decentralized settings still remains under\nexplored, particularly for the theoretical underpinnings due to the lack of\nsmoothness guarantee and model consensus interference (defined formally below).\nThis work improves the convergence rate of decentralized LoRA (DLoRA) to match\nthe rate of decentralized SGD by ensuring gradient smoothness. We also\nintroduce DeCAF, a novel algorithm integrating DLoRA with truncated singular\nvalue decomposition (TSVD)-based matrix factorization to resolve consensus\ninterference. Theoretical analysis shows TSVD's approximation error is bounded\nand consensus differences between DLoRA and DeCAF vanish as rank increases,\nyielding DeCAF's matching convergence rate. Extensive experiments across\nvision/language tasks demonstrate our algorithms outperform local training and\nrivals federated learning under both IID and non-IID data distributions.", "authors": ["Nastaran Saadati", "Zhanhong Jiang", "Joshua R. Waite", "Shreyan Ganguly", "Aditya Balu", "Chinmay Hegde", "Soumik Sarkar"], "published_date": "2025-05-27", "title_zh": "DeCAF：用於基礎模型低秩適應的分散式共識與分解", "summary_zh": "低秩適應(LoRA)已成為訓練視覺語言模型(VLM)和大型語言模型(LLM)最有效且計算可行的微調方法之一。LoRA凍結預訓練模型權重並注入可訓練的低秩矩陣，從而在邊緣設備上實現基礎模型的高效學習。然而，分散式環境中的LoRA仍未充分探索，尤其在理論基礎方面，原因在於缺乏平滑性保證和模型共識干擾。本研究透過確保梯度平滑性，改進分散式LoRA(DLoRA)的收斂速度，使其與分散式SGD的速度相匹配。此外，我們提出DeCAF，一種將DLoRA與基於截斷奇異值分解(TSVD)的矩陣分解相結合的新演算法，以解決共識干擾。理論分析表明，TSVD的近似誤差有界，且隨著秩的增加，DLoRA和DeCAF之間的共識差異消失，從而使DeCAF的收斂速度與之匹配。跨視覺/語言任務的大量實驗表明，我們的演算法優於本地訓練，且在獨立同分布(IID)和非獨立同分布(non-IID)資料分佈下均可與聯邦學習相媲美。", "audio": "audios/2505.21382v1.mp3", "timestamp": "2025-05-28T04:23:40.857751"}
{"query": "Diffusion Model", "id": "2505.21469v1", "url": "http://arxiv.org/abs/2505.21469v1", "title": "PropMolFlow: Property-guided Molecule Generation with Geometry-Complete Flow Matching", "summary": "Molecule generation is advancing rapidly in chemical discovery and drug\ndesign. Flow matching methods have recently set the state of the art (SOTA) in\nunconditional molecule generation, surpassing score-based diffusion models.\nHowever, diffusion models still lead in property-guided generation. In this\nwork, we introduce PropMolFlow, a novel approach for property-guided molecule\ngeneration based on geometry-complete SE(3)-equivariant flow matching.\nIntegrating five different property embedding methods with a Gaussian expansion\nof scalar properties, PropMolFlow outperforms previous SOTA diffusion models in\nconditional molecule generation across various properties while preserving the\nstability and validity of the generated molecules, consistent with its\nunconditional counterpart. Additionally, it enables faster inference with\nsignificantly fewer time steps compared to baseline models. We highlight the\nimportance of validating the properties of generated molecules through DFT\ncalculations performed at the same level of theory as the training data.\nSpecifically, our analysis identifies properties that require DFT validation\nand others where a pretrained SE(3) geometric vector perceptron regressors\nprovide sufficiently accurate predictions on generated molecules. Furthermore,\nwe introduce a new property metric designed to assess the model's ability to\npropose molecules with underrepresented property values, assessing its capacity\nfor out-of-distribution generalization. Our findings reveal shortcomings in\nexisting structural metrics, which mistakenly validate open-shell molecules or\nmolecules with invalid valence-charge configurations, underscoring the need for\nimproved evaluation frameworks. Overall, this work paves the way for developing\ntargeted property-guided generation methods, enhancing the design of molecular\ngenerative models for diverse applications.", "authors": ["Cheng Zeng", "Jirui Jin", "George Karypis", "Mark Transtrum", "Ellad B. Tadmor", "Richard G. Hennig", "Adrian Roitberg", "Stefano Martiniani", "Mingjie Liu"], "published_date": "2025-05-27", "title_zh": "PropMolFlow：基於幾何完整流匹配的性質導向分子生成", "summary_zh": "分子生成技術在化學發現和藥物設計領域快速發展。Flow matching方法在無條件分子生成方面已超越基於分數的擴散模型，達到最先進水平。然而，擴散模型在屬性導向生成方面仍具領先地位。本研究提出PropMolFlow，一種基於幾何完整SE(3)等變Flow matching的屬性導向分子生成新方法。PropMolFlow整合五種不同的屬性嵌入方法和標量屬性的高斯擴展，在多種屬性條件下，其條件分子生成效能超越先前的最先進擴散模型，同時保持生成分子的穩定性和有效性，與其無條件生成能力一致。此外，相較於基準模型，PropMolFlow能以更少的時間步長實現更快的推論。我們強調，驗證生成分子的屬性至關重要，應透過與訓練資料相同理論水平的DFT計算進行驗證。我們的分析特別指出哪些屬性需要DFT驗證，以及哪些屬性可透過預訓練的SE(3)幾何向量感知器迴歸器提供足夠精確的預測。我們還提出一種新的屬性指標，旨在評估模型提出具有代表性不足的屬性值分子的能力，從而評估其分佈外泛化能力。研究結果揭示了現有結構指標的缺點，它們會錯誤地驗證開殼層分子或具有無效價電荷配置的分子，突顯了改進評估框架的必要性。總體而言，這項工作為開發有針對性的屬性導向生成方法鋪平了道路，從而加強了分子生成模型在多樣化應用中的設計。", "audio": "audios/2505.21469v1.mp3", "timestamp": "2025-05-28T04:23:50.297453"}
{"query": "AI", "id": "2505.21482v1", "url": "http://arxiv.org/abs/2505.21482v1", "title": "Tissue-specific predictive performance: A unified estimation and inference framework for multi-category screening tests", "summary": "Multi-Cancer Early Detection (MCED) testing with tissue localization aims to\ndetect and identify multiple cancer types from a single blood sample. Such\ntests have the potential to aid clinical decisions and significantly improve\nhealth outcomes. Despite this promise, MCED testing has not yet achieved\nregulatory approval, reimbursement or broad clinical adoption. One major reason\nfor this shortcoming is uncertainty about test performance resulting from the\nreporting of clinically obtuse metrics. Traditionally, MCED tests report\naggregate measures of test performance, disregarding cancer type, that obscure\nbiological variability and underlying differences in the test's behavior,\nlimiting insight into true effectiveness. Clinically informative evaluation of\nan MCED test's performance requires metrics that are specific to cancer types.\nIn the context of a case-control sampling design, this paper derives analytical\nmethods that estimate cancer-specific intrinsic accuracy, tissue localization\nreadout-specific predictive value and the marginal test classification\ndistribution, each with corresponding confidence interval formulae. A\nsimulation study is presented that evaluates performance of the proposed\nmethodology and provides guidance for implementation. An application to a\npublished MCED test dataset is given. These statistical approaches allow for\nestimation and inference for the pointed metric of an MCED test that allow its\nevaluation to support a potential role in early cancer detection. This\nframework enables more precise clinical decision-making, supports optimized\ntrial designs across classical, digital, AI-driven, and hybrid stratified\ndiagnostic screening platforms, and facilitates informed healthcare decisions\nby clinicians, policymakers, regulators, scientists, and patients.", "authors": ["A. Gregory DiRienzo", "Elie Massaad", "Hutan Ashrafian"], "published_date": "2025-05-27", "title_zh": "組織特異性預測效能：多類別篩檢試驗之統一估計與推論框架", "summary_zh": "多癌早期檢測(MCED)旨在透過單一血液樣本檢測並定位多種癌症。此類檢測具潛力輔助臨床決策並顯著改善健康結果，但因臨床指標不明確導致的效能不確定性，尚未獲得監管批准、報銷或廣泛應用。傳統上，MCED檢測報告忽略癌症類型、彙總的效能指標，掩蓋了生物變異性和檢測行為的潛在差異，限制了對真實效用的了解。本研究針對病例對照抽樣設計，推導出估計癌症特異性固有準確度、組織定位讀數特異性預測值及邊緣檢測分類分佈的分析方法，並提供相應的置信區間公式。研究透過模擬評估所提方法的效能，並給予實施指導，同時應用於已發表的MCED檢測數據集。此統計方法可估計和推論MCED檢測的關鍵指標，支持其在早期癌症檢測中的潛在作用。此框架可實現更精確的臨床決策，支持跨傳統、數位、AI驅動和混合分層診斷篩查平台的優化試驗設計，並促進臨床醫生、政策制定者、監管機構、科學家和患者知情的醫療保健決策。", "audio": "audios/2505.21482v1.mp3", "timestamp": "2025-05-28T05:18:38.826250"}
{"query": "Foundation Model", "id": "2505.21375v1", "url": "http://arxiv.org/abs/2505.21375v1", "title": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution", "summary": "Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data\nfor Earth observation but pose challenges for existing multimodal foundation\nmodels due to two key bottlenecks: (1) limited availability of UHR training\ndata, and (2) token explosion caused by the large image size. To address data\nscarcity, we introduce SuperRS-VQA (avg. 8,376$\\times$8,376) and HighRS-VQA\n(avg. 2,000$\\times$1,912), the highest-resolution vision-language datasets in\nRS to date, covering 22 real-world dialogue tasks. To mitigate token explosion,\nour pilot studies reveal significant redundancy in RS images: crucial\ninformation is concentrated in a small subset of object-centric tokens, while\npruning background tokens (e.g., ocean or forest) can even improve performance.\nMotivated by these findings, we propose two strategies: Background Token\nPruning and Anchored Token Selection, to reduce the memory footprint while\npreserving key semantics.Integrating these techniques, we introduce\nGeoLLaVA-8K, the first RS-focused multimodal large language model capable of\nhandling inputs up to 8K$\\times$8K resolution, built on the LLaVA framework.\nTrained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art\non the XLRS-Bench.", "authors": ["Fengxiang Wang", "Mingshuo Chen", "Yueying Li", "Di Wang", "Haotian Wang", "Zonghao Guo", "Zefan Wang", "Boqi Shan", "Long Lan", "Yulin Wang", "Hongzhen Wang", "Wenjing Yang", "Bo Du", "Jing Zhang"], "published_date": "2025-05-27", "title_zh": "GeoLLaVA-8K：遙感多模態大型語言模型擴展至8K解析度", "summary_zh": "超高解析度遙感影像為地球觀測提供寶貴資料，但因訓練資料有限及影像尺寸過大導致的token爆炸，對現有多模態基礎模型構成挑戰。為解決資料稀缺問題，本研究推出SuperRS-VQA和HighRS-VQA，目前解析度最高的遙感視覺語言數據集，涵蓋22個真實對話任務。為緩解token爆炸，研究發現遙感影像存在顯著冗餘，關鍵訊息集中在少數以物件為中心的token中，刪除背景token甚至能提升效能。基於此，提出背景token剪枝和錨定token選擇策略，以減少記憶體佔用並保留關鍵語義。整合這些技術，本研究推出GeoLLaVA-8K，首個專注於遙感且能處理高達8K解析度輸入的多模態大型語言模型，基於LLaVA框架構建。GeoLLaVA-8K在SuperRS-VQA和HighRS-VQA上訓練，於XLRS-Bench上達到最先進水準。", "audio": "audios/2505.21375v1.mp3", "timestamp": "2025-05-28T05:18:48.157765"}
{"query": "Diffusion Model", "id": "2505.21467v1", "url": "http://arxiv.org/abs/2505.21467v1", "title": "Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion", "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains.", "authors": ["Zhanqiu Hu", "Jian Meng", "Yash Akhauri", "Mohamed S. Abdelfattah", "Jae-sun Seo", "Zhiru Zhang", "Udit Gupta"], "published_date": "2025-05-27", "title_zh": "透過高效鍵值快取與導引擴散加速擴散語言模型推論", "summary_zh": "擴散語言模型具平行生成詞元與內建雙向性之優勢，可望提升序列建模效率。然現有模型推論速度慢，需多次完整序列正向傳播，導致高運算成本與延遲，長文本情境尤甚。平行生成亦引發詞元不連貫問題，降低降噪步數更導致品質下降。本文提出兩種免訓練技術：FreeCache重複利用穩定之鍵值投射，有效降低運算成本；Guided Diffusion則利用輕量級自迴歸模型指導詞元解遮蔽，大幅減少降噪迭代次數而不犧牲品質。實驗顯示，結合此二方法可實現高達34倍之端對端加速，且不損害準確性。擴散語言模型首次達到與自迴歸模型相當甚至更快的延遲，為其於各領域之廣泛應用奠定基礎。", "audio": "audios/2505.21467v1.mp3", "timestamp": "2025-05-28T05:18:55.177356"}
{"query": "AI", "id": "2505.21479v1", "url": "http://arxiv.org/abs/2505.21479v1", "title": "Are Language Models Consequentialist or Deontological Moral Reasoners?", "summary": "As AI systems increasingly navigate applications in healthcare, law, and\ngovernance, understanding how they handle ethically complex scenarios becomes\ncritical. Previous work has mainly examined the moral judgments in large\nlanguage models (LLMs), rather than their underlying moral reasoning process.\nIn contrast, we focus on a large-scale analysis of the moral reasoning traces\nprovided by LLMs. Furthermore, unlike prior work that attempted to draw\ninferences from only a handful of moral dilemmas, our study leverages over 600\ndistinct trolley problems as probes for revealing the reasoning patterns that\nemerge within different LLMs. We introduce and test a taxonomy of moral\nrationales to systematically classify reasoning traces according to two main\nnormative ethical theories: consequentialism and deontology. Our analysis\nreveals that LLM chains-of-thought tend to favor deontological principles based\non moral obligations, while post-hoc explanations shift notably toward\nconsequentialist rationales that emphasize utility. Our framework provides a\nfoundation for understanding how LLMs process and articulate ethical\nconsiderations, an important step toward safe and interpretable deployment of\nLLMs in high-stakes decision-making environments. Our code is available at\nhttps://github.com/keenansamway/moral-lens .", "authors": ["Keenan Samway", "Max Kleiman-Weiner", "David Guzman Piedrahita", "Rada Mihalcea", "Bernhard Schölkopf", "Zhijing Jin"], "published_date": "2025-05-27", "title_zh": "語言模型是結果論抑或義務論的道德推理者？", "summary_zh": "人工智慧系統於醫療、法律及治理等領域應用日增，其倫理複雜情境之處理至關重要。既有研究多探討大型語言模型之道德判斷，而非其深層道德推理過程。本研究著重於大規模分析語言模型所提供之道德推理軌跡，並利用逾六百個電車難題，揭示不同語言模型內之推理模式。我們導入並測試一套道德理由分類法，依據結果主義與義務論兩大規範倫理理論，系統性地分類推理軌跡。分析顯示，語言模型之思維鏈傾向基於道德義務之義務論原則，事後解釋則顯著轉向強調效用之結果主義理由。本框架為理解語言模型如何處理及闡述倫理考量奠定基礎，有助於在高度風險決策環境中安全且可解釋地部署語言模型。程式碼已公開於https://github.com/keenansamway/moral-lens。", "audio": "audios/2505.21479v1.mp3", "timestamp": "2025-05-28T06:27:20.603558"}
{"query": "Foundation Model", "id": "2505.21357v1", "url": "http://arxiv.org/abs/2505.21357v1", "title": "AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping", "summary": "Accurate crop mapping fundamentally relies on modeling multi-scale\nspatiotemporal patterns, where spatial scales range from individual field\ntextures to landscape-level context, and temporal scales capture both\nshort-term phenological transitions and full growing-season dynamics.\nTransformer-based remote sensing foundation models (RSFMs) offer promising\npotential for crop mapping due to their innate ability for unified\nspatiotemporal processing. However, current RSFMs remain suboptimal for crop\nmapping: they either employ fixed spatiotemporal windows that ignore the\nmulti-scale nature of crop systems or completely disregard temporal information\nby focusing solely on spatial patterns. To bridge these gaps, we present\nAgriFM, a multi-source remote sensing foundation model specifically designed\nfor agricultural crop mapping. Our approach begins by establishing the\nnecessity of simultaneous hierarchical spatiotemporal feature extraction,\nleading to the development of a modified Video Swin Transformer architecture\nwhere temporal down-sampling is synchronized with spatial scaling operations.\nThis modified backbone enables efficient unified processing of long time-series\nsatellite inputs. AgriFM leverages temporally rich data streams from three\nsatellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is\npre-trained on a global representative dataset comprising over 25 million image\nsamples supervised by land cover products. The resulting framework incorporates\na versatile decoder architecture that dynamically fuses these learned\nspatiotemporal representations, supporting diverse downstream tasks.\nComprehensive evaluations demonstrate AgriFM's superior performance over\nconventional deep learning approaches and state-of-the-art general-purpose\nRSFMs across all downstream tasks. Codes will be available at\nurlhttps://github.com/flyakon/AgriFM.", "authors": ["Wenyuan Li", "Shunlin Liang", "Keyan Chen", "Yongzhe Chen", "Han Ma", "Jianglei Xu", "Yichuan Ma", "Shikang Guan", "Husheng Fang", "Zhenwei Shi"], "published_date": "2025-05-27", "title_zh": "AgriFM：用於作物mapping的多源時序遙感基礎模型", "summary_zh": "精準的農作物地圖繪製仰賴多尺度時空模式的建立，其中空間尺度涵蓋田地紋理至景觀層級，時間尺度則包含短期物候變化及完整生長季動態。基於Transformer的遙感基礎模型(RSFM)具備統一時空處理的潛力，然現有模型對農作物地圖繪製仍有不足：或採用固定時空窗而忽略多尺度特性，或僅關注空間模式而忽略時間資訊。為此，我們提出AgriFM，一款專為農業地圖繪製設計的多源遙感基礎模型。本研究首先確立同步階層式時空特徵提取的必要性，並研發改良版的Video Swin Transformer架構，使時間降採樣與空間縮放操作同步。此架構能有效處理長時間序列衛星輸入。AgriFM利用MODIS、Landsat-8/9和Sentinel-2等衛星資料，並以包含超過兩千五百萬影像樣本的全球代表性資料集進行預訓練，該資料集受土地覆蓋產品監督。此框架整合了能動態融合學習到的時空表徵之多功能解碼器架構，支援多樣的下游任務。綜合評估顯示，AgriFM在所有下游任務中均優於傳統深度學習方法及最先進的通用RSFM。", "audio": "audios/2505.21357v1.mp3", "timestamp": "2025-05-28T06:27:28.068773"}
{"query": "Diffusion Model", "id": "2505.21437v1", "url": "http://arxiv.org/abs/2505.21437v1", "title": "CoDA: Coordinated Diffusion Noise Optimization for Whole-Body Manipulation of Articulated Objects", "summary": "Synthesizing whole-body manipulation of articulated objects, including body\nmotion, hand motion, and object motion, is a critical yet challenging task with\nbroad applications in virtual humans and robotics. The core challenges are\ntwofold. First, achieving realistic whole-body motion requires tight\ncoordination between the hands and the rest of the body, as their movements are\ninterdependent during manipulation. Second, articulated object manipulation\ntypically involves high degrees of freedom and demands higher precision, often\nrequiring the fingers to be placed at specific regions to actuate movable\nparts. To address these challenges, we propose a novel coordinated diffusion\nnoise optimization framework. Specifically, we perform noise-space optimization\nover three specialized diffusion models for the body, left hand, and right\nhand, each trained on its own motion dataset to improve generalization.\nCoordination naturally emerges through gradient flow along the human kinematic\nchain, allowing the global body posture to adapt in response to hand motion\nobjectives with high fidelity. To further enhance precision in hand-object\ninteraction, we adopt a unified representation based on basis point sets (BPS),\nwhere end-effector positions are encoded as distances to the same BPS used for\nobject geometry. This unified representation captures fine-grained spatial\nrelationships between the hand and articulated object parts, and the resulting\ntrajectories serve as targets to guide the optimization of diffusion noise,\nproducing highly accurate interaction motion. We conduct extensive experiments\ndemonstrating that our method outperforms existing approaches in motion quality\nand physical plausibility, and enables various capabilities such as object pose\ncontrol, simultaneous walking and manipulation, and whole-body generation from\nhand-only data.", "authors": ["Huaijin Pi", "Zhi Cen", "Zhiyang Dou", "Taku Komura"], "published_date": "2025-05-27", "title_zh": "CoDA：用於具體物件全身操控的協同擴散雜訊最佳化", "summary_zh": "本文提出一種新型協調擴散雜訊優化框架，用於合成人體對關節物體的全身操控動作，包括身體、手部和物體運動。此框架針對身體、左手和右手分別訓練擴散模型，並在雜訊空間進行優化以提升泛化能力。透過人體運動鏈的梯度流，身體姿勢能自然適應手部運動目標。為提高手部與物體互動的精確度，採用基於基點集（BPS）的統一表示，將末端執行器的位置編碼為與物體幾何相同的BPS距離。實驗結果表明，該方法在動作質量和物理合理性方面優於現有方法，並能實現物體姿態控制、同步行走與操控等功能。", "audio": "audios/2505.21437v1.mp3", "timestamp": "2025-05-28T06:27:33.068444"}
{"query": "AI", "id": "2505.21448v1", "url": "http://arxiv.org/abs/2505.21448v1", "title": "OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers", "summary": "Lip synchronization is the task of aligning a speaker's lip movements in\nvideo with corresponding speech audio, and it is essential for creating\nrealistic, expressive video content. However, existing methods often rely on\nreference frames and masked-frame inpainting, which limit their robustness to\nidentity consistency, pose variations, facial occlusions, and stylized content.\nIn addition, since audio signals provide weaker conditioning than visual cues,\nlip shape leakage from the original video will affect lip sync quality. In this\npaper, we present OmniSync, a universal lip synchronization framework for\ndiverse visual scenarios. Our approach introduces a mask-free training paradigm\nusing Diffusion Transformer models for direct frame editing without explicit\nmasks, enabling unlimited-duration inference while maintaining natural facial\ndynamics and preserving character identity. During inference, we propose a\nflow-matching-based progressive noise initialization to ensure pose and\nidentity consistency, while allowing precise mouth-region editing. To address\nthe weak conditioning signal of audio, we develop a Dynamic Spatiotemporal\nClassifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance\nstrength over time and space. We also establish the AIGC-LipSync Benchmark, the\nfirst evaluation suite for lip synchronization in diverse AI-generated videos.\nExtensive experiments demonstrate that OmniSync significantly outperforms prior\nmethods in both visual quality and lip sync accuracy, achieving superior\nresults in both real-world and AI-generated videos.", "authors": ["Ziqiao Peng", "Jiwen Liu", "Haoxian Zhang", "Xiaoqiang Liu", "Songlin Tang", "Pengfei Wan", "Di Zhang", "Hongyan Liu", "Jun He"], "published_date": "2025-05-27", "title_zh": "OmniSync：基於擴散Transformer的通用唇語同步方法", "summary_zh": "唇語同步旨在使影片中講者的唇部動作與對應的語音音訊對齊，這對創造逼真且具表現力的影片內容至關重要。現有方法常依賴參考幀和遮罩幀修復，限制了其在身份一致性、姿勢變化、面部遮擋和風格化內容方面的穩健性。此外，音訊信號的條件約束弱於視覺線索，原始影片的唇形洩漏會影響唇語同步品質。本文提出OmniSync，一種適用於多樣視覺場景的通用唇語同步框架。該方法採用無遮罩訓練範式，使用Diffusion Transformer模型進行直接幀編輯，無需顯式遮罩，從而實現無限時長的推論，同時保持自然的面部動態並保留角色身份。在推論階段，我們提出基於流匹配的漸進式噪聲初始化，以確保姿勢和身份一致性，並允許精確的口部區域編輯。為了解決音訊的弱條件約束信號問題，我們開發了一種動態時空無分類器引導（DS-CFG）機制，可隨時間和空間自適應地調整引導強度。我們還建立了AIGC-LipSync基準，這是首個針對多樣人工智慧生成影片中唇語同步的評估套件。大量實驗表明，OmniSync在視覺品質和唇語同步準確性方面均顯著優於先前方法，在真實世界和人工智慧生成影片中均取得了卓越成果。", "audio": "audios/2505.21448v1.mp3", "timestamp": "2025-05-28T07:18:32.119650"}
{"query": "Foundation Model", "id": "2505.21356v1", "url": "http://arxiv.org/abs/2505.21356v1", "title": "Towards Robust Automated Perceptual Voice Quality Assessment with Deep Learning", "summary": "Objective: Perceptual voice quality assessment plays a critical role in\ndiagnosing and monitoring voice disorders by providing standardized evaluation\nof vocal function. Traditionally, this process relies on expert raters\nutilizing standard scales, such as the Consensus Auditory-Perceptual Evaluation\nof Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain\n(GRBAS). However, these metrics are inherently subjective and susceptible to\ninter-rater variability, motivating the need for automated and objective\nassessment methods. Methods: We propose Voice Quality Assessment Network\n(VOQANet), a deep learning-based framework with an attention mechanism that\nleverages a Speech Foundation Model (SFM) to capture high-level acoustic and\nprosodic information from raw speech. To enhance robustness and\ninterpretability, we present VOQANet+, which integrates handcrafted acoustic\nfeatures such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM\nembeddings. Results: Sentence-based input yields stronger performance than\nvowel-based input, especially at the patient level. VOQANet consistently\noutperforms baseline methods in RMSE and PCC, while VOQANet+ performs even\nbetter and maintains robustness under noisy conditions. Conclusion: Combining\nSFM embeddings with domain-informed acoustic features improves interpretability\nand resilience. Significance: VOQANet+ shows strong potential for deployment in\nreal-world and telehealth settings, addressing the limitations of subjective\nperceptual assessments with an interpretable and noise-resilient solution.", "authors": ["Whenty Ariyanti", "Kuan-Yu Chen", "Sabato Marco Siniscalchi", "Hsin-Min Wang", "Yu Tsao"], "published_date": "2025-05-27", "title_zh": "基於深度學習的穩健自動感知語音品質評估", "summary_zh": "目標：知覺語音品質評估在診斷和監測語音障礙中至關重要，傳統上依賴專家使用CAPE-V和GRBAS等標準量表，但這些指標具主觀性且易受評估者差異影響，故需自動化客觀評估方法。方法：提出基於深度學習的VOQANet，其具備注意力機制，利用語音基礎模型(SFM)捕捉原始語音中的高階聲學和韻律資訊。為提升穩健性和可解釋性，VOQANet+整合了人工設計的聲學特徵，如抖動、閃爍和諧波雜訊比(HNR)與SFM嵌入。結果：基於句子的輸入優於基於母音的輸入，尤其是在患者層面。VOQANet在RMSE和PCC方面始終優於基準方法，VOQANet+表現更佳，並在嘈雜條件下保持穩健性。結論：結合SFM嵌入和領域知識的聲學特徵可提高可解釋性和彈性。意義：VOQANet+在實際應用和遠程醫療環境中展現出強大的潛力，以可解釋且抗噪的解決方案克服了主觀知覺評估的局限性。", "audio": "audios/2505.21356v1.mp3", "timestamp": "2025-05-28T07:18:41.016156"}
{"query": "Diffusion Model", "id": "2505.21426v1", "url": "http://arxiv.org/abs/2505.21426v1", "title": "Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks", "summary": "Agent-Based Models (ABMs) are powerful tools for studying emergent properties\nin complex systems. In ABMs, agent behaviors are governed by local interactions\nand stochastic rules. However, these rules are, in general, non-differentiable,\nlimiting the use of gradient-based methods for optimization, and thus\nintegration with real-world data. We propose a novel framework to learn a\ndifferentiable surrogate of any ABM by observing its generated data. Our method\ncombines diffusion models to capture behavioral stochasticity and graph neural\nnetworks to model agent interactions. Distinct from prior surrogate approaches,\nour method introduces a fundamental shift: rather than approximating\nsystem-level outputs, it models individual agent behavior directly, preserving\nthe decentralized, bottom-up dynamics that define ABMs. We validate our\napproach on two ABMs (Schelling's segregation model and a Predator-Prey\necosystem) showing that it replicates individual-level patterns and accurately\nforecasts emergent dynamics beyond training. Our results demonstrate the\npotential of combining diffusion models and graph learning for data-driven ABM\nsimulation.", "authors": ["Francesco Cozzi", "Marco Pangallo", "Alan Perotti", "André Panisson", "Corrado Monti"], "published_date": "2025-05-27", "title_zh": "基於圖擴散網絡的個體行為學習於基於代理模型中", "summary_zh": "基於代理人模型（ABMs）擅長研究複雜系統中的湧現特性。ABMs中，代理人的行為受局部互動和隨機規則支配，但這些規則通常不可微，限制了梯度方法的優化和與真實數據的整合。本文提出一種新穎框架，透過觀察ABM生成的數據，學習其可微分的替代模型。該方法結合擴散模型捕捉行為隨機性，並利用圖神經網路模擬代理人互動。不同於以往的替代方法，本文直接模擬個體代理人行為，而非近似系統層級的輸出，保留了定義ABM的去中心化、自下而上的動態特性。在Schelling隔離模型和捕食者-獵物生態系統兩個ABM上的驗證表明，該方法能複製個體層級的模式，並準確預測訓練範圍外的湧現動態。研究結果展示了結合擴散模型和圖學習進行數據驅動ABM模擬的潛力。", "audio": "audios/2505.21426v1.mp3", "timestamp": "2025-05-28T07:18:47.788390"}
{"query": "AI", "id": "2505.21445v1", "url": "http://arxiv.org/abs/2505.21445v1", "title": "VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin", "summary": "The performance of speaker verification systems is adversely affected by\nspeaker aging. However, due to challenges in data collection, particularly the\nlack of sustained and large-scale longitudinal data for individuals, research\non speaker aging remains difficult. In this paper, we present VoxAging, a\nlarge-scale longitudinal dataset collected from 293 speakers (226 English\nspeakers and 67 Mandarin speakers) over several years, with the longest time\nspan reaching 17 years (approximately 900 weeks). For each speaker, the data\nwere recorded at weekly intervals. We studied the phenomenon of speaker aging\nand its effects on advanced speaker verification systems, analyzed individual\nspeaker aging processes, and explored the impact of factors such as age group\nand gender on speaker aging research.", "authors": ["Zhiqi Ai", "Meixuan Bao", "Zhiyong Chen", "Zhi Yang", "Xinnuo Li", "Shugong Xu"], "published_date": "2025-05-27", "title_zh": "VoxAging：基於大型縱向語料庫的英漢雙語持續追蹤說話人聲紋老化", "summary_zh": "語者驗證系統效能受語者老化影響。然因數據蒐集挑戰，特別是缺乏個人長期大規模縱向數據，語者老化研究困難重重。本文提出VoxAging，一大型縱向數據集，包含293位語者(226位英語語者及67位華語語者)歷年數據，最長跨度達17年(約900週)。每位語者數據按週記錄。研究語者老化現象及其對先進語者驗證系統之影響，分析個別語者老化過程，並探討年齡層及性別等因素對語者老化研究之影響。", "audio": "audios/2505.21445v1.mp3", "timestamp": "2025-05-28T08:25:53.699498"}
{"query": "Foundation Model", "id": "2505.21322v1", "url": "http://arxiv.org/abs/2505.21322v1", "title": "Assured Autonomy with Neuro-Symbolic Perception", "summary": "Many state-of-the-art AI models deployed in cyber-physical systems (CPS),\nwhile highly accurate, are simply pattern-matchers.~With limited security\nguarantees, there are concerns for their reliability in safety-critical and\ncontested domains. To advance assured AI, we advocate for a paradigm shift that\nimbues data-driven perception models with symbolic structure, inspired by a\nhuman's ability to reason over low-level features and high-level context. We\npropose a neuro-symbolic paradigm for perception (NeuSPaPer) and illustrate how\njoint object detection and scene graph generation (SGG) yields deep scene\nunderstanding.~Powered by foundation models for offline knowledge extraction\nand specialized SGG algorithms for real-time deployment, we design a framework\nleveraging structured relational graphs that ensures the integrity of\nsituational awareness in autonomy. Using physics-based simulators and\nreal-world datasets, we demonstrate how SGG bridges the gap between low-level\nsensor perception and high-level reasoning, establishing a foundation for\nresilient, context-aware AI and advancing trusted autonomy in CPS.", "authors": ["R. Spencer Hallyburton", "Miroslav Pajic"], "published_date": "2025-05-27", "title_zh": "基於神經符號感知的可靠自主性", "summary_zh": "當前網路實體系統中的先進AI模型雖精準，但僅為模式匹配，缺乏安全保障，難以在安全攸關領域應用。為提升AI可靠性，我們提倡將符號結構融入數據驅動的感知模型，模擬人類基於低階特徵和高階語境的推理能力。我們提出神經符號感知範例NeuSPaPer，藉由物件偵測和場景圖生成(SGG)實現深度場景理解。此框架利用基礎模型離線提取知識，並以專業SGG演算法即時部署，構建結構化關係圖，確保自主系統情境感知的完整性。透過物理模擬器和真實數據集，我們證明SGG能彌合低階感測器感知和高階推理之間的差距，為具韌性且感知語境的AI奠定基礎，進而推進網路實體系統中可信賴的自主性。", "audio": "audios/2505.21322v1.mp3", "timestamp": "2025-05-28T08:25:59.720613"}
{"query": "Diffusion Model", "id": "2505.21400v1", "url": "http://arxiv.org/abs/2505.21400v1", "title": "A Convergence Theory for Diffusion Language Models: An Information-Theoretic Perspective", "summary": "Diffusion models have emerged as a powerful paradigm for modern generative\nmodeling, demonstrating strong potential for large language models (LLMs).\nUnlike conventional autoregressive (AR) models that generate tokens\nsequentially, diffusion models enable parallel token sampling, leading to\nfaster generation and eliminating left-to-right generation constraints. Despite\ntheir empirical success, the theoretical understanding of diffusion model\napproaches remains underdeveloped. In this work, we develop convergence\nguarantees for diffusion language models from an information-theoretic\nperspective. Our analysis demonstrates that the sampling error, measured by the\nKullback-Leibler (KL) divergence, decays inversely with the number of\niterations $T$ and scales linearly with the mutual information between tokens\nin the target text sequence. In particular, we establish matching upper and\nlower bounds, up to some constant factor, to demonstrate the tightness of our\nconvergence analysis. These results offer novel theoretical insights into the\npractical effectiveness of diffusion language models.", "authors": ["Gen Li", "Changxiao Cai"], "published_date": "2025-05-27", "title_zh": "擴散語言模型收斂理論：資訊理論視角", "summary_zh": "擴散模型已成為強大的生成模型範例，對大型語言模型展現巨大潛力。相較於依序生成詞元的傳統自迴歸模型，擴散模型能平行取樣詞元，加速生成並消除由左至右的生成限制。儘管實證上獲得成功，對擴散模型方法的理論理解仍不充分。本研究從資訊理論角度發展擴散語言模型的收斂性保證。分析表明，以Kullback-Leibler散度衡量的取樣誤差，與迭代次數T成反比衰減，並與目標文本序列中詞元間的互信息成線性比例。具體而言，我們建立了匹配的上下界（僅相差常數因子），以證明收斂性分析的嚴謹性。這些結果為擴散語言模型的實際有效性提供了新的理論見解。", "audio": "audios/2505.21400v1.mp3", "timestamp": "2025-05-28T08:26:05.057172"}
{"query": "AI", "id": "2505.21419v1", "url": "http://arxiv.org/abs/2505.21419v1", "title": "Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs", "summary": "Today's cloud-hosted applications and services are complex systems, and a\nperformance or functional instability can have dozens or hundreds of potential\nroot causes. Our hypothesis is that by combining the pattern matching\ncapabilities of modern AI tools with a natural multi-modal RAG LLM interface,\nproblem identification and resolution can be simplified. ARCA is a new\nmulti-modal RAG LLM system that targets this domain. Step-wise evaluations show\nthat ARCA outperforms state-of-the-art alternatives.", "authors": ["Yifan Wang", "Kenneth P. Birman"], "published_date": "2025-05-27", "title_zh": "利用多模態RAG LLM診斷與解決雲平台不穩定性", "summary_zh": "現今雲端應用程式及服務複雜，效能或功能不穩定可能有多種潛在根源。本研究假設，結合現代AI工具的模式匹配能力與自然多模態RAG LLM介面，可簡化問題識別與解決。ARCA為一種針對此領域的新型多模態RAG LLM系統。逐步評估顯示，ARCA優於現有技術。", "audio": "audios/2505.21419v1.mp3", "timestamp": "2025-05-28T09:20:24.090597"}
{"query": "Foundation Model", "id": "2505.21317v1", "url": "http://arxiv.org/abs/2505.21317v1", "title": "A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features", "summary": "Understanding cellular responses to stimuli is crucial for biological\ndiscovery and drug development. Transcriptomics provides interpretable,\ngene-level insights, while microscopy imaging offers rich predictive features\nbut is harder to interpret. Weakly paired datasets, where samples share\nbiological states, enable multimodal learning but are scarce, limiting their\nutility for training and multimodal inference. We propose a framework to\nenhance transcriptomics by distilling knowledge from microscopy images. Using\nweakly paired data, our method aligns and binds modalities, enriching gene\nexpression representations with morphological information. To address data\nscarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal\ndistillation using pretrained foundation models, achieving state-of-the-art\nresults, and (2) PEA (Perturbation Embedding Augmentation), a novel\naugmentation technique that enhances transcriptomics data while preserving\ninherent biological information. These strategies improve the predictive power\nand retain the interpretability of transcriptomics, enabling rich unimodal\nrepresentations for complex biological tasks.", "authors": ["Ihab Bendidi", "Yassir El Mesbahi", "Alisandra K. Denton", "Karush Suri", "Kian Kenyon-Dean", "Auguste Genovesio", "Emmanuel Noutahi"], "published_date": "2025-05-27", "title_zh": "一種透過形態特徵提升轉錄組學表徵的跨模態知識蒸餾與數據增強方法", "summary_zh": "理解細胞對刺激的反應對於生物學發現和藥物開發至關重要。轉錄體學提供可解釋的基因層面洞察，而顯微鏡成像提供豐富的預測特徵，但難以解釋。弱配對數據集（樣本共享生物學狀態）支持多模態學習，但稀缺性限制了其訓練和多模態推斷的效用。我們提出一個框架，通過從顯微鏡圖像中提取知識來增強轉錄體學。利用弱配對數據，該方法對齊並結合模態，利用形態學資訊豐富基因表達表示。為了解決數據稀缺問題，我們引入了(1) Semi-Clipped，一種改編自CLIP的跨模態蒸餾方法，利用預訓練基礎模型實現最先進的成果，以及(2) PEA（擾動嵌入增強），一種新型增強技術，可在保留固有生物學資訊的同時增強轉錄體學數據。這些策略提高了轉錄體學的預測能力並保留其可解釋性，從而為複雜的生物學任務提供豐富的單模態表示。", "audio": "audios/2505.21317v1.mp3", "timestamp": "2025-05-28T09:20:31.050157"}
{"query": "Diffusion Model", "id": "2505.21325v1", "url": "http://arxiv.org/abs/2505.21325v1", "title": "MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on", "summary": "Video Virtual Try-On (VVT) aims to simulate the natural appearance of\ngarments across consecutive video frames, capturing their dynamic variations\nand interactions with human body motion. However, current VVT methods still\nface challenges in terms of spatiotemporal consistency and garment content\npreservation. First, they use diffusion models based on the U-Net, which are\nlimited in their expressive capability and struggle to reconstruct complex\ndetails. Second, they adopt a separative modeling approach for spatial and\ntemporal attention, which hinders the effective capture of structural\nrelationships and dynamic consistency across frames. Third, their expression of\ngarment details remains insufficient, affecting the realism and stability of\nthe overall synthesized results, especially during human motion. To address the\nabove challenges, we propose MagicTryOn, a video virtual try-on framework built\nupon the large-scale video diffusion Transformer.We replace the U-Net\narchitecture with a diffusion Transformer and combine full self-attention to\njointly model the spatiotemporal consistency of videos. We design a\ncoarse-to-fine garment preservation strategy. The coarse strategy integrates\ngarment tokens during the embedding stage, while the fine strategy incorporates\nmultiple garment-based conditions, such as semantics, textures, and contour\nlines during the denoising stage. Moreover, we introduce a mask-aware loss to\nfurther optimize garment region fidelity. Extensive experiments on both image\nand video try-on datasets demonstrate that our method outperforms existing SOTA\nmethods in comprehensive evaluations and generalizes to in-the-wild scenarios.", "authors": ["Guangyuan Li", "Siming Zheng", "Hao Zhang", "Jinwei Chen", "Junsheng Luan", "Binkai Ou", "Lei Zhao", "Bo Li", "Peng-Tao Jiang"], "published_date": "2025-05-27", "title_zh": "MagicTryOn：基於擴散Transformer的服裝保留式影片虛擬試穿", "summary_zh": "視訊虛擬試穿旨在模擬服裝在連續影格中的自然外觀，捕捉動態變化與人體動作互動。現有方法在時空一致性和服裝內容保留方面仍面臨挑戰。其一，基於U-Net的擴散模型表達能力有限，難以重建複雜細節。其二，空間與時間注意力分離建模阻礙跨影格結構關係與動態一致性的有效捕捉。其三，服裝細節表達不足，影響合成結果的真實性與穩定性，尤其在人體運動時。為解決上述問題，我們提出MagicTryOn，一個基於大規模視訊擴散Transformer的視訊虛擬試穿框架。採用擴散Transformer取代U-Net，並結合全自注意力聯合建模視訊的時空一致性。設計由粗到細的服裝保留策略，粗略策略在嵌入階段整合服裝標記，精細策略在去噪階段納入多種基於服裝的條件，如語義、紋理和輪廓線。此外，引入掩碼感知損失以優化服裝區域逼真度。在圖像和視訊試穿數據集上的大量實驗表明，該方法在綜合評估中優於現有技術，並能推廣至實際場景。", "audio": "audios/2505.21325v1.mp3", "timestamp": "2025-05-28T09:20:38.959911"}
{"query": "AI", "id": "2505.21418v1", "url": "http://arxiv.org/abs/2505.21418v1", "title": "Autonomous Multi-Modal LLM Agents for Treatment Planning in Focused Ultrasound Ablation Surgery", "summary": "Focused Ultrasound Ablation Surgery (FUAS) has emerged as a promising\nnon-invasive therapeutic modality, valued for its safety and precision.\nNevertheless, its clinical implementation entails intricate tasks such as\nmultimodal image interpretation, personalized dose planning, and real-time\nintraoperative decision-making processes that demand intelligent assistance to\nimprove efficiency and reliability. We introduce FUAS-Agents, an autonomous\nagent system that leverages the multimodal understanding and tool-using\ncapabilities of large language models (LLMs). By integrating patient profiles\nand MRI data, FUAS-Agents orchestrates a suite of specialized medical AI tools,\nincluding segmentation, treatment dose prediction, and clinical guideline\nretrieval, to generate personalized treatment plans comprising MRI image, dose\nparameters, and therapeutic strategies. We evaluate the system in a uterine\nfibroid treatment scenario. Human assessment by four senior FUAS experts\nindicates that 82.5%, 82.5%, 87.5%, and 97.5% of the generated plans were rated\n4 or above (on a 5-point scale) in terms of completeness, accuracy, fluency,\nand clinical compliance, respectively. These results demonstrate the potential\nof LLM-driven agents in enhancing decision-making across complex clinical\nworkflows, and exemplify a translational paradigm that combines general-purpose\nmodels with specialized expert systems to solve practical challenges in\nvertical healthcare domains.", "authors": ["Lina Zhao", "Jiaxing Bai", "Zihao Bian", "Qingyue Chen", "Yafang Li", "Guangbo Li", "Min He", "Huaiyuan Yao", "Zongjiu Zhang"], "published_date": "2025-05-27", "title_zh": "聚焦超音波消融手術治療計畫之自主多模態大型語言模型代理", "summary_zh": "聚焦超音波消融手術(FUAS)是一種具前景的非侵入性療法，但其臨床應用涉及複雜的多模態影像判讀、個人化劑量計畫和即時術中決策。為提升效率和可靠性，我們提出FUAS-Agents，一種利用大型語言模型(LLM)多模態理解和工具使用能力的自主代理系統。該系統整合患者資料和MRI影像，協調分割、治療劑量預測和臨床指南檢索等專業醫療AI工具，生成包含MRI影像、劑量參數和治療策略的個人化治療計畫。在子宮肌瘤治療案例中，四位資深FUAS專家評估結果顯示，生成計畫在完整性、準確性、流暢性和臨床合規性方面，分別有82.5%、82.5%、87.5%和97.5%獲得4分以上（5分制）。結果表明，LLM驅動的代理系統有潛力加強複雜臨床流程中的決策，並展示了一種將通用模型與專業知識系統結合，以解決垂直醫療領域實際挑戰的轉化範例。", "audio": "audios/2505.21418v1.mp3", "timestamp": "2025-05-28T10:20:55.376053"}
{"query": "Foundation Model", "id": "2505.21237v1", "url": "http://arxiv.org/abs/2505.21237v1", "title": "Unfolding A Few Structures for The Many: Memory-Efficient Compression of Conformer and Speech Foundation Models", "summary": "This paper presents a novel memory-efficient model compression approach for\nConformer ASR and speech foundation systems. Our approach features a unique\n\"small-to-large\" design. A compact \"seed\" model containing a few Conformer or\nTransformer blocks is trained and unfolded many times to emulate the\nperformance of larger uncompressed models with different logical depths. The\nseed model and many unfolded paths are jointly trained within a single\nunfolding cycle. The KL-divergence between the largest unfolded and smallest\nseed models is used in a self-distillation process to minimize their\nperformance disparity. Experimental results show that our foldable model\nproduces ASR performance comparable to individually constructed Conformer and\nwav2vec2/HuBERT speech foundation models under various depth configurations,\nwhile requiring only minimal memory and storage. Conformer and wav2vec2 models\nwith a reduction of 35% and 30% parameters are obtained without loss of\nperformance, respectively.", "authors": ["Zhaoqing Li", "Haoning Xu", "Xurong Xie", "Zengrui Jin", "Tianzi Wang", "Xunying Liu"], "published_date": "2025-05-27", "title_zh": "為眾多應用解構少數架構：Conformer與語音基礎模型之記憶體效率壓縮", "summary_zh": "本文提出一種針對Conformer語音辨識和語音基礎系統的新型記憶體效率模型壓縮方法。該方法採用獨特的「由小到大」設計，訓練一個包含少量Conformer或Transformer模塊的精簡「種子」模型，並多次展開以模擬具有不同邏輯深度的大型未壓縮模型的性能。種子模型和多個展開路徑在單個展開週期內聯合訓練。最大展開模型和最小種子模型之間的KL散度用於自我蒸餾過程，以最小化它們的性能差異。實驗結果表明，我們的可折疊模型在各種深度配置下產生與單獨構建的Conformer和wav2vec2/HuBERT語音基礎模型相當的語音辨識性能，同時僅需最小的記憶體和儲存空間。在不損失性能的情況下，分別獲得參數減少35%和30%的Conformer和wav2vec2模型。", "audio": "audios/2505.21237v1.mp3", "timestamp": "2025-05-28T10:21:00.674808"}
{"query": "Diffusion Model", "id": "2505.21223v1", "url": "http://arxiv.org/abs/2505.21223v1", "title": "Simulations of the churning mode: toroidally symmetric plasma convection and turbulence around the X-points in a snowflake divertor", "summary": "Using a reduced MHD model, extended to include field-aligned thermal\nconduction, we present numerical simulations of the churning mode (CM): a\ntoroidally symmetric, non-linear plasma vortex in the vicinity of the null\npoints in a snowflake (SF) divertor (Ryutov et al., Phys. Scr. 89 088002,\n2014). Simulations are carried out across a range of inter-null separations,\n$d_{xx}$, and inter-null orientations, $\\theta$, primarily in conditions\nrelevant to the MAST-U tokamak. We find that, when $d_{xx}$ is small, the CM\ninduces additional transport across the X-points when $\\beta_{pm} \\gtrsim 8$ %,\nwhere $\\beta_{pm}$ is the ratio of the plasma pressure in the null region to\npoloidal magnetic pressure at the midplane. This transport also increases\napproximately linearly as $d_{xx}$ is reduced. A diffusive model of this\ntransport is shown to predict the total transport across the null points, where\ndiffusion coefficients of up to $\\sim 10^2$ m$^2$s$^{-1}$ centred on a small\nregion around the X-points are used. However, the CM also results in\nsignificant changes to the flux surfaces in the null region which is not\ncaptured by this diffusive model. The changes in magnetic geometry mean the\nfractional exhaust power delivered to each divertor leg is highly sensitive to\n$\\beta_{pm}$, $d_{xx}$ and $\\theta$. For small values of $\\theta$, the CM can\ninduce a change in topology, redirecting exhaust power from a secondary\ndivertor leg on the high field side to one on the low field side. Similar\nbehaviour is found in the fraction of exhaust power going to the inner and\nouter divertor. Such changes in the flux surfaces may not be captured by\nGrad-Shafranov solvers and so may be a source of error in the magnetic\nreconstruction of SF experiments. We consistently find that the fractional\nexhaust power going to a secondary divertor leg on the high field side is\nsmall, consistent with SF experiments.", "authors": ["D Power", "M V Umansky", "V A Soukhanovskii"], "published_date": "2025-05-27", "title_zh": "攪盪模式模擬：雪花散熱器中環向對稱電漿對流與X點周圍的湍流", "summary_zh": "本研究利用包含場向熱傳導的簡化磁流體動力學模型，數值模擬了雪花型偏濾器零點附近的環向對稱非線性電漿渦旋——攪動模。模擬涵蓋不同零點間距$d_{xx}$與零點方向$\\theta$，主要針對MAST-U都卜勒實驗裝置的條件。研究發現，當$d_{xx}$較小時，若$\\beta_{pm}$（零點區域電漿壓力與中平面極向磁壓之比）超過8%，攪動模會在X點間產生額外傳輸，且此傳輸量隨著$d_{xx}$減小而近似線性增加。擴散模型能預測零點間的總傳輸，擴散係數高達$\\sim 10^2$ m$^2$s$^{-1}$，集中於X點周圍的小區域。然而，攪動模也顯著改變了零點區域的磁通面結構，此點擴散模型無法捕捉。磁場幾何結構的變化導致各偏濾器支腳的排氣功率分配比例對$\\beta_{pm}$、$d_{xx}$和$\\theta$高度敏感。對於較小的$\\theta$值，攪動模能誘發拓撲結構變化，將排氣功率從高場側的次級偏濾器支腳重新導向低場側。內外偏濾器的排氣功率分配比例也呈現類似行為。Grad-Shafranov解算器可能無法捕捉磁通面的這些變化，進而造成雪花型實驗磁場重構的誤差。研究一致發現，高場側次級偏濾器支腳的排氣功率比例較小，與雪花型實驗結果一致。", "audio": "audios/2505.21223v1.mp3", "timestamp": "2025-05-28T10:21:10.477577"}
{"query": "AI", "id": "2505.21398v1", "url": "http://arxiv.org/abs/2505.21398v1", "title": "A Structured Unplugged Approach for Foundational AI Literacy in Primary Education", "summary": "Younger generations are growing up in a world increasingly shaped by\nintelligent technologies, making early AI literacy crucial for developing the\nskills to critically understand and navigate them. However, education in this\nfield often emphasizes tool-based learning, prioritizing usage over\nunderstanding the underlying concepts. This lack of knowledge leaves\nnon-experts, especially children, prone to misconceptions, unrealistic\nexpectations, and difficulties in recognizing biases and stereotypes. In this\npaper, we propose a structured and replicable teaching approach that fosters\nfoundational AI literacy in primary students, by building upon core\nmathematical elements closely connected to and of interest in primary\ncurricula, to strengthen conceptualization, data representation, classification\nreasoning, and evaluation of AI. To assess the effectiveness of our approach,\nwe conducted an empirical study with thirty-one fifth-grade students across two\nclasses, evaluating their progress through a post-test and a satisfaction\nsurvey. Our results indicate improvements in terminology understanding and\nusage, features description, logical reasoning, and evaluative skills, with\nstudents showing a deeper comprehension of decision-making processes and their\nlimitations. Moreover, the approach proved engaging, with students particularly\nenjoying activities that linked AI concepts to real-world reasoning. Materials:\nhttps://github.com/tail-unica/ai-literacy-primary-ed.", "authors": ["Maria Cristina Carrisi", "Mirko Marras", "Sara Vergallo"], "published_date": "2025-05-27", "title_zh": "小學基礎人工智慧素養結構化離線教學法", "summary_zh": "年輕世代在智慧科技日益普及的環境中成長，早期的人工智慧素養至關重要，能培養批判性理解和應對能力。現有教育多著重工具使用，忽略底層概念理解，導致非專業人士，特別是兒童，容易產生誤解、不切實際的期望，以及難以辨識偏見和刻板印象。本文提出一套結構化且可複製的教學方法，透過與小學課程密切相關的核心數學元素，培養小學生的基礎人工智慧素養，強化概念化、資料呈現、分類推理和人工智慧評估能力。我們對兩班共31名五年級學生進行實證研究，通過後測和滿意度調查評估學習成效。結果顯示，學生在術語理解與使用、特徵描述、邏輯推理和評估技能方面均有提升，並對決策過程及其局限性有更深入的理解。此方法亦具吸引力，學生尤其喜愛將人工智慧概念與現實世界推理連結的活動。材料：https://github.com/tail-unica/ai-literacy-primary-ed", "audio": "audios/2505.21398v1.mp3", "timestamp": "2025-05-28T11:16:28.698552"}
{"query": "Foundation Model", "id": "2505.21137v1", "url": "http://arxiv.org/abs/2505.21137v1", "title": "Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction", "summary": "Spoken Grammatical Error Correction (SGEC) and Feedback (SGECF) are crucial\nfor second language learners, teachers and test takers. Traditional SGEC\nsystems rely on a cascaded pipeline consisting of an ASR, a module for\ndisfluency detection (DD) and removal and one for GEC. With the rise of\nend-to-end (E2E) speech foundation models, we investigate their effectiveness\nin SGEC and feedback generation. This work introduces a pseudo-labelling\nprocess to address the challenge of limited labelled data, expanding the\ntraining data size from 77 hours to approximately 2500 hours, leading to\nimproved performance. Additionally, we prompt an E2E Whisper-based SGEC model\nwith fluent transcriptions, showing a slight improvement in SGEC performance,\nwith more significant gains in feedback generation. Finally, we assess the\nimpact of increasing model size, revealing that while pseudo-labelled data does\nnot yield performance gain for a larger Whisper model, training with prompts\nproves beneficial.", "authors": ["Mengjie Qian", "Rao Ma", "Stefano Bannò", "Kate M. Knill", "Mark J. F. Gales"], "published_date": "2025-05-27", "title_zh": "規模化與提示工程：提升端到端語音語法錯誤校正", "summary_zh": "口語語法錯誤更正(SGEC)及回饋(SGECF)對二語學習者至關重要。傳統SGEC系統仰賴ASR、語暢性檢測與移除、及GEC模組串聯。本研究探討端到端(E2E)語音基礎模型在SGEC和回饋生成中的效用。為了解決標註數據有限的問題，引入偽標籤流程，將訓練數據從77小時擴展到約2500小時，提升性能。此外，使用流暢的文本提示E2E Whisper模型，略微改善SGEC性能，顯著提升回饋生成效果。評估模型尺寸影響，發現偽標籤數據對較大Whisper模型無助益，但提示訓練有效。", "audio": "audios/2505.21137v1.mp3", "timestamp": "2025-05-28T11:16:35.330447"}
{"query": "Diffusion Model", "id": "2505.21205v1", "url": "http://arxiv.org/abs/2505.21205v1", "title": "Sci-Fi: Symmetric Constraint for Frame Inbetweening", "summary": "Frame inbetweening aims to synthesize intermediate video sequences\nconditioned on the given start and end frames. Current state-of-the-art methods\nmainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs)\nby incorporating end-frame constraints via directly fine-tuning or omitting\ntraining. We identify a critical limitation in their design: Their injections\nof the end-frame constraint usually utilize the same mechanism that originally\nimposed the start-frame (single image) constraint. However, since the original\nI2V-DMs are adequately trained for the start-frame condition in advance,\nnaively introducing the end-frame constraint by the same mechanism with much\nless (even zero) specialized training probably can't make the end frame have a\nstrong enough impact on the intermediate content like the start frame. This\nasymmetric control strength of the two frames over the intermediate content\nlikely leads to inconsistent motion or appearance collapse in generated frames.\nTo efficiently achieve symmetric constraints of start and end frames, we\npropose a novel framework, termed Sci-Fi, which applies a stronger injection\nfor the constraint of a smaller training scale. Specifically, it deals with the\nstart-frame constraint as before, while introducing the end-frame constraint by\nan improved mechanism. The new mechanism is based on a well-designed\nlightweight module, named EF-Net, which encodes only the end frame and expands\nit into temporally adaptive frame-wise features injected into the I2V-DM. This\nmakes the end-frame constraint as strong as the start-frame constraint,\nenabling our Sci-Fi to produce more harmonious transitions in various\nscenarios. Extensive experiments prove the superiority of our Sci-Fi compared\nwith other baselines.", "authors": ["Liuhan Chen", "Xiaodong Cun", "Xiaoyu Li", "Xianyi He", "Shenghai Yuan", "Jie Chen", "Ying Shan", "Li Yuan"], "published_date": "2025-05-27", "title_zh": "科幻：用於幀間預測的對稱約束", "summary_zh": "影格補間旨在根據給定的起始和結束影格合成中間影片序列。現有方法主要擴展預訓練的圖像到影片擴散模型，透過微調或省略訓練來加入結束影格約束。然而，這些方法在設計上存在缺陷：它們通常使用與施加起始影格約束相同的機制來注入結束影格約束。由於原始圖像到影片擴散模型已充分訓練以適應起始影格，因此以相同機制引入訓練量不足的結束影格約束，可能無法像起始影格一樣對中間內容產生足夠強的影響。這種兩影格間控制強度的不對稱性可能導致生成影格中的運動不一致或外觀崩潰。為有效實現起始和結束影格的對稱約束，我們提出名為Sci-Fi的新框架，以更小的訓練規模實現更強的約束注入。具體而言，Sci-Fi沿用既有方式處理起始影格約束，同時透過改良機制引入結束影格約束。此機制基於精心設計的輕量級模組EF-Net，僅編碼結束影格並將其擴展為時序自適應的逐影格特徵，注入到圖像到影片擴散模型中。這使結束影格約束與起始影格約束同樣強大，使Sci-Fi能夠在各種場景中產生更協調的轉變。廣泛實驗證明Sci-Fi優於其他基準方法。", "audio": "audios/2505.21205v1.mp3", "timestamp": "2025-05-28T11:16:43.758405"}
{"query": "AI", "id": "2505.21355v1", "url": "http://arxiv.org/abs/2505.21355v1", "title": "Prostate Cancer Screening with Artificial Intelligence-Enhanced Micro-Ultrasound: A Comparative Study with Traditional Methods", "summary": "Background and objective: Micro-ultrasound (micro-US) is a novel imaging\nmodality with diagnostic accuracy comparable to MRI for detecting clinically\nsignificant prostate cancer (csPCa). We investigated whether artificial\nintelligence (AI) interpretation of micro-US can outperform clinical screening\nmethods using PSA and digital rectal examination (DRE). Methods: We\nretrospectively studied 145 men who underwent micro-US guided biopsy (79 with\ncsPCa, 66 without). A self-supervised convolutional autoencoder was used to\nextract deep image features from 2D micro-US slices. Random forest classifiers\nwere trained using five-fold cross-validation to predict csPCa at the slice\nlevel. Patients were classified as csPCa-positive if 88 or more consecutive\nslices were predicted positive. Model performance was compared with a\nclassifier using PSA, DRE, prostate volume, and age. Key findings and\nlimitations: The AI-based micro-US model and clinical screening model achieved\nAUROCs of 0.871 and 0.753, respectively. At a fixed threshold, the micro-US\nmodel achieved 92.5% sensitivity and 68.1% specificity, while the clinical\nmodel showed 96.2% sensitivity but only 27.3% specificity. Limitations include\na retrospective single-center design and lack of external validation.\nConclusions and clinical implications: AI-interpreted micro-US improves\nspecificity while maintaining high sensitivity for csPCa detection. This method\nmay reduce unnecessary biopsies and serve as a low-cost alternative to\nPSA-based screening. Patient summary: We developed an AI system to analyze\nprostate micro-ultrasound images. It outperformed PSA and DRE in detecting\naggressive cancer and may help avoid unnecessary biopsies.", "authors": ["Muhammad Imran", "Wayne G. Brisbane", "Li-Ming Su", "Jason P. Joseph", "Wei Shao"], "published_date": "2025-05-27", "title_zh": "人工智慧增強型微超音波前列腺癌篩檢：與傳統方法的比較研究", "summary_zh": "背景與目標：微型超聲(micro-US)是一種新的影像技術，診斷臨床顯著前列腺癌(csPCa)的準確性與MRI相當。本研究探討使用AI解讀微型超聲，能否優於PSA和肛門指檢(DRE)等臨床篩檢方法。方法：回顧性研究145名接受微型超聲引導活檢的男性（79名患有csPCa，66名未患病）。使用自監督卷積自編碼器從2D微型超聲切片中提取深度影像特徵。使用五重交叉驗證訓練隨機森林分類器，以預測切片層面的csPCa。若連續88個或更多切片預測為陽性，則將患者分類為csPCa陽性。模型效能與使用PSA、DRE、前列腺體積和年齡的分類器進行比較。主要發現與局限性：基於AI的微型超聲模型和臨床篩檢模型，其曲線下面積(AUROC)分別為0.871和0.753。在固定閾值下，微型超聲模型達到92.5%的敏感性和68.1%的特異性，而臨床模型顯示96.2%的敏感性，但特異性僅為27.3%。局限性包括回顧性單中心設計和缺乏外部驗證。結論與臨床意義：AI解讀的微型超聲提高了csPCa檢測的特異性，同時保持了高敏感性。此方法可能減少不必要的活檢，並可作為基於PSA篩檢的低成本替代方案。患者總結：我們開發了一種AI系統來分析前列腺微型超聲影像。它在檢測侵襲性癌症方面優於PSA和DRE，可能有助於避免不必要的活檢。", "audio": "audios/2505.21355v1.mp3", "timestamp": "2025-05-28T12:38:42.967318"}
{"query": "Foundation Model", "id": "2505.21050v1", "url": "http://arxiv.org/abs/2505.21050v1", "title": "Advancing high-fidelity 3D and Texture Generation with 2.5D latents", "summary": "Despite the availability of large-scale 3D datasets and advancements in 3D\ngenerative models, the complexity and uneven quality of 3D geometry and texture\ndata continue to hinder the performance of 3D generation techniques. In most\nexisting approaches, 3D geometry and texture are generated in separate stages\nusing different models and non-unified representations, frequently leading to\nunsatisfactory coherence between geometry and texture. To address these\nchallenges, we propose a novel framework for joint generation of 3D geometry\nand texture. Specifically, we focus in generate a versatile 2.5D\nrepresentations that can be seamlessly transformed between 2D and 3D. Our\napproach begins by integrating multiview RGB, normal, and coordinate images\ninto a unified representation, termed as 2.5D latents. Next, we adapt\npre-trained 2D foundation models for high-fidelity 2.5D generation, utilizing\nboth text and image conditions. Finally, we introduce a lightweight 2.5D-to-3D\nrefiner-decoder framework that efficiently generates detailed 3D\nrepresentations from 2.5D images. Extensive experiments demonstrate that our\nmodel not only excels in generating high-quality 3D objects with coherent\nstructure and color from text and image inputs but also significantly\noutperforms existing methods in geometry-conditioned texture generation.", "authors": ["Xin Yang", "Jiantao Lin", "Yingjie Xu", "Haodong Li", "Yingcong Chen"], "published_date": "2025-05-27", "title_zh": "以2.5D潛變量推進高保真3D與紋理生成", "summary_zh": "儘管大型3D數據集和3D生成模型有所進展，但3D幾何和紋理數據的複雜性及品質不均仍阻礙了3D生成技術的效能。現有方法多半分階段生成3D幾何和紋理，採用不同模型和非統一表示，導致幾何與紋理間的連貫性不佳。為了解決這些問題，本文提出一個聯合生成3D幾何和紋理的新框架，著重於生成可在2D和3D間無縫轉換的多功能2.5D表示。該方法首先整合多視角RGB、法線和座標圖像到一個統一的2.5D潛在表示中。接著，調整預訓練的2D基礎模型，用於高保真2.5D生成，並利用文本和圖像條件。最後，引入一個輕量級的2.5D到3D精煉器-解碼器框架，有效率地從2.5D圖像生成細緻的3D表示。實驗結果表明，該模型不僅擅長從文本和圖像輸入生成具有連貫結構和色彩的高品質3D物件，且在幾何條件紋理生成方面顯著優於現有方法。", "audio": "audios/2505.21050v1.mp3", "timestamp": "2025-05-28T12:38:49.885974"}
{"query": "Diffusion Model", "id": "2505.21179v1", "url": "http://arxiv.org/abs/2505.21179v1", "title": "Normalized Attention Guidance: Universal Negative Guidance for Diffusion Model", "summary": "Negative guidance -- explicitly suppressing unwanted attributes -- remains a\nfundamental challenge in diffusion models, particularly in few-step sampling\nregimes. While Classifier-Free Guidance (CFG) works well in standard settings,\nit fails under aggressive sampling step compression due to divergent\npredictions between positive and negative branches. We present Normalized\nAttention Guidance (NAG), an efficient, training-free mechanism that applies\nextrapolation in attention space with L1-based normalization and refinement.\nNAG restores effective negative guidance where CFG collapses while maintaining\nfidelity. Unlike existing approaches, NAG generalizes across architectures\n(UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image,\nvideo), functioning as a \\textit{universal} plug-in with minimal computational\noverhead. Through extensive experimentation, we demonstrate consistent\nimprovements in text alignment (CLIP Score), fidelity (FID, PFID), and\nhuman-perceived quality (ImageReward). Our ablation studies validate each\ndesign component, while user studies confirm significant preference for\nNAG-guided outputs. As a model-agnostic inference-time approach requiring no\nretraining, NAG provides effortless negative guidance for all modern diffusion\nframeworks -- pseudocode in the Appendix!", "authors": ["Dar-Yen Chen", "Hmrishav Bandyopadhyay", "Kai Zou", "Yi-Zhe Song"], "published_date": "2025-05-27", "title_zh": "正規化注意力引導：擴散模型的通用負引導", "summary_zh": "負向引導，即抑制不必要屬性，是擴散模型中的基本挑戰，尤其是在少步採樣中。無分類器引導(CFG)在標準設定下表現良好，但在激進的採樣步壓縮下會失效，因正向和負向分支的預測發散。我們提出正規化注意力引導(NAG)，一種高效、無需訓練的機制，在注意力空間中應用基於L1正規化和精煉的推斷。NAG恢復了CFG崩潰時的有效負向引導，同時保持了保真度。與現有方法不同，NAG可泛化於各種架構(UNet, DiT)、採樣方案(少步, 多步)和模態(圖像, 影片)，作為一個通用外掛程式，計算開銷極小。大量實驗證明，NAG在文字對齊(CLIP Score)、保真度(FID, PFID)和人類感知品質(ImageReward)方面均有持續提升。消融研究驗證了每個設計組件，使用者研究證實了對NAG引導輸出的顯著偏好。作為一種無需重新訓練的模型無關推論方法，NAG為所有現代擴散框架提供毫不費力的負向引導。", "audio": "audios/2505.21179v1.mp3", "timestamp": "2025-05-28T12:38:59.904530"}
