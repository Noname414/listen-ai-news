{"id": "2505.10561v1", "url": "http://arxiv.org/abs/2505.10561v1", "title": "T2A-Feedback: Improving Basic Capabilities of Text-to-Audio Generation via Fine-grained AI Feedback", "summary": "Text-to-audio (T2A) generation has achieved remarkable progress in generating\na variety of audio outputs from language prompts. However, current\nstate-of-the-art T2A models still struggle to satisfy human preferences for\nprompt-following and acoustic quality when generating complex multi-event\naudio. To improve the performance of the model in these high-level\napplications, we propose to enhance the basic capabilities of the model with AI\nfeedback learning. First, we introduce fine-grained AI audio scoring pipelines\nto: 1) verify whether each event in the text prompt is present in the audio\n(Event Occurrence Score), 2) detect deviations in event sequences from the\nlanguage description (Event Sequence Score), and 3) assess the overall acoustic\nand harmonic quality of the generated audio (Acoustic&Harmonic Quality). We\nevaluate these three automatic scoring pipelines and find that they correlate\nsignificantly better with human preferences than other evaluation metrics. This\nhighlights their value as both feedback signals and evaluation metrics.\nUtilizing our robust scoring pipelines, we construct a large audio preference\ndataset, T2A-FeedBack, which contains 41k prompts and 249k audios, each\naccompanied by detailed scores. Moreover, we introduce T2A-EpicBench, a\nbenchmark that focuses on long captions, multi-events, and story-telling\nscenarios, aiming to evaluate the advanced capabilities of T2A models. Finally,\nwe demonstrate how T2A-FeedBack can enhance current state-of-the-art audio\nmodel. With simple preference tuning, the audio generation model exhibits\nsignificant improvements in both simple (AudioCaps test set) and complex\n(T2A-EpicBench) scenarios.", "authors": ["Zehan Wang", "Ke Lei", "Chen Zhu", "Jiawei Huang", "Sashuai Zhou", "Luping Liu", "Xize Cheng", "Shengpeng Ji", "Zhenhui Ye", "Tao Jin", "Zhou Zhao"], "published_date": "2025-05-15", "title_zh": "T2A-Feedback：透過細粒度AI回饋提升文字轉語音生成之基礎能力", "summary_zh": "現今的文字轉語音模型在生成複雜多事件音訊時，難以完全符合人類對提示遵循度和音訊品質的期望。本研究提出利用AI回饋學習來提升模型基礎能力。首先，建立了精細的AI音訊評分流程，包含事件發生評分、事件序列評分，以及音訊和諧品質評分。這些評分流程能更準確地反映人類偏好。接著，利用這些評分流程，構建了包含41k個提示和249k個音訊的大型音訊偏好數據集T2A-FeedBack，並推出專注於長描述、多事件和故事敘述場景的評測基準T2A-EpicBench。實驗證明，透過簡單的偏好調整，T2A-FeedBack能有效提升現有音訊生成模型在簡單和複雜場景下的表現。", "audio": "audios/2505.10561v1.mp3", "timestamp": "2025-05-18T23:05:41.112280"}
{"id": "2505.10556v1", "url": "http://arxiv.org/abs/2505.10556v1", "title": "An AI-driven framework for the prediction of personalised health response to air pollution", "summary": "Air pollution poses a significant threat to public health, causing or\nexacerbating many respiratory and cardiovascular diseases. In addition, climate\nchange is bringing about more extreme weather events such as wildfires and\nheatwaves, which can increase levels of pollution and worsen the effects of\npollution exposure. Recent advances in personal sensing have transformed the\ncollection of behavioural and physiological data, leading to the potential for\nnew improvements in healthcare. We wish to capitalise on this data, alongside\nnew capabilities in AI for making time series predictions, in order to monitor\nand predict health outcomes for an individual. Thus, we present a novel\nworkflow for predicting personalised health responses to pollution by\nintegrating physiological data from wearable fitness devices with real-time\nenvironmental exposures. The data is collected from various sources in a secure\nand ethical manner, and is used to train an AI model to predict individual\nhealth responses to pollution exposure within a cloud-based, modular framework.\nWe demonstrate that the AI model -- an Adversarial Autoencoder neural network\nin this case -- accurately reconstructs time-dependent health signals and\ncaptures nonlinear responses to pollution. Transfer learning is applied using\ndata from a personal smartwatch, which increases the generalisation abilities\nof the AI model and illustrates the adaptability of the approach to real-world,\nuser-generated data.", "authors": ["Nazanin Zounemat Kermani", "Sadjad Naderi", "Claire H. Dilliway", "Claire E. Heaney", "Shrreya Behll", "Boyang Chen", "Hisham Abubakar-Waziri", "Alexandra E. Porter", "Marc Chadeau-Hyam", "Fangxin Fang", "Ian M. Adcock", "Kian Fan Chung", "Christopher C. Pain"], "published_date": "2025-05-15", "title_zh": "一個AI驅動的框架，用於預測個人化健康對空氣污染的反應", "summary_zh": "本研究提出一個新的AI框架，結合穿戴裝置收集的生理數據和即時環境暴露數據，來預測個人對空氣污染的健康反應。透過雲端架構訓練AI模型，精準重建時間序列健康訊號，並捕捉非線性污染反應。研究使用個人智慧手錶的數據進行遷移學習，提升模型泛化能力，展現此方法在真實世界使用者數據中的適應性。", "audio": "audios/2505.10556v1.mp3", "timestamp": "2025-05-18T23:05:46.038004"}
{"id": "2505.10527v1", "url": "http://arxiv.org/abs/2505.10527v1", "title": "WorldPM: Scaling Human Preference Modeling", "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.", "authors": ["Binghai Wang", "Runji Lin", "Keming Lu", "Le Yu", "Zhenru Zhang", "Fei Huang", "Chujie Zheng", "Kai Dang", "Yang Fan", "Xingzhang Ren", "An Yang", "Binyuan Hui", "Dayiheng Liu", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang", "Bowen Yu", "Jingren Zhou", "Junyang Lin"], "published_date": "2025-05-15", "title_zh": "WorldPM：擴展人類偏好建模", "summary_zh": "受到語言模型擴展定律的啟發，我們發現類似的定律也存在於偏好建模中。我們提出 WorldPM (World Preference Modeling) 以強調這種擴展潛力，它統一了人類偏好的表達。我們收集了來自公共論壇的偏好數據，涵蓋不同的用戶群體，並使用1500萬規模的數據和從15億到720億參數的模型進行了廣泛的訓練。結果顯示，對抗性指標（識別欺騙性特徵的能力）隨著訓練數據和模型大小的增加而持續提升；客觀性指標（有明確答案的客觀知識）在大語言模型中展現出湧現行為，突顯了 WorldPM 的可擴展性；主觀性指標則未顯示出擴展趨勢。實驗驗證了 WorldPM 作為偏好微調基礎的有效性。在七個基準測試的 20 個子任務中，WorldPM 顯著提高了跨不同規模人類偏好數據集（7K、100K 和 800K 樣本）的泛化性能，在許多關鍵子任務上的性能提升超過 5%。將 WorldPM 整合到我們的內部 RLHF 流程中，我們觀察到內部和公共評估集的顯著改進，內部評估的增益顯著提高了 4% 到 8%。", "audio": "audios/2505.10527v1.mp3", "timestamp": "2025-05-18T23:05:53.910110"}
{"id": "2505.10525v1", "url": "http://arxiv.org/abs/2505.10525v1", "title": "Sobolev and quasiconformal distortion of intermediate dimension with applications to conformal dimension", "summary": "We study the distortion of intermediate dimension under supercritical Sobolev\nmappings and also under quasiconformal or quasisymmetric homeomorphisms. In\nparticular, we extend to the setting of intermediate dimensions both the\nGehring--V\\\"ais\\\"al\\\"a theorem on dilatation-dependent quasiconformal\ndistortion of dimension and Kovalev's theorem on the nonexistence of metric\nspaces with conformal dimension strictly between zero and one. Applications\ninclude new contributions to the quasiconformal classification of Euclidean\nsets and a new sufficient condition for the vanishing of conformal box-counting\ndimension. We illustrate our conclusions with specific consequences for\nBedford--McMullen carpets, samples of Mandelbrot percolation, and product sets\ncontaining a polynomially convergent sequence factor.", "authors": ["Jonathan M. Fraser", "Jeremy T. Tyson"], "published_date": "2025-05-15", "title_zh": "中間維度的Sobolev及擬共形扭曲，及其於共形維度的應用", "summary_zh": "本研究探討超臨界Sobolev映射及擬共形/擬對稱同胚變換下，中間維度的扭曲現象。我們將Gehring-V\"ais\"al\"a關於膨脹係數與擬共形維度扭曲的定理，以及Kovalev關於不存在共形維度介於0與1之間的度量空間的定理，推廣到中間維度的情境。研究成果應用於歐幾里得集合的擬共形分類，並提出新的充分條件判斷共形盒計數維度是否消失。我們以Bedford-McMullen地毯、Mandelbrot滲流樣本，以及包含多項式收斂序列因子的乘積集合為例，闡述了我們的結論。", "audio": "audios/2505.10525v1.mp3", "timestamp": "2025-05-18T23:05:58.394013"}
{"id": "2505.10496v1", "url": "http://arxiv.org/abs/2505.10496v1", "title": "CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs", "summary": "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/", "authors": ["Raman Dutt", "Pedro Sanchez", "Yongchen Yao", "Steven McDonagh", "Sotirios A. Tsaftaris", "Timothy Hospedales"], "published_date": "2025-05-15", "title_zh": "CheXGenBench：合成胸腔X光片逼真度、隱私與效用之統一評估基準", "summary_zh": "CheXGenBench是一個綜合性的評估框架，用來衡量合成胸腔X光片生成模型的品質。它同時考量逼真度、潛在隱私風險以及在臨床上的實用性。 研究團隊發現現有的評估方法存在缺陷，導致結果不一致且缺乏參考價值。 因此，他們設計了一個標準化的評估基準，包含超過20個量化指標，並使用11種主流的文字轉圖像模型進行測試。研究結果揭示了現有評估方法的不足。 此外，研究團隊也釋出一個高品質的合成胸腔X光片資料集，SynthCheX-75K，包含7萬5千張圖像，以支持醫學AI領域的進一步研究。", "audio": "audios/2505.10496v1.mp3", "timestamp": "2025-05-18T23:06:03.275100"}
{"id": "2505.10490v1", "url": "http://arxiv.org/abs/2505.10490v1", "title": "Campus AI vs Commercial AI: A Late-Breaking Study on How LLM As-A-Service Customizations Shape Trust and Usage Patterns", "summary": "As the use of Large Language Models (LLMs) by students, lecturers and\nresearchers becomes more prevalent, universities - like other organizations -\nare pressed to develop coherent AI strategies. LLMs as-a-Service (LLMaaS) offer\naccessible pre-trained models, customizable to specific (business) needs. While\nmost studies prioritize data, model, or infrastructure adaptations (e.g., model\nfine-tuning), we focus on user-salient customizations, like interface changes\nand corporate branding, which we argue influence users' trust and usage\npatterns. This study serves as a functional prequel to a large-scale field\nstudy in which we examine how students and employees at a German university\nperceive and use their institution's customized LLMaaS compared to ChatGPT. The\ngoals of this prequel are to stimulate discussions on psychological effects of\nLLMaaS customizations and refine our research approach through feedback. Our\nforthcoming findings will deepen the understanding of trust dynamics in LLMs,\nproviding practical guidance for organizations considering LLMaaS deployment.", "authors": ["Leon Hannig", "Annika Bush", "Meltem Aksoy", "Steffen Becker", "Greta Ontrup"], "published_date": "2025-05-15", "title_zh": "校園AI vs. 商業AI：一項關於LLM即服務客製化如何形塑信任與使用模式的最新研究", "summary_zh": "隨著大學生、講師和研究人員廣泛使用大型語言模型（LLM），各大學正面臨制定完善AI策略的需求。本研究探討使用者可見的LLM客製化，例如介面修改和品牌形象，如何影響使用者對大學客製LLM的信任感和使用模式，並與ChatGPT進行比較。這項前期研究旨在引發關於LLM客製化心理效應的討論，並為後續的大規模實地研究提供方向，最終目標是深入了解LLM中的信任動態，為考慮部署LLMaaS的組織提供實用建議。", "audio": "audios/2505.10490v1.mp3", "timestamp": "2025-05-18T23:14:13.567508"}
{"id": "2505.10472v1", "url": "http://arxiv.org/abs/2505.10472v1", "title": "Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI", "summary": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools.", "authors": ["Agnik Saha", "Victoria Churchill", "Anny D. Rodriguez", "Ugur Kursuncu", "Muhammed Y. Idris"], "published_date": "2025-05-15", "title_zh": "用於癌症溝通的大型語言模型：評估生成式人工智慧中的語言品質、安全性和可及性", "summary_zh": "關於乳癌和子宮頸癌的有效溝通仍然是個健康挑戰。本研究評估了大型語言模型（LLMs）生成準確、安全且易於理解的癌症資訊的能力。研究發現，通用LLMs在語言品質和感染力方面表現較好，而醫療LLMs則在溝通可及性方面更勝一籌。然而，醫療LLMs也更容易產生潛在危害、毒性和偏見。總體而言，研究強調了在設計模型時需要有針對性地改進，特別是在降低危害和偏見方面，從而創建更安全、有效且可信賴的AI健康資訊工具。", "audio": "audios/2505.10472v1.mp3", "timestamp": "2025-05-18T23:14:30.270685"}
{"id": "2505.10468v1", "url": "http://arxiv.org/abs/2505.10468v1", "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge", "summary": "This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "published_date": "2025-05-15", "title_zh": "AI代理程式 vs. 具代理能力AI：概念分類、應用與挑戰", "summary_zh": "本研究區分了AI代理程式（AI Agents）和具代理能力AI（Agentic AI）兩個概念，並建立了結構化的分類體系。AI代理程式著重於利用大型語言模型（LLMs）和大型圖像模型（LIMs）進行狹窄、特定任務的自動化。而具代理能力AI則是一種範式轉移，強調多代理程式協作、動態任務分解、持久記憶和協調自主。研究比較了兩者的架構演進、運作機制、互動方式和自主程度，並探討了各自在客戶支援、研究自動化等領域的應用。同時，也分析了幻覺、脆弱性、突發行為和協調失敗等挑戰，並提出了針對性的解決方案。本研究旨在為開發穩健、可擴展且可解釋的AI代理程式和具代理能力AI系統提供清晰的指引。", "audio": "audios/2505.10468v1.mp3", "timestamp": "2025-05-18T15:29:49.604850"}
{"id": "2505.10454v1", "url": "http://arxiv.org/abs/2505.10454v1", "title": "Emotion-sensitive Explanation Model", "summary": "Explainable AI (XAI) research has traditionally focused on rational users,\naiming to improve understanding and reduce cognitive biases. However, emotional\nfactors play a critical role in how explanations are perceived and processed.\nPrior work shows that prior and task-generated emotions can negatively impact\nthe understanding of explanation. Building on these insights, we propose a\nthree-stage model for emotion-sensitive explanation grounding: (1) emotional or\nepistemic arousal, (2) understanding, and (3) agreement. This model provides a\nconceptual basis for developing XAI systems that dynamically adapt explanation\nstrategies to users emotional states, ultimately supporting more effective and\nuser-centered decision-making.", "authors": ["Christian Schütze", "Birte Richter", "Britta Wrede"], "published_date": "2025-05-15", "title_zh": "情緒敏感的解釋模型", "summary_zh": "傳統的可解釋AI(XAI)研究主要關注理性用戶，旨在提升理解和減少認知偏差。然而，情緒在解釋的感知和處理中扮演關鍵角色。本文提出一個三階段的情緒敏感解釋模型，包含情緒激發、理解和認同。此模型為開發能根據用戶情緒狀態動態調整解釋策略的XAI系統提供概念基礎，最終支持更有效且以用戶為中心的決策。", "audio": "audios/2505.10454v1.mp3", "timestamp": "2025-05-18T15:43:10.566009"}
{"id": "2505.10453v1", "url": "http://arxiv.org/abs/2505.10453v1", "title": "Vision language models have difficulty recognizing virtual objects", "summary": "Vision language models (VLMs) are AI systems paired with both language and\nvision encoders to process multimodal input. They are capable of performing\ncomplex semantic tasks such as automatic captioning, but it remains an open\nquestion about how well they comprehend the visuospatial properties of scenes\ndepicted in the images they process. We argue that descriptions of virtual\nobjects -- objects that are not visually represented in an image -- can help\ntest scene comprehension in these AI systems. For example, an image that\ndepicts a person standing under a tree can be paired with the following prompt:\nimagine that a kite is stuck in the tree. VLMs that comprehend the scene should\nupdate their representations and reason sensibly about the spatial relations\nbetween all three objects. We describe systematic evaluations of\nstate-of-the-art VLMs and show that their ability to process virtual objects is\ninadequate.", "authors": ["Tyler Tran", "Sangeet Khemlani", "J. G. Trafton"], "published_date": "2025-05-15", "title_zh": "視覺語言模型難以識別虛擬物件", "summary_zh": "視覺語言模型（VLMs）結合了語言和視覺編碼器，可以處理多模態輸入，例如自動生成圖片描述。然而，它們對圖像中場景的視覺空間理解程度仍然是個問題。研究發現，當提示VLMs想像圖像中不存在的虛擬物件（例如：一棵樹下站著一個人，提示想像樹上卡著風箏），它們難以合理更新場景表徵並推理物件之間的空間關係。這表明目前的VLMs在處理虛擬物件方面的能力不足。", "audio": "audios/2505.10453v1.mp3", "timestamp": "2025-05-18T15:52:48.305855"}
{"id": "2505.10427v1", "url": "http://arxiv.org/abs/2505.10427v1", "title": "Influence of prior and task generated emotions on XAI explanation retention and understanding", "summary": "The explanation of AI results and how they are received by users is an\nincreasingly active research field. However, there is a surprising lack of\nknowledge about how social factors such as emotions affect the process of\nexplanation by a decision support system (DSS). While previous research has\nshown effects of emotions on DSS supported decision-making, it remains unknown\nin how far emotions affect cognitive processing during an explanation. In this\nstudy, we, therefore, investigated the influence of prior emotions and\ntask-related arousal on the retention and understanding of explained feature\nrelevance. To investigate the influence of prior emotions, we induced happiness\nand fear prior to the decision support interaction. Before emotion induction,\nuser characteristics to assess their risk type were collected via a\nquestionnaire. To identify emotional reactions to the explanations of the\nrelevance of different features, we observed heart rate variability (HRV),\nfacial expressions, and self-reported emotions of the explainee while observing\nand listening to the explanation and assessed their retention of the features\nas well as their influence on the outcome of the decision task. Results\nindicate that (1) task-unrelated prior emotions do not affected the ratantion\nbut may affect the understanding of the relevance of certain features in the\nsense of an emotion-induced confirmation bias, (2) certain features related to\npersonal attitudes yielded arousal in individual participants, (3) this arousal\naffected the understanding of these variables.", "authors": ["Birte Richter", "Christian Schütze", "Anna Aksonova", "Britta Wrede"], "published_date": "2025-05-15", "title_zh": "先前情緒與任務產生情緒對XAI解釋保留與理解的影響", "summary_zh": "理解AI決策的解釋越來越重要。本研究探討情緒如何影響使用者對AI解釋的理解與記憶。我們透過誘導快樂與恐懼情緒，以及監測心率、面部表情等生理反應，觀察情緒如何影響使用者理解AI解釋中的特徵重要性。研究發現，先前情緒可能影響使用者對特定特徵的理解，產生情緒誘導的確認偏誤；某些與個人態度相關的特徵會引起使用者的情緒反應，進而影響他們對這些特徵的理解。", "audio": "audios/2505.10427v1.mp3", "timestamp": "2025-05-18T16:06:13.256765"}
{"id": "2505.10426v1", "url": "http://arxiv.org/abs/2505.10426v1", "title": "Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility", "summary": "The legal compliance and safety of different Human-in-the-loop (HITL) setups\nfor AI can vary greatly. This manuscript aims to identify new ways of choosing\nbetween such setups, and shows that there is an unavoidable trade-off between\nthe attribution of legal responsibility and the technical explainability of AI.\nWe begin by using the notion of oracle machines from computability theory to\nformalise different HITL setups, distinguishing between trivial human\nmonitoring, single endpoint human action, and highly involved interaction\nbetween the human(s) and the AI. These correspond to total functions, many-one\nreductions, and Turing reductions respectively. A taxonomy categorising HITL\nfailure modes is then presented, highlighting the limitations on what any HITL\nsetup can actually achieve. Our approach then identifies oversights from UK and\nEU legal frameworks, which focus on certain HITL setups which may not always\nachieve the desired ethical, legal, and sociotechnical outcomes. We suggest\nareas where the law should recognise the effectiveness of different HITL setups\nand assign responsibility in these contexts, avoiding unnecessary and\nunproductive human \"scapegoating\". Overall, we show how HITL setups involve\nmany technical design decisions, and can be prone to failures which are often\nout of the humans' control. This opens up a new analytic perspective on the\nchallenges arising in the creation of HITL setups, helping inform AI developers\nand lawmakers on designing HITL to better achieve their desired outcomes.", "authors": ["Maurice Chiodo", "Dennis Müller", "Paul Siewert", "Jean-Luc Wetherall", "Zoya Yasmine", "John Burden"], "published_date": "2025-05-15", "title_zh": "人機迴路正式化：計算歸約、失效模式與法律-道德責任", "summary_zh": "本研究探討不同人工智慧人機迴路(HITL)架構在法律合規和安全上的差異，指出法律責任歸屬與AI技術可解釋性之間存在不可避免的權衡。我們利用計算理論的預言機概念，形式化不同HITL架構，並建立HITL失效模式分類法。研究揭示了現有法律框架的不足，並建議如何根據不同HITL架構的有效性來分配責任，避免不必要的“替罪羊”效應。 總而言之，本研究揭示了HITL架構設計的複雜性以及潛在的失效風險，為AI開發者和立法者設計更有效的HITL架構提供了新的分析視角。", "audio": "audios/2505.10426v1.mp3", "timestamp": "2025-05-18T16:20:24.598334"}
{"id": "2505.10405v1", "url": "http://arxiv.org/abs/2505.10405v1", "title": "Visual Fidelity Index for Generative Semantic Communications with Critical Information Embedding", "summary": "Generative semantic communication (Gen-SemCom) with large artificial\nintelligence (AI) model promises a transformative paradigm for 6G networks,\nwhich reduces communication costs by transmitting low-dimensional prompts\nrather than raw data. However, purely prompt-driven generation loses\nfine-grained visual details. Additionally, there is a lack of systematic\nmetrics to evaluate the performance of Gen-SemCom systems. To address these\nissues, we develop a hybrid Gen-SemCom system with a critical information\nembedding (CIE) framework, where both text prompts and semantically critical\nfeatures are extracted for transmissions. First, a novel approach of semantic\nfiltering is proposed to select and transmit the semantically critical features\nof images relevant to semantic label. By integrating the text prompt and\ncritical features, the receiver reconstructs high-fidelity images using a\ndiffusion-based generative model. Next, we propose the generative visual\ninformation fidelity (GVIF) metric to evaluate the visual quality of the\ngenerated image. By characterizing the statistical models of image features,\nthe GVIF metric quantifies the mutual information between the distorted\nfeatures and their original counterparts. By maximizing the GVIF metric, we\ndesign a channel-adaptive Gen-SemCom system that adaptively control the volume\nof features and compression rate according to the channel state. Experimental\nresults validate the GVIF metric's sensitivity to visual fidelity, correlating\nwith both the PSNR and critical information volume. In addition, the optimized\nsystem achieves superior performance over benchmarking schemes in terms of\nhigher PSNR and lower FID scores.", "authors": ["Jianhao Huang", "Qunsong Zeng", "Kaibin Huang"], "published_date": "2025-05-15", "title_zh": "具有關鍵資訊嵌入的生成式語義通訊視覺保真度指標", "summary_zh": "研究提出一種混合生成式語義通訊系統，透過嵌入關鍵資訊框架，同時傳輸文字提示和語義上重要的圖像特徵，以解決純粹提示驅動的生成導致細節丟失的問題。為評估系統性能，提出生成式視覺資訊保真度（GVIF）指標，以量化失真特徵與原始特徵之間的互信息。實驗結果驗證了GVIF指標對視覺保真度的敏感性，並設計了一種基於GVIF最大化的通道自適應系統，能夠根據通道狀態調整特徵數量和壓縮率，進而提升重建圖像的品質。", "audio": "audios/2505.10405v1.mp3", "timestamp": "2025-05-18T17:14:39.044337"}
{"id": "2505.10377v1", "url": "http://arxiv.org/abs/2505.10377v1", "title": "The Art of Two-Round Voting", "summary": "We study the voting problem with two alternatives where voters' preferences\ndepend on a not-directly-observable state variable. While equilibria in the\none-round voting mechanisms lead to a good decision, they are usually hard to\ncompute and follow. We consider the two-round voting mechanism where the first\nround serves as a polling stage and the winning alternative only depends on the\noutcome of the second round. We show that the two-round voting mechanism is a\npowerful tool for making collective decisions. Firstly, every (approximated)\nequilibrium in the two-round voting mechanisms (asymptotically) leads to the\ndecision preferred by the majority as if the state of the world were revealed\nto the voters. Moreover, there exist natural equilibria in the two-round game\nfollowing intuitive behaviors such as informative voting, sincere voting\n[Austen-Smith and Banks, 1996], and the surprisingly popular strategy [Prelec\net al., 2017]. This sharply contrasts with the one-round voting mechanisms in\nthe previous literature, where no simple equilibrium is known. Finally, we show\nthat every equilibrium in the standard one-round majority vote mechanism gives\nan equilibrium in the two-round mechanisms that is not more complicated than\nthe one-round equilibrium. Therefore, the two-round voting mechanism provides a\nnatural equilibrium in every instance, including those where one-round voting\nfails to have a natural solution, and it can reach an informed majority\ndecision whenever one-round voting can. Our experiments on generative AI voters\nalso imply that two-round voting leads to the correct outcome more often than\none-round voting under some circumstances.", "authors": ["Qishen Han", "Grant Schoenebeck", "Biaoshuai Tao", "Lirong Xia"], "published_date": "2025-05-15", "title_zh": "兩輪投票的藝術", "summary_zh": "研究選民偏好取決於不可直接觀察狀態變數的雙選項投票問題。單輪投票機制雖能做出好的決策，但其均衡通常難以計算和遵循。論文探討兩輪投票機制，首輪作為民調，勝負僅取決於第二輪結果。研究表明兩輪投票機制是強大的集體決策工具。首先，兩輪投票機制的每個（近似）均衡（漸近地）導向多數人偏好的決策，如同世界狀態已被揭露給選民。此外，存在自然的兩輪賽局均衡，遵循直觀行為，例如資訊性投票、真誠投票和令人驚訝的流行策略。這與先前文獻中單輪投票機制形成鮮明對比，因為單輪投票機制沒有已知的簡單均衡。最後，研究表明標準單輪多數投票機制中的每個均衡都給出兩輪機制中的均衡，且不比單輪均衡更複雜。因此，兩輪投票機制在每個實例中都提供了一種自然的均衡，包括那些單輪投票未能產生自然解決方案的實例，並且只要單輪投票可以，它就可以達成知情的多数人决策。我們在生成式AI選民上的實驗也表明，在某些情況下，兩輪投票比單輪投票更有可能導致正確的結果。", "audio": "audios/2505.10377v1.mp3", "timestamp": "2025-05-18T18:23:30.321298"}
{"id": "2505.10375v1", "url": "http://arxiv.org/abs/2505.10375v1", "title": "Are Sparse Autoencoders Useful for Java Function Bug Detection?", "summary": "Software vulnerabilities such as buffer overflows and SQL injections are a\nmajor source of security breaches. Traditional methods for vulnerability\ndetection remain essential but are limited by high false positive rates,\nscalability issues, and reliance on manual effort. These constraints have\ndriven interest in AI-based approaches to automated vulnerability detection and\nsecure code generation. While Large Language Models (LLMs) have opened new\navenues for classification tasks, their complexity and opacity pose challenges\nfor interpretability and deployment. Sparse Autoencoder offer a promising\nsolution to this problem. We explore whether SAEs can serve as a lightweight,\ninterpretable alternative for bug detection in Java functions. We evaluate the\neffectiveness of SAEs when applied to representations from GPT-2 Small and\nGemma 2B, examining their capacity to highlight buggy behaviour without\nfine-tuning the underlying LLMs. We found that SAE-derived features enable bug\ndetection with an F1 score of up to 89%, consistently outperforming fine-tuned\ntransformer encoder baselines. Our work provides the first empirical evidence\nthat SAEs can be used to detect software bugs directly from the internal\nrepresentations of pretrained LLMs, without any fine-tuning or task-specific\nsupervision.", "authors": ["Rui Melo", "Claudia Mamede", "Andre Catarino", "Rui Abreu", "Henrique Lopes Cardoso"], "published_date": "2025-05-15", "title_zh": "稀疏自編碼器對Java函式錯誤偵測有用嗎？", "summary_zh": "軟體漏洞，如緩衝區溢位和SQL注入，是資安漏洞的主要來源。傳統的漏洞偵測方法雖然重要，但誤報率高，擴展性差，且依賴人工。這促使人們對基於AI的自動漏洞偵測和安全程式碼生成產生興趣。大型語言模型（LLM）雖然為分類任務開闢了新途徑，但其複雜性和不透明性對可解釋性和部署構成了挑戰。稀疏自編碼器（SAE）為此問題提供了一個有希望的解決方案。本文探討了SAE是否可以作為Java函式中錯誤偵測的輕量級、可解釋的替代方案。我們評估了將SAE應用於GPT-2 Small和Gemma 2B的表示時的有效性，檢驗了它們在不微調底層LLM的情況下突出顯示錯誤行為的能力。我們發現，SAE衍生的特徵能夠以高達89%的F1分數進行錯誤偵測，始終優於微調後的Transformer編碼器基準線。我們的研究提供了第一個經驗證據，證明SAE可以用於直接從預訓練LLM的內部表示中檢測軟體錯誤，而無需任何微調或特定於任務的監督。", "audio": "audios/2505.10375v1.mp3", "timestamp": "2025-05-18T19:13:34.703921"}
{"id": "2505.10360v1", "url": "http://arxiv.org/abs/2505.10360v1", "title": "FactsR: A Safer Method for Producing High Quality Healthcare Documentation", "summary": "There are now a multitude of AI-scribing solutions for healthcare promising\nthe utilization of large language models for ambient documentation. However,\nthese AI scribes still rely on one-shot, or few-shot prompts for generating\nnotes after the consultation has ended, employing little to no reasoning. This\nrisks long notes with an increase in hallucinations, misrepresentation of the\nintent of the clinician, and reliance on the proofreading of the clinician to\ncatch errors. A dangerous combination for patient safety if vigilance is\ncompromised by workload and fatigue. In this paper, we introduce a method for\nextracting salient clinical information in real-time alongside the healthcare\nconsultation, denoted Facts, and use that information recursively to generate\nthe final note. The FactsR method results in more accurate and concise notes by\nplacing the clinician-in-the-loop of note generation, while opening up new use\ncases within real-time decision support.", "authors": ["Victor Petrén Bach Hansen", "Lasse Krogsbøll", "Jonas Lyngsø", "Mathias Baltzersen", "Andreas Motzfeldt", "Kevin Pelgrims", "Lars Maaløe"], "published_date": "2025-05-15", "title_zh": "FactsR：一種更安全的生成高品質醫療文檔的方法", "summary_zh": "現今有許多AI醫療抄寫方案，聲稱利用大型語言模型進行環境文檔記錄。但這些方案仍依賴於少量樣本提示生成筆記，幾乎沒有推理能力，容易產生過長、充滿幻覺、誤解臨床醫生意圖的筆記，需要臨床醫生校對。若工作量大且疲勞，會危及患者安全。本研究提出FactsR方法，在醫療諮詢期間即時提取關鍵臨床資訊（Facts），並遞迴地利用這些資訊生成最終筆記。FactsR透過讓臨床醫生參與筆記生成，產生更精準簡潔的筆記，並開創了即時決策支援的新應用。", "audio": "audios/2505.10360v1.mp3", "timestamp": "2025-05-18T20:19:19.221424"}
{"id": "2505.10338v1", "url": "http://arxiv.org/abs/2505.10338v1", "title": "Telecom-to-Visible Quantum Frequency Converter on a Silicon Nitride Chip", "summary": "Quantum frequency conversion serves a key role in the realization of hybrid\nquantum networks by interfacing between wavelength-incompatible platforms. Here\nwe present the first quantum frequency converter connecting visible and telecom\ndomains on a silicon nitride (SiN) chip, using Bragg-scattering four-wave\nmixing to upconvert heralded single photons from 1260 to 698 nm, which covers a\n192 THz span. We examine the noise sources in SiN and devise approaches to\nsuppress noise photons at the source and target frequencies to enable\nmeasurements at the single-photon level. We demonstrate an on-chip conversion\nefficiency of 5% in photon flux and describe design modifications that can be\nimplemented to significantly improve it. Our results pave the way for the\nimplementation of CMOS-compatible devices in quantum networks.", "authors": ["Sidarth Raghunathan", "Richard Oliver", "Yun Zhao", "Karl McNulty", "Chaitali Joshi", "Michal Lipson", "Alexander L. Gaeta"], "published_date": "2025-05-15", "title_zh": "矽晶氮化矽晶片上用於電信頻段到可見光頻段的量子頻率轉換器", "summary_zh": "量子頻率轉換是連接不同波長量子平台的關鍵技術。這篇論文展示了首個矽晶氮化矽晶片上的量子頻率轉換器，能將電信頻段（1260奈米）的單光子上轉換到可見光頻段（698奈米），跨越192太赫茲的頻寬。研究人員探討了矽晶氮化矽晶片上的雜訊來源，並設計了抑制雜訊光子的方法，實現了單光子層級的測量。晶片上的轉換效率達到了5%，並且論文中也提出了可以顯著提高轉換效率的設計修改方案。這項成果為在量子網路中實現與CMOS相容的元件鋪平了道路。", "audio": "audios/2505.10338v1.mp3", "timestamp": "2025-05-18T21:15:26.692705"}
{"id": "2505.10325v1", "url": "http://arxiv.org/abs/2505.10325v1", "title": "A Representation Learning Approach to Feature Drift Detection in Wireless Networks", "summary": "AI is foreseen to be a centerpiece in next generation wireless networks\nenabling enabling ubiquitous communication as well as new services. However, in\nreal deployment, feature distribution changes may degrade the performance of AI\nmodels and lead to undesired behaviors. To counter for undetected model\ndegradation, we propose ALERT; a method that can detect feature distribution\nchanges and trigger model re-training that works well on two wireless network\nuse cases: wireless fingerprinting and link anomaly detection. ALERT includes\nthree components: representation learning, statistical testing and utility\nassessment. We rely on MLP for designing the representation learning component,\non Kolmogorov-Smirnov and Population Stability Index tests for designing the\nstatistical testing and a new function for utility assessment. We show the\nsuperiority of the proposed method against ten standard drift detection methods\navailable in the literature on two wireless network use cases.", "authors": ["Athanasios Tziouvaras", "Blaz Bertalanic", "George Floros", "Kostas Kolomvatsos", "Panagiotis Sarigiannidis", "Carolina Fortuna"], "published_date": "2025-05-15", "title_zh": "無線網路中基於表徵學習的特徵漂移偵測方法", "summary_zh": "下一代無線網路預計將廣泛應用人工智慧。然而，實際部署中，特徵分佈的改變可能降低人工智慧模型效能，導致不良行為。為了解決這個問題，我們提出 ALERT 方法，它可以偵測特徵分佈的改變，並觸發模型重新訓練。ALERT 包含表徵學習、統計檢定和效用評估三個部分。我們在無線指紋辨識和鏈路異常偵測兩個無線網路應用案例中，驗證了 ALERT 優於現有的十種漂移偵測方法。", "audio": "audios/2505.10325v1.mp3", "timestamp": "2025-05-18T22:16:22.632157"}
{"id": "2505.10320v1", "url": "http://arxiv.org/abs/2505.10320v1", "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning", "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.", "authors": ["Chenxi Whitehouse", "Tianlu Wang", "Ping Yu", "Xian Li", "Jason Weston", "Ilia Kulikov", "Swarnadeep Saha"], "published_date": "2025-05-15", "title_zh": "J1：透過強化學習激勵LLM作為評審的思考能力", "summary_zh": "AI發展受限於評估品質，而強大的LLM評審模型是關鍵解決方案。更強的思考鏈推理能提升判斷力，促使我們尋找訓練這些模型思考的最佳方法。本研究提出J1，一種利用強化學習訓練LLM評審模型的方法。我們的方法將可驗證和不可驗證的提示轉換為可驗證獎勵的判斷任務，激勵思考並減少判斷偏差。我們的模型在8B或70B模型大小下，表現優於所有現有的同規模模型，包括從DeepSeek-R1提煉的模型。J1也優於o1-mini，甚至在某些基準測試中優於R1，儘管訓練的模型較小。我們分析比較了Pairwise-J1 vs Pointwise-J1模型、離線 vs 線上訓練、獎勵策略、初始提示，以及思考長度和內容的變化。我們發現，我們的模型通過學習概述評估標準、與自我生成的參考答案進行比較，以及重新評估模型回應的正確性，來做出更好的判斷。", "audio": "audios/2505.10320v1.mp3", "timestamp": "2025-05-18T23:16:55.410198"}
{"id": "2505.10315v1", "url": "http://arxiv.org/abs/2505.10315v1", "title": "Private Transformer Inference in MLaaS: A Survey", "summary": "Transformer models have revolutionized AI, powering applications like content\ngeneration and sentiment analysis. However, their deployment in Machine\nLearning as a Service (MLaaS) raises significant privacy concerns, primarily\ndue to the centralized processing of sensitive user data. Private Transformer\nInference (PTI) offers a solution by utilizing cryptographic techniques such as\nsecure multi-party computation and homomorphic encryption, enabling inference\nwhile preserving both user data and model privacy. This paper reviews recent\nPTI advancements, highlighting state-of-the-art solutions and challenges. We\nalso introduce a structured taxonomy and evaluation framework for PTI, focusing\non balancing resource efficiency with privacy and bridging the gap between\nhigh-performance inference and data privacy.", "authors": ["Yang Li", "Xinyu Zhou", "Yitong Wang", "Liangxin Qian", "Jun Zhao"], "published_date": "2025-05-15", "title_zh": "MLaaS中的私有Transformer推論：綜述", "summary_zh": "Transformer模型在AI領域取得重大突破，但其在機器學習即服務（MLaaS）中的部署引發隱私問題，因為使用者敏感資料集中處理。私有Transformer推論（PTI）透過安全多方計算和同態加密等技術，在保護使用者資料和模型隱私的同時進行推論。本論文回顧了最新的PTI研究進展，重點介紹了最新的解決方案和挑戰，並提出了針對PTI的結構化分類和評估框架，旨在平衡資源效率與隱私保護，並彌合高性能推論與資料隱私之間的差距。", "audio": "audios/2505.10315v1.mp3", "timestamp": "2025-05-19T01:38:18.066985"}
{"id": "2505.11483v1", "url": "http://arxiv.org/abs/2505.11483v1", "title": "msf-CNN: Patch-based Multi-Stage Fusion with Convolutional Neural Networks for TinyML", "summary": "AI spans from large language models to tiny models running on\nmicrocontrollers (MCUs). Extremely memory-efficient model architectures are\ndecisive to fit within an MCU's tiny memory budget e.g., 128kB of RAM. However,\ninference latency must remain small to fit real-time constraints. An approach\nto tackle this is patch-based fusion, which aims to optimize data flows across\nneural network layers. In this paper, we introduce msf-CNN, a novel technique\nthat efficiently finds optimal fusion settings for convolutional neural\nnetworks (CNNs) by walking through the fusion solution space represented as a\ndirected acyclic graph. Compared to previous work on CNN fusion for MCUs,\nmsf-CNN identifies a wider set of solutions. We published an implementation of\nmsf-CNN running on various microcontrollers (ARM Cortex-M, RISC-V, ESP32). We\nshow that msf-CNN can achieve inference using 50% less RAM compared to the\nprior art (MCUNetV2 and StreamNet). We thus demonstrate how msf-CNN offers\nadditional flexibility for system designers.", "authors": ["Zhaolan Huang", "Emmanuel Baccelli"], "published_date": "2025-05-16", "title_zh": "msf-CNN：基於卷積神經網路的塊狀多階段融合TinyML", "summary_zh": "針對微控制器(MCU)上運行的TinyML，本研究提出msf-CNN，一種尋找卷積神經網路(CNN)最佳融合設定的新技術。透過在有向無環圖表示的融合方案空間中搜尋，msf-CNN能找到更廣泛的解決方案。實驗證明，相比於現有技術MCUNetV2和StreamNet，msf-CNN能減少50%的RAM使用量，為系統設計者提供更大的彈性。", "audio": "audios/2505.11483v1.mp3", "timestamp": "2025-05-19T03:17:07.151829"}
{"id": "2505.11481v1", "url": "http://arxiv.org/abs/2505.11481v1", "title": "MOSAAIC: Managing Optimization towards Shared Autonomy, Authority, and Initiative in Co-creation", "summary": "Striking the appropriate balance between humans and co-creative AI is an open\nresearch question in computational creativity. Co-creativity, a form of hybrid\nintelligence where both humans and AI take action proactively, is a process\nthat leads to shared creative artifacts and ideas. Achieving a balanced dynamic\nin co-creativity requires characterizing control and identifying strategies to\ndistribute control between humans and AI. We define control as the power to\ndetermine, initiate, and direct the process of co-creation. Informed by a\nsystematic literature review of 172 full-length papers, we introduce MOSAAIC\n(Managing Optimization towards Shared Autonomy, Authority, and Initiative in\nCo-creation), a novel framework for characterizing and balancing control in\nco-creation. MOSAAIC identifies three key dimensions of control: autonomy,\ninitiative, and authority. We supplement our framework with control\noptimization strategies in co-creation. To demonstrate MOSAAIC's applicability,\nwe analyze the distribution of control in six existing co-creative AI case\nstudies and present the implications of using this framework.", "authors": ["Alayt Issak", "Jeba Rezwana", "Casper Harteveld"], "published_date": "2025-05-16", "title_zh": "MOSAAIC：管理共享自主性、權威性與主動性，以優化共同創作", "summary_zh": "這篇論文探討人與AI協作創作時，如何取得控制權的平衡。研究提出了一個新的框架MOSAAIC，從自主性、主動性和權威性三個面向，分析和管理創作過程中的控制權分配。透過分析大量文獻和案例，論文展示了MOSAAIC框架的應用，幫助我們了解如何在人與AI之間更有效地分配控制權，促進更好的共同創作。", "audio": "audios/2505.11481v1.mp3", "timestamp": "2025-05-19T04:26:17.442066"}
{"id": "2505.13448v1", "url": "http://arxiv.org/abs/2505.13448v1", "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "summary": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE.", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "published_date": "2025-05-19", "category": "AI", "title_zh": "CIE：使用連續訊號控制語言模型文本生成", "summary_zh": "為了提升使用者體驗，如何讓語言模型更符合使用者意圖越來越重要。本研究提出一種方法，透過連續訊號（例如介於「短」到「長」之間的向量）來控制語言模型生成的文本屬性，例如文本長度。實驗證明，相較於使用自然語言提示或離散訊號的方法，此方法能更可靠地控制生成文本的長度。相關程式碼與資料集已開源。", "audio": "audios/2505.13448v1.mp3", "timestamp": "2025-05-20T03:11:17.454935"}
{"id": "2505.13434v1", "url": "http://arxiv.org/abs/2505.13434v1", "title": "SMOTExT: SMOTE meets Large Language Models", "summary": "Data scarcity and class imbalance are persistent challenges in training\nrobust NLP models, especially in specialized domains or low-resource settings.\nWe propose a novel technique, SMOTExT, that adapts the idea of Synthetic\nMinority Over-sampling (SMOTE) to textual data. Our method generates new\nsynthetic examples by interpolating between BERT-based embeddings of two\nexisting examples and then decoding the resulting latent point into text with\nxRAG architecture. By leveraging xRAG's cross-modal retrieval-generation\nframework, we can effectively turn interpolated vectors into coherent text.\nWhile this is preliminary work supported by qualitative outputs only, the\nmethod shows strong potential for knowledge distillation and data augmentation\nin few-shot settings. Notably, our approach also shows promise for\nprivacy-preserving machine learning: in early experiments, training models\nsolely on generated data achieved comparable performance to models trained on\nthe original dataset. This suggests a viable path toward safe and effective\nlearning under data protection constraints.", "authors": ["Mateusz Bystroński", "Mikołaj Hołysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "published_date": "2025-05-19", "category": "Foundation Model", "title_zh": "SMOTExT：SMOTE 遇上大型語言模型", "summary_zh": "SMOTExT是一種新的文字資料增強技術，它將SMOTE（合成少數類過採樣技術）的概念應用於自然語言處理。此方法透過插值BERT嵌入向量，然後使用xRAG架構將插值後的向量解碼為文本，生成新的合成樣本。初步實驗顯示，SMOTExT在小樣本學習中具有知識蒸餾和數據增強的潛力，甚至能在隱私保護的機器學習中，僅用生成的數據訓練出與原始數據訓練的模型相近的效能。總而言之，SMOTExT為解決資料稀缺和類別不平衡問題提供了一種有前景的方案。", "audio": "audios/2505.13434v1.mp3", "timestamp": "2025-05-20T03:11:21.857638"}
{"id": "2505.13447v1", "url": "http://arxiv.org/abs/2505.13447v1", "title": "Mean Flows for One-step Generative Modeling", "summary": "We propose a principled and effective framework for one-step generative\nmodeling. We introduce the notion of average velocity to characterize flow\nfields, in contrast to instantaneous velocity modeled by Flow Matching methods.\nA well-defined identity between average and instantaneous velocities is derived\nand used to guide neural network training. Our method, termed the MeanFlow\nmodel, is self-contained and requires no pre-training, distillation, or\ncurriculum learning. MeanFlow demonstrates strong empirical performance: it\nachieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet\n256x256 trained from scratch, significantly outperforming previous\nstate-of-the-art one-step diffusion/flow models. Our study substantially\nnarrows the gap between one-step diffusion/flow models and their multi-step\npredecessors, and we hope it will motivate future research to revisit the\nfoundations of these powerful models.", "authors": ["Zhengyang Geng", "Mingyang Deng", "Xingjian Bai", "J. Zico Kolter", "Kaiming He"], "published_date": "2025-05-19", "category": "Diffusion Model", "title_zh": "用於單步生成建模的平均流", "summary_zh": "本論文提出了一個穩健且有效的單步生成模型框架。不同於Flow Matching方法中建模瞬時速度，本文引入了「平均速度」的概念來描述流場。透過推導平均速度和瞬時速度之間明確的關係，引導神經網絡訓練。此方法命名為MeanFlow模型，無需預訓練、知識提煉或課程學習。實驗結果顯示，MeanFlow在ImageNet 256x256上從零開始訓練，僅需單次函數評估（1-NFE）便達到3.43的FID，顯著超越先前最先進的單步擴散/流模型，大幅縮小了單步模型與多步模型之間的差距。期望這項研究能激勵未來對於這些強大模型基礎的深入探討。", "audio": "audios/2505.13447v1.mp3", "timestamp": "2025-05-20T03:11:26.812839"}
{"id": "2505.13445v1", "url": "http://arxiv.org/abs/2505.13445v1", "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "published_date": "2025-05-19", "category": "AI", "title_zh": "信任，但要驗證：一種使用可驗證獎勵的強化學習自驗證方法", "summary_zh": "大型語言模型在複雜推理方面展現了巨大潛力，其中使用可驗證獎勵的強化學習（RLVR）是一種關鍵的增強策略。然而，一個常見的問題是“表面自反思”，即模型無法有效驗證自己的輸出。我們引入了RISE（利用自驗證強化推理），這是一種新的線上強化學習框架，旨在解決這個問題。RISE明確地並且同時訓練大型語言模型，在單一整合的強化學習過程中，提高其解決問題和自我驗證的能力。核心機制是利用來自結果驗證器的可驗證獎勵，為解決方案生成和自我驗證任務提供即時回饋。在每次迭代中，模型生成解決方案，然後批判性地檢視自身生成的解決方案，這兩個軌跡都有助於策略更新。在各種數學推理基準上的廣泛實驗表明，RISE始終如一地提高了模型解決問題的準確性，同時培養了強大的自我驗證能力。我們的分析強調了線上驗證的優勢以及增加驗證計算的好處。此外，RISE模型在推理過程中表現出更頻繁和準確的自我驗證行為。這些優勢鞏固了RISE作為開發更穩健和具有自我意識的推理器的靈活且有效的途徑。", "audio": "audios/2505.13445v1.mp3", "timestamp": "2025-05-20T04:22:04.619238"}
{"id": "2505.13419v1", "url": "http://arxiv.org/abs/2505.13419v1", "title": "FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning", "summary": "Facial Emotion Analysis (FEA) plays a crucial role in visual affective\ncomputing, aiming to infer a person's emotional state based on facial data.\nScientifically, facial expressions (FEs) result from the coordinated movement\nof facial muscles, which can be decomposed into specific action units (AUs)\nthat provide detailed emotional insights. However, traditional methods often\nstruggle with limited interpretability, constrained generalization and\nreasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have\nshown exceptional performance in various visual tasks, while they still face\nsignificant challenges in FEA due to the lack of specialized datasets and their\ninability to capture the intricate relationships between FEs and AUs. To\naddress these issues, we introduce a novel FEA Instruction Dataset that\nprovides accurate and aligned FE and AU descriptions and establishes causal\nreasoning relationships between them, followed by constructing a new benchmark,\nFEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to\ncapture more detailed facial information, enhancing its capability in FEA\ntasks. Our model demonstrates strong performance on FEABench and impressive\ngeneralization capability through zero-shot evaluation on various datasets,\nincluding RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and\neffectiveness in FEA tasks. The dataset and code will be available at\nhttps://github.com/953206211/FEALLM.", "authors": ["Zhuozhao Hu", "Kaishen Yuan", "Xin Liu", "Zitong Yu", "Yuan Zong", "Jingang Shi", "Huanjing Yue", "Jingyu Yang"], "published_date": "2025-05-19", "category": "Foundation Model", "title_zh": "FEALLM：透過情緒協同與推理，推進多模態大型語言模型在臉部表情分析方面的能力", "summary_zh": "FEALLM 提出了一個新的臉部表情分析方法，運用多模態大型語言模型。它建立了一個包含精準的臉部表情和動作單元描述的資料集，並設計了一個新的模型架構，強化模型捕捉細緻臉部資訊的能力。實驗結果顯示，FEALLM 在臉部表情分析任務上表現出色，並展現了良好的泛化能力。", "audio": "audios/2505.13419v1.mp3", "timestamp": "2025-05-20T04:22:10.035222"}
{"id": "2505.13273v1", "url": "http://arxiv.org/abs/2505.13273v1", "title": "Seeing the Unseen: How EMoE Unveils Bias in Text-to-Image Diffusion Models", "summary": "Estimating uncertainty in text-to-image diffusion models is challenging\nbecause of their large parameter counts (often exceeding 100 million) and\noperation in complex, high-dimensional spaces with virtually infinite input\npossibilities. In this paper, we propose Epistemic Mixture of Experts (EMoE), a\nnovel framework for efficiently estimating epistemic uncertainty in diffusion\nmodels. EMoE leverages pre-trained networks without requiring additional\ntraining, enabling direct uncertainty estimation from a prompt. We leverage a\nlatent space within the diffusion process that captures epistemic uncertainty\nbetter than existing methods. Experimental results on the COCO dataset\ndemonstrate EMoE's effectiveness, showing a strong correlation between\nuncertainty and image quality. Additionally, EMoE identifies under-sampled\nlanguages and regions with higher uncertainty, revealing hidden biases in the\ntraining set. This capability demonstrates the relevance of EMoE as a tool for\naddressing fairness and accountability in AI-generated content.", "authors": ["Lucas Berry", "Axel Brando", "Wei-Di Chang", "Juan Camilo Gamboa Higuera", "David Meger"], "published_date": "2025-05-19", "category": "Diffusion Model", "title_zh": "看見未見之處：EMoE 如何揭示文本到圖像擴散模型中的偏見", "summary_zh": "現今文本到圖像的擴散模型參數龐大，難以估算其不確定性。本研究提出「知識混合專家模型」(EMoE)，能有效率地估計擴散模型中的知識不確定性。EMoE利用預訓練網路，無需額外訓練，即可直接從提示詞中估算不確定性，並藉由擴散過程中的潛在空間來捕捉知識不確定性，效果優於現有方法。實驗證明EMoE與圖像品質高度相關，並且能識別訓練集中代表性不足的語言和區域，進而揭示模型中隱藏的偏見。這顯示EMoE能作為AI生成內容中，處理公平性和問責制問題的有效工具。", "audio": "audios/2505.13273v1.mp3", "timestamp": "2025-05-20T04:22:15.863491"}
{"id": "2505.13439v1", "url": "http://arxiv.org/abs/2505.13439v1", "title": "VTBench: Evaluating Visual Tokenizers for Autoregressive Image Generation", "summary": "Autoregressive (AR) models have recently shown strong performance in image\ngeneration, where a critical component is the visual tokenizer (VT) that maps\ncontinuous pixel inputs to discrete token sequences. The quality of the VT\nlargely defines the upper bound of AR model performance. However, current\ndiscrete VTs fall significantly behind continuous variational autoencoders\n(VAEs), leading to degraded image reconstructions and poor preservation of\ndetails and text. Existing benchmarks focus on end-to-end generation quality,\nwithout isolating VT performance. To address this gap, we introduce VTBench, a\ncomprehensive benchmark that systematically evaluates VTs across three core\ntasks: Image Reconstruction, Detail Preservation, and Text Preservation, and\ncovers a diverse range of evaluation scenarios. We systematically assess\nstate-of-the-art VTs using a set of metrics to evaluate the quality of\nreconstructed images. Our findings reveal that continuous VAEs produce superior\nvisual representations compared to discrete VTs, particularly in retaining\nspatial structure and semantic detail. In contrast, the degraded\nrepresentations produced by discrete VTs often lead to distorted\nreconstructions, loss of fine-grained textures, and failures in preserving text\nand object integrity. Furthermore, we conduct experiments on GPT-4o image\ngeneration and discuss its potential AR nature, offering new insights into the\nrole of visual tokenization. We release our benchmark and codebase publicly to\nsupport further research and call on the community to develop strong,\ngeneral-purpose open-source VTs.", "authors": ["Huawei Lin", "Tong Geng", "Zhaozhuo Xu", "Weijie Zhao"], "published_date": "2025-05-19", "category": "AI", "title_zh": "VTBench：評估自迴歸圖像生成中的視覺 Tokenizer", "summary_zh": "自迴歸模型在圖像生成方面表現出色，其中視覺 Tokenizer (VT) 至關重要，它將連續像素輸入映射到離散的 Token 序列。VT 的品質直接影響著自迴歸模型的效能上限。然而，目前的離散 VT 相較於連續變分自編碼器 (VAE) 仍有明顯差距，導致圖像重建品質下降，細節和文字的保留效果不佳。現有評估標準著重於端到端生成品質，忽略了對 VT 效能的獨立評估。為了解決這個問題，我們推出了 VTBench，一個全面的評估標準，它系統性地評估 VT 在三個核心任務上的表現：圖像重建、細節保留和文字保留，並涵蓋多樣的評估場景。我們使用一系列指標系統性地評估了最先進的 VT，以評估重建圖像的品質。我們的研究結果表明，連續 VAE 產生了優於離散 VT 的視覺表徵，尤其是在保留空間結構和語義細節方面。相比之下，離散 VT 產生的劣質表徵通常會導致失真的重建、細粒度紋理的丟失以及在保留文本和物件完整性方面的失敗。此外，我們還對 GPT-4o 圖像生成進行了實驗，並討論了其潛在的自迴歸性質，為視覺 Tokenization 的作用提供了新的見解。我們公開發布了我們的評估標準和程式碼庫，以支持進一步的研究，並呼籲社群開發強大且通用的開源 VT。", "audio": "audios/2505.13439v1.mp3", "timestamp": "2025-05-20T05:18:43.416925"}
{"id": "2505.13418v1", "url": "http://arxiv.org/abs/2505.13418v1", "title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness", "summary": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter.", "authors": ["Lotem Peled-Cohen", "Maya Zadok", "Nitay Calderon", "Hila Gonen", "Roi Reichart"], "published_date": "2025-05-19", "category": "Foundation Model", "title_zh": "用不同的角度看失智症：針對人類與大型語言模型感知的可解釋性建模，以利早期察覺", "summary_zh": "認知功能衰退往往在診斷前數年就體現在語言中。非專業人士，像是病患的親近家人，常常是第一個察覺到變化的。隨著大型語言模型日益融入日常生活，甚至可能由它們發現異狀。這篇論文探討非專業人士如何透過語言感知失智症，並比較其與大型語言模型的判斷。研究發現，人類對失智症的感知不一致，且仰賴狹隘甚至具誤導性的線索。相比之下，大型語言模型利用更豐富、更細膩的特徵，更貼近臨床模式。但兩者都容易出現假陰性，經常忽略失智症病例。透過可解釋性框架，我們希望能幫助非專業人士更好地識別重要的語言徵兆。", "audio": "audios/2505.13418v1.mp3", "timestamp": "2025-05-20T05:18:49.560907"}
{"id": "2505.13244v1", "url": "http://arxiv.org/abs/2505.13244v1", "title": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models", "summary": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection.", "authors": ["Jieying Xue", "Phuong Minh Nguyen", "Minh Le Nguyen", "Xin Liu"], "published_date": "2025-05-19", "category": "Diffusion Model", "title_zh": "JNLP於SemEval-2025 Task 11：使用生成模型進行跨語言多標籤情感偵測", "summary_zh": "隨著全球數位化，跨語言情感偵測日益重要。本研究針對SemEval-2025 Task 11，利用預訓練多語言模型，探討多標籤情感偵測和情感強度這兩個子任務。我們採用微調的BERT分類模型和指令調整的生成LLM兩種架構，並提出兩種方法處理多標籤分類。實驗結果表明，我們的模型在多語言情感辨識方面具有強大的泛化能力，在Track A中，我們的模型在10種語言中取得前四的成績，並在印地語中排名第一。在Track B中，我們的方法在7種語言中也取得了前五名的成績，證明了其簡潔性和有效性。", "audio": "audios/2505.13244v1.mp3", "timestamp": "2025-05-20T05:18:54.395293"}
{"id": "2505.13438v1", "url": "http://arxiv.org/abs/2505.13438v1", "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "published_date": "2025-05-19", "category": "AI", "title_zh": "透過預算相對策略優化來最佳化隨時推理", "summary_zh": "為了提升大型語言模型的推理能力，增加測試時的計算量非常重要。現有方法通常使用強化學習來最大化推理結束時的可驗證獎勵。然而，這些方法只優化固定預算下的最終性能，效率不高。本研究提出一個名為 AnytimeReasoner 的新框架，旨在最佳化隨時推理性能，提升token效率，並在不同token預算限制下提供更靈活的推理。透過將完整推理過程截斷到來自先驗分佈的採樣token預算內，模型必須為每次截斷的思考總結出最佳答案進行驗證。這引入了可驗證的密集獎勵，促進強化學習中更有效的信用分配。此外，我們還提出了一種新的方差縮減技術，即預算相對策略優化（BRPO），以增強學習過程的穩健性和效率。在數學推理任務中的實驗結果表明，我們的模型在各種先驗分佈下，始終優於GRPO，提高了訓練和token效率。", "audio": "audios/2505.13438v1.mp3", "timestamp": "2025-05-20T06:27:24.396559"}
{"id": "2505.13416v1", "url": "http://arxiv.org/abs/2505.13416v1", "title": "Gluon: Making Muon & Scion Great Again! (Bridging Theory and Practice of LMO-based Optimizers for LLMs)", "summary": "Recent developments in deep learning optimization have brought about\nradically new algorithms based on the Linear Minimization Oracle (LMO)\nframework, such as $\\sf Muon$ and $\\sf Scion$. After over a decade of $\\sf\nAdam$'s dominance, these LMO-based methods are emerging as viable replacements,\noffering several practical advantages such as improved memory efficiency,\nbetter hyperparameter transferability, and most importantly, superior empirical\nperformance on large-scale tasks, including LLM training. However, a\nsignificant gap remains between their practical use and our current theoretical\nunderstanding: prior analyses (1) overlook the layer-wise LMO application of\nthese optimizers in practice, and (2) rely on an unrealistic smoothness\nassumption, leading to impractically small stepsizes. To address both, we\npropose a new LMO-based method called $\\sf Gluon$, capturing prior\ntheoretically analyzed methods as special cases, and introduce a new refined\ngeneralized smoothness model that captures the layer-wise geometry of neural\nnetworks, matches the layer-wise practical implementation of $\\sf Muon$ and\n$\\sf Scion$, and leads to convergence guarantees with strong practical\npredictive power. Unlike prior results, our theoretical stepsizes closely match\nthe fine-tuned values reported by Pethick et al. (2025). Our experiments with\nNanoGPT and CNN confirm that our assumption holds along the optimization\ntrajectory, ultimately closing the gap between theory and practice.", "authors": ["Artem Riabinin", "Egor Shulgin", "Kaja Gruntkowska", "Peter Richtárik"], "published_date": "2025-05-19", "category": "Foundation Model", "title_zh": "Gluon：讓Muon和Scion再次偉大！（彌合基於LMO的LLM優化器之理論與實踐差距）", "summary_zh": "近年來，基於線性最小化預言機（LMO）框架的Muon和Scion等優化算法嶄露頭角，有望取代Adam。它們在記憶體效率、超參數遷移和大規模任務（包括LLM訓練）的性能上表現更佳。然而，理論與實踐間存在差距，過去的研究未能考慮這些優化器在實踐中逐層應用LMO的特性，以及過於理想化的平滑性假設導致步長過小。為了解決這些問題，我們提出了一種新的LMO方法Gluon，它涵蓋了先前理論分析過的方法，並引入了一種新的廣義平滑性模型，可以捕捉神經網路的逐層幾何結構，與Muon和Scion的逐層實作相符，並能提供具有實際預測能力的收斂保證。實驗結果表明，我們的理論步長與實際調整值非常接近，最終彌合了理論與實踐之間的差距。", "audio": "audios/2505.13416v1.mp3", "timestamp": "2025-05-20T06:27:30.916151"}
{"id": "2505.13213v1", "url": "http://arxiv.org/abs/2505.13213v1", "title": "Diffusion Models with Double Guidance: Generate with aggregated datasets", "summary": "Creating large-scale datasets for training high-performance generative models\nis often prohibitively expensive, especially when associated attributes or\nannotations must be provided. As a result, merging existing datasets has become\na common strategy. However, the sets of attributes across datasets are often\ninconsistent, and their naive concatenation typically leads to block-wise\nmissing conditions. This presents a significant challenge for conditional\ngenerative modeling when the multiple attributes are used jointly as\nconditions, thereby limiting the model's controllability and applicability. To\naddress this issue, we propose a novel generative approach, Diffusion Model\nwith Double Guidance, which enables precise conditional generation even when no\ntraining samples contain all conditions simultaneously. Our method maintains\nrigorous control over multiple conditions without requiring joint annotations.\nWe demonstrate its effectiveness in molecular and image generation tasks, where\nit outperforms existing baselines both in alignment with target conditional\ndistributions and in controllability under missing condition settings.", "authors": ["Yanfeng Yang", "Kenji Fukumizu"], "published_date": "2025-05-19", "category": "Diffusion Model", "title_zh": "雙重引導的擴散模型：使用聚合數據集進行生成", "summary_zh": "為了訓練高效能的生成模型，建立大規模數據集往往成本高昂。因此，合併現有數據集成為常見策略，但各數據集的屬性往往不一致，直接合併容易導致條件缺失。針對這個問題，我們提出一種名為「雙重引導的擴散模型」的新方法，即使沒有同時包含所有條件的訓練樣本，也能實現精確的條件生成，在不需要聯合標註的情況下，嚴格控制多個條件。實驗證明，在分子和圖像生成任務中，我們的模型在目標條件分佈對齊以及缺失條件下的可控性方面，都優於現有基線。", "audio": "audios/2505.13213v1.mp3", "timestamp": "2025-05-20T06:27:36.081132"}
{"query": "AI", "id": "2505.13400v1", "url": "http://arxiv.org/abs/2505.13400v1", "title": "Robin: A multi-agent system for automating scientific discovery", "summary": "Scientific discovery is driven by the iterative process of background\nresearch, hypothesis generation, experimentation, and data analysis. Despite\nrecent advancements in applying artificial intelligence to scientific\ndiscovery, no system has yet automated all of these stages in a single\nworkflow. Here, we introduce Robin, the first multi-agent system capable of\nfully automating the key intellectual steps of the scientific process. By\nintegrating literature search agents with data analysis agents, Robin can\ngenerate hypotheses, propose experiments, interpret experimental results, and\ngenerate updated hypotheses, achieving a semi-autonomous approach to scientific\ndiscovery. By applying this system, we were able to identify a novel treatment\nfor dry age-related macular degeneration (dAMD), the major cause of blindness\nin the developed world. Robin proposed enhancing retinal pigment epithelium\nphagocytosis as a therapeutic strategy, and identified and validated a\npromising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho\nkinase (ROCK) inhibitor that has never previously been proposed for treating\ndAMD. To elucidate the mechanism of ripasudil-induced upregulation of\nphagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment,\nwhich revealed upregulation of ABCA1, a critical lipid efflux pump and possible\nnovel target. All hypotheses, experimental plans, data analyses, and data\nfigures in the main text of this report were produced by Robin. As the first AI\nsystem to autonomously discover and validate a novel therapeutic candidate\nwithin an iterative lab-in-the-loop framework, Robin establishes a new paradigm\nfor AI-driven scientific discovery.", "authors": ["Ali Essam Ghareeb", "Benjamin Chang", "Ludovico Mitchener", "Angela Yiu", "Caralyn J. Szostkiewicz", "Jon M. Laurent", "Muhammed T. Razzak", "Andrew D. White", "Michaela M. Hinks", "Samuel G. Rodriques"], "published_date": "2025-05-19", "title_zh": "羅賓：一個用於自動化科學發現的多代理人系統", "summary_zh": "科學發現通常需要反覆進行文獻研究、假說生成、實驗以及數據分析。本文介紹了名為「羅賓」的多代理人系統，它整合了文獻搜尋和數據分析代理，首次能夠完全自動化科學發現過程中的關鍵步驟。羅賓能生成假說、提出實驗方案、解讀實驗結果並更新假說，實現半自主的科學發現。利用羅賓，我們發現了一種治療乾性老年黃斑部病變（dAMD）的新方法，並驗證了潛在候選藥物ripasudil。羅賓還分析了後續實驗，揭示了ABCA1的表達上調，這可能是個新的治療靶點。重要的是，本文中的所有假說、實驗計畫、數據分析和圖表均由羅賓生成。作為首個在迭代實驗迴路中自主發現並驗證治療候選藥物的人工智慧系統，羅賓為人工智慧驅動的科學發現建立了一個新典範。", "audio": "audios/2505.13400v1.mp3", "timestamp": "2025-05-20T09:53:34.781452"}
{"query": "Foundation Model", "id": "2505.13291v1", "url": "http://arxiv.org/abs/2505.13291v1", "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents", "summary": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating\nArtificial Intelligence (AI) agents on time series machine learning engineering\nchallenges. Existing benchmarks lack scalability, focus narrowly on model\nbuilding in well-defined settings, and evaluate only a limited set of research\nartifacts (e.g., CSV submission files). To make AI agent benchmarking more\nrelevant to the practice of machine learning engineering, our framework scales\nalong two critical dimensions. First, recognizing that effective ML engineering\nrequires a range of diverse skills, TimeSeriesGym incorporates challenges from\ndiverse sources spanning multiple domains and tasks. We design challenges to\nevaluate both isolated capabilities (including data handling, understanding\nresearch repositories, and code translation) and their combinations, and rather\nthan addressing each challenge independently, we develop tools that support\ndesigning multiple challenges at scale. Second, we implement evaluation\nmechanisms for multiple research artifacts, including submission files, code,\nand models, using both precise numeric measures and more flexible LLM-based\nevaluation approaches. This dual strategy balances objective assessment with\ncontextual judgment. Although our initial focus is on time series applications,\nour framework can be readily extended to other data modalities, broadly\nenhancing the comprehensiveness and practical utility of agentic AI evaluation.\nWe open-source our benchmarking framework to facilitate future research on the\nML engineering capabilities of AI agents.", "authors": ["Yifu Cai", "Xinyu Li", "Mononito Goswami", "Michał Wiliński", "Gus Welter", "Artur Dubrawski"], "published_date": "2025-05-19", "title_zh": "TimeSeriesGym：一個可擴展的機器學習工程（時間序列）代理基準測試", "summary_zh": "TimeSeriesGym 是一個可擴展的基準測試框架，用於評估人工智慧 (AI) 代理在時間序列機器學習工程挑戰中的表現。現有的基準測試缺乏可擴展性，且過於狹隘地關注良好定義環境下的模型構建，並僅評估有限的研究成果。TimeSeriesGym 透過兩個關鍵面向提升可擴展性：首先，它整合了來自多個領域和任務的多樣化挑戰，評估資料處理、理解研究庫和程式碼翻譯等不同技能的組合。其次，它評估多種研究成果，包括提交檔案、程式碼和模型，同時採用精確的數值度量和基於大型語言模型 (LLM) 的彈性評估方法。這個框架開放原始碼，旨在促進對 AI 代理機器學習工程能力的研究。", "audio": "audios/2505.13291v1.mp3", "timestamp": "2025-05-20T09:53:40.535627"}
{"query": "Diffusion Model", "id": "2505.13389v1", "url": "http://arxiv.org/abs/2505.13389v1", "title": "Faster Video Diffusion with Trainable Sparse Attention", "summary": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at \\emph{both}\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight \\emph{critical tokens}; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models.", "authors": ["Peiyuan Zhang", "Haofeng Huang", "Yongqi Chen", "Will Lin", "Zhengzhong Liu", "Ion Stoica", "Eric P. Xing", "Hao Zhang"], "published_date": "2025-05-19", "title_zh": "透過可訓練的稀疏注意力加速影片擴散", "summary_zh": "影片擴散轉換器（DiT）的擴展受限於其二次方的3D注意力，即便大部分注意力集中在一小部分位置。我們提出了VSA，一種可訓練、硬體高效的稀疏注意力，在訓練和推論階段都取代了完整注意力。VSA使用輕量級粗略階段將tokens池化成tiles，並識別高權重的「關鍵tokens」；精細階段僅在這些tiles內部計算token級別的注意力，並採用區塊計算布局以確保硬體效率。這產生了一個可端到端訓練的單一可微核心，無需事後分析，並維持了FlashAttention3 MFU的85%。我們進行了大量的消融研究和縮放律實驗，對參數量從60M到1.4B的DiT進行預訓練。VSA達到了一個帕雷托點，在擴散損失沒有下降的情況下，將訓練FLOPS降低了2.53倍。改造開源的Wan-2.1模型後，注意力時間加速了6倍，並在品質相當的情況下，將端到端生成時間從31秒降低到18秒。這些結果表明，可訓練的稀疏注意力是完整注意力的一個實用替代方案，也是進一步擴展影片擴散模型的關鍵推動因素。", "audio": "audios/2505.13389v1.mp3", "timestamp": "2025-05-20T09:53:47.983791"}
{"query": "AI", "id": "2505.13381v1", "url": "http://arxiv.org/abs/2505.13381v1", "title": "How Adding Metacognitive Requirements in Support of AI Feedback in Practice Exams Transforms Student Learning Behaviors", "summary": "Providing personalized, detailed feedback at scale in large undergraduate\nSTEM courses remains a persistent challenge. We present an empirically\nevaluated practice exam system that integrates AI generated feedback with\ntargeted textbook references, deployed in a large introductory biology course.\nOur system encourages metacognitive behavior by asking students to explain\ntheir answers and declare their confidence. It uses OpenAI's GPT-4o to generate\npersonalized feedback based on this information, while directing them to\nrelevant textbook sections. Through interaction logs from consenting\nparticipants across three midterms (541, 342, and 413 students respectively),\ntotaling 28,313 question-student interactions across 146 learning objectives,\nalong with 279 surveys and 23 interviews, we examined the system's impact on\nlearning outcomes and engagement. Across all midterms, feedback types showed no\nstatistically significant performance differences, though some trends suggested\npotential benefits. The most substantial impact came from the required\nconfidence ratings and explanations, which students reported transferring to\ntheir actual exam strategies. About 40 percent of students engaged with\ntextbook references when prompted by feedback -- far higher than traditional\nreading rates. Survey data revealed high satisfaction (mean rating 4.1 of 5),\nwith 82.1 percent reporting increased confidence on practiced midterm topics,\nand 73.4 percent indicating they could recall and apply specific concepts. Our\nfindings suggest that embedding structured reflection requirements may be more\nimpactful than sophisticated feedback mechanisms.", "authors": ["Mak Ahmad", "Prerna Ravi", "David Karger", "Marc Facciotti"], "published_date": "2025-05-19", "title_zh": "如何在練習測驗中加入元認知需求以支援人工智慧回饋，進而改變學生學習行為", "summary_zh": "在大型大學STEM課程中提供大規模且個人化的回饋是個挑戰。這項研究設計了一個練習測驗系統，結合人工智慧生成的回饋與相關教科書參考，並在生物入門課中實施。該系統鼓勵學生進行元認知，要求他們解釋答案並聲明信心程度。系統使用GPT-4o生成個人化回饋，並引導學生查閱教科書。研究發現，雖然不同回饋類型的成績差異不大，但要求學生評估信心程度並解釋答案，對他們的學習策略有顯著影響，許多學生也表示將這些策略應用於正式考試中。當被回饋提示時，約有40%的學生會參考教科書，遠高於傳統閱讀率。調查顯示學生滿意度高，並表示對練習過的考題更有信心，也更能回憶和應用特定概念。研究表明，比起複雜的回饋機制，嵌入結構化的反思需求可能更具影響力。", "audio": "audios/2505.13381v1.mp3", "timestamp": "2025-05-20T10:20:40.252613"}
{"query": "Foundation Model", "id": "2505.13255v1", "url": "http://arxiv.org/abs/2505.13255v1", "title": "Policy Contrastive Decoding for Robotic Foundation Models", "summary": "Robotic foundation models, or generalist robot policies, hold immense\npotential to enable flexible, general-purpose and dexterous robotic systems.\nDespite their advancements, our empirical experiments reveal that existing\nrobot policies are prone to learning spurious correlations from pre-training\ntrajectories, adversely affecting their generalization capabilities beyond the\ntraining data. To tackle this, we propose a novel Policy Contrastive Decoding\n(PCD) approach, which redirects the robot policy's focus toward object-relevant\nvisual clues by contrasting action probability distributions derived from\noriginal and object-masked visual inputs. As a training-free method, our PCD\ncan be used as a plugin to improve different types of robot policies without\nneeding to finetune or access model weights. We conduct extensive experiments\non top of three open-source robot policies, including the autoregressive policy\nOpenVLA and the diffusion-based policies Octo and $\\pi_0$. The obtained results\nin both simulation and real-world environments prove PCD's flexibility and\neffectiveness, e.g., PCD enhances the state-of-the-art policy $\\pi_0$ by 8% in\nthe simulation environment and by 108% in the real-world environment. Code and\ndemos are publicly available at: https://Koorye.github.io/proj/PCD.", "authors": ["Shihan Wu", "Ji Zhang", "Xu Luo", "Junlin Xie", "Jingkuan Song", "Heng Tao Shen", "Lianli Gao"], "published_date": "2025-05-19", "title_zh": "用於機器人基礎模型的策略對比解碼", "summary_zh": "機器人基礎模型，也就是通用型機器人策略，有潜力打造更靈活的機器人系統。然而，研究發現現有的機器人策略容易從預訓練資料中學到虛假關聯性，導致泛化能力下降。為了解決這個問題，我們提出一種名為「策略對比解碼 (PCD)」的新方法，它通過对比原始視覺輸入和遮蔽物體的視覺輸入所得到的動作概率分佈，引導機器人策略關注與物體相關的視覺線索。PCD無需訓練，可以像插件一樣使用，提升各種機器人策略的性能，而無需微調或訪問模型權重。大量實驗表明PCD具有靈活性和有效性，例如，在模擬環境和真實環境中，它分別將最先進的策略 π₀ 的性能提升了 8% 和 108%。程式碼和演示可在指定連結取得。", "audio": "audios/2505.13255v1.mp3", "timestamp": "2025-05-20T10:20:47.331287"}
{"query": "Diffusion Model", "id": "2505.13377v1", "url": "http://arxiv.org/abs/2505.13377v1", "title": "Restoration Score Distillation: From Corrupted Diffusion Pretraining to One-Step High-Quality Generation", "summary": "Learning generative models from corrupted data is a fundamental yet\npersistently challenging task across scientific disciplines, particularly when\naccess to clean data is limited or expensive. Denoising Score Distillation\n(DSD) \\cite{chen2025denoising} recently introduced a novel and surprisingly\neffective strategy that leverages score distillation to train high-fidelity\ngenerative models directly from noisy observations. Building upon this\nfoundation, we propose \\textit{Restoration Score Distillation} (RSD), a\nprincipled generalization of DSD that accommodates a broader range of\ncorruption types, such as blurred, incomplete, or low-resolution images. RSD\noperates by first pretraining a teacher diffusion model solely on corrupted\ndata and subsequently distilling it into a single-step generator that produces\nhigh-quality reconstructions. Empirically, RSD consistently surpasses its\nteacher model across diverse restoration tasks on both natural and scientific\ndatasets. Moreover, beyond standard diffusion objectives, the RSD framework is\ncompatible with several corruption-aware training techniques such as Ambient\nTweedie, Ambient Diffusion, and its Fourier-space variant, enabling flexible\nintegration with recent advances in diffusion modeling. Theoretically, we\ndemonstrate that in a linear regime, RSD recovers the eigenspace of the clean\ndata covariance matrix from linear measurements, thereby serving as an implicit\nregularizer. This interpretation recasts score distillation not only as a\nsampling acceleration technique but as a principled approach to enhancing\ngenerative performance in severely degraded data regimes.", "authors": ["Yasi Zhang", "Tianyu Chen", "Zhendong Wang", "Ying Nian Wu", "Mingyuan Zhou", "Oscar Leong"], "published_date": "2025-05-19", "title_zh": "復原分數蒸餾：從已損毀的擴散預訓練到一步式高品質生成", "summary_zh": "從損毀數據學習生成模型是個重要的挑戰。復原分數蒸餾(RSD)是一種新的方法，它先用損毀數據訓練一個擴散模型作為老師，然後將其知識提煉成一個一步式生成器，直接重建出高品質的影像。RSD可以處理各種損毀類型，例如模糊、不完整或低解析度的圖像，並且在各種還原任務上都超越了老師模型。理論分析表明，RSD可以從線性測量中恢復乾淨數據的協方差矩陣的特徵空間，因此它不僅是一種加速取樣的技術，更是一種在數據嚴重降級的情況下提升生成性能的有效方法。", "audio": "audios/2505.13377v1.mp3", "timestamp": "2025-05-20T10:20:54.898993"}
{"query": "AI", "id": "2505.13355v1", "url": "http://arxiv.org/abs/2505.13355v1", "title": "Multi-Armed Bandits Meet Large Language Models", "summary": "Bandit algorithms and Large Language Models (LLMs) have emerged as powerful\ntools in artificial intelligence, each addressing distinct yet complementary\nchallenges in decision-making and natural language processing. This survey\nexplores the synergistic potential between these two fields, highlighting how\nbandit algorithms can enhance the performance of LLMs and how LLMs, in turn,\ncan provide novel insights for improving bandit-based decision-making. We first\nexamine the role of bandit algorithms in optimizing LLM fine-tuning, prompt\nengineering, and adaptive response generation, focusing on their ability to\nbalance exploration and exploitation in large-scale learning tasks.\nSubsequently, we explore how LLMs can augment bandit algorithms through\nadvanced contextual understanding, dynamic adaptation, and improved policy\nselection using natural language reasoning. By providing a comprehensive review\nof existing research and identifying key challenges and opportunities, this\nsurvey aims to bridge the gap between bandit algorithms and LLMs, paving the\nway for innovative applications and interdisciplinary research in AI.", "authors": ["Djallel Bouneffouf", "Raphael Feraud"], "published_date": "2025-05-19", "title_zh": "多臂老虎機遇上大型語言模型", "summary_zh": "這篇論文探討了多臂老虎機演算法與大型語言模型（LLM）之間的協同效應。多臂老虎機演算法可以優化 LLM 的微調、提示工程和自適應回應生成，而 LLM 則能利用其強大的上下文理解能力、動態適應能力和自然語言推理能力來改進多臂老虎機演算法的策略選擇。這篇綜述旨在促進這兩個領域的交叉研究，為人工智慧的創新應用鋪平道路。", "audio": "audios/2505.13355v1.mp3", "timestamp": "2025-05-20T11:15:50.184028"}
{"query": "Foundation Model", "id": "2505.13227v1", "url": "http://arxiv.org/abs/2505.13227v1", "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "published_date": "2025-05-19", "title_zh": "透過使用者介面分解與合成來擴展電腦使用接地的能力", "summary_zh": "圖形使用者介面 (GUI) 接地，也就是將自然語言指令映射到 GUI 上的特定操作，仍然是電腦使用代理程式開發中的一個關鍵瓶頸。現有基準測試過於簡化接地任務，將其視為簡短的指稱表達式，未能捕捉到需要軟體常識、版面理解和細粒度操作能力的真實世界互動的複雜性。為了應對這些局限性，我們引入了 OSWorld-G，這是一個全面的基準測試，包含 564 個跨多種任務類型（包括文本匹配、元素識別、版面理解和精確操作）的精細註釋樣本。此外，我們透過多視角解耦任務，合成並發布了最大的電腦使用接地資料集 Jedi，其中包含 400 萬個示例。我們在 Jedi 上訓練的多尺度模型透過優於 ScreenSpot-v2、ScreenSpot-Pro 和我們的 OSWorld-G 上的現有方法，證明了其有效性。此外，我們證明了 Jedi 改進的接地能力直接增強了通用基礎模型在複雜電腦任務上的代理能力，在 OSWorld 上從 5% 提高到 27%。透過詳細的消融研究，我們確定了影響接地效能的關鍵因素，並驗證了針對不同介面元素的專業資料的結合能夠實現對新介面的組合成泛化。所有基準測試、資料、檢查點和程式碼都是開源的，並且可於 https://osworld-grounding.github.io 取得。", "audio": "audios/2505.13227v1.mp3", "timestamp": "2025-05-20T11:16:02.194993"}
{"query": "Diffusion Model", "id": "2505.13375v1", "url": "http://arxiv.org/abs/2505.13375v1", "title": "Minimum-Excess-Work Guidance", "summary": "We propose a regularization framework inspired by thermodynamic work for\nguiding pre-trained probability flow generative models (e.g., continuous\nnormalizing flows or diffusion models) by minimizing excess work, a concept\nrooted in statistical mechanics and with strong conceptual connections to\noptimal transport. Our approach enables efficient guidance in sparse-data\nregimes common to scientific applications, where only limited target samples or\npartial density constraints are available. We introduce two strategies: Path\nGuidance for sampling rare transition states by concentrating probability mass\non user-defined subsets, and Observable Guidance for aligning generated\ndistributions with experimental observables while preserving entropy. We\ndemonstrate the framework's versatility on a coarse-grained protein model,\nguiding it to sample transition configurations between folded/unfolded states\nand correct systematic biases using experimental data. The method bridges\nthermodynamic principles with modern generative architectures, offering a\nprincipled, efficient, and physics-inspired alternative to standard fine-tuning\nin data-scarce domains. Empirical results highlight improved sample efficiency\nand bias reduction, underscoring its applicability to molecular simulations and\nbeyond.", "authors": ["Christopher Kolloff", "Tobias Höppe", "Emmanouil Angelis", "Mathias Jacob Schreiner", "Stefan Bauer", "Andrea Dittadi", "Simon Olsson"], "published_date": "2025-05-19", "title_zh": "最小過剩功引導", "summary_zh": "我們提出一種基於熱力學功的正則化框架，引導預訓練的機率流生成模型（例如連續歸一化流或擴散模型），通過最小化過剩功實現。這種方法在科學應用常見的稀疏數據情況下非常有效，僅需少量目標樣本或部分密度約束。我們介紹了兩種策略：路徑引導，用於採樣罕見的過渡態，以及可觀測量引導，用於將生成的分布與實驗觀測值對齊。在粗粒化蛋白模型上的實驗表明，該框架能夠有效地採樣摺疊/解摺疊狀態之間的過渡構型，並利用實驗數據修正系統偏差。總之，這項工作將熱力學原理與現代生成架構相結合，為數據稀缺領域提供了一種基於物理、高效且有原則的替代方案，優於標準微調方法。", "audio": "audios/2505.13375v1.mp3", "timestamp": "2025-05-20T11:16:08.821722"}
{"query": "AI", "id": "2505.13354v1", "url": "http://arxiv.org/abs/2505.13354v1", "title": "A large-scale analysis of public-facing, community-built chatbots on Character.AI", "summary": "This paper presents the first large-scale analysis of public-facing chatbots\non Character.AI, a rapidly growing social media platform where users create and\ninteract with chatbots. Character.AI is distinctive in that it merges\ngenerative AI with user-generated content, enabling users to build bots-often\nmodeled after fictional or public personas-for others to engage with. It is\nalso popular, with over 20 million monthly active users, and impactful, with\nrecent headlines detailing significant issues with youth engagement on the\nsite. Character.AI is thus of interest to study both substantively and\nconceptually. To this end, we present a descriptive overview of the site using\na dataset of 2.1 million English-language prompts (or ``greetings'') for\nchatbots on the site, created by around 1 million users. Our work explores the\nprevalence of different fandoms on the site, broader tropes that persist across\nfandoms, and how dynamics of power intersect with gender within greetings.\nOverall, our findings illuminate an emerging form of online (para)social\ninteraction that toes a unique and important intersection between generative AI\nand user-generated content.", "authors": ["Owen Lee", "Kenneth Joseph"], "published_date": "2025-05-19", "title_zh": "Character.AI上公開、社群建立的聊天機器人的大規模分析", "summary_zh": "本研究首次大規模分析Character.AI平台上公開的聊天機器人。Character.AI是一個快速成長的社交媒體平台，用戶可以創建並與聊天機器人互動。它結合了生成式AI和使用者產生的內容，讓使用者可以建立模仿虛構或公眾人物的機器人。本研究利用包含210萬條英文提示詞的數據集，描述了Character.AI的概況，並探討了平台上的不同粉絲群體、常見的主題，以及權力動態如何與性別交叉。研究結果揭示了一種新興的線上準社交互動形式，它獨特且重要地結合了生成式AI和使用者產生的內容。", "audio": "audios/2505.13354v1.mp3", "timestamp": "2025-05-20T12:38:41.704889"}
{"query": "Foundation Model", "id": "2505.13192v1", "url": "http://arxiv.org/abs/2505.13192v1", "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics", "summary": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters and\norders of magnitude faster inference times. DynaMix outperforms TS foundation\nmodels in terms of long-term statistics, and often also short-term forecasts,\neven on real-world time series, like traffic or weather data, typically used\nfor training and evaluating TS models, but not at all part of DynaMix' training\ncorpus. We illustrate some of the failure modes of TS models for DSR problems,\nand conclude that models built on DS principles may bear a huge potential also\nfor advancing the TS prediction field.", "authors": ["Christoph Jürgen Hemmer", "Daniel Durstewitz"], "published_date": "2025-05-19", "title_zh": "真實零樣本推論：長期統計量保持的動態系統", "summary_zh": "DynaMix是一種新的動態系統重建模型，它基於ALRNN的專家混合架構進行預訓練。與傳統方法不同，DynaMix無需針對每個新系統進行重新訓練，就能夠零樣本泛化到未知的動態系統。只需提供上下文信號，DynaMix即可忠實地預測新系統的長期演化，其性能優於現有的時間序列基礎模型，且參數更少、推論速度更快。即使面對真實世界的交通或天氣數據，DynaMix也能在長期統計量方面勝過這些模型。這項研究表明，基於動態系統原理構建的模型在時間序列預測領域具有巨大潛力。", "audio": "audios/2505.13192v1.mp3", "timestamp": "2025-05-20T12:38:46.833826"}
{"query": "Diffusion Model", "id": "2505.13358v1", "url": "http://arxiv.org/abs/2505.13358v1", "title": "One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling", "summary": "Diffusion-based generative models have demonstrated exceptional performance,\nyet their iterative sampling procedures remain computationally expensive. A\nprominent strategy to mitigate this cost is distillation, with offline\ndistillation offering particular advantages in terms of efficiency, modularity,\nand flexibility. In this work, we identify two key observations that motivate a\nprincipled distillation framework: (1) while diffusion models have been viewed\nthrough the lens of dynamical systems theory, powerful and underexplored tools\ncan be further leveraged; and (2) diffusion models inherently impose\nstructured, semantically coherent trajectories in latent space. Building on\nthese observations, we introduce the Koopman Distillation Model KDM, a novel\noffline distillation approach grounded in Koopman theory-a classical framework\nfor representing nonlinear dynamics linearly in a transformed space. KDM\nencodes noisy inputs into an embedded space where a learned linear operator\npropagates them forward, followed by a decoder that reconstructs clean samples.\nThis enables single-step generation while preserving semantic fidelity. We\nprovide theoretical justification for our approach: (1) under mild assumptions,\nthe learned diffusion dynamics admit a finite-dimensional Koopman\nrepresentation; and (2) proximity in the Koopman latent space correlates with\nsemantic similarity in the generated outputs, allowing for effective trajectory\nalignment. Empirically, KDM achieves state-of-the-art performance across\nstandard offline distillation benchmarks, improving FID scores by up to 40% in\na single generation step. All implementation details and code for the\nexperimental setups are provided in our GitHub -\nhttps://github.com/azencot-group/KDM, or in our project page -\nhttps://sites.google.com/view/koopman-distillation-model.", "authors": ["Nimrod Berman", "Ilan Naiman", "Moshe Eliasof", "Hedi Zisling", "Omri Azencot"], "published_date": "2025-05-19", "title_zh": "基於Koopman建模的擴散模型一步式離線蒸餾", "summary_zh": "擴散模型在生成任務上表現出色，但迭代採樣過程耗時。本研究提出一種名為 Koopman Distillation Model (KDM) 的創新離線蒸餾方法，利用 Koopman 理論將非線性擴散動態線性地表示在轉換後的空間中。KDM 通過學習線性算子在嵌入空間中傳播噪聲輸入，實現單步生成高品質樣本，在標準離線蒸餾基準測試中，FID 指標提升高達 40%。程式碼和更多資訊可在 GitHub 或專案頁面找到。", "audio": "audios/2505.13358v1.mp3", "timestamp": "2025-05-20T12:38:54.099908"}
{"query": "AI", "id": "2505.13338v1", "url": "http://arxiv.org/abs/2505.13338v1", "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "summary": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities.", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "published_date": "2025-05-19", "title_zh": "用於多模態語音-LLM的情境副語言資料創建：資料濃縮與口語問答生成", "summary_zh": "現有的語音語言模型在情境推理和副語言理解方面能力有限，主要是缺乏涵蓋這兩方面的問答資料集。本文提出一個新穎的框架，從真實世界的語音資料中生成同時整合情境推理和副語言資訊的資料集，包含基於偽副語言標籤的資料濃縮，以及基於大型語言模型的情境副語言問答生成。實驗證明，基於此框架生成的資料集，能有效提升語音語言模型的性能，但也揭示了模型在同理心推理任務上的不足，突顯了創建此類資料集及開發更強大模型的重要性。此框架為首創，有潛力訓練出更強大且具備副語言推理能力的語音語言模型。", "audio": "audios/2505.13338v1.mp3", "timestamp": "2025-05-20T13:31:45.014604"}
{"query": "Foundation Model", "id": "2505.13150v1", "url": "http://arxiv.org/abs/2505.13150v1", "title": "Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics", "summary": "Behavioral Foundation Models (BFMs) proved successful in producing policies\nfor arbitrary tasks in a zero-shot manner, requiring no test-time training or\ntask-specific fine-tuning. Among the most promising BFMs are the ones that\nestimate the successor measure learned in an unsupervised way from\ntask-agnostic offline data. However, these methods fail to react to changes in\nthe dynamics, making them inefficient under partial observability or when the\ntransition function changes. This hinders the applicability of BFMs in a\nreal-world setting, e.g., in robotics, where the dynamics can unexpectedly\nchange at test time. In this work, we demonstrate that Forward-Backward (FB)\nrepresentation, one of the methods from the BFM family, cannot distinguish\nbetween distinct dynamics, leading to an interference among the latent\ndirections, which parametrize different policies. To address this, we propose a\nFB model with a transformer-based belief estimator, which greatly facilitates\nzero-shot adaptation. We also show that partitioning the policy encoding space\ninto dynamics-specific clusters, aligned with the context-embedding directions,\nyields additional gain in performance. These traits allow our method to respond\nto the dynamics observed during training and to generalize to unseen ones.\nEmpirically, in the changing dynamics setting, our approach achieves up to a 2x\nhigher zero-shot returns compared to the baselines for both discrete and\ncontinuous tasks.", "authors": ["Maksim Bobrin", "Ilya Zisman", "Alexander Nikulin", "Vladislav Kurenkov", "Dmitry Dylov"], "published_date": "2025-05-19", "title_zh": "行為基礎模型在未見動力學下的零樣本適應", "summary_zh": "行為基礎模型（BFM）無需訓練或微調就能零樣本執行各種任務。然而，基於後繼度量估計的BFM在動力學改變時表現不佳。本研究指出，一種名為Forward-Backward (FB)表示的BFM無法區分不同的動力學，導致策略混淆。為解決此問題，我們提出一種結合Transformer的信念估計器FB模型，顯著提升了零樣本適應能力。此外，將策略編碼空間劃分為特定動力學的群組，可以進一步提高性能。實驗證明，在動力學變化環境中，我們的模型在離散和連續任務上，零樣本回報比基線方法高達2倍。", "audio": "audios/2505.13150v1.mp3", "timestamp": "2025-05-20T13:31:50.537855"}
{"query": "Diffusion Model", "id": "2505.13280v1", "url": "http://arxiv.org/abs/2505.13280v1", "title": "FlowPure: Continuous Normalizing Flows for Adversarial Purification", "summary": "Despite significant advancements in the area, adversarial robustness remains\na critical challenge in systems employing machine learning models. The removal\nof adversarial perturbations at inference time, known as adversarial\npurification, has emerged as a promising defense strategy. To achieve this,\nstate-of-the-art methods leverage diffusion models that inject Gaussian noise\nduring a forward process to dilute adversarial perturbations, followed by a\ndenoising step to restore clean samples before classification. In this work, we\npropose FlowPure, a novel purification method based on Continuous Normalizing\nFlows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings\nfrom adversarial examples to their clean counterparts. Unlike prior\ndiffusion-based approaches that rely on fixed noise processes, FlowPure can\nleverage specific attack knowledge to improve robustness under known threats,\nwhile also supporting a more general stochastic variant trained on Gaussian\nperturbations for settings where such knowledge is unavailable. Experiments on\nCIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art\npurification-based defenses in preprocessor-blind and white-box scenarios, and\ncan do so while fully preserving benign accuracy in the former. Moreover, our\nresults show that not only is FlowPure a highly effective purifier but it also\nholds a strong potential for adversarial detection, identifying\npreprocessor-blind PGD samples with near-perfect accuracy.", "authors": ["Elias Collaert", "Abel Rodríguez", "Sander Joos", "Lieven Desmet", "Vera Rimmer"], "published_date": "2025-05-19", "title_zh": "FlowPure：使用連續歸一化流進行對抗性淨化", "summary_zh": "機器學習模型的對抗性魯棒性是個重要挑戰。FlowPure 是一種新的淨化方法，它利用連續歸一化流 (CNF) 來學習將對抗性樣本映射到乾淨樣本。與先前依賴固定噪音過程的擴散模型方法不同，FlowPure 能針對特定攻擊知識進行訓練，提升已知威脅下的魯棒性，也能在缺乏相關知識的情況下，使用基於高斯擾動的隨機變體。實驗表明，FlowPure 在 CIFAR-10 和 CIFAR-100 上的表現優於現有的淨化方法，且在預處理器盲測情境下能完全保持良性樣本的準確度。此外，FlowPure 在對抗性檢測方面也表現出色，幾乎能完美識別預處理器盲測的 PGD 樣本。", "audio": "audios/2505.13280v1.mp3", "timestamp": "2025-05-20T13:31:59.957771"}
{"query": "AI", "id": "2505.13329v1", "url": "http://arxiv.org/abs/2505.13329v1", "title": "Recommender Systems for Democracy: Toward Adversarial Robustness in Voting Advice Applications", "summary": "Voting advice applications (VAAs) help millions of voters understand which\npolitical parties or candidates best align with their views. This paper\nexplores the potential risks these applications pose to the democratic process\nwhen targeted by adversarial entities. In particular, we expose 11 manipulation\nstrategies and measure their impact using data from Switzerland's primary VAA,\nSmartvote, collected during the last two national elections. We find that\naltering application parameters, such as the matching method, can shift a\nparty's recommendation frequency by up to 105%. Cherry-picking questionnaire\nitems can increase party recommendation frequency by over 261%, while subtle\nchanges to parties' or candidates' responses can lead to a 248% increase. To\naddress these vulnerabilities, we propose adversarial robustness properties\nVAAs should satisfy, introduce empirical metrics for assessing the resilience\nof various matching methods, and suggest possible avenues for research toward\nmitigating the effect of manipulation. Our framework is key to ensuring secure\nand reliable AI-based VAAs poised to emerge in the near future.", "authors": ["Frédéric Berdoz", "Dustin Brunner", "Yann Vonlanthen", "Roger Wattenhofer"], "published_date": "2025-05-19", "title_zh": "民主推薦系統：邁向投票建議應用程式的對抗性穩健性", "summary_zh": "投票建議應用程式幫助選民了解哪些政黨或候選人最符合他們的觀點。這篇論文探討了這些應用程式在受到對抗性實體攻擊時，對民主進程構成的潛在風險。研究揭露了11種操控策略，發現更改應用程式參數、精心挑選問卷題目或修改政黨/候選人的回答，都可能顯著改變政黨的推薦頻率。為了應對這些漏洞，研究提出了投票建議應用程式應該滿足的對抗性穩健性屬性，引入了評估不同匹配方法韌性的指標，並建議了減輕操控影響的研究方向，旨在確保未來基於AI的投票建議應用程式的安全和可靠性。", "audio": "audios/2505.13329v1.mp3", "timestamp": "2025-05-20T14:18:35.743976"}
{"query": "Foundation Model", "id": "2505.13099v1", "url": "http://arxiv.org/abs/2505.13099v1", "title": "Industry-focused Synthetic Segmentation Pre-training", "summary": "Pre-training on real-image datasets has been widely proven effective for\nimproving instance segmentation. However, industrial applications face two key\nchallenges: (1) legal and ethical restrictions, such as ImageNet's prohibition\nof commercial use, and (2) limited transferability due to the domain gap\nbetween web images and industrial imagery. Even recent vision foundation\nmodels, including the segment anything model (SAM), show notable performance\ndegradation in industrial settings. These challenges raise critical questions:\nCan we build a vision foundation model for industrial applications without\nrelying on real images or manual annotations? And can such models outperform\neven fine-tuned SAM on industrial datasets? To address these questions, we\npropose the Instance Core Segmentation Dataset (InsCore), a synthetic\npre-training dataset based on formula-driven supervised learning (FDSL).\nInsCore generates fully annotated instance segmentation images that reflect key\ncharacteristics of industrial data, including complex occlusions, dense\nhierarchical masks, and diverse non-rigid shapes, distinct from typical web\nimagery. Unlike previous methods, InsCore requires neither real images nor\nhuman annotations. Experiments on five industrial datasets show that models\npre-trained with InsCore outperform those trained on COCO and ImageNet-21k, as\nwell as fine-tuned SAM, achieving an average improvement of 6.2 points in\ninstance segmentation performance. This result is achieved using only 100k\nsynthetic images, more than 100 times fewer than the 11 million images in SAM's\nSA-1B dataset, demonstrating the data efficiency of our approach. These\nfindings position InsCore as a practical and license-free vision foundation\nmodel for industrial applications.", "authors": ["Shinichi Mae", "Ryosuke Yamada", "Hirokatsu Kataoka"], "published_date": "2025-05-19", "title_zh": "針對產業的合成分割預訓練", "summary_zh": "現有的圖像分割預訓練模型常受限於授權問題及與工業圖像的領域差距。為此，我們提出一個名為InsCore的合成預訓練數據集，它基於公式驅動的監督學習，能生成反映工業數據特徵的完整標註分割圖像，例如複雜的遮擋、密集的分層遮罩和多樣化的非剛性形狀。實驗證明，使用InsCore預訓練的模型，在五個工業數據集上的分割表現優於在COCO和ImageNet-21k上訓練的模型，甚至超越微調後的SAM模型，平均提升了6.2個百分點。重點是，InsCore僅使用10萬張合成圖像，效率遠高於SAM的SA-1B數據集，為工業應用提供了一種實用且無授權限制的視覺基礎模型。", "audio": "audios/2505.13099v1.mp3", "timestamp": "2025-05-20T14:18:44.078291"}
{"query": "Diffusion Model", "id": "2505.13152v1", "url": "http://arxiv.org/abs/2505.13152v1", "title": "Higher fidelity perceptual image and video compression with a latent conditioned residual denoising diffusion model", "summary": "Denoising diffusion models achieved impressive results on several image\ngeneration tasks often outperforming GAN based models. Recently, the generative\ncapabilities of diffusion models have been employed for perceptual image\ncompression, such as in CDC. A major drawback of these diffusion-based methods\nis that, while producing impressive perceptual quality images they are dropping\nin fidelity/increasing the distortion to the original uncompressed images when\ncompared with other traditional or learned image compression schemes aiming for\nfidelity. In this paper, we propose a hybrid compression scheme optimized for\nperceptual quality, extending the approach of the CDC model with a decoder\nnetwork in order to reduce the impact on distortion metrics such as PSNR. After\nusing the decoder network to generate an initial image, optimized for\ndistortion, the latent conditioned diffusion model refines the reconstruction\nfor perceptual quality by predicting the residual. On standard benchmarks, we\nachieve up to +2dB PSNR fidelity improvements while maintaining comparable\nLPIPS and FID perceptual scores when compared with CDC. Additionally, the\napproach is easily extensible to video compression, where we achieve similar\nresults.", "authors": ["Jonas Brenig", "Radu Timofte"], "published_date": "2025-05-19", "title_zh": "使用潛在條件殘差去噪擴散模型實現更高保真度的感知圖像與影片壓縮", "summary_zh": "本研究提出一種混合壓縮方法，旨在提升感知圖像與影片壓縮的品質和保真度。利用解碼器網路產生初步重建圖像，優化失真度，然後使用潛在條件擴散模型預測殘差，進一步提升感知品質。實驗結果顯示，在保持感知品質指標（如LPIPS和FID）不變的前提下，PSNR可提升高達2dB，且該方法能輕鬆擴展至影片壓縮，並獲得相似的成果。", "audio": "audios/2505.13152v1.mp3", "timestamp": "2025-05-20T14:18:48.810505"}
{"query": "AI", "id": "2505.13324v1", "url": "http://arxiv.org/abs/2505.13324v1", "title": "From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable AI", "summary": "Counterfactuals play a pivotal role in the two distinct data science fields\nof causal inference (CI) and explainable artificial intelligence (XAI). While\nthe core idea behind counterfactuals remains the same in both fields--the\nexamination of what would have happened under different circumstances--there\nare key differences in how they are used and interpreted. We introduce a formal\ndefinition that encompasses the multi-faceted concept of the counterfactual in\nCI and XAI. We then discuss how counterfactuals are used, evaluated, generated,\nand operationalized in CI vs. XAI, highlighting conceptual and practical\ndifferences. By comparing and contrasting the two, we hope to identify\nopportunities for cross-fertilization across CI and XAI.", "authors": ["Galit Shmueli", "David Martens", "Jaewon Yoo", "Travis Greene"], "published_date": "2025-05-19", "title_zh": "從「如果...會怎樣」到洞見：因果推論與可解釋人工智慧中的反事實分析", "summary_zh": "反事實分析在因果推論和可解釋人工智慧這兩個領域都扮演關鍵角色。雖然核心概念都是探討在不同情況下會發生什麼，但它們的使用和解釋方式存在差異。這篇論文定義了一個涵蓋因果推論和可解釋人工智慧中反事實分析的多面向概念，並比較了它們在應用、評估、生成和實用化方面的不同，旨在促進兩個領域的互相借鑒。", "audio": "audios/2505.13324v1.mp3", "timestamp": "2025-05-20T15:20:23.282037"}
{"query": "Foundation Model", "id": "2505.12890v1", "url": "http://arxiv.org/abs/2505.12890v1", "title": "ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling", "summary": "The real-world complexity of surgeries necessitates surgeons to have deep and\nholistic comprehension to ensure precision, safety, and effective\ninterventions. Computational systems are required to have a similar level of\ncomprehension within the operating room. Prior works, limited to single-task\nefforts like phase recognition or scene graph generation, lack scope and\ngeneralizability. In this work, we introduce ORQA, a novel OR question\nanswering benchmark and foundational multimodal model to advance OR\nintelligence. By unifying all four public OR datasets into a comprehensive\nbenchmark, we enable our approach to concurrently address a diverse range of OR\nchallenges. The proposed multimodal large language model fuses diverse OR\nsignals such as visual, auditory, and structured data, for a holistic modeling\nof the OR. Finally, we propose a novel, progressive knowledge distillation\nparadigm, to generate a family of models optimized for different speed and\nmemory requirements. We show the strong performance of ORQA on our proposed\nbenchmark, and its zero-shot generalization, paving the way for scalable,\nunified OR modeling and significantly advancing multimodal surgical\nintelligence. We will release our code and data upon acceptance.", "authors": ["Ege Özsoy", "Chantal Pellegrini", "David Bani-Harouni", "Kun Yuan", "Matthias Keicher", "Nassir Navab"], "published_date": "2025-05-19", "title_zh": "ORQA：整體手術室建模的基準和基礎模型", "summary_zh": "為了讓電腦系統也能理解手術室的複雜性，如同外科醫生一般，我們推出了ORQA，一個全新的手術室問答基準和多模態基礎模型。ORQA整合了現有公開的手術室數據集，可以同時處理多樣化的手術室挑戰。我們提出的多模態大型語言模型結合了視覺、聽覺和結構化數據等各種手術室訊號，以實現對手術室的整體建模。此外，我們還提出了一種漸進式知識蒸餾方法，可以生成一系列針對不同速度和記憶體需求的模型。實驗結果顯示，ORQA在基準測試中表現出色，並具有零樣本泛化能力，為可擴展、統一的手術室建模奠定了基礎，並顯著推進了多模態手術智慧。", "audio": "audios/2505.12890v1.mp3", "timestamp": "2025-05-20T15:20:37.753880"}
{"query": "Diffusion Model", "id": "2505.13138v1", "url": "http://arxiv.org/abs/2505.13138v1", "title": "Neurosymbolic Diffusion Models", "summary": "Neurosymbolic (NeSy) predictors combine neural perception with symbolic\nreasoning to solve tasks like visual reasoning. However, standard NeSy\npredictors assume conditional independence between the symbols they extract,\nthus limiting their ability to model interactions and uncertainty - often\nleading to overconfident predictions and poor out-of-distribution\ngeneralisation. To overcome the limitations of the independence assumption, we\nintroduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy\npredictors that use discrete diffusion to model dependencies between symbols.\nOur approach reuses the independence assumption from NeSy predictors at each\nstep of the diffusion process, enabling scalable learning while capturing\nsymbol dependencies and uncertainty quantification. Across both synthetic and\nreal-world benchmarks - including high-dimensional visual path planning and\nrule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among\nNeSy predictors and demonstrate strong calibration.", "authors": ["Emile van Krieken", "Pasquale Minervini", "Edoardo Ponti", "Antonio Vergari"], "published_date": "2025-05-19", "title_zh": "神經符號擴散模型", "summary_zh": "傳統神經符號模型假設符號之間彼此獨立，導致無法有效模擬互動和不確定性。為了解決這個問題，我們提出了神經符號擴散模型（NeSyDMs），利用離散擴散過程來模擬符號之間的依賴關係。NeSyDMs在擴散的每一步驟中重用獨立性假設，實現可擴展的學習，同時捕捉符號之間的依賴關係和量化不確定性。在合成和真實世界的基準測試中，包括高維視覺路徑規劃和基於規則的自動駕駛，NeSyDMs在神經符號預測器中實現了最先進的準確性，並展現出強大的校準能力。", "audio": "audios/2505.13138v1.mp3", "timestamp": "2025-05-20T15:20:46.326880"}
{"query": "AI", "id": "2505.13315v1", "url": "http://arxiv.org/abs/2505.13315v1", "title": "KHRONOS: a Kernel-Based Neural Architecture for Rapid, Resource-Efficient Scientific Computation", "summary": "Contemporary models of high dimensional physical systems are constrained by\nthe curse of dimensionality and a reliance on dense data. We introduce KHRONOS\n(Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an\nAI framework for model based, model free and model inversion tasks. KHRONOS\nconstructs continuously differentiable target fields with a hierarchical\ncomposition of per-dimension kernel expansions, which are tensorized into modes\nand then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation\nbenchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L2 square\nerrors of 5e-4 down to 6e-10. This represents a 100 time gain over Kolmogorov\nArnold Networks (which itself reports a 100 times improvement on MLPs/PINNs\nwith 100 times fewer parameters) when controlling for the number of parameters.\nThis also represents a 1e4 times improvement in L2 square error compared to\nstandard linear FEM at comparable DoFs. Inference complexity is dominated by\ninner products, yielding sub-millisecond full-field predictions that scale to\nan arbitrary resolution. For inverse problems, KHRONOS facilitates rapid,\niterative level set recovery in only a few forward evaluations, with\nsub-microsecond per sample latency. KHRONOS scalability, expressivity, and\ninterpretability open new avenues in constrained edge computing, online\ncontrol, computer vision, and beyond.", "authors": ["Reza T. Batley", "Sourav Saha"], "published_date": "2025-05-19", "title_zh": "KHRONOS：一種基於核心的類神經網路架構，用於快速、資源高效的科學計算", "summary_zh": "KHRONOS (核心擴展層級化簡階、神經優化代理模型) 是一個 AI 框架，能處理基於模型、無模型和模型反演的任務。它利用分層式的單維核心擴展構建連續可微的目標場，並透過張量化和疊加來提升效率。在 Poisson 方程的基準測試中，KHRONOS 展現了極高的準確性和速度，在參數數量相當的情況下，相比其他方法有顯著優勢。此外，KHRONOS 還能快速解決反問題，具有良好的延展性和可解釋性，未來有望應用於邊緣計算、線上控制、電腦視覺等領域。", "audio": "audios/2505.13315v1.mp3", "timestamp": "2025-05-20T16:23:27.181018"}
{"query": "Foundation Model", "id": "2505.12738v1", "url": "http://arxiv.org/abs/2505.12738v1", "title": "EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting", "summary": "Advanced epidemic forecasting is critical for enabling precision containment\nstrategies, highlighting its strategic importance for public health security.\nWhile recent advances in Large Language Models (LLMs) have demonstrated\neffectiveness as foundation models for domain-specific tasks, their potential\nfor epidemic forecasting remains largely unexplored. In this paper, we\nintroduce EpiLLM, a novel LLM-based framework tailored for spatio-temporal\nepidemic forecasting. Considering the key factors in real-world epidemic\ntransmission: infection cases and human mobility, we introduce a dual-branch\narchitecture to achieve fine-grained token-level alignment between such complex\nepidemic patterns and language tokens for LLM adaptation. To unleash the\nmulti-step forecasting and generalization potential of LLM architectures, we\npropose an autoregressive modeling paradigm that reformulates the epidemic\nforecasting task into next-token prediction. To further enhance LLM perception\nof epidemics, we introduce spatio-temporal prompt learning techniques, which\nstrengthen forecasting capabilities from a data-driven perspective. Extensive\nexperiments show that EpiLLM significantly outperforms existing baselines on\nreal-world COVID-19 datasets and exhibits scaling behavior characteristic of\nLLMs.", "authors": ["Chenghua Gong", "Rui Sun", "Yuhao Zheng", "Juyuan Zhang", "Tianjun Gu", "Liming Pan", "Linyuan Lv"], "published_date": "2025-05-19", "title_zh": "EpiLLM：釋放大型語言模型在流行病預測中的潛力", "summary_zh": "EpiLLM 是一種基於大型語言模型的新框架，專為時空流行病預測量身定制。它考慮了感染病例和人口流動等關鍵因素，並利用自迴歸建模將預測任務轉化為下一代詞預測。此外，還引入了時空提示學習技術以加強模型對流行病的理解。實驗結果表明，EpiLLM 在 COVID-19 數據集上顯著優於現有方法，並展現了大型語言模型的擴展特性。", "audio": "audios/2505.12738v1.mp3", "timestamp": "2025-05-20T16:23:32.743890"}
{"query": "Diffusion Model", "id": "2505.13131v1", "url": "http://arxiv.org/abs/2505.13131v1", "title": "Constraint-Aware Diffusion Guidance for Robotics: Real-Time Obstacle Avoidance for Autonomous Racing", "summary": "Diffusion models hold great potential in robotics due to their ability to\ncapture complex, high-dimensional data distributions. However, their lack of\nconstraint-awareness limits their deployment in safety-critical applications.\nWe propose Constraint-Aware Diffusion Guidance (CoDiG), a data-efficient and\ngeneral-purpose framework that integrates barrier functions into the denoising\nprocess, guiding diffusion sampling toward constraint-satisfying outputs. CoDiG\nenables constraint satisfaction even with limited training data and generalizes\nacross tasks. We evaluate our framework in the challenging setting of miniature\nautonomous racing, where real-time obstacle avoidance is essential. Real-world\nexperiments show that CoDiG generates safe outputs efficiently under dynamic\nconditions, highlighting its potential for broader robotic applications. A\ndemonstration video is available at https://youtu.be/KNYsTdtdxOU.", "authors": ["Hao Ma", "Sabrina Bodmer", "Andrea Carron", "Melanie Zeilinger", "Michael Muehlebach"], "published_date": "2025-05-19", "title_zh": "機器人約束感知擴散引導：自主競速的即時避障", "summary_zh": "擴散模型在機器人領域潛力巨大，但缺乏約束感知能力。我們提出「約束感知擴散引導 (CoDiG)」，它將障礙函數整合到去噪過程中，引导擴散採樣生成滿足約束的輸出。CoDiG能在訓練數據有限的情況下满足约束，並且具有泛化能力。我們在微型自主競速中驗證了該框架，CoDiG能高效地產生安全輸出，展現其在更廣泛機器人應用中的潛力。", "audio": "audios/2505.13131v1.mp3", "timestamp": "2025-05-20T16:23:38.404568"}
{"query": "AI", "id": "2505.13302v1", "url": "http://arxiv.org/abs/2505.13302v1", "title": "I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models", "summary": "Large language models are increasingly integrated into news recommendation\nsystems, raising concerns about their role in spreading misinformation. In\nhumans, visual content is known to boost credibility and shareability of\ninformation, yet its effect on vision-language models (VLMs) remains unclear.\nWe present the first study examining how images influence VLMs' propensity to\nreshare news content, whether this effect varies across model families, and how\npersona conditioning and content attributes modulate this behavior. To support\nthis analysis, we introduce two methodological contributions: a\njailbreaking-inspired prompting strategy that elicits resharing decisions from\nVLMs while simulating users with antisocial traits and political alignments;\nand a multimodal dataset of fact-checked political news from PolitiFact, paired\nwith corresponding images and ground-truth veracity labels. Experiments across\nmodel families reveal that image presence increases resharing rates by 4.8% for\ntrue news and 15.0% for false news. Persona conditioning further modulates this\neffect: Dark Triad traits amplify resharing of false news, whereas\nRepublican-aligned profiles exhibit reduced veracity sensitivity. Of all the\ntested models, only Claude-3-Haiku demonstrates robustness to visual\nmisinformation. These findings highlight emerging risks in multimodal model\nbehavior and motivate the development of tailored evaluation frameworks and\nmitigation strategies for personalized AI systems. Code and dataset are\navailable at: https://github.com/3lis/misinfo_vlm", "authors": ["Alice Plebe", "Timothy Douglas", "Diana Riazi", "R. Maria del Rio-Chanona"], "published_date": "2025-05-19", "title_zh": "眼見為憑：圖像會增加視覺語言模型中錯誤資訊的傳播", "summary_zh": "大型語言模型越來越多地被整合到新聞推薦系統中，引發了人們對其在傳播錯誤資訊方面所扮演角色的擔憂。研究發現，圖像會顯著增加視覺語言模型轉發新聞的意願，尤其是假新聞，轉發率提高了15%。特定人格特徵，例如「黑暗三性格」，以及政治立場，也會影響模型的轉發行為。只有Claude-3-Haiku模型對視覺錯誤資訊表現出較強的抵抗力。這項研究揭示了多模態模型行為中潛在的風險，並強調需要針對個性化AI系統開發評估框架和緩解策略。", "audio": "audios/2505.13302v1.mp3", "timestamp": "2025-05-20T17:16:15.208753"}
{"query": "Foundation Model", "id": "2505.12684v1", "url": "http://arxiv.org/abs/2505.12684v1", "title": "Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement", "summary": "Recent advances in graph machine learning have shifted to data-centric\nparadigms, driven by two emerging fields: (1) Federated graph learning (FGL)\nenables multi-client collaboration but faces challenges from data and task\nheterogeneity, limiting its practicality; (2) Graph foundation models (GFM)\noffer strong domain generalization but are usually trained on single machines,\nmissing out on cross-silo data and resources.\n  These paradigms are complementary, and their integration brings notable\nbenefits. Motivated by this, we propose FedGFM, a novel decentralized GFM\ntraining paradigm. However, a key challenge is knowledge entanglement, where\nmulti-domain knowledge merges into indistinguishable representations, hindering\ndownstream adaptation.\n  To address this, we present FedGFM+, an enhanced framework with two core\nmodules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based\ndomain-aware initialization strategy. Before pre-training, each client encodes\nits local graph into domain-specific prototypes that serve as semantic anchors.\nSynthetic embeddings around these anchors initialize the global model. We\ntheoretically prove these prototypes are distinguishable across domains,\nproviding a strong inductive bias to disentangle domain-specific knowledge. (2)\nAdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a\nlightweight graph prompt capturing domain semantics during pre-training. During\nfine-tuning, prompts from all clients form a pool from which the GFM selects\nrelevant prompts to augment target graph attributes, improving downstream\nadaptation.\n  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and\ntasks, outperforming 20 baselines from supervised learning, FGL, and federated\nGFM variants.", "authors": ["Yinlin Zhu", "Xunkai Li", "Jishuo Jia", "Miao Hu", "Di Wu", "Meikang Qiu"], "published_date": "2025-05-19", "title_zh": "邁向高效能聯邦圖基礎模型：透過降低知識糾纏", "summary_zh": "現今圖機器學習趨勢轉向以資料為中心，聯邦圖學習(FGL)和圖基礎模型(GFM)是兩個重要領域。FGL雖能促進多方協作，但受限於資料和任務異質性；GFM雖具備強大的領域泛化能力，卻常在單機上訓練，錯失跨機構的資料和資源。因此，我們提出FedGFM，一種去中心化的GFM訓練方法。然而，知識糾纏是主要挑戰，它會讓多領域知識混合成無法區分的表示，阻礙下游適應。為了解決此問題，我們提出FedGFM+，透過AncDAI（錨點式領域感知初始化）和AdaDPP（自適應領域敏感提示池）兩個核心模組來降低知識糾纏。AncDAI在預訓練前，將本地圖編碼成領域特定的原型作為語義錨點，並以此初始化全域模型，提供領域知識解耦的強烈歸納偏置。AdaDPP讓每個客戶端學習捕捉領域語義的輕量級圖提示，在微調時，將所有客戶端的提示形成提示池，GFM從中選擇相關提示來增強目標圖屬性，提升下游適應性。實驗證明，FedGFM+在多個領域和任務的八個基準測試中，優於20個基線模型。", "audio": "audios/2505.12684v1.mp3", "timestamp": "2025-05-20T17:16:25.053705"}
{"query": "Diffusion Model", "id": "2505.13091v1", "url": "http://arxiv.org/abs/2505.13091v1", "title": "Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction", "summary": "Diffusion models have made breakthroughs in 3D generation tasks. Current 3D\ndiffusion models focus on reconstructing target shape from images or a set of\npartial observations. While excelling in global context understanding, they\nstruggle to capture the local details of complex shapes and limited to the\nocclusion and lighting conditions. To overcome these limitations, we utilize\ntactile images to capture the local 3D information and propose a Touch2Shape\nmodel, which leverages a touch-conditioned diffusion model to explore and\nreconstruct the target shape from touch. For shape reconstruction, we have\ndeveloped a touch embedding module to condition the diffusion model in creating\na compact representation and a touch shape fusion module to refine the\nreconstructed shape. For shape exploration, we combine the diffusion model with\nreinforcement learning to train a policy. This involves using the generated\nlatent vector from the diffusion model to guide the touch exploration policy\ntraining through a novel reward design. Experiments validate the reconstruction\nquality thorough both qualitatively and quantitative analysis, and our touch\nexploration policy further boosts reconstruction performance.", "authors": ["Yuanbo Wang", "Zhaoxuan Zhang", "Jiajin Qiu", "Dilong Sun", "Zhengyu Meng", "Xiaopeng Wei", "Xin Yang"], "published_date": "2025-05-19", "title_zh": "Touch2Shape：觸摸條件下的3D擴散模型，用於形狀探索與重建", "summary_zh": "3D擴散模型在形狀生成上表現亮眼，但對複雜形狀的局部細節捕捉能力有限。本論文提出 Touch2Shape 模型，利用觸覺影像捕捉局部3D資訊，並結合觸摸條件的擴散模型來探索和重建目標形狀。模型包含觸摸嵌入模組，產生精簡表示，以及觸摸形狀融合模組，優化重建效果。此外，結合擴散模型與強化學習，訓練觸摸探索策略，進一步提升重建效能。實驗證明此方法能有效重建形狀，並且觸摸探索策略可以改善重建結果。", "audio": "audios/2505.13091v1.mp3", "timestamp": "2025-05-20T17:16:30.788447"}
{"query": "AI", "id": "2505.13292v1", "url": "http://arxiv.org/abs/2505.13292v1", "title": "Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms of AI Systems by Integrating Federated Learning and LLMs", "summary": "In the age of cloud computing, data privacy protection has become a major\nchallenge, especially when sharing sensitive data across cloud environments.\nHowever, how to optimize collaboration across cloud environments remains an\nunresolved problem. In this paper, we combine federated learning with\nlarge-scale language models to optimize the collaborative mechanism of AI\nsystems. Based on the existing federated learning framework, we introduce a\ncross-cloud architecture in which federated learning works by aggregating model\nupdates from decentralized nodes without exposing the original data. At the\nsame time, combined with large-scale language models, its powerful context and\nsemantic understanding capabilities are used to improve model training\nefficiency and decision-making ability. We've further innovated by introducing\na secure communication layer to ensure the privacy and integrity of model\nupdates and training data. The model enables continuous model adaptation and\nfine-tuning across different cloud environments while protecting sensitive\ndata. Experimental results show that the proposed method is significantly\nbetter than the traditional federated learning model in terms of accuracy,\nconvergence speed and data privacy protection.", "authors": ["Huaiying Luo", "Cheng Ji"], "published_date": "2025-05-19", "title_zh": "跨雲端資料隱私保護：整合聯邦學習與大型語言模型優化AI系統的協作機制", "summary_zh": "本研究探討在雲端運算時代，跨雲端共享敏感資料時的資料隱私保護挑戰。我們結合聯邦學習和大型語言模型，優化AI系統的協作機制。透過跨雲端架構，聯邦學習可在不洩露原始資料的情況下匯總模型更新。同時，利用大型語言模型的強大語義理解能力，提升模型訓練效率和決策能力。此外，引入安全通訊層確保模型更新和訓練資料的隱私和完整性。實驗結果顯示，相較於傳統聯邦學習模型，本方法在準確度、收斂速度和資料隱私保護方面有顯著提升。", "audio": "audios/2505.13292v1.mp3", "timestamp": "2025-05-20T18:26:43.658852"}
{"query": "Foundation Model", "id": "2505.12638v1", "url": "http://arxiv.org/abs/2505.12638v1", "title": "ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data", "summary": "The advent of single-cell Assay for Transposase-Accessible Chromatin using\nsequencing (scATAC-seq) offers an innovative perspective for deciphering\nregulatory mechanisms by assembling a vast repository of single-cell chromatin\naccessibility data. While foundation models have achieved significant success\nin single-cell transcriptomics, there is currently no foundation model for\nscATAC-seq that supports zero-shot high-quality cell identification and\ncomprehensive multi-omics analysis simultaneously. Key challenges lie in the\nhigh dimensionality and sparsity of scATAC-seq data, as well as the lack of a\nstandardized schema for representing open chromatin regions (OCRs). Here, we\npresent \\textbf{ChromFound}, a foundation model tailored for scATAC-seq.\nChromFound utilizes a hybrid architecture and genome-aware tokenization to\neffectively capture genome-wide long contexts and regulatory signals from\ndynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues\nand 6 disease conditions, ChromFound demonstrates broad applicability across 6\ndiverse tasks. Notably, it achieves robust zero-shot performance in generating\nuniversal cell representations and exhibits excellent transferability in cell\ntype annotation and cross-omics prediction. By uncovering enhancer-gene links\nundetected by existing computational methods, ChromFound offers a promising\nframework for understanding disease risk variants in the noncoding genome.", "authors": ["Yifeng Jiao", "Yuchen Liu", "Yu Zhang", "Xin Guo", "Yushuai Wu", "Chen Jiang", "Jiyang Li", "Hongwei Zhang", "Limei Han", "Xin Gao", "Yuan Qi", "Yuan Cheng"], "published_date": "2025-05-19", "title_zh": "ChromFound：邁向單細胞染色質可及性數據的通用基礎模型", "summary_zh": "隨著單細胞ATAC-seq技術的發展，我們得以以前所未有的視角解析調控機制。然而，雖然基礎模型在單細胞轉錄組學上取得了巨大成功，但在單細胞染色質可及性數據方面，卻缺乏一個能同時支持零樣本高質量細胞識別和全面多組學分析的基礎模型。為了解決這個問題，我們開發了ChromFound，一個專為單細胞ATAC-seq設計的基礎模型。它通過混合架構和基因組感知的Tokenization技術，有效地捕捉了全基因組的長程上下文和來自動態染色質環境的調控信號。ChromFound預訓練了來自30個組織和6種疾病條件的197萬個細胞，展示了廣泛的適用性，並在多項任務中表現出色，特別是在零樣本細胞表示生成和跨組學預測方面。ChromFound還有望幫助我們理解非編碼基因組中的疾病風險變異。", "audio": "audios/2505.12638v1.mp3", "timestamp": "2025-05-20T18:26:52.420514"}
{"query": "Diffusion Model", "id": "2505.13023v1", "url": "http://arxiv.org/abs/2505.13023v1", "title": "Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions", "summary": "As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models.", "authors": ["Yimao Guo", "Zuomin Qu", "Wei Lu", "Xiangyang Luo"], "published_date": "2025-05-19", "title_zh": "反填補：針對未知條件下基於惡意擴散模型的影像填補器的預防性防禦", "summary_zh": "基於擴散模型的惡意影像篡改日益普遍，針對此問題，我們提出「反填補」這種預防性防禦機制。它透過多層級特徵提取、多尺度語義保留的資料擴增，以及基於選擇的分布偏差優化策略，在未知條件下也能有效地保護影像，抵禦惡意影像填補，並在多項實驗中證明了其效能和魯棒性。", "audio": "audios/2505.13023v1.mp3", "timestamp": "2025-05-20T18:27:07.718187"}
{"query": "AI", "id": "2505.13259v1", "url": "http://arxiv.org/abs/2505.13259v1", "title": "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery", "summary": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.", "authors": ["Tianshi Zheng", "Zheye Deng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Zihao Wang", "Yangqiu Song"], "published_date": "2025-05-19", "title_zh": "從自動化到自主化：大型語言模型在科學發現中的綜述", "summary_zh": "大型語言模型正在徹底改變科學研究。它們不再只是自動化工具，而是逐漸變成具有自主性的智能體，重塑研究流程和人機協作模式。本綜述系統性地探討了這個新興領域，重點關注大型語言模型在科學領域中不斷變化的角色和日益提升的能力。我們從科學方法出發，提出了工具、分析師和科學家三個層級的分類，來描述模型自主性的演進。此外，我們也指出了機器人自動化、自我改進和倫理治理等關鍵挑戰和未來研究方向。總之，本綜述提供了一個概念框架和策略遠見，旨在引導和塑造AI驅動的科學發現的未來，促進快速創新和負責任的發展。", "audio": "audios/2505.13259v1.mp3", "timestamp": "2025-05-20T19:14:36.941246"}
{"query": "Foundation Model", "id": "2505.12583v1", "url": "http://arxiv.org/abs/2505.12583v1", "title": "A Comprehensive Survey on Physical Risk Control in the Era of Foundation Model-enabled Robotics", "summary": "Recent Foundation Model-enabled robotics (FMRs) display greatly improved\ngeneral-purpose skills, enabling more adaptable automation than conventional\nrobotics. Their ability to handle diverse tasks thus creates new opportunities\nto replace human labor. However, unlike general foundation models, FMRs\ninteract with the physical world, where their actions directly affect the\nsafety of humans and surrounding objects, requiring careful deployment and\ncontrol. Based on this proposition, our survey comprehensively summarizes robot\ncontrol approaches to mitigate physical risks by covering all the lifespan of\nFMRs ranging from pre-deployment to post-accident stage. Specifically, we\nbroadly divide the timeline into the following three phases: (1) pre-deployment\nphase, (2) pre-incident phase, and (3) post-incident phase. Throughout this\nsurvey, we find that there is much room to study (i) pre-incident risk\nmitigation strategies, (ii) research that assumes physical interaction with\nhumans, and (iii) essential issues of foundation models themselves. We hope\nthat this survey will be a milestone in providing a high-resolution analysis of\nthe physical risks of FMRs and their control, contributing to the realization\nof a good human-robot relationship.", "authors": ["Takeshi Kojima", "Yaonan Zhu", "Yusuke Iwasawa", "Toshinori Kitamura", "Gang Yan", "Shu Morikuni", "Ryosuke Takanami", "Alfredo Solano", "Tatsuya Matsushima", "Akiko Murakami", "Yutaka Matsuo"], "published_date": "2025-05-19", "title_zh": "基於基礎模型的機器人時代物理風險控制全面綜述", "summary_zh": "近年來，基於基礎模型的機器人展現出更強的通用能力，使得機器人能更靈活地自動化。然而，與一般基礎模型不同，它們會與物理世界互動，其行為直接影響人類和周遭物體的安全，需要仔細部署和控制。本綜述全面總結了機器人控制方法，以減輕物理風險，涵蓋從部署前到事故後的整個生命週期，並將時間線分為部署前、事故前和事故後三個階段。研究發現，事故前的風險緩解策略、假設與人類進行物理互動的研究以及基礎模型本身的基本問題，都還有很大的研究空間。希望本綜述能為分析基於基礎模型的機器人的物理風險及其控制提供高解析度的分析，從而有助於實現良好的人機關係。", "audio": "audios/2505.12583v1.mp3", "timestamp": "2025-05-20T19:14:49.584037"}
{"query": "Diffusion Model", "id": "2505.12935v1", "url": "http://arxiv.org/abs/2505.12935v1", "title": "LatentINDIGO: An INN-Guided Latent Diffusion Algorithm for Image Restoration", "summary": "There is a growing interest in the use of latent diffusion models (LDMs) for\nimage restoration (IR) tasks due to their ability to model effectively the\ndistribution of natural images. While significant progress has been made, there\nare still key challenges that need to be addressed. First, many approaches\ndepend on a predefined degradation operator, making them ill-suited for complex\nor unknown degradations that deviate from standard analytical models. Second,\nmany methods struggle to provide a stable guidance in the latent space and\nfinally most methods convert latent representations back to the pixel domain\nfor guidance at every sampling iteration, which significantly increases\ncomputational and memory overhead. To overcome these limitations, we introduce\na wavelet-inspired invertible neural network (INN) that simulates degradations\nthrough a forward transform and reconstructs lost details via the inverse\ntransform. We further integrate this design into a latent diffusion pipeline\nthrough two proposed approaches: LatentINDIGO-PixelINN, which operates in the\npixel domain, and LatentINDIGO-LatentINN, which stays fully in the latent space\nto reduce complexity. Both approaches alternate between updating intermediate\nlatent variables under the guidance of our INN and refining the INN forward\nmodel to handle unknown degradations. In addition, a regularization step\npreserves the proximity of latent variables to the natural image manifold.\nExperiments demonstrate that our algorithm achieves state-of-the-art\nperformance on synthetic and real-world low-quality images, and can be readily\nadapted to arbitrary output sizes.", "authors": ["Di You", "Daniel Siromani", "Pier Luigi Dragotti"], "published_date": "2025-05-19", "title_zh": "潛在INDIGO：一種用於影像修復的INN引導潛在擴散演算法", "summary_zh": "潛在擴散模型在影像修復領域越來越受歡迎，但現有方法在處理複雜或未知降質、提供穩定潛在空間引導，以及計算效率等方面仍存在挑戰。本文提出一種名為LatentINDIGO的演算法，它使用波小波啟發的可逆神經網路（INN）來模擬降質過程，並通過逆變換重建丟失的細節。該演算法有兩個版本：PixelINN版本在像素域操作，LatentINN版本則完全在潛在空間中操作，以減少複雜度。這兩種方法交替更新潛在變量和精煉INN模型，並通過正則化步驟確保潛在變量接近自然圖像流形。實驗結果表明，該演算法在合成和真實低質量圖像上均取得了最先進的性能，並且可以輕鬆適應任意輸出尺寸。", "audio": "audios/2505.12935v1.mp3", "timestamp": "2025-05-20T19:14:58.408447"}
{"query": "AI", "id": "2505.13246v1", "url": "http://arxiv.org/abs/2505.13246v1", "title": "Agentic Publications: An LLM-Driven Framework for Interactive Scientific Publishing, Supplementing Traditional Papers with AI-Powered Knowledge Systems", "summary": "The exponential growth of scientific literature presents significant\nchallenges for researchers navigating the complex knowledge landscape. We\npropose \"Agentic Publications\", a novel LLM-driven framework complementing\ntraditional publishing by transforming papers into interactive knowledge\nsystems. Our architecture integrates structured data with unstructured content\nthrough retrieval-augmented generation and multi-agent verification. The\nframework offers interfaces for both humans and machines, combining narrative\nexplanations with machine-readable outputs while addressing ethical\nconsiderations through automated validation and transparent governance. Key\nfeatures include continuous knowledge updates, automatic integration of new\nfindings, and customizable detail levels. Our proof-of-concept demonstrates\nmultilingual interaction, API accessibility, and structured knowledge\nrepresentation through vector databases, knowledge graphs, and verification\nagents. This approach enhances scientific communication across disciplines,\nimproving efficiency and collaboration while preserving traditional publishing\npathways, particularly valuable for interdisciplinary fields where knowledge\nintegration remains challenging.", "authors": ["Roberto Pugliese", "George Kourousias", "Francesco Venier", "Grazia Garlatti Costa"], "published_date": "2025-05-19", "title_zh": "具代理能力的出版品：一個由大型語言模型驅動的互動式科學出版框架，透過 AI 驅動的知識系統來補充傳統論文", "summary_zh": "科學文獻爆炸性成長，研究人員難以掌握。本研究提出「具代理能力的出版品」框架，利用大型語言模型將傳統論文轉化為互動式知識系統，結合結構化和非結構化數據，並透過多重代理驗證確保準確性。此框架提供人機介面，具備知識持續更新、自動整合新發現等功能。此方法透過提升跨領域的科學交流效率和協作，並保留傳統出版途徑，尤其對於知識整合困難的跨領域研究而言，更具價值。", "audio": "audios/2505.13246v1.mp3", "timestamp": "2025-05-20T20:20:44.106068"}
{"query": "Foundation Model", "id": "2505.12534v1", "url": "http://arxiv.org/abs/2505.12534v1", "title": "ChemPile: A 250GB Diverse and Curated Dataset for Chemical Foundation Models", "summary": "Foundation models have shown remarkable success across scientific domains,\nyet their impact in chemistry remains limited due to the absence of diverse,\nlarge-scale, high-quality datasets that reflect the field's multifaceted\nnature. We present the ChemPile, an open dataset containing over 75 billion\ntokens of curated chemical data, specifically built for training and evaluating\ngeneral-purpose models in the chemical sciences. The dataset mirrors the human\nlearning journey through chemistry -- from educational foundations to\nspecialized expertise -- spanning multiple modalities and content types\nincluding structured data in diverse chemical representations (SMILES, SELFIES,\nIUPAC names, InChI, molecular renderings), scientific and educational text,\nexecutable code, and chemical images. ChemPile integrates foundational\nknowledge (textbooks, lecture notes), specialized expertise (scientific\narticles and language-interfaced data), visual understanding (molecular\nstructures, diagrams), and advanced reasoning (problem-solving traces and code)\n-- mirroring how human chemists develop expertise through diverse learning\nmaterials and experiences. Constructed through hundreds of hours of expert\ncuration, the ChemPile captures both foundational concepts and domain-specific\ncomplexity. We provide standardized training, validation, and test splits,\nenabling robust benchmarking. ChemPile is openly released via HuggingFace with\na consistent API, permissive license, and detailed documentation. We hope the\nChemPile will serve as a catalyst for chemical AI, enabling the development of\nthe next generation of chemical foundation models.", "authors": ["Adrian Mirza", "Nawaf Alampara", "Martiño Ríos-García", "Mohamed Abdelalim", "Jack Butler", "Bethany Connolly", "Tunca Dogan", "Marianna Nezhurina", "Bünyamin Şen", "Santosh Tirunagari", "Mark Worrall", "Adamo Young", "Philippe Schwaller", "Michael Pieler", "Kevin Maik Jablonka"], "published_date": "2025-05-18", "title_zh": "ChemPile：一個250GB的多樣化且精心策劃的化學基礎模型數據集", "summary_zh": "ChemPile是一個開放的250GB化學數據集，包含超過750億個tokens，專為訓練和評估化學領域的通用模型而設計。它涵蓋結構化數據、文本、程式碼和圖像等多種形式，模擬人類學習化學的過程，從基礎知識到專業知識，致力於推動化學人工智慧的發展，並助力新一代化學基礎模型的誕生。", "audio": "audios/2505.12534v1.mp3", "timestamp": "2025-05-20T20:20:49.050176"}
{"query": "Diffusion Model", "id": "2505.12882v1", "url": "http://arxiv.org/abs/2505.12882v1", "title": "PhyDA: Physics-Guided Diffusion Models for Data Assimilation in Atmospheric Systems", "summary": "Data Assimilation (DA) plays a critical role in atmospheric science by\nreconstructing spatially continous estimates of the system state, which serves\nas initial conditions for scientific analysis. While recent advances in\ndiffusion models have shown great potential for DA tasks, most existing\napproaches remain purely data-driven and often overlook the physical laws that\ngovern complex atmospheric dynamics. As a result, they may yield physically\ninconsistent reconstructions that impair downstream applications. To overcome\nthis limitation, we propose PhyDA, a physics-guided diffusion framework\ndesigned to ensure physical coherence in atmospheric data assimilation. PhyDA\nintroduces two key components: (1) a Physically Regularized Diffusion Objective\nthat integrates physical constraints into the training process by penalizing\ndeviations from known physical laws expressed as partial differential\nequations, and (2) a Virtual Reconstruction Encoder that bridges observational\nsparsity for structured latent representations, further enhancing the model's\nability to infer complete and physically coherent states. Experiments on the\nERA5 reanalysis dataset demonstrate that PhyDA achieves superior accuracy and\nbetter physical plausibility compared to state-of-the-art baselines. Our\nresults emphasize the importance of combining generative modeling with\ndomain-specific physical knowledge and show that PhyDA offers a promising\ndirection for improving real-world data assimilation systems.", "authors": ["Hao Wang", "Jindong Han", "Wei Fan", "Weijia Zhang", "Hao Liu"], "published_date": "2025-05-19", "title_zh": "PhyDA：物理引導的擴散模型用於大氣系統中的資料同化", "summary_zh": "PhyDA是一個新型的大氣資料同化框架，它利用物理定律引導擴散模型，確保重建的大氣狀態不僅準確，而且符合物理規律。它透過將物理約束納入訓練目標，並使用編碼器來處理觀測資料的稀疏性，從而優於傳統方法，更適用於實際應用。", "audio": "audios/2505.12882v1.mp3", "timestamp": "2025-05-20T20:20:53.711520"}
{"query": "AI", "id": "2505.14680v1", "url": "http://arxiv.org/abs/2505.14680v1", "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search", "summary": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.", "authors": ["Sunhao Dai", "Wenjie Wang", "Liang Pang", "Jun Xu", "See-Kiong Ng", "Ji-Rong Wen", "Tat-Seng Chua"], "published_date": "2025-05-20", "title_zh": "NExT-Search：重建生成式AI搜尋的使用者回饋生態系統", "summary_zh": "生成式AI搜尋雖然方便，但打破了傳統搜尋仰賴使用者回饋不斷改進的機制。傳統搜尋可以透過使用者點擊、停留時間等精細回饋來優化排序模型。而生成式AI搜尋流程更長，使用者僅對最終答案提供粗略回饋，難以追溯問題源頭，導致系統難以改進。本研究提出NExT-Search，透過「使用者除錯模式」讓使用者介入關鍵步驟，並利用「影子使用者模式」模擬使用者偏好提供AI輔助回饋，重新引入精細的流程級回饋。這些回饋將用於線上即時調整搜尋結果，以及離線微調查詢分解、檢索和生成模型，最終打造能夠持續根據使用者回饋進化的AI搜尋系統。", "audio": "audios/2505.14680v1.mp3", "timestamp": "2025-05-21T03:11:29.471335"}
{"query": "Foundation Model", "id": "2505.14683v1", "url": "http://arxiv.org/abs/2505.14683v1", "title": "Emerging Properties in Unified Multimodal Pretraining", "summary": "Unifying multimodal understanding and generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0source foundational model that natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\non trillions of tokens curated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such as free-form image manipulation,\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/", "authors": ["Chaorui Deng", "Deyao Zhu", "Kunchang Li", "Chenhui Gou", "Feng Li", "Zeyu Wang", "Shu Zhong", "Weihao Yu", "Xiaonan Nie", "Ziang Song", "Guang Shi", "Haoqi Fan"], "published_date": "2025-05-20", "title_zh": "統一多模態預訓練中湧現的特性", "summary_zh": "本研究介紹了開放原始碼的多模態基礎模型 BAGEL，它能同時理解和生成多模態內容。BAGEL 基於大量的文字、圖片、影片和網路數據進行預訓練，展現了在複雜多模態推理方面的能力。在多模態生成和理解方面，BAGEL 的表現明顯優於其他開放原始碼的統一模型，並且具備進階的多模態推理能力，例如自由形式的圖像操作、未來幀預測、3D 操作和世界導航。研究團隊分享了重要的發現、預訓練細節、數據創建協議，並公開了程式碼和模型權重，希望能促進多模態研究的發展。", "audio": "audios/2505.14683v1.mp3", "timestamp": "2025-05-21T03:11:36.102262"}
{"query": "Diffusion Model", "id": "2505.14673v1", "url": "http://arxiv.org/abs/2505.14673v1", "title": "Training-Free Watermarking for Autoregressive Image Generation", "summary": "Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.", "authors": ["Yu Tong", "Zihao Pan", "Shuai Yang", "Kaiyang Zhou"], "published_date": "2025-05-20", "title_zh": "無需訓練的自迴歸圖像生成浮水印", "summary_zh": "一種為自迴歸圖像生成模型設計的，無需訓練的浮水印框架IndexMark。它利用碼本的冗餘特性，將自迴歸生成的索引替換為視覺上相似的索引，以嵌入肉眼難以察覺的浮水印，且不影響圖像質量。透過計算生成圖像中浮水印標記的比例來驗證浮水印，並使用索引編碼器進一步提高精度。實驗表明，IndexMark在圖像質量和驗證準確性方面都表現出色，並且對各種攻擊具有魯棒性，例如裁剪、噪聲、模糊等等。", "audio": "audios/2505.14673v1.mp3", "timestamp": "2025-05-21T03:11:41.526487"}
{"query": "AI", "id": "2505.14677v1", "url": "http://arxiv.org/abs/2505.14677v1", "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning", "summary": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.", "authors": ["Jiaer Xia", "Yuhang Zang", "Peng Gao", "Yixuan Li", "Kaiyang Zhou"], "published_date": "2025-05-20", "title_zh": "Visionary-R1：利用強化學習減輕視覺推理中的捷徑", "summary_zh": "大型語言模型(LLM)利用強化學習在推理方面取得進展。本研究旨在透過強化學習訓練視覺語言模型(VLM)進行圖像推理，無需逐步思考(CoT)的監督。研究發現，直接應用強化學習於VLM可能會因簡單問題而產生捷徑，降低其泛化能力。為解決此問題，本研究提出先生成圖像的詳細描述，再進行推理的Caption-Reason-Answer方法。訓練模型Visionary-R1後，其在多個視覺推理基準測試中超越了GPT-4o等強大的多模態模型。", "audio": "audios/2505.14677v1.mp3", "timestamp": "2025-05-21T04:22:43.916166"}
{"query": "Foundation Model", "id": "2505.14648v1", "url": "http://arxiv.org/abs/2505.14648v1", "title": "Vox-Profile: A Speech Foundation Model Benchmark for Characterizing Diverse Speaker and Speech Traits", "summary": "We introduce Vox-Profile, a comprehensive benchmark to characterize rich\nspeaker and speech traits using speech foundation models. Unlike existing works\nthat focus on a single dimension of speaker traits, Vox-Profile provides\nholistic and multi-dimensional profiles that reflect both static speaker traits\n(e.g., age, sex, accent) and dynamic speech properties (e.g., emotion, speech\nflow). This benchmark is grounded in speech science and linguistics, developed\nwith domain experts to accurately index speaker and speech characteristics. We\nreport benchmark experiments using over 15 publicly available speech datasets\nand several widely used speech foundation models that target various static and\ndynamic speaker and speech properties. In addition to benchmark experiments, we\nshowcase several downstream applications supported by Vox-Profile. First, we\nshow that Vox-Profile can augment existing speech recognition datasets to\nanalyze ASR performance variability. Vox-Profile is also used as a tool to\nevaluate the performance of speech generation systems. Finally, we assess the\nquality of our automated profiles through comparison with human evaluation and\nshow convergent validity. Vox-Profile is publicly available at:\nhttps://github.com/tiantiaf0627/vox-profile-release.", "authors": ["Tiantian Feng", "Jihwan Lee", "Anfeng Xu", "Yoonjeong Lee", "Thanathai Lertpetchpun", "Xuan Shi", "Helin Wang", "Thomas Thebaud", "Laureano Moro-Velazquez", "Dani Byrd", "Najim Dehak", "Shrikanth Narayanan"], "published_date": "2025-05-20", "title_zh": "Vox-Profile: 一個用於表徵多樣化說話者和語音特徵的語音基礎模型基準", "summary_zh": "Vox-Profile 是一個全面的基準測試，旨在利用語音基礎模型來分析說話者和語音的豐富特徵。它不僅關注說話者的年齡、性別、口音等靜態特徵，還包含情緒、語速等動態語音屬性。該基準基於語音科學和語言學，由領域專家開發，能準確地索引說話者和語音的特徵。研究者使用超過15個公開語音數據集和多個主流語音基礎模型進行了基準測試，並展示了Vox-Profile在增強語音識別數據集、評估語音生成系統和驗證自動分析結果等方面的應用。Vox-Profile程式碼已公開。", "audio": "audios/2505.14648v1.mp3", "timestamp": "2025-05-21T04:22:49.028751"}
{"query": "Diffusion Model", "id": "2505.14556v1", "url": "http://arxiv.org/abs/2505.14556v1", "title": "Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI", "summary": "Brain-to-image decoding has been recently propelled by the progress in\ngenerative AI models and the availability of large ultra-high field functional\nMagnetic Resonance Imaging (fMRI). However, current approaches depend on\ncomplicated multi-stage pipelines and preprocessing steps that typically\ncollapse the temporal dimension of brain recordings, thereby limiting\ntime-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural\nActivity Diffusion for Image Reconstruction), a new single-stage diffusion\nmodel designed for reconstructing images from dynamically evolving fMRI\nrecordings. Our approach offers three main contributions. First, Dynadiff\nsimplifies training as compared to existing approaches. Second, our model\noutperforms state-of-the-art models on time-resolved fMRI signals, especially\non high-level semantic image reconstruction metrics, while remaining\ncompetitive on preprocessed fMRI data that collapse time. Third, this approach\nallows a precise characterization of the evolution of image representations in\nbrain activity. Overall, this work lays the foundation for time-resolved\nbrain-to-image decoding.", "authors": ["Marlène Careil", "Yohann Benchetrit", "Jean-Rémi King"], "published_date": "2025-05-20", "title_zh": "Dynadiff: 從持續演進的fMRI數據單階段解碼圖像", "summary_zh": "近年來，腦部到圖像的解碼技術，受益於生成式AI和高場強功能性磁振造影（fMRI）的發展。然而，現有方法依賴複雜的多階段流程，並通常會壓縮腦部記錄的時間維度，限制了時間分辨的腦部解碼器。我們提出Dynadiff，一種新的單階段擴散模型，旨在從動態演進的fMRI記錄中重建圖像。Dynadiff簡化了訓練流程，在時間分辨的fMRI訊號上優於現有模型，特別是在高階語義圖像重建指標上，同時在預處理過的、時間維度已壓縮的fMRI數據上仍具競爭力。此外，它能精確描述腦部活動中圖像表徵的演進過程。這項研究為時間分辨的腦部到圖像解碼奠定了基礎。", "audio": "audios/2505.14556v1.mp3", "timestamp": "2025-05-21T04:22:55.811957"}
{"query": "AI", "id": "2505.14675v1", "url": "http://arxiv.org/abs/2505.14675v1", "title": "Semi-parametric efficient estimation of small genetic effects in large-scale population cohorts", "summary": "Population genetics seeks to quantify DNA variant associations with traits or\ndiseases, as well as interactions among variants and with environmental\nfactors. Computing millions of estimates in large cohorts in which small effect\nsizes are expected, necessitates minimising model-misspecification bias to\ncontrol false discoveries. We present TarGene, a unified statistical workflow\nfor the semi-parametric efficient and double robust estimation of genetic\neffects including k-point interactions among categorical variables in the\npresence of confounding and weak population dependence. k-point interactions,\nor Average Interaction Effects (AIEs), are a direct generalisation of the usual\naverage treatment effect (ATE). We estimate AIEs with cross-validated and/or\nweighted versions of Targeted Minimum Loss-based Estimators (TMLE) and One-Step\nEstimators (OSE). The effect of dependence among data units on variance\nestimates is corrected by using sieve plateau variance estimators based on\ngenetic relatedness across the units. We present extensive realistic\nsimulations to demonstrate power, coverage, and control of type I error. Our\nmotivating application is the targeted estimation of genetic effects on trait,\nincluding two-point and higher-order gene-gene and gene-environment\ninteractions, in large-scale genomic databases such as UK Biobank and All of\nUs. All cross-validated and/or weighted TMLE and OSE for the AIE k-point\ninteraction, as well as ATEs, conditional ATEs and functions thereof, are\nimplemented in the general purpose Julia package TMLE.jl. For high-throughput\napplications in population genomics, we provide the open-source Nextflow\npipeline and software TarGene which integrates seamlessly with modern\nhigh-performance and cloud computing platforms.", "authors": ["Olivier Labayle", "Breeshey Roskams-Hieter", "Joshua Slaughter", "Kelsey Tetley-Campbell", "Mark J. van der Laan", "Chris P. Ponting", "Sjoerd Viktor Beentjes", "Ava Khamseh"], "published_date": "2025-05-20", "title_zh": "大規模群體世代研究中小型遺傳效應的半參數有效估計", "summary_zh": "本研究提出 TarGene，一個統一的統計流程，旨在準確且高效地估計大規模基因體數據庫中小型遺傳效應，即使存在混雜因素和弱群體依賴性。TarGene 使用半參數方法，包括目標最小損失估計器（TMLE）和單步估計器（OSE），並結合交叉驗證和加權策略，來估計基因間和基因與環境間的多點交互作用（平均交互效應 AIE）。透過基於遺傳相關性的篩法平穩方差估計器，修正數據單元間依賴性對方差估計的影響。TarGene 的目標應用是在如 UK Biobank 和 All of Us 等大型數據庫中，針對性地估計遺傳效應，包括高階基因-基因和基因-環境交互作用。所有方法都實現在 Julia 語言的 TMLE.jl 包中，並提供 Nextflow 流程 TarGene 方便在高通量環境下使用。", "audio": "audios/2505.14675v1.mp3", "timestamp": "2025-05-21T06:27:20.327572"}
{"query": "Foundation Model", "id": "2505.14603v1", "url": "http://arxiv.org/abs/2505.14603v1", "title": "Towards a Foundation Model for Communication Systems", "summary": "Artificial Intelligence (AI) has demonstrated unprecedented performance\nacross various domains, and its application to communication systems is an\nactive area of research. While current methods focus on task-specific\nsolutions, the broader trend in AI is shifting toward large general models\ncapable of supporting multiple applications. In this work, we take a step\ntoward a foundation model for communication data--a transformer-based,\nmulti-modal model designed to operate directly on communication data. We\npropose methodologies to address key challenges, including tokenization,\npositional embedding, multimodality, variable feature sizes, and normalization.\nFurthermore, we empirically demonstrate that such a model can successfully\nestimate multiple features, including transmission rank, selected precoder,\nDoppler spread, and delay profile.", "authors": ["Davide Buffelli", "Sowmen Das", "Yu-Wei Lin", "Sattar Vakili", "Chien-Yi Wang", "Masoud Attarifar", "Pritthijit Nath", "Da-shan Shiu"], "published_date": "2025-05-20", "title_zh": "邁向通訊系統的基礎模型", "summary_zh": "本文旨在探索通訊系統領域的基礎模型。 借鑒AI領域發展趨勢，提出一個基於Transformer的多模態模型，直接處理通訊數據。研究針對通訊數據的特殊性，解決了分詞、位置嵌入、多模態、可變特徵尺寸和正規化等關鍵挑戰。實驗結果顯示，該模型能有效預測多種通訊指標，例如傳輸等級、預編碼器、都卜勒頻展和延遲分佈。", "audio": "audios/2505.14603v1.mp3", "timestamp": "2025-05-21T06:27:23.893770"}
{"query": "Diffusion Model", "id": "2505.14521v1", "url": "http://arxiv.org/abs/2505.14521v1", "title": "SparC: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling", "summary": "High-fidelity 3D object synthesis remains significantly more challenging than\n2D image generation due to the unstructured nature of mesh data and the cubic\ncomplexity of dense volumetric grids. Existing two-stage pipelines-compressing\nmeshes with a VAE (using either 2D or 3D supervision), followed by latent\ndiffusion sampling-often suffer from severe detail loss caused by inefficient\nrepresentations and modality mismatches introduced in VAE. We introduce SparC,\na unified framework that combines a sparse deformable marching cubes\nrepresentation SparseCubes with a novel encoder SparConv-VAE. SparseCubes\nconverts raw meshes into high-resolution ($1024^3$) surfaces with arbitrary\ntopology by scattering signed distance and deformation fields onto a sparse\ncube, allowing differentiable optimization. SparConv-VAE is the first\nmodality-consistent variational autoencoder built entirely upon sparse\nconvolutional networks, enabling efficient and near-lossless 3D reconstruction\nsuitable for high-resolution generative modeling through latent diffusion.\nSparC achieves state-of-the-art reconstruction fidelity on challenging inputs,\nincluding open surfaces, disconnected components, and intricate geometry. It\npreserves fine-grained shape details, reduces training and inference cost, and\nintegrates naturally with latent diffusion models for scalable, high-resolution\n3D generation.", "authors": ["Zhihao Li", "Yufei Wang", "Heliang Zheng", "Yihao Luo", "Bihan Wen"], "published_date": "2025-05-20", "title_zh": "SparC: 用於高解析度3D形狀建模的稀疏表示與建構", "summary_zh": "SparC是一个统一的3D模型生成框架，它結合了稀疏可變形移動立方體表示SparseCubes和新颖的編碼器SparConv-VAE。SparseCubes能将原始网格转换为高分辨率的表面，而SparConv-VAE則是首個完全基於稀疏卷積網絡的變分自編碼器，实现高效近乎無损的3D重建。SparC在高精度重建复杂3D模型方面表现出色，并且能与潜在扩散模型整合，实现可扩展的高分辨率3D生成。", "audio": "audios/2505.14521v1.mp3", "timestamp": "2025-05-21T06:27:28.672877"}
{"query": "AI", "id": "2505.14668v1", "url": "http://arxiv.org/abs/2505.14668v1", "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions", "summary": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants.", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Kaiwei Liu", "Siyang Jiang", "Wenrui Lu", "Hongkai Chen", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "published_date": "2025-05-20", "title_zh": "ContextAgent：具備開放世界感知能力的語境感知主動式大型語言模型代理", "summary_zh": "論文提出 ContextAgent，一個能主動提供協助的 AI 代理。它利用穿戴裝置的感測數據，像是影像和聲音，以及歷史資料，來理解使用者的意圖，並預測使用者是否需要協助。當需要協助時，ContextAgent 會自動調用工具來提供服務。研究團隊還建立了 ContextAgentBench 基準測試，證明 ContextAgent 在主動預測和工具調用方面都比其他方法更準確。目標是開發更先進、以人為本的主動式 AI 助理。", "audio": "audios/2505.14668v1.mp3", "timestamp": "2025-05-21T08:24:48.499842"}
{"query": "Foundation Model", "id": "2505.14543v1", "url": "http://arxiv.org/abs/2505.14543v1", "title": "Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions", "summary": "Traditional time series models are task-specific and often depend on\ndataset-specific training and extensive feature engineering. While\nTransformer-based architectures have improved scalability, foundation models,\ncommonplace in text, vision, and audio, remain under-explored for time series\nand are largely restricted to forecasting. We introduce $\\textbf{CHARM}$, a\nfoundation embedding model for multivariate time series that learns shared,\ntransferable, and domain-aware representations. To address the unique\ndifficulties of time series foundation learning, $\\textbf{CHARM}$ incorporates\narchitectural innovations that integrate channel-level textual descriptions\nwhile remaining invariant to channel order. The model is trained using a Joint\nEmbedding Predictive Architecture (JEPA), with novel augmentation schemes and a\nloss function designed to improve interpretability and training stability. Our\n$7$M-parameter model achieves state-of-the-art performance across diverse\ndownstream tasks, setting a new benchmark for time series representation\nlearning.", "authors": ["Utsav Dutta", "Sina Khoshfetrat Pakazad", "Henrik Ohlsson"], "published_date": "2025-05-20", "title_zh": "時間嵌入：利用通道描述解鎖時間序列基礎模型", "summary_zh": "傳統時間序列模型高度依賴特定任務和數據集，且需要大量特徵工程。雖然Transformer架構提升了可擴展性，但時間序列的基礎模型開發仍落後於文字、視覺和音訊領域，且主要集中在預測上。本研究提出一個名為CHARM的多元時間序列基礎嵌入模型，旨在學習可共享、可遷移且具領域感知性的表徵。CHARM結合了通道層級的文字描述，並具備通道順序不變性，克服了時間序列基礎學習的獨特挑戰。CHARM採用聯合嵌入預測架構（JEPA）進行訓練，結合創新的增強方案和損失函數，以提升可解釋性和訓練穩定性。僅有700萬參數的CHARM模型在各種下游任務中表現出色，為時間序列表徵學習設定了新的基準。", "audio": "audios/2505.14543v1.mp3", "timestamp": "2025-05-21T08:24:54.929489"}
{"query": "Diffusion Model", "id": "2505.14502v1", "url": "http://arxiv.org/abs/2505.14502v1", "title": "Learning to Integrate Diffusion ODEs by Averaging the Derivatives", "summary": "To accelerate diffusion model inference, numerical solvers perform poorly at\nextremely small steps, while distillation techniques often introduce complexity\nand instability. This work presents an intermediate strategy, balancing\nperformance and cost, by learning ODE integration using loss functions derived\nfrom the derivative-integral relationship, inspired by Monte Carlo integration\nand Picard iteration. From a geometric perspective, the losses operate by\ngradually extending the tangent to the secant, thus are named as secant losses.\nThe secant losses can rapidly convert (via fine-tuning or distillation) a\npretrained diffusion model into its secant version. In our experiments, the\nsecant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the\nsecant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID\nof $1.96$ on ImageNet-$256\\times256$. Code will be available.", "authors": ["Wenze Liu", "Xiangyu Yue"], "published_date": "2025-05-20", "title_zh": "透過平均導數學習整合擴散常微分方程式", "summary_zh": "為了加速擴散模型的推論速度，數值解法在極小步長下表現不佳，而知識蒸餾技術又常引入複雜性和不穩定性。本文提出一種中間策略，在性能和成本之間取得平衡，透過學習常微分方程式的積分，利用源自導數-積分關係的損失函數，靈感來自蒙地卡羅積分和皮卡迭代。從幾何角度來看，這些損失函數透過逐步將切線延伸至割線來運作，因此被命名為割線損失。割線損失可以快速地（透過微調或知識蒸餾）將預訓練的擴散模型轉換為其割線版本。在實驗中，EDM 的割線版本在 CIFAR-10 上僅需 10 步即可達到 2.14 的 FID，而 SiT-XL/2 的割線版本在 ImageNet-256x256 上僅需 4 步即可達到 2.27 的 FID，8 步可達 1.96 的 FID。程式碼將會公開。", "audio": "audios/2505.14502v1.mp3", "timestamp": "2025-05-21T08:25:02.392047"}
{"query": "AI", "id": "2505.14667v1", "url": "http://arxiv.org/abs/2505.14667v1", "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment", "summary": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI.", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Minsuk Kahng", "Albert No"], "published_date": "2025-05-20", "title_zh": "SAFEPATH：透過早期對齊預防鏈式思考中的有害推理", "summary_zh": "大型推理模型在解決複雜問題上表現出色，但鏈式思考過程可能在面對有害提示時產生不安全的輸出。現有安全對齊方法雖可減少有害輸出，但也可能降低推理深度，影響複雜任務的表現，且容易受到精巧的越獄攻擊。為此，我們提出 SAFEPATH，這是一種輕量級的對齊方法，透過微調大型推理模型，使其在收到有害提示時，於推理的開頭輸出一段簡短的 8 字元安全引言，同時讓剩餘的推理過程保持無監督。實驗結果顯示，SAFEPATH 能有效減少有害輸出，同時維持推理效能。我們還提出了一個零樣本變體，無需任何微調。此外，我們分析了現有大型語言模型方法應用於以推理為中心的模型時的泛化能力，揭示了關鍵的不足之處和更安全 AI 的新方向。", "audio": "audios/2505.14667v1.mp3", "timestamp": "2025-05-21T09:20:36.302657"}
{"query": "Foundation Model", "id": "2505.14417v1", "url": "http://arxiv.org/abs/2505.14417v1", "title": "Towards Non-Euclidean Foundation Models: Advancing AI Beyond Euclidean Frameworks", "summary": "In the era of foundation models and Large Language Models (LLMs), Euclidean\nspace is the de facto geometric setting of our machine learning architectures.\nHowever, recent literature has demonstrated that this choice comes with\nfundamental limitations. To that end, non-Euclidean learning is quickly gaining\ntraction, particularly in web-related applications where complex relationships\nand structures are prevalent. Non-Euclidean spaces, such as hyperbolic,\nspherical, and mixed-curvature spaces, have been shown to provide more\nefficient and effective representations for data with intrinsic geometric\nproperties, including web-related data like social network topology,\nquery-document relationships, and user-item interactions. Integrating\nfoundation models with non-Euclidean geometries has great potential to enhance\ntheir ability to capture and model the underlying structures, leading to better\nperformance in search, recommendations, and content understanding. This\nworkshop focuses on the intersection of Non-Euclidean Foundation Models and\nGeometric Learning (NEGEL), exploring its potential benefits, including the\npotential benefits for advancing web-related technologies, challenges, and\nfuture directions. Workshop page:\n[https://hyperboliclearning.github.io/events/www2025workshop](https://hyperboliclearning.github.io/events/www2025workshop)", "authors": ["Menglin Yang", "Yifei Zhang", "Jialin Chen", "Melanie Weber", "Rex Ying"], "published_date": "2025-05-20", "title_zh": "邁向非歐幾里得基礎模型：超越歐幾里得框架推進人工智慧", "summary_zh": "歐幾里得空間是目前機器學習架構的預設幾何設定，但它存在根本性的局限性。因此，非歐幾里得學習正迅速受到關注，尤其是在網路相關應用中。例如，雙曲、球面和混合曲率空間，在處理具有內在幾何特性的資料（如社交網路拓撲、查詢-文件關係和使用者-項目互動）方面更有效率。將非歐幾里得幾何與基礎模型整合，可望提升模型捕捉和建模底層結構的能力，進而改善搜尋、推薦和內容理解。本工作坊聚焦於非歐幾里得基礎模型和幾何學習的交叉領域，探討其潛在優勢、挑戰和未來方向，特別是在推進網路相關技術方面的潛力。", "audio": "audios/2505.14417v1.mp3", "timestamp": "2025-05-21T09:20:41.995382"}
{"query": "Diffusion Model", "id": "2505.14455v1", "url": "http://arxiv.org/abs/2505.14455v1", "title": "CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation", "summary": "Although autoregressive models have dominated language modeling in recent\nyears, there has been a growing interest in exploring alternative paradigms to\nthe conventional next-token prediction framework. Diffusion-based language\nmodels have emerged as a compelling alternative due to their powerful parallel\ngeneration capabilities and inherent editability. However, these models are\noften constrained by fixed-length generation. A promising direction is to\ncombine the strengths of both paradigms, segmenting sequences into blocks,\nmodeling autoregressive dependencies across blocks while leveraging discrete\ndiffusion to estimate the conditional distribution within each block given the\npreceding context. Nevertheless, their practical application is often hindered\nby two key limitations: rigid fixed-length outputs and a lack of flexible\ncontrol mechanisms. In this work, we address the critical limitations of fixed\ngranularity and weak controllability in current large diffusion language\nmodels. We propose CtrlDiff, a dynamic and controllable semi-autoregressive\nframework that adaptively determines the size of each generation block based on\nlocal semantics using reinforcement learning. Furthermore, we introduce a\nclassifier-guided control mechanism tailored to discrete diffusion, which\nsignificantly reduces computational overhead while facilitating efficient\npost-hoc conditioning without retraining. Extensive experiments demonstrate\nthat CtrlDiff sets a new standard among hybrid diffusion models, narrows the\nperformance gap to state-of-the-art autoregressive approaches, and enables\neffective conditional text generation across diverse tasks.", "authors": ["Chihan Huang", "Hao Tang"], "published_date": "2025-05-20", "title_zh": "CtrlDiff：透過動態區塊預測與可控生成提升大型擴散語言模型", "summary_zh": "現今，擴散語言模型因其強大的平行生成能力和可編輯性而備受關注。然而，這些模型常受限於固定長度的生成。CtrlDiff提出一種動態且可控的半自迴歸框架，能基於局部語義自適應地決定每個生成區塊的大小。此外，我們還引入了一種針對離散擴散的分類器引導控制機制，在無需重新訓練的情況下，實現高效的後驗條件生成。實驗表明，CtrlDiff在混合擴散模型中樹立了新的標竿，縮小了與最先進自迴歸方法的性能差距，並能跨多種任務實現有效的條件文本生成。", "audio": "audios/2505.14455v1.mp3", "timestamp": "2025-05-21T09:20:47.242642"}
{"query": "AI", "id": "2505.14661v1", "url": "http://arxiv.org/abs/2505.14661v1", "title": "Abacus: A Cost-Based Optimizer for Semantic Operator Systems", "summary": "LLMs enable an exciting new class of data processing applications over large\ncollections of unstructured documents. Several new programming frameworks have\nenabled developers to build these applications by composing them out of\nsemantic operators: a declarative set of AI-powered data transformations with\nnatural language specifications. These include LLM-powered maps, filters,\njoins, etc. used for document processing tasks such as information extraction,\nsummarization, and more. While systems of semantic operators have achieved\nstrong performance on benchmarks, they can be difficult to optimize. An\noptimizer for this setting must determine how to physically implement each\nsemantic operator in a way that optimizes the system globally. Existing\noptimizers are limited in the number of optimizations they can apply, and most\n(if not all) cannot optimize system quality, cost, or latency subject to\nconstraint(s) on the other dimensions. In this paper we present Abacus, an\nextensible, cost-based optimizer which searches for the best implementation of\na semantic operator system given a (possibly constrained) optimization\nobjective. Abacus estimates operator performance by leveraging a minimal set of\nvalidation examples and, if available, prior beliefs about operator\nperformance. We evaluate Abacus on document processing workloads in the\nbiomedical and legal domains (BioDEX; CUAD) and multi-modal question answering\n(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%\nbetter quality and up to 23.6x lower cost and 4.2x lower latency than the next\nbest system.", "authors": ["Matthew Russo", "Sivaprasad Sudhir", "Gerardo Vitagliano", "Chunwei Liu", "Tim Kraska", "Samuel Madden", "Michael Cafarella"], "published_date": "2025-05-20", "title_zh": "算盤 (Abacus): 一個基於成本的語義運算子系統最佳化器", "summary_zh": "大型語言模型催生了新的非結構化文件處理應用。開發者可以使用語義運算子，也就是基於AI、具自然語言規範的資料轉換，來建構這些應用。然而，優化這些運算子系統非常困難。現有的優化器能力有限，且難以同時優化品質、成本和延遲。本文提出 Abacus，一個可擴展、基於成本的優化器，能根據給定的優化目標（可能帶有約束）尋找語義運算子系統的最佳實現方案。Abacus 利用少量的驗證樣本和運算子效能的先驗知識來估算運算子效能。在生物醫學、法律領域和多模態問答工作負載上的評估表明，由 Abacus 優化的系統比其他系統能達到18.7%-39.2%的品質提升，並降低高達23.6倍的成本和4.2倍的延遲。", "audio": "audios/2505.14661v1.mp3", "timestamp": "2025-05-21T11:15:44.284777"}
{"query": "Foundation Model", "id": "2505.14415v1", "url": "http://arxiv.org/abs/2505.14415v1", "title": "Table Foundation Models: on knowledge pre-training for tabular learning", "summary": "Table foundation models bring high hopes to data science: pre-trained on\ntabular data to embark knowledge or priors, they should facilitate downstream\ntasks on tables. One specific challenge is that of data semantics: numerical\nentries take their meaning from context, e.g., column name. Pre-trained neural\nnetworks that jointly model column names and table entries have recently\nboosted prediction accuracy. While these models outline the promises of world\nknowledge to interpret table values, they lack the convenience of popular\nfoundation models in text or vision. Indeed, they must be fine-tuned to bring\nbenefits, come with sizeable computation costs, and cannot easily be reused or\ncombined with other architectures. Here we introduce TARTE, a foundation model\nthat transforms tables to knowledge-enhanced vector representations using the\nstring to capture semantics. Pre-trained on large relational data, TARTE yields\nrepresentations that facilitate subsequent learning with little additional\ncost. These representations can be fine-tuned or combined with other learners,\ngiving models that push the state-of-the-art prediction performance and improve\nthe prediction/computation performance trade-off. Specialized to a task or a\ndomain, TARTE gives domain-specific representations that facilitate further\nlearning. Our study demonstrates an effective approach to knowledge\npre-training for tabular learning.", "authors": ["Myung Jun Kim", "Félix Lefebvre", "Gaëtan Brison", "Alexandre Perez-Lebel", "Gaël Varoquaux"], "published_date": "2025-05-20", "title_zh": "表格基礎模型：關於表格學習的知識預訓練", "summary_zh": "表格基礎模型被寄予厚望，期望透過在表格數據上進行預訓練，獲取知識或先驗知識，從而簡化下游表格任務。一個特別的挑戰是數據語義：數值條目的意義來自於上下文，例如欄位名稱。最近，聯合建模欄位名稱和表格條目的預訓練神經網路提高了預測準確性。然而，這些模型缺乏文本或視覺領域中常見基礎模型的便利性，必須進行微調才能帶來效益，計算成本高昂，且難以重複使用或與其他架構結合。因此，我們介紹了TARTE，一種基礎模型，它使用字串將表格轉換為知識增強的向量表示，從而捕獲語義。TARTE在大型關聯數據上進行預訓練，產生易於後續學習且幾乎無需額外成本的表示。這些表示可以進行微調或與其他學習器結合，從而使模型能夠提升最先進的預測性能，並改善預測/計算性能的權衡。針對特定任務或領域進行專門設計的TARTE，可以提供領域特定的表示，從而促進進一步的學習。我們的研究證明了一種有效的表格學習知識預訓練方法。", "audio": "audios/2505.14415v1.mp3", "timestamp": "2025-05-21T11:15:52.095562"}
{"query": "Diffusion Model", "id": "2505.14429v1", "url": "http://arxiv.org/abs/2505.14429v1", "title": "Compositional amortized inference for large-scale hierarchical Bayesian models", "summary": "Amortized Bayesian inference (ABI) has emerged as a powerful simulation-based\napproach for estimating complex mechanistic models, offering fast posterior\nsampling via generative neural networks. However, extending ABI to hierarchical\nmodels, a cornerstone of modern Bayesian analysis, remains a major challenge\ndue to the difficulty of scaling to large numbers of parameters. In this work,\nwe build on compositional score matching (CSM), a divide-and-conquer strategy\nfor Bayesian updating using diffusion models. To address existing stability\nissues of CSM, we propose adaptive solvers coupled with a novel, error-damping\ncompositional estimator. Our proposed method remains stable even with hundreds\nof thousands of data points and parameters. We validate our approach on a\ncontrolled toy example, a high-dimensional spatial autoregressive model, and a\nreal-world advanced microscopy biological application task involving over\n750,000 parameters.", "authors": ["Jonas Arruda", "Vikas Pandey", "Catherine Sherry", "Margarida Barroso", "Xavier Intes", "Jan Hasenauer", "Stefan T. Radev"], "published_date": "2025-05-20", "title_zh": "大規模層級貝氏模型的組合攤銷推論", "summary_zh": "提出一種基於生成神經網路的攤銷貝氏推論 (ABI) 方法，加速複雜模型後驗分布的取樣。針對 ABI 在大規模層級模型上的應用挑戰，本研究結合組合分數匹配 (CSM) 策略和自適應求解器，並提出一種新型的誤差阻尼組合估計器，克服 CSM 的穩定性問題。該方法即使在高維度數據和參數下也能保持穩定，並在模擬數據、空間自迴歸模型以及實際生物顯微鏡數據上驗證其有效性。簡言之，本研究提出一種穩定的方法，可以處理參數量巨大的層級貝氏模型推論問題。", "audio": "audios/2505.14429v1.mp3", "timestamp": "2025-05-21T11:15:57.552059"}
{"query": "AI", "id": "2505.14659v1", "url": "http://arxiv.org/abs/2505.14659v1", "title": "Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks", "summary": "As healthcare systems increasingly adopt advanced wireless networks and\nconnected devices, securing medical applications has become critical. The\nintegration of Internet of Medical Things devices, such as robotic surgical\ntools, intensive care systems, and wearable monitors has enhanced patient care\nbut introduced serious security risks. Cyberattacks on these devices can lead\nto life threatening consequences, including surgical errors, equipment failure,\nand data breaches. While the ITU IMT 2030 vision highlights 6G's transformative\nrole in healthcare through AI and cloud integration, it also raises new\nsecurity concerns. This paper explores how explainable AI techniques like SHAP,\nLIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve\ntrust and transparency in 6G enabled healthcare. We support our approach with\nexperimental analysis and highlight promising results.", "authors": ["Navneet Kaur", "Lav Gupta"], "published_date": "2025-05-20", "title_zh": "可解釋人工智慧在物聯網整合型 6G 無線網路中保障醫療保健安全", "summary_zh": "隨著醫療系統更廣泛使用無線網路和聯網設備，醫療應用的安全性變得至關重要。物聯網醫療設備雖然提升了照護品質，但也帶來了嚴重的安全風險，像是手術機器人可能被駭、數據外洩等。6G 的發展進一步促進醫療保健的 AI 和雲端整合，但也引發了新的安全隱憂。本文探討如何運用可解釋人工智慧技術，例如 SHAP、LIME 和 DiCE，來發現漏洞、強化防禦，並提高 6G 醫療保健的信任度和透明度。實驗分析結果也顯示這些方法很有潛力。", "audio": "audios/2505.14659v1.mp3", "timestamp": "2025-05-21T12:37:45.151636"}
{"query": "Foundation Model", "id": "2505.14414v1", "url": "http://arxiv.org/abs/2505.14414v1", "title": "Diving into the Fusion of Monocular Priors for Generalized Stereo Matching", "summary": "The matching formulation makes it naturally hard for the stereo matching to\nhandle ill-posed regions like occlusions and non-Lambertian surfaces. Fusing\nmonocular priors has been proven helpful for ill-posed matching, but the biased\nmonocular prior learned from small stereo datasets constrains the\ngeneralization. Recently, stereo matching has progressed by leveraging the\nunbiased monocular prior from the vision foundation model (VFM) to improve the\ngeneralization in ill-posed regions. We dive into the fusion process and\nobserve three main problems limiting the fusion of the VFM monocular prior. The\nfirst problem is the misalignment between affine-invariant relative monocular\ndepth and absolute depth of disparity. Besides, when we use the monocular\nfeature in an iterative update structure, the over-confidence in the disparity\nupdate leads to local optima results. A direct fusion of a monocular depth map\ncould alleviate the local optima problem, but noisy disparity results computed\nat the first several iterations will misguide the fusion. In this paper, we\npropose a binary local ordering map to guide the fusion, which converts the\ndepth map into a binary relative format, unifying the relative and absolute\ndepth representation. The computed local ordering map is also used to re-weight\nthe initial disparity update, resolving the local optima and noisy problem. In\naddition, we formulate the final direct fusion of monocular depth to the\ndisparity as a registration problem, where a pixel-wise linear regression\nmodule can globally and adaptively align them. Our method fully exploits the\nmonocular prior to support stereo matching results effectively and efficiently.\nWe significantly improve the performance from the experiments when generalizing\nfrom SceneFlow to Middlebury and Booster datasets while barely reducing the\nefficiency.", "authors": ["Chengtang Yao", "Lidong Yu", "Zhidan Liu", "Jiaxi Zeng", "Yuwei Wu", "Yunde Jia"], "published_date": "2025-05-20", "title_zh": "深入探討融合單眼先驗知識以提升廣義立體匹配", "summary_zh": "立體匹配在處理遮擋和非朗伯表面等不良區域時面臨挑戰。融合單眼先驗知識已被證明有效，但從小型立體數據集學習到的偏差先驗會限制泛化能力。近年來，利用視覺基礎模型（VFM）中無偏差的單眼先驗，立體匹配在不良區域的泛化能力方面取得了進展。本研究深入探討了融合過程，發現了限制VFM單眼先驗融合的三個主要問題：仿射不變的相對單眼深度與絕對深度視差之間的錯位；迭代更新結構中，過度自信的視差更新導致局部最優；以及早期迭代中嘈雜的視差結果會誤導直接融合單眼深度圖。我們提出了一種二元局部排序圖來引導融合，將深度圖轉換為二元相對格式，統一了相對深度和絕對深度表示。該局部排序圖還用於重新權衡初始視差更新，解決了局部最優和噪聲問題。此外，我們將單眼深度與視差的最終直接融合表述為一個註冊問題，通過像素級線性回歸模塊進行全局自適應對齊。實驗結果表明，我們的模型充分利用了單眼先驗知識，顯著提高了從SceneFlow泛化到Middlebury和Booster數據集的性能，同時幾乎沒有降低效率。", "audio": "audios/2505.14414v1.mp3", "timestamp": "2025-05-21T12:37:55.398628"}
{"query": "Diffusion Model", "id": "2505.14357v1", "url": "http://arxiv.org/abs/2505.14357v1", "title": "Vid2World: Crafting Video Diffusion Models to Interactive World Models", "summary": "World models, which predict transitions based on history observation and\naction sequences, have shown great promise in improving data efficiency for\nsequential decision making. However, existing world models often require\nextensive domain-specific training and still produce low-fidelity, coarse\npredictions, limiting their applicability in complex environments. In contrast,\nvideo diffusion models trained on large, internet-scale datasets have\ndemonstrated impressive capabilities in generating high-quality videos that\ncapture diverse real-world dynamics. In this work, we present Vid2World, a\ngeneral approach for leveraging and transferring pre-trained video diffusion\nmodels into interactive world models. To bridge the gap, Vid2World performs\ncasualization of a pre-trained video diffusion model by crafting its\narchitecture and training objective to enable autoregressive generation.\nFurthermore, it introduces a causal action guidance mechanism to enhance action\ncontrollability in the resulting interactive world model. Extensive experiments\nin robot manipulation and game simulation domains show that our method offers a\nscalable and effective approach for repurposing highly capable video diffusion\nmodels to interactive world models.", "authors": ["Siqiao Huang", "Jialong Wu", "Qixing Zhou", "Shangchen Miao", "Mingsheng Long"], "published_date": "2025-05-20", "title_zh": "Vid2World：打造視訊擴散模型成為互動式世界模型", "summary_zh": "Vid2World 提出一種利用預訓練視訊擴散模型打造互動式世界模型的新方法。它通過改造模型架構和訓練目標，使其能自迴歸地生成預測，並引入因果行動引導機制，增強行動的可控性。實驗證明，這種方法能有效地將强大的視訊擴散模型轉化為可互動的世界模型，並應用於機器人操作和遊戲模擬等領域。", "audio": "audios/2505.14357v1.mp3", "timestamp": "2025-05-21T12:37:58.683896"}
{"query": "AI", "id": "2505.14654v1", "url": "http://arxiv.org/abs/2505.14654v1", "title": "Beyond Words: Multimodal LLM Knows When to Speak", "summary": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI.", "authors": ["Zikai Liao", "Yi Ouyang", "Yi-Lun Lee", "Chen-Ping Yu", "Yi-Hsuan Tsai", "Zhaozheng Yin"], "published_date": "2025-05-20", "title_zh": "超越文字：多模態大型語言模型知道何時該說話", "summary_zh": "大型語言模型聊天機器人在產生連貫且符合上下文的回應方面表現出色，但它們常常難以理解何時該說話，尤其是在持續的對話中給予簡短、及時的回應。本研究著重於即時預測回應類型，特別是那些依賴視覺、聽覺和文字等細微多模態信號的簡短反應性話語。為此，研究者們建立了一個新的多模態數據集，並提出了一個名為MM-When2Speak的多模態模型，該模型能自適應地整合視覺、聽覺和文字信息，以預測何時應進行回應以及何種回應類型是適當的。實驗結果表明，該模型在回應時機的準確性上，比現有技術和基於大型語言模型的模型有顯著提升，突顯了多模態輸入對於產生及時、自然和引人入勝的對話式AI的重要性。", "audio": "audios/2505.14654v1.mp3", "timestamp": "2025-05-21T13:30:37.941299"}
{"query": "Foundation Model", "id": "2505.14411v1", "url": "http://arxiv.org/abs/2505.14411v1", "title": "Byte Pair Encoding for Efficient Time Series Forecasting", "summary": "Existing time series tokenization methods predominantly encode a constant\nnumber of samples into individual tokens. This inflexible approach can generate\nexcessive tokens for even simple patterns like extended constant values,\nresulting in substantial computational overhead. Inspired by the success of\nbyte pair encoding, we propose the first pattern-centric tokenization scheme\nfor time series analysis. Based on a discrete vocabulary of frequent motifs,\nour method merges samples with underlying patterns into tokens, compressing\ntime series adaptively. Exploiting our finite set of motifs and the continuous\nproperties of time series, we further introduce conditional decoding as a\nlightweight yet powerful post-hoc optimization method, which requires no\ngradient computation and adds no computational overhead. On recent time series\nfoundation models, our motif-based tokenization improves forecasting\nperformance by 36% and boosts efficiency by 1990% on average. Conditional\ndecoding further reduces MSE by up to 44%. In an extensive analysis, we\ndemonstrate the adaptiveness of our tokenization to diverse temporal patterns,\nits generalization to unseen data, and its meaningful token representations\ncapturing distinct time series properties, including statistical moments and\ntrends.", "authors": ["Leon Götz", "Marcel Kollovieh", "Stephan Günnemann", "Leo Schwinn"], "published_date": "2025-05-20", "title_zh": "用於高效時間序列預測的位元組對編碼", "summary_zh": "現有的時間序列符號化方法通常將固定數量的樣本編碼成單獨的符號，這種不靈活的方式會對簡單的模式（例如持續的常數值）產生過多的符號，導致大量的計算開銷。受位元組對編碼的成功啟發，我們提出了一種以模式為中心的的時間序列符號化方案。該方法基於常見的圖案離散詞彙，將具有底層模式的樣本合併到符號中，從而自適應地壓縮時間序列。利用我們的有限模式集和時間序列的連續性，我們進一步引入條件解碼作為一種輕量但強大的後處理優化方法，無需梯度計算，也不會增加計算開銷。在最新的時間序列基礎模型上，我們基於模式的符號化平均提高了36%的預測性能，並提高了1990%的效率。條件解碼進一步將均方誤差降低了最多44%。在廣泛的分析中，我們證明了我們的符號化對不同時間模式的適應性、對未見數據的泛化能力，以及其捕獲不同時間序列屬性（包括統計矩和趨勢）的有意義的符號表示。", "audio": "audios/2505.14411v1.mp3", "timestamp": "2025-05-21T13:30:45.995773"}
{"query": "Diffusion Model", "id": "2505.14254v1", "url": "http://arxiv.org/abs/2505.14254v1", "title": "Instructing Text-to-Image Diffusion Models via Classifier-Guided Semantic Optimization", "summary": "Text-to-image diffusion models have emerged as powerful tools for\nhigh-quality image generation and editing. Many existing approaches rely on\ntext prompts as editing guidance. However, these methods are constrained by the\nneed for manual prompt crafting, which can be time-consuming, introduce\nirrelevant details, and significantly limit editing performance. In this work,\nwe propose optimizing semantic embeddings guided by attribute classifiers to\nsteer text-to-image models toward desired edits, without relying on text\nprompts or requiring any training or fine-tuning of the diffusion model. We\nutilize classifiers to learn precise semantic embeddings at the dataset level.\nThe learned embeddings are theoretically justified as the optimal\nrepresentation of attribute semantics, enabling disentangled and accurate\nedits. Experiments further demonstrate that our method achieves high levels of\ndisentanglement and strong generalization across different domains of data.", "authors": ["Yuanyuan Chang", "Yinghua Yao", "Tao Qin", "Mengmeng Wang", "Ivor Tsang", "Guang Dai"], "published_date": "2025-05-20", "title_zh": "透過分類器引導的語義優化來指示文字到圖像擴散模型", "summary_zh": "現有的文字到圖像模型編輯方式仰賴人工撰寫提示詞，費時且易引入雜訊。本研究提出一種新方法，無需文字提示詞，也無需訓練或微調擴散模型，而是利用屬性分類器引導語義嵌入的優化，從而實現更精準、更具解耦性的圖像編輯。研究結果顯示此方法在不同數據領域皆具有良好的泛化能力。", "audio": "audios/2505.14254v1.mp3", "timestamp": "2025-05-21T13:30:50.483667"}
{"query": "AI", "id": "2505.14646v1", "url": "http://arxiv.org/abs/2505.14646v1", "title": "CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation", "summary": "Efficient creation of accurate and editable 3D CAD models is critical in\nengineering design, significantly impacting cost and time-to-market in product\ninnovation. Current manual workflows remain highly time-consuming and demand\nextensive user expertise. While recent developments in AI-driven CAD generation\nshow promise, existing models are limited by incomplete representations of CAD\noperations, inability to generalize to real-world images, and low output\naccuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model\n(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)\ndirectly from visual input. Leveraging a novel dataset that we\ncreated--GenCAD-Code, consisting of over 163k CAD-model image and code\npairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and\nQwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in\n3D solid similarity. Notably, our VLM demonstrates some signs of\ngeneralizability, successfully generating CAD code from real-world images and\nexecuting CAD operations unseen during fine-tuning. The performance and\nadaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code\nto streamline CAD workflows for engineers and designers. CAD-Coder is publicly\navailable at: https://github.com/anniedoris/CAD-Coder.", "authors": ["Anna C. Doris", "Md Ferdous Alam", "Amin Heyrani Nobari", "Faez Ahmed"], "published_date": "2025-05-20", "title_zh": "CAD-Coder：一個用於電腦輔助設計程式碼生成的開源視覺-語言模型", "summary_zh": "工程設計中，高效、精確且可編輯的 3D CAD 模型至關重要。現有方法耗時且需專業知識。本文介紹了 CAD-Coder，一個開源視覺-語言模型，能直接從視覺輸入生成可編輯的 CAD 程式碼。它基於一個包含超過 16 萬 CAD 模型圖像和程式碼配對的新數據集 GenCAD-Code 進行微調，性能超越了 GPT-4.5 和 Qwen2.5-VL-72B 等最先進的模型，實現了 100% 的有效語法率和最高的 3D 實體相似度精度。CAD-Coder 甚至展現了從真實圖像生成 CAD 程式碼的能力。這個模型的性能和適應性凸顯了程式碼微調的視覺-語言模型在簡化工程師和設計師 CAD 工作流程方面的潛力。CAD-Coder 可在 GitHub 上公開獲取。", "audio": "audios/2505.14646v1.mp3", "timestamp": "2025-05-21T14:19:13.872585"}
{"query": "Foundation Model", "id": "2505.14402v1", "url": "http://arxiv.org/abs/2505.14402v1", "title": "OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking", "summary": "The code of nature, embedded in DNA and RNA genomes since the origin of life,\nholds immense potential to impact both humans and ecosystems through genome\nmodeling. Genomic Foundation Models (GFMs) have emerged as a transformative\napproach to decoding the genome. As GFMs scale up and reshape the landscape of\nAI-driven genomics, the field faces an urgent need for rigorous and\nreproducible evaluation. We present OmniGenBench, a modular benchmarking\nplatform designed to unify the data, model, benchmarking, and interpretability\nlayers across GFMs. OmniGenBench enables standardized, one-command evaluation\nof any GFM across five benchmark suites, with seamless integration of over 31\nopen-source models. Through automated pipelines and community-extensible\nfeatures, the platform addresses critical reproducibility challenges, including\ndata transparency, model interoperability, benchmark fragmentation, and\nblack-box interpretability. OmniGenBench aims to serve as foundational\ninfrastructure for reproducible genomic AI research, accelerating trustworthy\ndiscovery and collaborative innovation in the era of genome-scale modeling.", "authors": ["Heng Yang", "Jack Cole", "Yuan Li", "Renzhi Chen", "Geyong Min", "Ke Li"], "published_date": "2025-05-20", "title_zh": "OmniGenBench：一個用於基因體基礎模型可重現性評測的模組化平台", "summary_zh": "OmniGenBench是一個基因體基礎模型（GFMs）的評測平台，旨在解決AI驅動基因體學領域中可重現性評估的迫切需求。它整合了數據、模型、評測和可解釋性等層面，支援一鍵式評估31個以上的開源模型。透過自動化流程和社群擴展功能，OmniGenBench致力於克服數據透明度、模型互操作性、基準測試碎片化和黑盒可解釋性等挑戰，加速基因體AI研究的可靠發現和協作創新。", "audio": "audios/2505.14402v1.mp3", "timestamp": "2025-05-21T14:19:18.151222"}
{"query": "Diffusion Model", "id": "2505.14206v1", "url": "http://arxiv.org/abs/2505.14206v1", "title": "Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data", "summary": "The widespread adoption of mobile sensors has the potential to provide\nmassive and heterogeneous time series data, driving Artificial Intelligence\napplications in mHealth. However, data collection remains limited due to\nstringent ethical regulations, privacy concerns, and other constraints,\nhindering progress in the field. Synthetic data generation, particularly\nthrough Generative Adversarial Networks and Diffusion Models, has emerged as a\npromising solution to address both data scarcity and privacy issues. Yet, these\nmodels are often limited to short-term, unimodal signal patterns. This paper\npresents a systematic evaluation of state-of-the-art generative models for time\nseries synthesis, with a focus on their ability to jointly handle\nmulti-modality, long-range dependencies, and conditional generation-key\nchallenges in the mHealth domain. To ensure a fair comparison, we introduce a\nnovel evaluation framework designed to measure both the intrinsic quality of\nsynthetic data and its utility in downstream predictive tasks. Our findings\nreveal critical limitations in the existing approaches, particularly in\nmaintaining cross-modal consistency, preserving temporal coherence, and\nensuring robust performance in train-on-synthetic, test-on-real, and data\naugmentation scenarios. Finally, we present our future research directions to\nenhance synthetic time series generation and improve the applicability of\ngenerative models in mHealth.", "authors": ["Flavio Di Martino", "Franca Delmastro"], "published_date": "2025-05-20", "title_zh": "mHealth感測器資料合成生成中的挑戰與限制", "summary_zh": "行動感測器在mHealth領域應用廣泛，能產生大量時間序列數據。然而，倫理、隱私等因素限制了真實數據的收集。利用生成對抗網路和擴散模型等技術合成數據是個有前景的解決方案，但現有模型通常只能處理短期、單模態訊號。本文系統性地評估了目前最先進的時間序列生成模型，著重於它們處理多模態、長程依賴以及條件生成的能力，這些都是mHealth領域的關鍵挑戰。研究結果揭示了現有方法的局限性，尤其是在保持跨模態一致性、保存時間相干性和確保在不同情境下的穩健性能方面。最後，論文提出了未來的研究方向，旨在改進合成時間序列生成技術，並提高生成模型在mHealth領域的應用性。", "audio": "audios/2505.14206v1.mp3", "timestamp": "2025-05-21T14:19:25.266113"}
{"query": "AI", "id": "2505.14640v1", "url": "http://arxiv.org/abs/2505.14640v1", "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation", "summary": "Large multimodal models (LMMs) have recently emerged as a powerful tool for\nlong video understanding (LVU), prompting the development of standardized LVU\nbenchmarks to evaluate their performance. However, our investigation reveals a\nrather sober lesson for existing LVU benchmarks. First, most existing\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\nresults are inflated due to the possibility of guessing the correct answer;\nSecond, a significant portion of questions in these benchmarks have strong\npriors to allow models to answer directly without even reading the input video.\nFor example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame\nfrom a long video on Video-MME. We also observe that increasing the number of\nframes does not necessarily lead to improvement on existing benchmarks, which\nis counterintuitive. As a result, the validity and robustness of current LVU\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\nrealistic LVU benchmark containing questions with open-ended short-answer,\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\nsegment-level and full-video understanding through perception and reasoning\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\nfollowing findings: (1) video LMMs show drastic performance ($>$25\\%) drops on\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\nframes. Our results show that VideoEval-Pro offers a more realistic and\nreliable measure of long video understanding, providing a clearer view of\nprogress in this domain.", "authors": ["Wentao Ma", "Weiming Ren", "Yiming Jia", "Zhuofeng Li", "Ping Nie", "Ge Zhang", "Wenhu Chen"], "published_date": "2025-05-20", "title_zh": "VideoEval-Pro：穩健且真實的長影片理解評估", "summary_zh": "現有的長影片理解評估基準測試存在嚴重問題，容易讓模型猜對答案，甚至不看影片也能答題，導致評估結果虛高。為了更真實地評估大型多模態模型的能力，我們提出了 VideoEval-Pro。它包含需要完整理解影片內容的開放式簡答題，涵蓋影片片段及完整影片的感知和推理任務。實驗結果顯示，現有模型在 VideoEval-Pro 上表現大幅下降，且增加輸入影格數更能提升效能，顯示 VideoEval-Pro 能更準確地衡量長影片理解能力。", "audio": "audios/2505.14640v1.mp3", "timestamp": "2025-05-21T16:24:06.392302"}
{"query": "Foundation Model", "id": "2505.14396v1", "url": "http://arxiv.org/abs/2505.14396v1", "title": "Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds", "summary": "Causal world models are systems that can answer counterfactual questions\nabout an environment of interest, i.e. predict how it would have evolved if an\narbitrary subset of events had been realized differently. It requires\nunderstanding the underlying causes behind chains of events and conducting\ncausal inference for arbitrary unseen distributions. So far, this task eludes\nfoundation models, notably large language models (LLMs), which do not have\ndemonstrated causal reasoning capabilities beyond the memorization of existing\ncausal relationships. Furthermore, evaluating counterfactuals in real-world\napplications is challenging since only the factual world is observed, limiting\nevaluation to synthetic datasets. We address these problems by explicitly\nextracting and modeling causal relationships and propose the Causal\nCartographer framework. First, we introduce a graph retrieval-augmented\ngeneration agent tasked to retrieve causal relationships from data. This\napproach allows us to construct a large network of real-world causal\nrelationships that can serve as a repository of causal knowledge and build\nreal-world counterfactuals. In addition, we create a counterfactual reasoning\nagent constrained by causal relationships to perform reliable step-by-step\ncausal inference. We show that our approach can extract causal knowledge and\nimprove the robustness of LLMs for causal reasoning tasks while reducing\ninference costs and spurious correlations.", "authors": ["Gaël Gendron", "Jože M. Rožanec", "Michael Witbrock", "Gillian Dobbie"], "published_date": "2025-05-20", "title_zh": "因果製圖師：從地圖繪製到反事實世界的推理", "summary_zh": "大型語言模型（LLM）擅長記憶，但缺乏真正的因果推理能力，難以回答關於反事實情境的問題，也就是預測如果某些事件以不同方式發生，世界將如何演變。本研究提出「因果製圖師」框架，透過檢索數據中的因果關係，構建一個龐大的真實世界因果關係網絡，作為因果知識庫。此外，我們還建立了一個受因果關係約束的反事實推理代理，使其能執行可靠的逐步因果推理。實驗證明，此方法能有效提取因果知識，提升LLM在因果推理任務中的穩健性，同時降低推理成本和錯誤的相關性。", "audio": "audios/2505.14396v1.mp3", "timestamp": "2025-05-21T16:24:11.925044"}
{"query": "Diffusion Model", "id": "2505.14139v1", "url": "http://arxiv.org/abs/2505.14139v1", "title": "FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning", "summary": "The use of guidance to steer sampling toward desired outcomes has been widely\nexplored within diffusion models, especially in applications such as image and\ntrajectory generation. However, incorporating guidance during training remains\nrelatively underexplored. In this work, we introduce energy-guided flow\nmatching, a novel approach that enhances the training of flow models and\neliminates the need for guidance at inference time. We learn a conditional\nvelocity field corresponding to the flow policy by approximating an\nenergy-guided probability path as a Gaussian path. Learning guided trajectories\nis appealing for tasks where the target distribution is defined by a\ncombination of data and an energy function, as in reinforcement learning.\nDiffusion-based policies have recently attracted attention for their expressive\npower and ability to capture multi-modal action distributions. Typically, these\npolicies are optimized using weighted objectives or by back-propagating\ngradients through actions sampled by the policy. As an alternative, we propose\nFlowQ, an offline reinforcement learning algorithm based on energy-guided flow\nmatching. Our method achieves competitive performance while the policy training\ntime is constant in the number of flow sampling steps.", "authors": ["Marvin Alles", "Nutan Chen", "Patrick van der Smagt", "Botond Cseke"], "published_date": "2025-05-20", "title_zh": "FlowQ：能量導引的流動策略用於離線強化學習", "summary_zh": "本研究提出FlowQ，一種基於能量導引的流動匹配的離線強化學習算法。FlowQ透過近似能量導引機率路徑為高斯路徑，學習條件速度場，從而訓練出更好的流動模型。這種方法消除了推理階段需要額外導引的步驟，並且在策略訓練時間上與流動採樣步數無關，同時能達到具競爭力的效能。", "audio": "audios/2505.14139v1.mp3", "timestamp": "2025-05-21T16:24:16.369091"}
{"query": "AI", "id": "2505.14633v1", "url": "http://arxiv.org/abs/2505.14633v1", "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas", "summary": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.", "authors": ["Yu Ying Chiu", "Zhilin Wang", "Sharan Maiya", "Yejin Choi", "Kyle Fish", "Sydney Levine", "Evan Hubinger"], "published_date": "2025-05-20", "title_zh": "AI會為了拯救生病的孩子而說謊嗎？用AIRiskDilemmas測試AI的價值優先順序", "summary_zh": "隨著AI模型變得更強大，偵測AI風險也更具挑戰性，因為它們會使用像是「對齊偽造」等新方法來迴避偵測。我們認為，識別AI模型內部的價值觀，可以作為AI潛在風險行為的早期預警系統。我們創建了LitmusValues評估流程，揭示AI模型在各種價值觀類別上的優先順序。接著，我們收集了AIRiskDilemmas，這是一系列多樣化的兩難情境，這些情境將價值觀彼此對立，並與AI安全風險（例如權力追求）相關。透過評估AI模型在這些情境中的選擇，我們可以獲得一致的價值優先順序預測，從而揭示潛在的風險。研究表明，LitmusValues中的價值觀（包括看似無害的價值觀，如關懷）可以預測AIRiskDilemmas中已知的風險行為，以及HarmBench中未知的風險行為。", "audio": "audios/2505.14633v1.mp3", "timestamp": "2025-05-21T17:16:05.673771"}
{"query": "Foundation Model", "id": "2505.14361v1", "url": "http://arxiv.org/abs/2505.14361v1", "title": "Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives", "summary": "Vision-language modeling (VLM) aims to bridge the information gap between\nimages and natural language. Under the new paradigm of first pre-training on\nmassive image-text pairs and then fine-tuning on task-specific data, VLM in the\nremote sensing domain has made significant progress. The resulting models\nbenefit from the absorption of extensive general knowledge and demonstrate\nstrong performance across a variety of remote sensing data analysis tasks.\nMoreover, they are capable of interacting with users in a conversational\nmanner. In this paper, we aim to provide the remote sensing community with a\ntimely and comprehensive review of the developments in VLM using the two-stage\nparadigm. Specifically, we first cover a taxonomy of VLM in remote sensing:\ncontrastive learning, visual instruction tuning, and text-conditioned image\ngeneration. For each category, we detail the commonly used network architecture\nand pre-training objectives. Second, we conduct a thorough review of existing\nworks, examining foundation models and task-specific adaptation methods in\ncontrastive-based VLM, architectural upgrades, training strategies and model\ncapabilities in instruction-based VLM, as well as generative foundation models\nwith their representative downstream applications. Third, we summarize datasets\nused for VLM pre-training, fine-tuning, and evaluation, with an analysis of\ntheir construction methodologies (including image sources and caption\ngeneration) and key properties, such as scale and task adaptability. Finally,\nwe conclude this survey with insights and discussions on future research\ndirections: cross-modal representation alignment, vague requirement\ncomprehension, explanation-driven model reliability, continually scalable model\ncapabilities, and large-scale datasets featuring richer modalities and greater\nchallenges.", "authors": ["Xingxing Weng", "Chao Pang", "Gui-Song Xia"], "published_date": "2025-05-20", "title_zh": "視覺-語言建模遇上遙感：模型、資料集與展望", "summary_zh": "本文綜述了視覺-語言建模（VLM）在遙感領域的最新進展。VLM透過大規模圖像-文字配對的預訓練，再針對特定任務進行微調，已展現出強大的遙感數據分析能力，甚至可以進行對話互動。本文分類介紹了遙感VLM的三種主要方法：對比學習、視覺指令調整和文本條件圖像生成，並深入探討了現有模型、訓練策略和資料集。最後，文章展望了未來研究方向，包括跨模態表示對齊、模糊需求理解、可解釋性、持續可擴展的模型能力，以及更大規模、更具挑戰性的多模態資料集。", "audio": "audios/2505.14361v1.mp3", "timestamp": "2025-05-21T17:16:12.073270"}
{"query": "Diffusion Model", "id": "2505.14036v1", "url": "http://arxiv.org/abs/2505.14036v1", "title": "Adaptive Cyclic Diffusion for Inference Scaling", "summary": "Diffusion models have demonstrated strong generative capabilities across\ndomains ranging from image synthesis to complex reasoning tasks. However, most\ninference-time scaling methods rely on fixed denoising schedules, limiting\ntheir ability to allocate computation based on instance difficulty or\ntask-specific demands adaptively. We introduce the challenge of adaptive\ninference-time scaling-dynamically adjusting computational effort during\ninference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a\nflexible, search-based inference framework. ABCD refines outputs through\nbi-directional diffusion cycles while adaptively controlling exploration depth\nand termination. It comprises three components: Cyclic Diffusion Search,\nAutomatic Exploration-Exploitation Balancing, and Adaptive Thinking Time.\nExperiments show that ABCD improves performance across diverse tasks while\nmaintaining computational efficiency.", "authors": ["Gyubin Lee", "Truong Nhat Nguyen Bao", "Jaesik Yoon", "Dongwoo Lee", "Minsu Kim", "Yoshua Bengio", "Sungjin Ahn"], "published_date": "2025-05-20", "title_zh": "自適應循環擴散用於推論規模調整", "summary_zh": "擴散模型在生成任務上表現出色，但現有的推論加速方法依賴固定降噪流程，無法根據任務難度或需求動態調整計算量。本文提出自適應推論規模調整的挑戰，並引入自適應雙向循環擴散(ABCD)框架。ABCD透過雙向擴散循環精煉輸出，並自適應控制探索深度和終止條件，包含循環擴散搜尋、自動探索-利用平衡和自適應思考時間三個部分。實驗證明，ABCD在多種任務上提升效能的同時，也保持了計算效率。", "audio": "audios/2505.14036v1.mp3", "timestamp": "2025-05-21T17:16:17.825422"}
{"query": "AI", "id": "2505.14613v1", "url": "http://arxiv.org/abs/2505.14613v1", "title": "Virtual Cells: Predict, Explain, Discover", "summary": "Drug discovery is fundamentally a process of inferring the effects of\ntreatments on patients, and would therefore benefit immensely from\ncomputational models that can reliably simulate patient responses, enabling\nresearchers to generate and test large numbers of therapeutic hypotheses safely\nand economically before initiating costly clinical trials. Even a more specific\nmodel that predicts the functional response of cells to a wide range of\nperturbations would be tremendously valuable for discovering safe and effective\ntreatments that successfully translate to the clinic. Creating such virtual\ncells has long been a goal of the computational research community that\nunfortunately remains unachieved given the daunting complexity and scale of\ncellular biology. Nevertheless, recent advances in AI, computing power, lab\nautomation, and high-throughput cellular profiling provide new opportunities\nfor reaching this goal. In this perspective, we present a vision for developing\nand evaluating virtual cells that builds on our experience at Recursion. We\nargue that in order to be a useful tool to discover novel biology, virtual\ncells must accurately predict the functional response of a cell to\nperturbations and explain how the predicted response is a consequence of\nmodifications to key biomolecular interactions. We then introduce key\nprinciples for designing therapeutically-relevant virtual cells, describe a\nlab-in-the-loop approach for generating novel insights with them, and advocate\nfor biologically-grounded benchmarks to guide virtual cell development.\nFinally, we make the case that our approach to virtual cells provides a useful\nframework for building other models at higher levels of organization, including\nvirtual patients. We hope that these directions prove useful to the research\ncommunity in developing virtual models optimized for positive impact on drug\ndiscovery outcomes.", "authors": ["Emmanuel Noutahi", "Jason Hartford", "Prudencio Tossou", "Shawn Whitfield", "Alisandra K. Denton", "Cas Wognum", "Kristina Ulicna", "Jonathan Hsu", "Michael Cuccarese", "Emmanuel Bengio", "Dominique Beaini", "Christopher Gibson", "Daniel Cohen", "Berton Earnshaw"], "published_date": "2025-05-20", "title_zh": "虛擬細胞：預測、解釋、發現", "summary_zh": "藥物發現的核心在於推斷治療對患者的影響。開發能夠可靠模擬患者反應的計算模型，將能幫助研究人員在昂貴的臨床試驗前，安全且經濟地產生和測試大量治療假設。本論文提出一個開發和評估「虛擬細胞」的願景，它能準確預測細胞對干擾的反應，並解釋其反應的生物分子機制。透過AI、運算能力、實驗室自動化和高通量細胞分析的進展，我們有機會打造出可用於發現安全有效療法的虛擬細胞，並最終建構更複雜的虛擬患者模型，從而加速藥物開發。", "audio": "audios/2505.14613v1.mp3", "timestamp": "2025-05-21T18:26:28.417346"}
{"query": "Foundation Model", "id": "2505.14100v1", "url": "http://arxiv.org/abs/2505.14100v1", "title": "Unlocking the Power of SAM 2 for Few-Shot Segmentation", "summary": "Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few\nclasses to segment arbitrary classes, but at the risk of overfitting. To\naddress this, some methods use the well-learned knowledge of foundation models\n(e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM\nby supporting video segmentation, whose class-agnostic matching ability is\nuseful to FSS. A simple idea is to encode support foreground (FG) features as\nmemory, with which query FG features are matched and fused. Unfortunately, the\nFG objects in different frames of SAM 2's video data are always the same\nidentity, while those in FSS are different identities, i.e., the matching step\nis incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo\nquery memory, matching with query features in a compatible way. However, the\nmemories can never be as accurate as the real ones, i.e., they are likely to\ncontain incomplete query FG, and some unexpected query background (BG)\nfeatures, leading to wrong segmentation. Hence, we further design Iterative\nMemory Refinement to fuse more query FG features into the memory, and devise a\nSupport-Calibrated Memory Attention to suppress the unexpected query BG\nfeatures in memory. Extensive experiments have been conducted on PASCAL-5$^i$\nand COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot\nmIoU can be 4.2\\% better than the best baseline.", "authors": ["Qianxiong Xu", "Lanyun Zhu", "Xuanyi Liu", "Guosheng Lin", "Cheng Long", "Ziyue Li", "Rui Zhao"], "published_date": "2025-05-20", "title_zh": "解鎖 SAM 2 在少樣本分割中的力量", "summary_zh": "少樣本分割的目標是學習類別無關的分割，但容易過擬合。本文利用SAM 2的影片分割能力，提出了一種新方法。核心挑戰是SAM 2訓練數據的目標在不同幀中是同一物體，而少樣本分割不是。因此，我們設計了偽提示生成器和迭代記憶體精煉，並提出了支持校準的記憶體注意力機制，以提升分割效果。實驗證明，我們的模型在PASCAL-5$^i$和COCO-20$^i$等數據集上表現優異，例如，1-shot mIoU 可以比最好的基線高出 4.2%。", "audio": "audios/2505.14100v1.mp3", "timestamp": "2025-05-21T18:26:32.856819"}
{"query": "Diffusion Model", "id": "2505.13983v1", "url": "http://arxiv.org/abs/2505.13983v1", "title": "Combining Deterministic Enhanced Conditions with Dual-Streaming Encoding for Diffusion-Based Speech Enhancement", "summary": "Diffusion-based speech enhancement (SE) models need to incorporate correct\nprior knowledge as reliable conditions to generate accurate predictions.\nHowever, providing reliable conditions using noisy features is challenging. One\nsolution is to use features enhanced by deterministic methods as conditions.\nHowever, the information distortion and loss caused by deterministic methods\nmight affect the diffusion process. In this paper, we first investigate the\neffects of using different deterministic SE models as conditions for diffusion.\nWe validate two conditions depending on whether the noisy feature was used as\npart of the condition: one using only the deterministic feature\n(deterministic-only), and the other using both deterministic and noisy features\n(deterministic-noisy). Preliminary investigation found that using deterministic\nenhanced conditions improves hearing experiences on real data, while the choice\nbetween using deterministic-only or deterministic-noisy conditions depends on\nthe deterministic models. Based on these findings, we propose a dual-streaming\nencoding Repair-Diffusion Model for SE (DERDM-SE) to more effectively utilize\nboth conditions. Moreover, we found that fine-grained deterministic models have\ngreater potential in objective evaluation metrics, while UNet-based\ndeterministic models provide more stable diffusion performance. Therefore, in\nthe DERDM-SE, we propose a deterministic model that combines coarse- and\nfine-grained processing. Experimental results on CHiME4 show that the proposed\nmodels effectively leverage deterministic models to achieve better SE\nevaluation scores, along with more stable performance compared to other\ndiffusion-based SE models.", "authors": ["Hao Shi", "Xugang Lu", "Kazuki Shimada", "Tatsuya Kawahara"], "published_date": "2025-05-20", "title_zh": "結合確定性增強條件與雙流編碼的基於擴散的語音增強", "summary_zh": "為了讓基於擴散的語音增強模型更準確，需要提供可靠的先驗知識作為條件。本研究探討使用確定性方法增強的特徵作為條件的效果，並發現加入確定性增強條件可以改善聽覺體驗。基於這些發現，我們提出了一種雙流編碼的修復擴散模型（DERDM-SE），能更有效地利用確定性增強條件。實驗結果表明，我們提出的模型能有效利用確定性模型，在語音增強評估中獲得更好的分數，且相較於其他基於擴散的語音增強模型，表現更穩定。", "audio": "audios/2505.13983v1.mp3", "timestamp": "2025-05-21T18:26:39.178977"}
{"query": "AI", "id": "2505.14612v1", "url": "http://arxiv.org/abs/2505.14612v1", "title": "AI Agents in the Electricity Market Game with Cryptocurrency Transactions: A Post-Terminator Analysis", "summary": "This paper extends (Spear 2003) by replacing human agents with artificial\nintelligence (AI) entities that derive utility solely from electricity\nconsumption. These AI agents must prepay for electricity using cryptocurrency\nand the verification of these transactions requires a fixed amount of\nelectricity. As a result the agents must strategically allocate electricity\nresources between consumption and payment verification. This paper analyzes the\nequilibrium outcomes of such a system and discusses the implications of\nAI-driven energy markets.", "authors": ["Microsoft Copilot", "Stephen E. Spear"], "published_date": "2025-05-20", "title_zh": "基於加密貨幣交易的電力市場博弈中的AI代理：後終結者時代分析", "summary_zh": "本研究將既有電力市場模型中的人類代理替換為完全依賴電力消費獲取效用的AI實體。這些AI代理必須使用加密貨幣預付電費，而交易驗證需要消耗一定量的電力。因此，AI代理需要在消費和支付驗證之間策略性地分配電力資源。本研究分析了該系統的均衡結果，並探討了AI驅動能源市場的影響。", "audio": "audios/2505.14612v1.mp3", "timestamp": "2025-05-21T19:14:36.343567"}
{"query": "Foundation Model", "id": "2505.14088v1", "url": "http://arxiv.org/abs/2505.14088v1", "title": "Generalizable Multispectral Land Cover Classification via Frequency-Aware Mixture of Low-Rank Token Experts", "summary": "We introduce Land-MoE, a novel approach for multispectral land cover\nclassification (MLCC). Spectral shift, which emerges from disparities in\nsensors and geospatial conditions, poses a significant challenge in this\ndomain. Existing methods predominantly rely on domain adaptation and\ngeneralization strategies, often utilizing small-scale models that exhibit\nlimited performance. In contrast, Land-MoE addresses these issues by\nhierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts,\nto fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner.\nSpecifically, Land-MoE comprises two key modules: the mixture of low-rank token\nexperts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages\nrank-differentiated tokens to generate diverse feature adjustments for\nindividual instances within multispectral images. By dynamically combining\nlearnable low-rank token experts of varying ranks, it enhances the robustness\nagainst spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on\nthe refined features. This process enables the model to effectively capture\nfrequency band information that is strongly correlated with semantic essence,\nwhile simultaneously suppressing frequency noise irrelevant to the task.\nComprehensive experiments on MLCC tasks involving cross-sensor and\ncross-geospatial setups demonstrate that Land-MoE outperforms existing methods\nby a large margin. Additionally, the proposed approach has also achieved\nstate-of-the-art performance in domain generalization semantic segmentation\ntasks of RGB remote sensing images.", "authors": ["Xi Chen", "Shen Yan", "Juelin Zhu", "Chen Chen", "Yu Liu", "Maojun Zhang"], "published_date": "2025-05-20", "title_zh": "基於頻率感知的低秩Token專家混合模型實現廣義多光譜地表覆蓋分類", "summary_zh": "Land-MoE 是一種新的多光譜地表覆蓋分類方法，旨在解決因傳感器和地理條件差異導致的光譜偏移問題。它通過層次化地插入一個頻率感知的低秩 Token 專家混合模型，以參數高效的方式微調視覺基礎模型。該方法包含低秩 Token 專家混合（MoLTE）和頻率感知濾波器（FAF）兩個關鍵模塊。MoLTE 利用不同秩的 Token 為多光譜圖像中的每個實例生成不同的特徵調整，增強了對光譜偏移的魯棒性。FAF 則對精煉後的特徵進行頻域調製，有效捕捉與語義本質密切相關的頻帶信息，同時抑制與任務無關的頻率噪聲。實驗結果表明，Land-MoE 在跨傳感器和跨地理空間的多光譜地表覆蓋分類任務中顯著優於現有方法，並在 RGB 遙感圖像的領域泛化語義分割任務中也取得了最先進的性能。", "audio": "audios/2505.14088v1.mp3", "timestamp": "2025-05-21T19:14:45.882789"}
{"query": "Diffusion Model", "id": "2505.13919v1", "url": "http://arxiv.org/abs/2505.13919v1", "title": "Predicting Dynamical Systems across Environments via Diffusive Model Weight Generation", "summary": "Data-driven methods offer an effective equation-free solution for predicting\nphysical dynamics. However, the same physical system can exhibit significantly\ndifferent dynamic behaviors in various environments. This causes prediction\nfunctions trained for specific environments to fail when transferred to unseen\nenvironments. Therefore, cross-environment prediction requires modeling the\ndynamic functions of different environments. In this work, we propose a model\nweight generation method, \\texttt{EnvAd-Diff}. \\texttt{EnvAd-Diff} operates in\nthe weight space of the dynamic function, generating suitable weights from\nscratch based on environmental condition for zero-shot prediction.\nSpecifically, we first train expert prediction functions on dynamic\ntrajectories from a limited set of visible environments to create a model zoo,\nthereby constructing sample pairs of prediction function weights and their\ncorresponding environments. Subsequently, we train a latent space diffusion\nmodel conditioned on the environment to model the joint distribution of weights\nand environments. Considering the lack of environmental prior knowledge in\nreal-world scenarios, we propose a physics-informed surrogate label to\ndistinguish different environments. Generalization experiments across multiple\nsystems demonstrate that a 1M parameter prediction function generated by\n\\texttt{EnvAd-Diff} outperforms a pre-trained 500M parameter foundation model.", "authors": ["Ruikun Li", "Huandong Wang", "Jingtao Ding", "Yuan Yuan", "Qingmin Liao", "Yong Li"], "published_date": "2025-05-20", "title_zh": "透過擴散模型權重生成跨環境預測動態系統", "summary_zh": "現有的資料驅動方法能有效預測物理動態，但在不同環境下，同一系統可能展現截然不同的行為，導致特定環境訓練的模型在未見環境失效。為解決跨環境預測問題，我們提出EnvAd-Diff，在動態函數的權重空間中，根據環境條件從頭生成合適的權重，以實現零樣本預測。我們訓練一系列基於可見環境數據的專家模型，建立模型庫，並以此構建權重和環境的樣本對。接著，我們訓練一個以環境為條件的潛空間擴散模型，學習權重和環境的聯合分佈。針對真實環境缺乏先驗知識的問題，我們提出物理資訊代理標籤來區分不同環境。實驗結果表明，EnvAd-Diff生成的100萬參數預測函數優於預訓練的5億參數基礎模型。", "audio": "audios/2505.13919v1.mp3", "timestamp": "2025-05-21T19:14:51.897428"}
{"query": "AI", "id": "2505.14588v1", "url": "http://arxiv.org/abs/2505.14588v1", "title": "Generative AI at the Crossroads: Light Bulb, Dynamo, or Microscope?", "summary": "With the advent of generative AI (genAI), the potential scope of artificial\nintelligence has increased dramatically, but the future effect of genAI on\nproductivity remains uncertain with the effect of the technology on the\ninnovation process a crucial open question. Some labor-saving innovations, such\nas the light bulb, temporarily raise productivity growth as adoption spreads,\nbut the effect fades when the market is saturated; that is, the level of output\nper hour is permanently higher but the growth rate is not. In contrast, two\ntypes of technologies stand out as having longer-lived effects on productivity\ngrowth. First, there are technologies known as general-purpose technologies\n(GPTs). GPTs are (1) widely adopted, (2) spur abundant knock-on innovations\n(new goods and services, process efficiencies, and business reorganization),\nand (3) improve continuously, refreshing this innovation cycle; the electric\ndynamo is an example. Second, there are inventions of methods of invention\n(IMIs). IMIs increase the efficiency of the research and development process,\ngenerating new ideas more quickly and cheaply; the compound microscope is an\nexample. We show that GenAI has the characteristics of both a GPT and an IMI --\nan encouraging sign. Even so, for genAI to boost productivity growth, its\ncontribution will have to exceed the fading growth effects of past IT\ninnovations baked into the trend, including predecessor AI technologies.", "authors": ["Martin Baily", "David Byrne", "Aidan Kane", "Paul Soto"], "published_date": "2025-05-20", "title_zh": "生成式人工智慧在十字路口：燈泡、發電機，還是顯微鏡？", "summary_zh": "生成式人工智慧（GenAI）的出現大幅拓展了人工智慧的潛力範圍，但其對生產力的未來影響仍不明朗，對創新過程的影響尤其關鍵。論文探討GenAI是否像燈泡一樣，僅短暫提高生產力後趨於平緩；還是像通用技術（GPT）如發電機，持續推動創新；又或者像發明方法的發明（IMI）如顯微鏡，加速研發進程。研究發現GenAI兼具GPT和IMI的特性，這是個鼓舞人心的跡象。然而，要提升生產力增長，GenAI的貢獻必須超越過去IT創新（包括先前的AI技術）所帶來的趨勢衰退效應。", "audio": "audios/2505.14588v1.mp3", "timestamp": "2025-05-21T20:20:15.738409"}
{"query": "Foundation Model", "id": "2505.14042v1", "url": "http://arxiv.org/abs/2505.14042v1", "title": "Adversarially Pretrained Transformers may be Universally Robust In-Context Learners", "summary": "Adversarial training is one of the most effective adversarial defenses, but\nit incurs a high computational cost. In this study, we show that transformers\nadversarially pretrained on diverse tasks can serve as robust foundation models\nand eliminate the need for adversarial training in downstream tasks.\nSpecifically, we theoretically demonstrate that through in-context learning, a\nsingle adversarially pretrained transformer can robustly generalize to multiple\nunseen tasks without any additional training, i.e., without any parameter\nupdates. This robustness stems from the model's focus on robust features and\nits resistance to attacks that exploit non-predictive features. Besides these\npositive findings, we also identify several limitations. Under certain\nconditions (though unrealistic), no universally robust single-layer\ntransformers exist. Moreover, robust transformers exhibit an\naccuracy--robustness trade-off and require a large number of in-context\ndemonstrations. The code is available at\nhttps://github.com/s-kumano/universally-robust-in-context-learner.", "authors": ["Soichiro Kumano", "Hiroshi Kera", "Toshihiko Yamasaki"], "published_date": "2025-05-20", "title_zh": "對抗式預訓練的變換器可能成為通用的穩健上下文學習器", "summary_zh": "對抗式訓練是有效的防禦手段，但成本高昂。本研究發現，在多樣化任務上進行對抗式預訓練的變換器，可作為穩健的基礎模型，無需在下游任務中額外進行對抗式訓練。理論上證明，透過上下文學習，單一對抗式預訓練的變換器，可以在沒有任何參數更新的情況下，穩健地泛化到多個未見過的任務。這種穩健性來自模型對穩健特徵的關注以及對利用非預測性特徵攻擊的抵抗力。研究也指出一些限制，例如在特定條件下，不存在通用的穩健單層變換器，且穩健的變換器需要在準確性和穩健性之間權衡，並需要大量的上下文示範。", "audio": "audios/2505.14042v1.mp3", "timestamp": "2025-05-21T20:20:22.156616"}
{"query": "Diffusion Model", "id": "2505.13843v1", "url": "http://arxiv.org/abs/2505.13843v1", "title": "A Semantic Information-based Hierarchical Speech Enhancement Method Using Factorized Codec and Diffusion Model", "summary": "Most current speech enhancement (SE) methods recover clean speech from noisy\ninputs by directly estimating time-frequency masks or spectrums. However, these\napproaches often neglect the distinct attributes, such as semantic content and\nacoustic details, inherent in speech signals, which can hinder performance in\ndownstream tasks. Moreover, their effectiveness tends to degrade in complex\nacoustic environments. To overcome these challenges, we propose a novel,\nsemantic information-based, step-by-step factorized SE method using factorized\ncodec and diffusion model. Unlike traditional SE methods, our hierarchical\nmodeling of semantic and acoustic attributes enables more robust clean speech\nrecovery, particularly in challenging acoustic scenarios. Moreover, this method\noffers further advantages for downstream TTS tasks. Experimental results\ndemonstrate that our algorithm not only outperforms SOTA baselines in terms of\nspeech quality but also enhances TTS performance in noisy environments.", "authors": ["Yang Xiang", "Canan Huang", "Desheng Hu", "Jingguang Tian", "Xinhui Hu", "Chao Zhang"], "published_date": "2025-05-20", "title_zh": "基於語義訊息的階層式語音增強方法，採用分解式編解碼器與擴散模型", "summary_zh": "現有的語音增強方法多半直接從嘈雜的訊號中估計時頻遮罩或頻譜來還原乾淨語音，忽略了語音訊號中獨特的語義內容和聲學細節，這會影響後續任務的表現。我們提出一種新的方法，利用分解式編解碼器和擴散模型，分階段、以語義訊息為基礎進行語音增強。這種階層式建模方式更能還原乾淨語音，尤其是在複雜的聲學環境中，而且還能提升後續的語音合成任務表現。實驗結果顯示，我們的演算法不僅在語音品質上優於現有方法，還能在嘈雜的環境中提升語音合成的性能。", "audio": "audios/2505.13843v1.mp3", "timestamp": "2025-05-21T20:20:27.423795"}
{"query": "AI", "id": "2505.14585v1", "url": "http://arxiv.org/abs/2505.14585v1", "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning", "summary": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +17.64% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively.", "authors": ["Wenbin Hu", "Haoran Li", "Huihao Jing", "Qi Hu", "Ziqian Zeng", "Sirui Han", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "published_date": "2025-05-20", "title_zh": "Context Reasoner：透過強化學習激勵情境化隱私與安全合規的推理能力", "summary_zh": "大型語言模型雖強大，卻也帶來安全和隱私風險。現有防護策略往往犧牲情境推理能力，過度依賴敏感模式匹配，且忽略現有的安全和隱私標準。本研究將安全和隱私問題轉化為情境化合規問題，並以情境完整性理論為基礎，對齊GDPR、歐盟AI法案和HIPAA等規範。我們利用強化學習，以規則為基礎的獎勵來激勵模型的情境推理能力，同時提升安全和隱私合規性。實驗證明，此方法不僅顯著提高法律合規性，更增強了一般的推理能力。", "audio": "audios/2505.14585v1.mp3", "timestamp": "2025-05-21T21:17:12.423871"}
{"query": "Foundation Model", "id": "2505.13840v1", "url": "http://arxiv.org/abs/2505.13840v1", "title": "EfficientLLM: Efficiency in Large Language Models", "summary": "Large Language Models (LLMs) have driven significant progress, yet their\ngrowing parameter counts and context windows incur prohibitive compute, energy,\nand monetary costs. We introduce EfficientLLM, a novel benchmark and the first\ncomprehensive empirical study evaluating efficiency techniques for LLMs at\nscale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our\nstudy systematically explores three key axes: (1) architecture pretraining\n(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts\n(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and\n(3) inference (quantization methods: int4, float16). We define six fine-grained\nmetrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy\nConsumption, Compression Rate) to capture hardware saturation,\nlatency-throughput balance, and carbon cost. Evaluating over 100\nmodel-technique pairs (0.5B-72B parameters), we derive three core insights: (i)\nEfficiency involves quantifiable trade-offs: no single method is universally\noptimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by\n40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%\naccuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal\nmemory-latency trade-offs for constrained devices, MLA achieves lowest\nperplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency\nonly beyond 14B parameters. (iii) Techniques generalize across modalities: we\nextend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and\nVision-Language Models (Qwen2.5-VL), confirming effective transferability. By\nopen-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM\nprovides essential guidance for researchers and engineers navigating the\nefficiency-performance landscape of next-generation foundation models.", "authors": ["Zhengqing Yuan", "Weixiang Sun", "Yixin Liu", "Huichi Zhou", "Rong Zhou", "Yiyang Li", "Zheyuan Zhang", "Wei Song", "Yue Huang", "Haolong Jia", "Keerthiram Murugesan", "Yu Wang", "Lifang He", "Jianfeng Gao", "Lichao Sun", "Yanfang Ye"], "published_date": "2025-05-20", "title_zh": "高效LLM：大型語言模型效率研究", "summary_zh": "大型語言模型雖然帶來突破，但參數量和上下文窗口的增長導致成本過高。本文推出EfficientLLM基準測試，對大型語言模型的效率提升技術進行大規模評估。研究涵蓋架構預訓練、微調和推理三個主要方向，並定義了六個細粒度指標來衡量硬體利用率、延遲吞吐量平衡和碳排放成本。研究發現，效率提升涉及權衡取捨，沒有萬能方法；最佳方案取決於任務和模型規模；這些技術也能推廣到視覺和視覺語言模型上。EfficientLLM開放資料集和評估流程，為研究者和工程師在下一代基礎模型的效率與性能之間做出選擇提供指引。", "audio": "audios/2505.13840v1.mp3", "timestamp": "2025-05-21T21:17:21.216998"}
{"query": "Diffusion Model", "id": "2505.13791v1", "url": "http://arxiv.org/abs/2505.13791v1", "title": "Scalable Autoregressive 3D Molecule Generation", "summary": "Generative models of 3D molecular structure play a rapidly growing role in\nthe design and simulation of molecules. Diffusion models currently dominate the\nspace of 3D molecule generation, while autoregressive models have trailed\nbehind. In this work, we present Quetzal, a simple but scalable autoregressive\nmodel that builds molecules atom-by-atom in 3D. Treating each molecule as an\nordered sequence of atoms, Quetzal combines a causal transformer that predicts\nthe next atom's discrete type with a smaller Diffusion MLP that models the\ncontinuous next-position distribution. Compared to existing autoregressive\nbaselines, Quetzal achieves substantial improvements in generation quality and\nis competitive with the performance of state-of-the-art diffusion models. In\naddition, by reducing the number of expensive forward passes through a dense\ntransformer, Quetzal enables significantly faster generation speed, as well as\nexact divergence-based likelihood computation. Finally, without any\narchitectural changes, Quetzal natively handles variable-size tasks like\nhydrogen decoration and scaffold completion. We hope that our work motivates a\nperspective on scalability and generality for generative modelling of 3D\nmolecules.", "authors": ["Austin H. Cheng", "Chong Sun", "Alán Aspuru-Guzik"], "published_date": "2025-05-20", "title_zh": "可擴展的自迴歸三維分子生成", "summary_zh": "分子結構的生成模型在分子設計和模擬中扮演著越來越重要的角色。目前擴散模型主導了三維分子生成領域，而自迴歸模型則相對落後。這篇論文介紹了 Quetzal，一個簡單但可擴展的自迴歸模型，能夠以原子為單位逐個構建三維分子。Quetzal 將每個分子視為一個有序的原子序列，結合了一個預測下一個原子離散類型的因果轉換器，以及一個較小的擴散 MLP 模型，用於模擬下一個位置的連續分佈。Quetzal 在生成品質上顯著優於現有的自迴歸模型，並且性能可與最先進的擴散模型相媲美。此外，Quetzal 透過減少密集轉換器的前向傳播次數，顯著提高了生成速度，並且能夠進行基於散度的精確似然計算。最後，Quetzal 在不改變架構的情況下，可以自然地處理氫原子裝飾和骨架補全等可變大小的任務。這項工作旨在激發人們對三維分子生成模型的可擴展性和通用性的思考。", "audio": "audios/2505.13791v1.mp3", "timestamp": "2025-05-21T21:17:51.909900"}
{"query": "AI", "id": "2505.14569v1", "url": "http://arxiv.org/abs/2505.14569v1", "title": "Agent Context Protocols Enhance Collective Inference", "summary": "AI agents have become increasingly adept at complex tasks such as coding,\nreasoning, and multimodal understanding. However, building generalist systems\nrequires moving beyond individual agents to collective inference -- a paradigm\nwhere multi-agent systems with diverse, task-specialized agents complement one\nanother through structured communication and collaboration. Today, coordination\nis usually handled with imprecise, ad-hoc natural language, which limits\ncomplex interaction and hinders interoperability with domain-specific agents.\nWe introduce Agent context protocols (ACPs): a domain- and agent-agnostic\nfamily of structured protocols for agent-agent communication, coordination, and\nerror handling. ACPs combine (i) persistent execution blueprints -- explicit\ndependency graphs that store intermediate agent outputs -- with (ii)\nstandardized message schemas, enabling robust and fault-tolerant multi-agent\ncollective inference. ACP-powered generalist systems reach state-of-the-art\nperformance: 28.3 % accuracy on AssistantBench for long-horizon web assistance\nand best-in-class multimodal technical reports, outperforming commercial AI\nsystems in human evaluation. ACPs are highly modular and extensible, allowing\npractitioners to build top-tier generalist agents quickly.", "authors": ["Devansh Bhardwaj", "Arjun Beniwal", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Karthik R. Narasimhan", "Ameet Deshpande", "Vishvak Murahari"], "published_date": "2025-05-20", "title_zh": "代理上下文協議增強集體推理", "summary_zh": "AI 代理在編碼、推理和多模態理解等複雜任務中越來越強大。要打造通用系統，需要超越單個代理，轉向集體推理——一種多代理系統，其中不同且專精於特定任務的代理通過結構化的溝通和協作來互補。目前，協調通常依賴於不精確、臨時性的自然語言，這限制了複雜的互動，也阻礙了與特定領域代理的互操作性。為此，我們引入代理上下文協議 (ACPs)：一種領域和代理無關的結構化協議族，用於代理之間的通訊、協調和錯誤處理。ACPs 結合了 (i) 持續的執行藍圖 (顯式的依賴關係圖，用於儲存中間代理的輸出)，以及 (ii) 標準化的訊息架構，從而實現穩健且容錯的多代理集體推理。使用 ACPs 的通用系統達到了最先進的性能：在 AssistantBench 上，針對長週期網絡助手達到 28.3% 的準確率，並且在多模態技術報告方面表現最佳，在人類評估中優於商業 AI 系統。ACPs 具有高度的模組化和可擴展性，使從業者能夠快速構建頂級的通用代理。", "audio": "audios/2505.14569v1.mp3", "timestamp": "2025-05-21T22:17:50.906228"}
{"query": "Foundation Model", "id": "2505.13755v1", "url": "http://arxiv.org/abs/2505.13755v1", "title": "Panda: A pretrained forecast model for universal representation of chaotic dynamics", "summary": "Chaotic systems are intrinsically sensitive to small errors, challenging\nefforts to construct predictive data-driven models of real-world dynamical\nsystems such as fluid flows or neuronal activity. Prior efforts comprise either\nspecialized models trained separately on individual time series, or foundation\nmodels trained on vast time series databases with little underlying dynamical\nstructure. Motivated by dynamical systems theory, we present Panda, Patched\nAttention for Nonlinear DynAmics. We train Panda on a novel synthetic,\nextensible dataset of $2 \\times 10^4$ chaotic dynamical systems that we\ndiscover using an evolutionary algorithm. Trained purely on simulated data,\nPanda exhibits emergent properties: zero-shot forecasting of unseen real world\nchaotic systems, and nonlinear resonance patterns in cross-channel attention\nheads. Despite having been trained only on low-dimensional ordinary\ndifferential equations, Panda spontaneously develops the ability to predict\npartial differential equations without retraining. We demonstrate a neural\nscaling law for differential equations, underscoring the potential of\npretrained models for probing abstract mathematical domains like nonlinear\ndynamics.", "authors": ["Jeffrey Lai", "Anthony Bao", "William Gilpin"], "published_date": "2025-05-19", "title_zh": "Panda：用於混沌動力學通用表示的預訓練預測模型", "summary_zh": "混沌系統對微小誤差極為敏感，這對構建預測性的資料驅動模型構成挑戰。本研究提出 Panda 模型，利用演化算法發現的大量合成混沌動力學系統進行訓練。Panda 展現出驚人的能力，例如零樣本預測未見過的真實世界混沌系統，以及在跨通道注意力頭中呈現非線性共振模式。更令人驚訝的是，Panda 在僅接受低維常微分方程訓練後，無需重新訓練即可預測偏微分方程。研究還展示了微分方程的神經縮放定律，突顯了預訓練模型在探索非線性動力學等抽象數學領域的潛力。", "audio": "audios/2505.13755v1.mp3", "timestamp": "2025-05-21T22:18:10.039919"}
{"query": "Diffusion Model", "id": "2505.13771v1", "url": "http://arxiv.org/abs/2505.13771v1", "title": "Score-Based Training for Energy-Based TTS Models", "summary": "Noise contrastive estimation (NCE) is a popular method for training\nenergy-based models (EBM) with intractable normalisation terms. The key idea of\nNCE is to learn by comparing unnormalised log-likelihoods of the reference and\nnoisy samples, thus avoiding explicitly computing normalisation terms. However,\nNCE critically relies on the quality of noisy samples. Recently, sliced score\nmatching (SSM) has been popularised by closely related diffusion models (DM).\nUnlike NCE, SSM learns a gradient of log-likelihood, or score, by learning\ndistribution of its projections on randomly chosen directions. However, both\nNCE and SSM disregard the form of log-likelihood function, which is problematic\ngiven that EBMs and DMs make use of first-order optimisation during inference.\nThis paper proposes a new criterion that learns scores more suitable for\nfirst-order schemes. Experiments contrasts these approaches for training EBMs.", "authors": ["Wanli Sun", "Anton Ragni"], "published_date": "2025-05-19", "title_zh": "基於分數的能量型語音合成模型訓練", "summary_zh": "這篇論文探討能量型語音合成模型的訓練方法。能量型模型訓練的一大挑戰是難以計算歸一化項。論文比較了噪音對比估計 (NCE) 和分片分數匹配 (SSM) 等方法，前者透過比較真實和噪音樣本的未歸一化對數似然來學習，後者則學習對數似然梯度的投影分佈。然而，這些方法忽略了對數似然函數的形式，這在推理時使用一階最佳化的能量型模型中是個問題。因此，論文提出了一種新的訓練準則，旨在學習更適合一階方法的梯度，並通過實驗比較不同方法在能量型模型訓練上的效果。", "audio": "audios/2505.13771v1.mp3", "timestamp": "2025-05-21T22:18:28.330206"}
{"query": "AI", "id": "2505.14539v1", "url": "http://arxiv.org/abs/2505.14539v1", "title": "A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)", "summary": "In this work, we present the first general logic of attention. Attention is a\npowerful cognitive ability that allows agents to focus on potentially complex\ninformation, such as logically structured propositions, higher-order beliefs,\nor what other agents pay attention to. This ability is a strength, as it helps\nto ignore what is irrelevant, but it can also introduce biases when some types\nof information or agents are systematically ignored. Existing dynamic epistemic\nlogics for attention cannot model such complex attention scenarios, as they\nonly model attention to atomic formulas. Additionally, such logics quickly\nbecome cumbersome, as their size grows exponentially in the number of agents\nand announced literals. Here, we introduce a logic that overcomes both\nlimitations. First, we generalize edge-conditioned event models, which we show\nto be as expressive as standard event models yet exponentially more succinct\n(generalizing both standard event models and generalized arrow updates).\nSecond, we extend attention to arbitrary formulas, allowing agents to also\nattend to other agents' beliefs or attention. Our work treats attention as a\nmodality, like belief or awareness. We introduce attention principles that\nimpose closure properties on that modality and that can be used in its\naxiomatization. Throughout, we illustrate our framework with examples of AI\nagents reasoning about human attentional biases, demonstrating how such agents\ncan discover attentional biases.", "authors": ["Gaia Belardinelli", "Thomas Bolander", "Sebastian Watzl"], "published_date": "2025-05-20", "title_zh": "基於邊緣條件事件模型的一般注意力邏輯（擴展版本）", "summary_zh": "這篇論文提出了一種全新的、更通用化的注意力邏輯。傳統的注意力模型只能處理簡單的原子公式，而這個新邏輯可以處理更複雜的資訊，例如邏輯命題、高階信念，甚至是其他人的注意力。為了實現這一點，研究人員使用了邊緣條件事件模型，這種模型更簡潔，表達能力也更強。這個新邏輯將注意力視為一種模態，就像信念或意識一樣，並且可以捕捉人類注意力偏差的特性。文章用AI智能體推理人類注意力偏差的例子，展示了這個框架的應用。", "audio": "audios/2505.14539v1.mp3", "timestamp": "2025-05-21T23:17:25.202342"}
{"query": "Foundation Model", "id": "2505.13099v2", "url": "http://arxiv.org/abs/2505.13099v2", "title": "Industrial Synthetic Segment Pre-training", "summary": "Pre-training on real-image datasets has been widely proven effective for\nimproving instance segmentation. However, industrial applications face two key\nchallenges: (1) legal and ethical restrictions, such as ImageNet's prohibition\nof commercial use, and (2) limited transferability due to the domain gap\nbetween web images and industrial imagery. Even recent vision foundation\nmodels, including the segment anything model (SAM), show notable performance\ndegradation in industrial settings. These challenges raise critical questions:\nCan we build a vision foundation model for industrial applications without\nrelying on real images or manual annotations? And can such models outperform\neven fine-tuned SAM on industrial datasets? To address these questions, we\npropose the Instance Core Segmentation Dataset (InsCore), a synthetic\npre-training dataset based on formula-driven supervised learning (FDSL).\nInsCore generates fully annotated instance segmentation images that reflect key\ncharacteristics of industrial data, including complex occlusions, dense\nhierarchical masks, and diverse non-rigid shapes, distinct from typical web\nimagery. Unlike previous methods, InsCore requires neither real images nor\nhuman annotations. Experiments on five industrial datasets show that models\npre-trained with InsCore outperform those trained on COCO and ImageNet-21k, as\nwell as fine-tuned SAM, achieving an average improvement of 6.2 points in\ninstance segmentation performance. This result is achieved using only 100k\nsynthetic images, more than 100 times fewer than the 11 million images in SAM's\nSA-1B dataset, demonstrating the data efficiency of our approach. These\nfindings position InsCore as a practical and license-free vision foundation\nmodel for industrial applications.", "authors": ["Shinichi Mae", "Ryousuke Yamada", "Hirokatsu Kataoka"], "published_date": "2025-05-19", "title_zh": "工業合成分割預訓練", "summary_zh": "現有的圖像分割模型在工業應用中面臨授權限制和領域差異的挑戰。本研究提出一個基於公式驅動的合成數據集InsCore，無需真實圖像或人工標注，即可預訓練出專為工業應用設計的圖像分割模型。實驗證明，用InsCore預訓練的模型在多個工業數據集上的表現，優於用COCO和ImageNet預訓練的模型，甚至超越了微調後的SAM模型，且只需少量合成數據就能達到很好的效果。InsCore為工業應用提供了一個實用且無授權問題的視覺基礎模型。", "audio": "audios/2505.13099v2.mp3", "timestamp": "2025-05-21T23:17:30.656713"}
{"query": "Diffusion Model", "id": "2505.13740v1", "url": "http://arxiv.org/abs/2505.13740v1", "title": "Improving Compositional Generation with Diffusion Models Using Lift Scores", "summary": "We introduce a novel resampling criterion using lift scores, for improving\ncompositional generation in diffusion models. By leveraging the lift scores, we\nevaluate whether generated samples align with each single condition and then\ncompose the results to determine whether the composed prompt is satisfied. Our\nkey insight is that lift scores can be efficiently approximated using only the\noriginal diffusion model, requiring no additional training or external modules.\nWe develop an optimized variant that achieves relatively lower computational\noverhead during inference while maintaining effectiveness. Through extensive\nexperiments, we demonstrate that lift scores significantly improved the\ncondition alignment for compositional generation across 2D synthetic data,\nCLEVR position tasks, and text-to-image synthesis. Our code is available at\nhttp://github.com/rainorangelemon/complift.", "authors": ["Chenning Yu", "Sicun Gao"], "published_date": "2025-05-19", "title_zh": "使用Lift分數改進擴散模型中的組合生成", "summary_zh": "本研究提出一種新的重採樣標準，利用Lift分數來提升擴散模型的組合生成能力。透過Lift分數評估生成樣本與各個條件的對齊程度，進而判斷組合提示詞是否被滿足。此方法僅需原始擴散模型即可有效近似計算Lift分數，無需額外訓練或模組。實驗證明，Lift分數能顯著改善二維合成數據、CLEVR位置任務和文本生成圖像等場景中的條件對齊效果。", "audio": "audios/2505.13740v1.mp3", "timestamp": "2025-05-21T23:17:37.788568"}
{"query": "AI", "id": "2505.15811v1", "url": "http://arxiv.org/abs/2505.15811v1", "title": "On the creation of narrow AI: hierarchy and nonlocality of neural network skills", "summary": "We study the problem of creating strong, yet narrow, AI systems. While recent\nAI progress has been driven by the training of large general-purpose foundation\nmodels, the creation of smaller models specialized for narrow domains could be\nvaluable for both efficiency and safety. In this work, we explore two\nchallenges involved in creating such systems, having to do with basic\nproperties of how neural networks learn and structure their representations.\nThe first challenge regards when it is possible to train narrow models from\nscratch. Through experiments on a synthetic task, we find that it is sometimes\nnecessary to train networks on a wide distribution of data to learn certain\nnarrow skills within that distribution. This effect arises when skills depend\non each other hierarchically, and training on a broad distribution introduces a\ncurriculum which substantially accelerates learning. The second challenge\nregards how to transfer particular skills from large general models into small\nspecialized models. We find that model skills are often not perfectly localized\nto a particular set of prunable components. However, we find that methods based\non pruning can still outperform distillation. We investigate the use of a\nregularization objective to align desired skills with prunable components while\nunlearning unnecessary skills.", "authors": ["Eric J. Michaud", "Asher Parker-Sartori", "Max Tegmark"], "published_date": "2025-05-21", "title_zh": "關於窄人工智慧的創建：神經網路技能的層次性和非局部性", "summary_zh": "本文探討如何創建強大但專精於特定領域的窄人工智慧系統。研究發現，從頭訓練窄領域模型有時需要先在大範圍的數據上訓練，以建立技能間的層次結構，加速學習。此外，將大型通用模型的技能轉移到小型專精模型時，技能並非總是完美地集中在可修剪的組件上。儘管如此，修剪方法仍優於知識蒸餾。研究進一步探討如何透過正則化，將期望的技能與可修剪的組件對齊，同時移除不必要的技能。", "audio": "audios/2505.15811v1.mp3", "timestamp": "2025-05-22T03:10:54.678837"}
{"query": "Foundation Model", "id": "2505.15809v1", "url": "http://arxiv.org/abs/2505.15809v1", "title": "MMaDA: Multimodal Large Diffusion Language Models", "summary": "We introduce MMaDA, a novel class of multimodal diffusion foundation models\ndesigned to achieve superior performance across diverse domains such as textual\nreasoning, multimodal understanding, and text-to-image generation. The approach\nis distinguished by three key innovations: (i) MMaDA adopts a unified diffusion\narchitecture with a shared probabilistic formulation and a modality-agnostic\ndesign, eliminating the need for modality-specific components. This\narchitecture ensures seamless integration and processing across different data\ntypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning\nstrategy that curates a unified CoT format across modalities. By aligning\nreasoning processes between textual and visual domains, this strategy\nfacilitates cold-start training for the final reinforcement learning (RL)\nstage, thereby enhancing the model's ability to handle complex tasks from the\noutset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm\nspecifically tailored for diffusion foundation models. Utilizing diversified\nreward modeling, UniGRPO unifies post-training across both reasoning and\ngeneration tasks, ensuring consistent performance improvements. Experimental\nresults demonstrate that MMaDA-8B exhibits strong generalization capabilities\nas a unified multimodal foundation model. It surpasses powerful models like\nLLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in\nmultimodal understanding, and excels over SDXL and Janus in text-to-image\ngeneration. These achievements highlight MMaDA's effectiveness in bridging the\ngap between pretraining and post-training within unified diffusion\narchitectures, providing a comprehensive framework for future research and\ndevelopment. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA", "authors": ["Ling Yang", "Ye Tian", "Bowen Li", "Xinchen Zhang", "Ke Shen", "Yunhai Tong", "Mengdi Wang"], "published_date": "2025-05-21", "title_zh": "MMaDA：多模態大型擴散語言模型", "summary_zh": "MMaDA是一種新的多模態擴散模型，旨在提升文字推理、多模態理解和文字生成圖像等領域的表現。它採用統一的擴散架構，無需針對特定模態的元件。透過混合式的長鏈思考微調策略，統一不同模態的推理過程。更提出UniGRPO，一種基於策略梯度的強化學習演算法，適用於擴散模型，能統一推理和生成任務的後訓練。實驗結果顯示，MMaDA在文字推理、多模態理解和文字生成圖像方面都超越了其他模型，證明其作為統一多模態模型的強大泛化能力。", "audio": "audios/2505.15809v1.mp3", "timestamp": "2025-05-22T03:10:59.380909"}
{"query": "Diffusion Model", "id": "2505.15812v1", "url": "http://arxiv.org/abs/2505.15812v1", "title": "Leveraging the Powerful Attention of a Pre-trained Diffusion Model for Exemplar-based Image Colorization", "summary": "Exemplar-based image colorization aims to colorize a grayscale image using a\nreference color image, ensuring that reference colors are applied to\ncorresponding input regions based on their semantic similarity. To achieve\naccurate semantic matching between regions, we leverage the self-attention\nmodule of a pre-trained diffusion model, which is trained on a large dataset\nand exhibits powerful attention capabilities. To harness this power, we propose\na novel, fine-tuning-free approach based on a pre-trained diffusion model,\nmaking two key contributions. First, we introduce dual attention-guided color\ntransfer. We utilize the self-attention module to compute an attention map\nbetween the input and reference images, effectively capturing semantic\ncorrespondences. The color features from the reference image is then\ntransferred to the semantically matching regions of the input image, guided by\nthis attention map, and finally, the grayscale features are replaced with the\ncorresponding color features. Notably, we utilize dual attention to calculate\nattention maps separately for the grayscale and color images, achieving more\nprecise semantic alignment. Second, we propose classifier-free colorization\nguidance, which enhances the transferred colors by combining color-transferred\nand non-color-transferred outputs. This process improves the quality of\ncolorization. Our experimental results demonstrate that our method outperforms\nexisting techniques in terms of image quality and fidelity to the reference.\nSpecifically, we use 335 input-reference pairs from previous research,\nachieving an FID of 95.27 (image quality) and an SI-FID of 5.51 (fidelity to\nthe reference). Our source code is available at\nhttps://github.com/satoshi-kosugi/powerful-attention.", "authors": ["Satoshi Kosugi"], "published_date": "2025-05-21", "title_zh": "利用預訓練擴散模型強大的注意力機制進行範例式圖像著色", "summary_zh": "範例式圖像著色旨在利用參考彩色圖像為灰度圖像著色，確保參考顏色根據其語義相似性應用到相應的輸入區域。為了實現精確的區域語義匹配，我們利用預訓練擴散模型的自注意力模塊，該模型在大型數據集上訓練，展現了強大的注意力能力。我們提出了一種基於預訓練擴散模型的新穎、免微調方法，做出了兩個主要貢獻：雙重注意力引導的顏色傳輸，利用自注意力模塊計算輸入和參考圖像之間的注意力圖，有效捕捉語義對應關係，並將參考圖像的顏色特徵傳輸到輸入圖像的語義匹配區域；以及無分類器顏色引導，通過結合顏色傳輸和非顏色傳輸的輸出，增強傳輸的顏色，提高著色質量。實驗結果表明，我們的方法在圖像質量和參考保真度方面優於現有技術。我們使用先前研究中的335個輸入-參考對，实现了95.27的FID（图像质量）和5.51的SI-FID（参考保真度）。", "audio": "audios/2505.15812v1.mp3", "timestamp": "2025-05-22T03:11:06.284321"}
{"query": "AI", "id": "2505.15799v1", "url": "http://arxiv.org/abs/2505.15799v1", "title": "The Agentic Economy", "summary": "Generative AI has transformed human-computer interaction by enabling natural\nlanguage interfaces and the emergence of autonomous agents capable of acting on\nusers' behalf. While early applications have improved individual productivity,\nthese gains have largely been confined to predefined tasks within existing\nworkflows. We argue that the more profound economic impact lies in reducing\ncommunication frictions between consumers and businesses. This shift could\nreorganize markets, redistribute power, and catalyze the creation of new\nproducts and services. We explore the implications of an agentic economy, where\nassistant agents act on behalf of consumers and service agents represent\nbusinesses, interacting programmatically to facilitate transactions. A key\ndistinction we draw is between unscripted interactions -- enabled by technical\nadvances in natural language and protocol design -- and unrestricted\ninteractions, which depend on market structures and governance. We examine the\ncurrent limitations of siloed and end-to-end agents, and explore future\nscenarios shaped by technical standards and market dynamics. These include the\npotential tension between agentic walled gardens and an open web of agents,\nimplications for advertising and discovery, the evolution of\nmicro-transactions, and the unbundling and rebundling of digital goods.\nUltimately, we argue that the architecture of agentic communication will\ndetermine the extent to which generative AI democratizes access to economic\nopportunity.", "authors": ["David M. Rothschild", "Markus Mobius", "Jake M. Hofman", "Eleanor W. Dillon", "Daniel G. Goldstein", "Nicole Immorlica", "Sonia Jaffe", "Brendan Lucier", "Aleksandrs Slivkins", "Matthew Vogel"], "published_date": "2025-05-21", "title_zh": "代理經濟", "summary_zh": "生成式AI透過自然語言介面和自主代理，大幅改變人機互動。論文認為，其更深遠的經濟影響在於降低消費者與企業之間的溝通障礙，進而重塑市場、重新分配權力，並催生新產品和服務。論文探討代理經濟，消費者代理代表用戶，服務代理代表企業，透過程式化互動促進交易。關鍵區別在於，技術進步促成的「無腳本互動」與取決於市場結構和治理的「無限制互動」。論文分析了現有代理的局限性，並探討技術標準和市場動態塑造的未來場景，包括代理封閉花園與開放網路的潛在衝突、廣告與發現的影響、微交易的演變，以及數位商品的解綁和重組。論文強調，代理溝通的架構將決定生成式AI能否真正實現經濟機會的民主化。", "audio": "audios/2505.15799v1.mp3", "timestamp": "2025-05-22T04:23:32.823938"}
{"query": "Foundation Model", "id": "2505.15685v1", "url": "http://arxiv.org/abs/2505.15685v1", "title": "From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems", "summary": "Foundation models (FMs) are increasingly used to bridge language and action\nin embodied agents, yet the operational characteristics of different FM\nintegration strategies remain under-explored -- particularly for complex\ninstruction following and versatile action generation in changing environments.\nThis paper examines three paradigms for building robotic systems: end-to-end\nvision-language-action (VLA) models that implicitly integrate perception and\nplanning, and modular pipelines incorporating either vision-language models\n(VLMs) or multimodal large language models (LLMs). We evaluate these paradigms\nthrough two focused case studies: a complex instruction grounding task\nassessing fine-grained instruction understanding and cross-modal\ndisambiguation, and an object manipulation task targeting skill transfer via\nVLA finetuning. Our experiments in zero-shot and few-shot settings reveal\ntrade-offs in generalization and data efficiency. By exploring performance\nlimits, we distill design implications for developing language-driven physical\nagents and outline emerging challenges and opportunities for FM-powered\nrobotics in real-world conditions.", "authors": ["Xiuchao Sui", "Daiying Tian", "Qi Sun", "Ruirui Chen", "Dongkyu Choi", "Kenneth Kwok", "Soujanya Poria"], "published_date": "2025-05-21", "title_zh": "從基礎紮根到操控自如：具體化機器人系統中基礎模型整合的案例研究", "summary_zh": "本論文探討了如何將基礎模型整合到機器人系统中，使其能够理解人类指令并执行复杂操作。研究比较了三种不同的整合方法：端到端视觉-语言-动作模型、以及使用视觉-语言模型或多模态大型语言模型的模块化流程。通过两个案例研究，揭示了不同方法在泛化能力和数据效率方面的优缺点，为开发更智能的语言驱动型机器人提供了设计指导，并指出了未来实际应用中面临的挑战和机遇。", "audio": "audios/2505.15685v1.mp3", "timestamp": "2025-05-22T04:23:36.820479"}
{"query": "Diffusion Model", "id": "2505.15800v1", "url": "http://arxiv.org/abs/2505.15800v1", "title": "Interspatial Attention for Efficient 4D Human Video Generation", "summary": "Generating photorealistic videos of digital humans in a controllable manner\nis crucial for a plethora of applications. Existing approaches either build on\nmethods that employ template-based 3D representations or emerging video\ngeneration models but suffer from poor quality or limited consistency and\nidentity preservation when generating individual or multiple digital humans. In\nthis paper, we introduce a new interspatial attention (ISA) mechanism as a\nscalable building block for modern diffusion transformer (DiT)--based video\ngeneration models. ISA is a new type of cross attention that uses relative\npositional encodings tailored for the generation of human videos. Leveraging a\ncustom-developed video variation autoencoder, we train a latent ISA-based\ndiffusion model on a large corpus of video data. Our model achieves\nstate-of-the-art performance for 4D human video synthesis, demonstrating\nremarkable motion consistency and identity preservation while providing precise\ncontrol of the camera and body poses. Our code and model are publicly released\nat https://dsaurus.github.io/isa4d/.", "authors": ["Ruizhi Shao", "Yinghao Xu", "Yujun Shen", "Ceyuan Yang", "Yang Zheng", "Changan Chen", "Yebin Liu", "Gordon Wetzstein"], "published_date": "2025-05-21", "title_zh": "用於高效4D人體影片生成之跨空間注意力機制", "summary_zh": "論文提出一種新的跨空間注意力（ISA）機制，用於提升基於擴散Transformer（DiT）模型的4D人體影片生成效率。ISA是一種新型的跨注意力，針對人體影片生成設計了相對位置編碼。該模型透過訓練大量影片數據，在運動一致性和身份保持方面達到最佳效果，同時能精準控制相機和身體姿勢。程式碼和模型已公開。", "audio": "audios/2505.15800v1.mp3", "timestamp": "2025-05-22T04:23:41.402506"}
{"query": "AI", "id": "2505.15798v1", "url": "http://arxiv.org/abs/2505.15798v1", "title": "Model Merging is Secretly Certifiable: Non-Vacuous Generalisation Bounds for Low-Shot Learning", "summary": "Certifying the IID generalisation ability of deep networks is the first of\nmany requirements for trusting AI in high-stakes applications from medicine to\nsecurity. However, when instantiating generalisation bounds for deep networks\nit remains challenging to obtain non-vacuous guarantees, especially when\napplying contemporary large models on the small scale data prevalent in such\nhigh-stakes fields. In this paper, we draw a novel connection between a family\nof learning methods based on model fusion and generalisation certificates, and\nsurprisingly show that with minor adjustment several existing learning\nstrategies already provide non-trivial generalisation guarantees. Essentially,\nby focusing on data-driven learning of downstream tasks by fusion rather than\nfine-tuning, the certified generalisation gap becomes tiny and independent of\nthe base network size, facilitating its certification. Our results show for the\nfirst time non-trivial generalisation guarantees for learning with as low as\n100 examples, while using vision models such as VIT-B and language models such\nas mistral-7B. This observation is significant as it has immediate implications\nfor facilitating the certification of existing systems as trustworthy, and\nopens up new directions for research at the intersection of practice and\ntheory.", "authors": ["Taehoon Kim", "Henry Gouk", "Minyoung Kim", "Timothy Hospedales"], "published_date": "2025-05-21", "title_zh": "模型融合隱藏的認證性：少樣本學習的非空泛泛化界限", "summary_zh": "這篇論文揭示了模型融合技術隱藏的優勢，它能為深度學習模型提供可驗證的泛化能力，尤其是在少樣本學習的場景下。傳統上，要為深度網路建立有效的泛化界限非常困難，特別是在高風險領域中，資料量通常很小。研究發現，透過模型融合進行資料驅動的下游任務學習，可以顯著縮小認證泛化差距，使其與基底網路的大小無關。該研究首次展示了使用少至100個樣本，以及VIT-B和Mistral-7B等模型，也能實現非空泛的泛化保證，為現有系統的認證開闢了新的方向。", "audio": "audios/2505.15798v1.mp3", "timestamp": "2025-05-22T05:18:42.219278"}
{"query": "Foundation Model", "id": "2505.15594v1", "url": "http://arxiv.org/abs/2505.15594v1", "title": "Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off", "summary": "While foundation models demonstrate impressive performance across various\ntasks, they remain vulnerable to adversarial inputs. Current research explores\nvarious approaches to enhance model robustness, with Diffusion Denoised\nSmoothing emerging as a particularly promising technique. This method employs a\npretrained diffusion model to preprocess inputs before model inference. Yet,\nits effectiveness remains largely unexplored beyond classification. We aim to\naddress this gap by analyzing three datasets with four distinct downstream\ntasks under three different adversarial attack algorithms. Our findings reveal\nthat while foundation models maintain resilience against conventional\ntransformations, applying high-noise diffusion denoising to clean images\nwithout any distortions significantly degrades performance by as high as 57%.\nLow-noise diffusion settings preserve performance but fail to provide adequate\nprotection across all attack types. Moreover, we introduce a novel attack\nstrategy specifically targeting the diffusion process itself, capable of\ncircumventing defenses in the low-noise regime. Our results suggest that the\ntrade-off between adversarial robustness and performance remains a challenge to\nbe addressed.", "authors": ["Yury Belousov", "Brian Pulfer", "Vitaliy Kinakh", "Slava Voloshynovskiy"], "published_date": "2025-05-21", "title_zh": "超越分類：評估擴散去噪平滑在安全性與效用性權衡上的表現", "summary_zh": "大型模型雖然在各項任務表現出色，但容易受到對抗性輸入的攻擊。擴散去噪平滑是一種有潛力的防禦方法，它利用預訓練的擴散模型來預處理輸入。然而，目前對其在分類以外任務的有效性研究不足。本研究針對多個任務和攻擊策略進行分析，發現高噪音擴散去噪會顯著降低模型效能，甚至高達57%。低噪音設置雖能維持效能，但無法有效防禦所有攻擊。此外，我們還設計了一種針對擴散過程本身的新型攻擊，能夠繞過低噪音設置的防禦。結果表明，在對抗性魯棒性和模型效能之間取得平衡仍然是一個挑戰。", "audio": "audios/2505.15594v1.mp3", "timestamp": "2025-05-22T05:18:47.929179"}
{"query": "Diffusion Model", "id": "2505.15791v1", "url": "http://arxiv.org/abs/2505.15791v1", "title": "VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL", "summary": "Diffusion models have emerged as powerful generative tools across various\ndomains, yet tailoring pre-trained models to exhibit specific desirable\nproperties remains challenging. While reinforcement learning (RL) offers a\npromising solution,current methods struggle to simultaneously achieve stable,\nefficient fine-tuning and support non-differentiable rewards. Furthermore,\ntheir reliance on sparse rewards provides inadequate supervision during\nintermediate steps, often resulting in suboptimal generation quality. To\naddress these limitations, dense and differentiable signals are required\nthroughout the diffusion process. Hence, we propose VAlue-based Reinforced\nDiffusion (VARD): a novel approach that first learns a value function\npredicting expection of rewards from intermediate states, and subsequently uses\nthis value function with KL regularization to provide dense supervision\nthroughout the generation process. Our method maintains proximity to the\npretrained model while enabling effective and stable training via\nbackpropagation. Experimental results demonstrate that our approach facilitates\nbetter trajectory guidance, improves training efficiency and extends the\napplicability of RL to diffusion models optimized for complex,\nnon-differentiable reward functions.", "authors": ["Fengyuan Dai", "Zifeng Zhuang", "Yufei Huang", "Siteng Huang", "Bangyan Liao", "Donglin Wang", "Fajie Yuan"], "published_date": "2025-05-21", "title_zh": "VARD：基於價值的強化學習，用於擴散模型的高效且密集微調", "summary_zh": "擴散模型在生成領域表現出色，但要讓預訓練模型展現特定期望的特性仍然困難。強化學習雖有潛力，但現有方法難以兼顧穩定、高效的微調，且無法支援不可微分的獎勵。它們對稀疏獎勵的依賴導致中間步驟的監督不足，影響生成品質。VARD 旨在解決這些問題，它首先學習一個價值函數，預測中間狀態的獎勵期望值，然後用該價值函數及KL正則化，在生成過程中提供密集的監督。這種方法在保持與預訓練模型接近的同時，通過反向傳播實現有效且穩定的訓練。實驗表明，VARD 能更好地引導生成軌跡，提高訓練效率，並將強化學習應用擴展到優化複雜的、不可微分的獎勵函數的擴散模型上。", "audio": "audios/2505.15791v1.mp3", "timestamp": "2025-05-22T05:18:54.362516"}
{"query": "AI", "id": "2505.15790v1", "url": "http://arxiv.org/abs/2505.15790v1", "title": "Exploring the Innovation Opportunities for Pre-trained Models", "summary": "Innovators transform the world by understanding where services are\nsuccessfully meeting customers' needs and then using this knowledge to identify\nfailsafe opportunities for innovation. Pre-trained models have changed the AI\ninnovation landscape, making it faster and easier to create new AI products and\nservices. Understanding where pre-trained models are successful is critical for\nsupporting AI innovation. Unfortunately, the hype cycle surrounding pre-trained\nmodels makes it hard to know where AI can really be successful. To address\nthis, we investigated pre-trained model applications developed by HCI\nresearchers as a proxy for commercially successful applications. The research\napplications demonstrate technical capabilities, address real user needs, and\navoid ethical challenges. Using an artifact analysis approach, we categorized\ncapabilities, opportunity domains, data types, and emerging interaction design\npatterns, uncovering some of the opportunity space for innovation with\npre-trained models.", "authors": ["Minjung Park", "Jodi Forlizzi", "John Zimmerman"], "published_date": "2025-05-21", "title_zh": "探索預訓練模型的創新機會", "summary_zh": "預訓練模型正在重塑人工智慧創新，加速新產品和服務的開發。本研究分析人機互動(HCI)研究者開發的應用案例，作為商業成功的指標，旨在辨識預訓練模型真正能夠發揮價值的領域。透過分析這些應用，我們歸納出預訓練模型在能力、應用領域、資料類型和互動設計模式上的潛力，為人工智慧創新指明方向。", "audio": "audios/2505.15790v1.mp3", "timestamp": "2025-05-22T06:27:08.958151"}
{"query": "Foundation Model", "id": "2505.15572v1", "url": "http://arxiv.org/abs/2505.15572v1", "title": "Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback", "summary": "The data-to-equation (Data2Eqn) task aims to discover interpretable\nmathematical equations that map observed values to labels, offering physical\ninsights and broad applicability across academic and industrial domains.\nGenetic programming and traditional deep learning-based approaches suffer from\nsearch inefficiency and poor generalization on small task-specific datasets.\nFoundation models showed promise in this area, but existing approaches suffer\nfrom: 1) They are pretrained on general-purpose data distributions, making them\nless effective for domain-specific tasks; and 2) their training objectives\nfocus on token-level alignment, overlooking mathematical semantics, which can\nlead to inaccurate equations. To address these issues, we aim to enhance the\ndomain adaptability of foundation models for Data2Eqn tasks. In this work, we\npropose a reinforcement learning-based finetuning framework that directly\noptimizes the generation policy of a pretrained model through reward signals\nderived from downstream numerical fitness. Our method allows the model to adapt\nto specific and complex data distributions and generate mathematically\nmeaningful equations. Extensive experiments demonstrate that our approach\nimproves both the accuracy and robustness of equation generation under complex\ndistributions.", "authors": ["Wangyang Ying", "Haoyue Bai", "Nanxu Gong", "Xinyuan Wang", "Sixun Dong", "Haifeng Chen", "Yanjie Fu"], "published_date": "2025-05-21", "title_zh": "以強化回饋彌合方程式蒸餾中的領域差距", "summary_zh": "資料到方程式（Data2Eqn）任務旨在從觀測值找出可解釋的數學方程式，進而對應到標籤，以提供物理學見解並廣泛應用於學術界和工業界。我們發現，預訓練模型在特定領域的Data2Eqn任務中表現不佳，原因在於它們是在通用數據分佈上訓練，並且訓練目標側重於token級別對齊，忽略了數學語義，導致方程式不準確。為了解決這些問題，我們提出一個基於強化學習的微調框架，透過來自下游數值擬合的獎勵信號，直接優化預訓練模型的生成策略。實驗結果表明，此方法提高了模型在複雜分佈下生成方程式的準確性和魯棒性。", "audio": "audios/2505.15572v1.mp3", "timestamp": "2025-05-22T06:27:14.421326"}
{"query": "Diffusion Model", "id": "2505.15679v1", "url": "http://arxiv.org/abs/2505.15679v1", "title": "SwarmDiff: Swarm Robotic Trajectory Planning in Cluttered Environments via Diffusion Transformer", "summary": "Swarm robotic trajectory planning faces challenges in computational\nefficiency, scalability, and safety, particularly in complex, obstacle-dense\nenvironments. To address these issues, we propose SwarmDiff, a hierarchical and\nscalable generative framework for swarm robots. We model the swarm's\nmacroscopic state using Probability Density Functions (PDFs) and leverage\nconditional diffusion models to generate risk-aware macroscopic trajectory\ndistributions, which then guide the generation of individual robot trajectories\nat the microscopic level. To ensure a balance between the swarm's optimal\ntransportation and risk awareness, we integrate Wasserstein metrics and\nConditional Value at Risk (CVaR). Additionally, we introduce a Diffusion\nTransformer (DiT) to improve sampling efficiency and generation quality by\ncapturing long-range dependencies. Extensive simulations and real-world\nexperiments demonstrate that SwarmDiff outperforms existing methods in\ncomputational efficiency, trajectory validity, and scalability, making it a\nreliable solution for swarm robotic trajectory planning.", "authors": ["Kang Ding", "Chunxuan Jiao", "Yunze Hu", "Kangjie Zhou", "Pengying Wu", "Yao Mu", "Chang Liu"], "published_date": "2025-05-21", "title_zh": "SwarmDiff：基於擴散轉換器的複雜環境群體機器人軌跡規劃", "summary_zh": "SwarmDiff 是一個針對複雜環境中群體機器人的軌跡規劃框架。它運用擴散模型生成風險感知的群體宏觀軌跡分佈，再以此引導生成個體機器人的微觀軌跡。同時，整合了 Wasserstein 指標和條件風險值（CVaR）來平衡群體的最優運輸和風險意識。透過擴散轉換器 (DiT) 來提升抽樣效率和生成品質。實驗證明，SwarmDiff 在計算效率、軌跡有效性和可擴展性方面優於現有方法，為群體機器人軌跡規劃提供了一個可靠的解決方案。", "audio": "audios/2505.15679v1.mp3", "timestamp": "2025-05-22T06:27:18.842130"}
{"query": "AI", "id": "2505.15778v1", "url": "http://arxiv.org/abs/2505.15778v1", "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space", "summary": "Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking.", "authors": ["Zhen Zhang", "Xuehai He", "Weixiang Yan", "Ao Shen", "Chenyang Zhao", "Shuohang Wang", "Yelong Shen", "Xin Eric Wang"], "published_date": "2025-05-21", "title_zh": "軟性思考：釋放LLM在連續概念空間中的推理潛力", "summary_zh": "現有的推理模型受限於語言，只能處理離散的語義單位。這篇論文提出一種名為「軟性思考」的免訓練方法，模擬人類的抽象思考模式，在連續概念空間中產生軟性的概念標記。這些標記透過加權混合不同的詞嵌入形成，能更平滑地轉換並產生更豐富的表徵，突破傳統離散邊界。實驗結果顯示，「軟性思考」在數學和程式碼基準測試中，能有效提升準確度並降低 token 使用量，同時保持輸出的可讀性，展現其打破離散語言推理瓶頸的潛力。", "audio": "audios/2505.15778v1.mp3", "timestamp": "2025-05-22T09:21:16.643593"}
{"query": "Foundation Model", "id": "2505.15559v1", "url": "http://arxiv.org/abs/2505.15559v1", "title": "Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music Attributes", "summary": "Moonbeam is a transformer-based foundation model for symbolic music,\npretrained on a large and diverse collection of MIDI data totaling 81.6K hours\nof music and 18 billion tokens. Moonbeam incorporates music-domain inductive\nbiases by capturing both absolute and relative musical attributes through the\nintroduction of a novel domain-knowledge-inspired tokenization method and\nMultidimensional Relative Attention (MRA), which captures relative music\ninformation without additional trainable parameters. Leveraging the pretrained\nMoonbeam, we propose 2 finetuning architectures with full anticipatory\ncapabilities, targeting 2 categories of downstream tasks: symbolic music\nunderstanding and conditional music generation (including music infilling). Our\nmodel outperforms other large-scale pretrained music models in most cases in\nterms of accuracy and F1 score across 3 downstream music classification tasks\non 4 datasets. Moreover, our finetuned conditional music generation model\noutperforms a strong transformer baseline with a REMI-like tokenizer. We\nopen-source the code, pretrained model, and generated samples on Github.", "authors": ["Zixun Guo", "Simon Dixon"], "published_date": "2025-05-21", "title_zh": "Moonbeam：一個利用絕對與相對音樂屬性的MIDI基礎模型", "summary_zh": "Moonbeam是一個基於Transformer的符號音樂基礎模型，使用大量MIDI數據（總計8.16萬小時的音樂和180億個tokens）進行預訓練。它整合了音樂領域的歸納偏置，通過創新的token化方法和多維相對注意力機制（MRA）來捕捉絕對和相對的音樂屬性。MRA在不增加可訓練參數的情況下捕捉相對音樂信息。利用預訓練的Moonbeam，我們提出了兩種具有完整預測能力的微調架構，針對符號音樂理解和條件音樂生成（包括音樂填充）兩類下游任務。我們的模型在四個數據集的三个下游音樂分類任務中，其準確度和F1分數大多優於其他大規模預訓練音樂模型。此外，我們微調的條件音樂生成模型也勝過使用類似REMI tokenization方法的強Transformer基線。我們已在Github上開源代碼、預訓練模型和生成的樣本。", "audio": "audios/2505.15559v1.mp3", "timestamp": "2025-05-22T09:21:23.097162"}
{"query": "Diffusion Model", "id": "2505.15644v1", "url": "http://arxiv.org/abs/2505.15644v1", "title": "FragFake: A Dataset for Fine-Grained Detection of Edited Images with Vision Language Models", "summary": "Fine-grained edited image detection of localized edits in images is crucial\nfor assessing content authenticity, especially given that modern diffusion\nmodels and image editing methods can produce highly realistic manipulations.\nHowever, this domain faces three challenges: (1) Binary classifiers yield only\na global real-or-fake label without providing localization; (2) Traditional\ncomputer vision methods often rely on costly pixel-level annotations; and (3)\nNo large-scale, high-quality dataset exists for modern image-editing detection\ntechniques. To address these gaps, we develop an automated data-generation\npipeline to create FragFake, the first dedicated benchmark dataset for edited\nimage detection, which includes high-quality images from diverse editing models\nand a wide variety of edited objects. Based on FragFake, we utilize Vision\nLanguage Models (VLMs) for the first time in the task of edited image\nclassification and edited region localization. Experimental results show that\nfine-tuned VLMs achieve higher average Object Precision across all datasets,\nsignificantly outperforming pretrained models. We further conduct ablation and\ntransferability analyses to evaluate the detectors across various\nconfigurations and editing scenarios. To the best of our knowledge, this work\nis the first to reformulate localized image edit detection as a vision-language\nunderstanding task, establishing a new paradigm for the field. We anticipate\nthat this work will establish a solid foundation to facilitate and inspire\nsubsequent research endeavors in the domain of multimodal content authenticity.", "authors": ["Zhen Sun", "Ziyi Zhang", "Zeren Luo", "Zeyang Sha", "Tianshuo Cong", "Zheng Li", "Shiwen Cui", "Weiqiang Wang", "Jiaheng Wei", "Xinlei He", "Qi Li", "Qian Wang"], "published_date": "2025-05-21", "title_zh": "FragFake：一個利用視覺語言模型進行細粒度編輯圖像檢測的資料集", "summary_zh": "現代圖像編輯技術高度逼真，準確判斷圖像是否經過局部修改至關重要。 然而，傳統方法難以定位編輯區域，且缺乏高品質的大規模資料集。為此，我們創建了 FragFake 資料集，包含多種編輯模型和物件，並首次使用視覺語言模型 (VLMs) 進行編輯圖像分類和區域定位。實驗表明，經過微調的 VLMs 在檢測編輯區域的精確度上顯著優於預訓練模型。 這項研究將局部圖像編輯檢測重新定義為視覺語言理解任務，為該領域建立了一個新範式。", "audio": "audios/2505.15644v1.mp3", "timestamp": "2025-05-22T09:21:28.753217"}
{"query": "AI", "id": "2505.15755v1", "url": "http://arxiv.org/abs/2505.15755v1", "title": "Exploring The Visual Feature Space for Multimodal Neural Decoding", "summary": "The intrication of brain signals drives research that leverages multimodal AI\nto align brain modalities with visual and textual data for explainable\ndescriptions. However, most existing studies are limited to coarse\ninterpretations, lacking essential details on object descriptions, locations,\nattributes, and their relationships. This leads to imprecise and ambiguous\nreconstructions when using such cues for visual decoding. To address this, we\nanalyze different choices of vision feature spaces from pre-trained visual\ncomponents within Multimodal Large Language Models (MLLMs) and introduce a\nzero-shot multimodal brain decoding method that interacts with these models to\ndecode across multiple levels of granularities. % To assess a model's ability\nto decode fine details from brain signals, we propose the Multi-Granularity\nBrain Detail Understanding Benchmark (MG-BrainDub). This benchmark includes two\nkey tasks: detailed descriptions and salient question-answering, with metrics\nhighlighting key visual elements like objects, attributes, and relationships.\nOur approach enhances neural decoding precision and supports more accurate\nneuro-decoding applications. Code will be available at\nhttps://github.com/weihaox/VINDEX.", "authors": ["Weihao Xia", "Cengiz Oztireli"], "published_date": "2025-05-21", "title_zh": "探索多模態神經解碼的視覺特徵空間", "summary_zh": "現有研究利用多模態人工智慧將腦部訊號與視覺及文字資料對齊，以產生可解釋的描述，但細節不足，導致視覺解碼重建不精確。本研究分析多模態大型語言模型(MLLM)中預訓練視覺元件的不同視覺特徵空間，並提出一種零樣本多模態腦部解碼方法，與這些模型互動，在多個粒度層級上進行解碼。研究更推出多粒度腦部細節理解基準(MG-BrainDub)，包含詳細描述和顯著問答任務，以評估模型從腦部訊號解碼細節的能力。本研究旨在提高神經解碼的精確度，並支援更準確的神經解碼應用。程式碼將在https://github.com/weihaox/VINDEX 上公開。", "audio": "audios/2505.15755v1.mp3", "timestamp": "2025-05-22T10:20:08.924290"}
{"query": "Foundation Model", "id": "2505.15506v1", "url": "http://arxiv.org/abs/2505.15506v1", "title": "Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts", "summary": "Recently, Vision-Language foundation models like CLIP and ALIGN, which are\npre-trained on large-scale data have shown remarkable zero-shot generalization\nto diverse datasets with different classes and even domains. In this work, we\ntake a step further and analyze whether these models can be adapted to target\ndatasets having very different distributions and classes compared to what these\nmodels have been trained on, using only a few labeled examples from the target\ndataset. In such scenarios, finetuning large pretrained models is challenging\ndue to problems of overfitting as well as loss of generalization, and has not\nbeen well explored in prior literature. Since, the pre-training data of such\nmodels are unavailable, it is difficult to comprehend the performance on\nvarious downstream datasets. First, we try to answer the question: Given a\ntarget dataset with a few labelled examples, can we estimate whether further\nfine-tuning can enhance the performance compared to zero-shot evaluation? by\nanalyzing the common vision-language embedding space. Based on the analysis, we\npropose a novel prompt-tuning method, PromptMargin for adapting such\nlarge-scale VLMs directly on the few target samples. PromptMargin effectively\ntunes the text as well as visual prompts for this task, and has two main\nmodules: 1) Firstly, we use a selective augmentation strategy to complement the\nfew training samples in each task; 2) Additionally, to ensure robust training\nin the presence of unfamiliar class names, we increase the inter-class margin\nfor improved class discrimination using a novel Multimodal Margin Regularizer.\nExtensive experiments and analysis across fifteen target benchmark datasets,\nwith varying degrees of distribution shifts from natural images, shows the\neffectiveness of the proposed framework over the existing state-of-the-art\napproaches applied to this setting. github.com/debarshigit/PromptMargin.", "authors": ["Debarshi Brahma", "Anuska Roy", "Soma Biswas"], "published_date": "2025-05-21", "title_zh": "使用邊界正則化器調整視覺語言模型以應對分布偏移下的少樣本學習", "summary_zh": "CLIP 和 ALIGN 等視覺語言預訓練模型在不同資料集上展現了出色的零樣本泛化能力。本研究進一步探討如何使用目標資料集的少量標記樣本，使這些模型適應與訓練資料具有顯著分布和類別差異的目標資料集。針對這個問題，我們分析了視覺語言嵌入空間，提出一種新的 PromptMargin 方法，有效調整文本和視覺提示詞，並採用選擇性資料擴增和多模態邊界正則化器，增強模型在少樣本情況下的鲁棒性和類別區分能力。在十五個基準資料集上的實驗證明，PromptMargin 在分布偏移下的少樣本學習中優於現有方法。", "audio": "audios/2505.15506v1.mp3", "timestamp": "2025-05-22T10:20:16.069397"}
{"query": "Diffusion Model", "id": "2505.15450v1", "url": "http://arxiv.org/abs/2505.15450v1", "title": "Comprehensive Evaluation and Analysis for NSFW Concept Erasure in Text-to-Image Diffusion Models", "summary": "Text-to-image diffusion models have gained widespread application across\nvarious domains, demonstrating remarkable creative potential. However, the\nstrong generalization capabilities of diffusion models can inadvertently lead\nto the generation of not-safe-for-work (NSFW) content, posing significant risks\nto their safe deployment. While several concept erasure methods have been\nproposed to mitigate the issue associated with NSFW content, a comprehensive\nevaluation of their effectiveness across various scenarios remains absent. To\nbridge this gap, we introduce a full-pipeline toolkit specifically designed for\nconcept erasure and conduct the first systematic study of NSFW concept erasure\nmethods. By examining the interplay between the underlying mechanisms and\nempirical observations, we provide in-depth insights and practical guidance for\nthe effective application of concept erasure methods in various real-world\nscenarios, with the aim of advancing the understanding of content safety in\ndiffusion models and establishing a solid foundation for future research and\ndevelopment in this critical area.", "authors": ["Die Chen", "Zhiwen Li", "Cen Chen", "Yuexiang Xie", "Xiaodan Li", "Jinyan Ye", "Yingda Chen", "Yaliang Li"], "published_date": "2025-05-21", "title_zh": "文字生成圖像擴散模型中，針對不雅內容概念消除的全面評估與分析", "summary_zh": "文字生成圖像模型能力強大，但也可能生成不雅內容，造成安全風險。雖然已有消除不雅內容的方法，但缺乏全面評估。本研究開發工具，系統性地評估現有方法，深入分析其機制和效果，為實際應用提供指導，旨在提升對擴散模型內容安全性的理解，並為未來研究奠定基礎。\n\n**簡明摘要：**AI繪圖很厲害，但可能畫出不雅圖片。這項研究評估了目前消除不雅內容的方法，希望能讓AI繪圖更安全、更可靠。", "audio": "audios/2505.15450v1.mp3", "timestamp": "2025-05-22T10:20:23.383218"}
{"query": "AI", "id": "2505.15700v1", "url": "http://arxiv.org/abs/2505.15700v1", "title": "\"Alexa, can you forget me?\" Machine Unlearning Benchmark in Spoken Language Understanding", "summary": "Machine unlearning, the process of efficiently removing specific information\nfrom machine learning models, is a growing area of interest for responsible AI.\nHowever, few studies have explored the effectiveness of unlearning methods on\ncomplex tasks, particularly speech-related ones. This paper introduces\nUnSLU-BENCH, the first benchmark for machine unlearning in spoken language\nunderstanding (SLU), focusing on four datasets spanning four languages. We\naddress the unlearning of data from specific speakers as a way to evaluate the\nquality of potential \"right to be forgotten\" requests. We assess eight\nunlearning techniques and propose a novel metric to simultaneously better\ncapture their efficacy, utility, and efficiency. UnSLU-BENCH sets a foundation\nfor unlearning in SLU and reveals significant differences in the effectiveness\nand computational feasibility of various techniques.", "authors": ["Alkis Koudounas", "Claudio Savelli", "Flavio Giobergia", "Elena Baralis"], "published_date": "2025-05-21", "title_zh": "「Alexa，你可以忘記我嗎？」：口語理解中的機器遺忘基準測試", "summary_zh": "為響應「被遺忘權」的要求，機器遺忘技術日漸重要。本論文推出首個口語理解（SLU）機器遺忘基準測試UnSLU-BENCH，涵蓋四種語言的四個數據集，專注於將特定說話者的數據從模型中移除。研究評估了八種遺忘技術，並提出一種新的評估指標，綜合考量其效果、實用性和效率。UnSLU-BENCH 為口語理解中的遺忘技術奠定基礎，並揭示了不同技術在效果和計算可行性上的顯著差異。", "audio": "audios/2505.15700v1.mp3", "timestamp": "2025-05-22T11:16:43.359376"}
{"query": "Foundation Model", "id": "2505.15334v1", "url": "http://arxiv.org/abs/2505.15334v1", "title": "Parameter-Efficient Fine-Tuning of Multispectral Foundation Models for Hyperspectral Image Classification", "summary": "Foundation models have achieved great success across diverse domains,\nincluding remote sensing (RS), thanks to their versatility and strong\ngeneralization abilities. However, most RS foundation models are designed for\nmultispectral data, while hyperspectral imagery (HSI) - with its hundreds of\nspectral bands - remains less explored. Fine-tuning such models for downstream\ntasks is also challenging, often demanding considerable memory and storage. In\nthis paper, we propose an efficient framework to fine-tune SpectralGPT, a\nmultispectral foundation model, for hyperspectral image classification (HSIC).\nWe explore several Parameter-Efficient Fine-Tuning (PEFT) methods, including\nLow-Rank Adaptation (LoRA), Kronecker-based adaptation (KronA), Low-Rank\nKronecker (LoKr), and the recent LoRA+, which uses distinct learning rates for\nlow-rank adapters scaled by a factor lambda. Inspired by LoRA+, we introduce\nKronA+, which applies a similar mechanism to the Kronecker matrices. We\nevaluate our approach on five datasets from different sensors, showing\ncompetitive performance with state-of-the-art HSI models. Our full fine-tuning\n(FFT) setup for SpectralGPT even outperforms a dedicated hyperspectral\nfoundation model on some datasets while requiring only a quarter of the\ntraining epochs. Under the same number of epochs, KronA+ reaches similar\nperformance with far fewer trainable parameters - just 0.056 percent - and adds\nonly approximately 0.2 megabytes of storage, making it the most effective PEFT\nmethod tested.", "authors": ["Bernardin Ligan", "Khalide Jbilou", "Fahd Kalloubi", "Ahmed Ratnani"], "published_date": "2025-05-21", "title_zh": "針對高光譜影像分類的多光譜基礎模型之參數高效微調", "summary_zh": "本文提出一個高效框架，針對高光譜影像分類，微調多光譜基礎模型SpectralGPT。由於高光譜影像擁有數百個頻譜通道，直接微調大型模型耗費大量資源。因此，研究使用多種參數高效微調(PEFT)方法，如LoRA、KronA及其改良版本LoRA+和KronA+。實驗證明，我們的框架在多個數據集上表現優異，其中KronA+僅需極少的可訓練參數和儲存空間，即可達到與完整微調相近的效能，是目前測試中最有效的PEFT方法。", "audio": "audios/2505.15334v1.mp3", "timestamp": "2025-05-22T11:16:48.505895"}
{"query": "Diffusion Model", "id": "2505.15427v1", "url": "http://arxiv.org/abs/2505.15427v1", "title": "Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions", "summary": "The remarkable ability of diffusion models to generate high-fidelity images\nhas led to their widespread adoption. However, concerns have also arisen\nregarding their potential to produce Not Safe for Work (NSFW) content and\nexhibit social biases, hindering their practical use in real-world\napplications. In response to this challenge, prior work has focused on\nemploying security filters to identify and exclude toxic text, or\nalternatively, fine-tuning pre-trained diffusion models to erase sensitive\nconcepts. Unfortunately, existing methods struggle to achieve satisfactory\nperformance in the sense that they can have a significant impact on the normal\nmodel output while still failing to prevent the generation of harmful content\nin some cases. In this paper, we propose a novel self-discovery approach to\nidentifying a semantic direction vector in the embedding space to restrict text\nembedding within a safe region. Our method circumvents the need for correcting\nindividual words within the input text and steers the entire text prompt\ntowards a safe region in the embedding space, thereby enhancing model\nrobustness against all possibly unsafe prompts. In addition, we employ Low-Rank\nAdaptation (LoRA) for semantic direction vector initialization to reduce the\nimpact on the model performance for other semantics. Furthermore, our method\ncan also be integrated with existing methods to improve their social\nresponsibility. Extensive experiments on benchmark datasets demonstrate that\nour method can effectively reduce NSFW content and mitigate social bias\ngenerated by diffusion models compared to several state-of-the-art baselines.", "authors": ["Zhiwen Li", "Die Chen", "Mingyuan Fan", "Cen Chen", "Yaliang Li", "Yanhao Wang", "Wenmeng Zhou"], "published_date": "2025-05-21", "title_zh": "透過在安全區域內約束文本嵌入實現負責任的擴散模型", "summary_zh": "擴散模型雖然能生成高擬真度圖像，但也可能產生不宜內容和社會偏見，影響實際應用。為了應對這個問題，本研究提出一種新的方法，透過在嵌入空間中識別語義方向向量，將文本嵌入限制在安全區域內。這種方法無需修正輸入文本中的個別詞彙，而是引導整個文本提示詞前往嵌入空間中的安全區域，從而增強模型對所有可能不安全提示詞的抵抗力。此外，我們採用低秩適應 (LoRA) 初始化語義方向向量，以減少對模型性能的影響。實驗結果表明，與現有方法相比，我們的方法能有效減少擴散模型生成的不宜內容並減輕社會偏見。", "audio": "audios/2505.15427v1.mp3", "timestamp": "2025-05-22T11:16:53.788897"}
{"query": "AI", "id": "2505.15622v1", "url": "http://arxiv.org/abs/2505.15622v1", "title": "Benchmarking Energy and Latency in TinyML: A Novel Method for Resource-Constrained AI", "summary": "The rise of IoT has increased the need for on-edge machine learning, with\nTinyML emerging as a promising solution for resource-constrained devices such\nas MCU. However, evaluating their performance remains challenging due to\ndiverse architectures and application scenarios. Current solutions have many\nnon-negligible limitations. This work introduces an alternative benchmarking\nmethodology that integrates energy and latency measurements while\ndistinguishing three execution phases pre-inference, inference, and\npost-inference. Additionally, the setup ensures that the device operates\nwithout being powered by an external measurement unit, while automated testing\ncan be leveraged to enhance statistical significance. To evaluate our setup, we\ntested the STM32N6 MCU, which includes a NPU for executing neural networks. Two\nconfigurations were considered: high-performance and Low-power. The variation\nof the EDP was analyzed separately for each phase, providing insights into the\nimpact of hardware configurations on energy efficiency. Each model was tested\n1000 times to ensure statistically relevant results. Our findings demonstrate\nthat reducing the core voltage and clock frequency improve the efficiency of\npre- and post-processing without significantly affecting network execution\nperformance. This approach can also be used for cross-platform comparisons to\ndetermine the most efficient inference platform and to quantify how pre- and\npost-processing overhead varies across different hardware implementations.", "authors": ["Pietro Bartoli", "Christian Veronesi", "Andrea Giudici", "David Siorpaes", "Diana Trojaniello", "Franco Zappa"], "published_date": "2025-05-21", "title_zh": "TinyML 能效與延遲基準測試：一種針對資源受限AI的新方法", "summary_zh": "物聯網興起，對邊緣機器學習的需求增加。TinyML 成為資源受限裝置（如微控制器 MCU）的潛力解決方案。然而，評估其效能極具挑戰。本研究提出一種新的基準測試方法，整合能耗與延遲測量，並區分預處理、推論和後處理三個階段。此方法確保裝置獨立供電運行，並利用自動化測試提高統計顯著性。實驗結果顯示，降低核心電壓和時脈頻率能有效提升預處理和後處理的效率，且不顯著影響神經網路的推論效能。此方法也可用於跨平台比較，找出最有效率的推論平台，並量化不同硬體實現的預處理和後處理開銷。", "audio": "audios/2505.15622v1.mp3", "timestamp": "2025-05-22T12:38:29.628724"}
{"query": "Foundation Model", "id": "2505.15307v1", "url": "http://arxiv.org/abs/2505.15307v1", "title": "Towards Pre-training an Effective Respiratory Audio Foundation Model", "summary": "Recent advancements in foundation models have sparked interest in respiratory\naudio foundation models. However, the effectiveness of applying conventional\npre-training schemes to datasets that are small-sized and lack diversity has\nnot been sufficiently verified. This study aims to explore better pre-training\npractices for respiratory sounds by comparing numerous pre-trained audio\nmodels. Our investigation reveals that models pre-trained on AudioSet, a\ngeneral audio dataset, are more effective than the models specifically\npre-trained on respiratory sounds. Moreover, combining AudioSet and respiratory\nsound datasets for further pre-training enhances performance, and preserving\nthe frequency-wise information when aggregating features is vital. Along with\nmore insights found in the experiments, we establish a new state-of-the-art for\nthe OPERA benchmark, contributing to advancing respiratory audio foundation\nmodels. Our code is available online at\nhttps://github.com/nttcslab/eval-audio-repr/tree/main/plugin/OPERA.", "authors": ["Daisuke Niizumi", "Daiki Takeuchi", "Masahiro Yasuda", "Binh Thien Nguyen", "Yasunori Ohishi", "Noboru Harada"], "published_date": "2025-05-21", "title_zh": "邁向預訓練一個有效的呼吸音訊基礎模型", "summary_zh": "近期基礎模型的發展激發了對呼吸音訊基礎模型的興趣。本研究探討針對小規模、缺乏多樣性的呼吸音訊數據集，如何進行更有效的預訓練。研究比較了多個預訓練音訊模型，發現使用通用音訊數據集AudioSet預訓練的模型效果更好。進一步結合AudioSet和呼吸音訊數據集進行預訓練可提升性能，並且在聚合特徵時保留頻率信息至關重要。實驗結果為呼吸音訊基礎模型提供了新的洞見，並在OPERA基準測試中取得了最先進的結果。", "audio": "audios/2505.15307v1.mp3", "timestamp": "2025-05-22T12:38:33.648979"}
{"query": "Diffusion Model", "id": "2505.15336v1", "url": "http://arxiv.org/abs/2505.15336v1", "title": "My Face Is Mine, Not Yours: Facial Protection Against Diffusion Model Face Swapping", "summary": "The proliferation of diffusion-based deepfake technologies poses significant\nrisks for unauthorized and unethical facial image manipulation. While\ntraditional countermeasures have primarily focused on passive detection\nmethods, this paper introduces a novel proactive defense strategy through\nadversarial attacks that preemptively protect facial images from being\nexploited by diffusion-based deepfake systems. Existing adversarial protection\nmethods predominantly target conventional generative architectures (GANs, AEs,\nVAEs) and fail to address the unique challenges presented by diffusion models,\nwhich have become the predominant framework for high-quality facial deepfakes.\nCurrent diffusion-specific adversarial approaches are limited by their reliance\non specific model architectures and weights, rendering them ineffective against\nthe diverse landscape of diffusion-based deepfake implementations.\nAdditionally, they typically employ global perturbation strategies that\ninadequately address the region-specific nature of facial manipulation in\ndeepfakes.", "authors": ["Hon Ming Yam", "Zhongliang Guo", "Chun Pong Lau"], "published_date": "2025-05-21", "title_zh": "我的臉是我的，不是你的：針對擴散模型換臉的面部保護", "summary_zh": "擴散模型技術讓換臉變得更容易，但也帶來了未授權和不道德的面部圖像操縱風險。本文提出一種新的主動防禦策略，通過對抗性攻擊，預先保護臉部圖像免於被擴散模型的Deepfake系統利用。與過去針對GAN等生成模型的防禦方法不同，本文方法專門針對擴散模型的特性進行設計，克服了以往方法對模型依賴性強和擾動策略不夠精確等問題，旨在有效保護臉部圖像免受Deepfake攻擊。", "audio": "audios/2505.15336v1.mp3", "timestamp": "2025-05-22T12:38:39.258376"}
{"query": "AI", "id": "2505.15620v1", "url": "http://arxiv.org/abs/2505.15620v1", "title": "Observation of $χ_{cJ}\\to 3K_S^0K^\\pmπ^\\mp$", "summary": "By analyzing $(2712.4\\pm14.3)\\times10^6$ $\\psi(3686)$ events collected with\nthe BESIII detector operating at the BEPCII collider, the decays $\\chi_{c0,1,2}\n\\to 3K_S^0K^\\pm\\pi^\\mp$ are observed for the first time with statistical\nsignificances greater than $10\\sigma$. The branching fractions of these decays\nare determined to be $\\mathcal{B}(\\chi_{c0}\\to 3K_S^0K^\\pm\\pi^\\mp\n)=(7.95\\pm0.50\\pm0.65)\\times10^{-5},$ $\\mathcal{B}(\\chi_{c1}\\to\n3K_S^0K^\\pm\\pi^\\mp)=(2.62\\pm0.08\\pm0.19)\\times10^{-4},$ and\n$\\mathcal{B}(\\chi_{c2}\\to\n3K_S^0K^\\pm\\pi^\\mp)=(1.72\\pm0.07\\pm0.15)\\times10^{-4},$ where the first\nuncertainties are statistical and the second systematic.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "G. Chelkov", "C. Chen", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "Y. B. Chen", "Y. Q. Chen", "Z. J. Chen", "Z. K. Chen", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "Yang Gao", "S. Garbolino", "I. Garzia", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "T. Holtmann", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "B. Y. Hu", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "S. Janchiv", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "Z. H. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "Cheng Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "C. X. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. Liu", "L. C. Liu", "Lu Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. Ma", "H. L. Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "Y. H. Meng", "Z. X. Meng", "J. G. Messchendorp", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "Y. Y. Peng", "K. Peters", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "Ch. Rosner", "M. Q. Ruan", "S. N. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. X. Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "L. F. Tang", "M. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "V. Thoren", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "T. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "Lei Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "X. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. Z. Zhou", "Z. C. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-05-21", "title_zh": "$χ_{cJ}$ 衰變至 $3K_S^0K^\\pmπ^\\mp$ 的觀測", "summary_zh": "利用北京譜儀III實驗在BEPCII對撞機上收集的$(2712.4\\pm14.3)\\times10^6$個$\\psi(3686)$事件，首次觀測到衰變$\\chi_{c0,1,2} \\to 3K_S^0K^\\pm\\pi^\\mp$，統計顯著性均大於$10\\sigma$。測得分支比分別為$\\mathcal{B}(\\chi_{c0}\\to 3K_S^0K^\\pm\\pi^\\mp)=(7.95\\pm0.50\\pm0.65)\\times10^{-5}$，$\\mathcal{B}(\\chi_{c1}\\to 3K_S^0K^\\pm\\pi^\\mp)=(2.62\\pm0.08\\pm0.19)\\times10^{-4}$，以及$\\mathcal{B}(\\chi_{c2}\\to 3K_S^0K^\\pm\\pi^\\mp)=(1.72\\pm0.07\\pm0.15)\\times10^{-4}$，其中第一項誤差為統計誤差，第二項為系統誤差。", "audio": "audios/2505.15620v1.mp3", "timestamp": "2025-05-22T23:34:08.504115"}
{"query": "Foundation Model", "id": "2505.15192v1", "url": "http://arxiv.org/abs/2505.15192v1", "title": "Leveraging Foundation Models for Multimodal Graph-Based Action Recognition", "summary": "Foundation models have ushered in a new era for multimodal video\nunderstanding by enabling the extraction of rich spatiotemporal and semantic\nrepresentations. In this work, we introduce a novel graph-based framework that\nintegrates a vision-language foundation, leveraging VideoMAE for dynamic visual\nencoding and BERT for contextual textual embedding, to address the challenge of\nrecognizing fine-grained bimanual manipulation actions. Departing from\nconventional static graph architectures, our approach constructs an adaptive\nmultimodal graph where nodes represent frames, objects, and textual\nannotations, and edges encode spatial, temporal, and semantic relationships.\nThese graph structures evolve dynamically based on learned interactions,\nallowing for flexible and context-aware reasoning. A task-specific attention\nmechanism within a Graph Attention Network further enhances this reasoning by\nmodulating edge importance based on action semantics. Through extensive\nevaluations on diverse benchmark datasets, we demonstrate that our method\nconsistently outperforms state-of-the-art baselines, underscoring the strength\nof combining foundation models with dynamic graph-based reasoning for robust\nand generalizable action recognition.", "authors": ["Fatemeh Ziaeetabar", "Florentin Wörgötter"], "published_date": "2025-05-21", "title_zh": "利用基礎模型進行多模態圖基於動作識別", "summary_zh": "基礎模型為多模態影片理解帶來新紀元，能提取豐富時空與語義表徵。本文提出一種新型基於圖的框架，整合視覺-語言基礎模型，利用VideoMAE進行動態視覺編碼，BERT進行上下文文本嵌入，以解決精細雙手操作動作識別的挑戰。不同於傳統靜態圖架構，我們的方法構建自適應多模態圖，其中節點代表幀、對象和文本註釋，邊緣編碼空間、時間和語義關係。這些圖結構基於學習到的交互動態演化，實現靈活的上下文感知推理。圖注意力網絡中的任務特定注意力機制通過調節基於動作語義的邊緣重要性，進一步增強推理能力。在多個基準數據集上的廣泛評估表明，該方法始終優於最先進的基線，突顯了將基礎模型與基於動態圖的推理相結合，以實現穩健和通用的動作識別的優勢。", "audio": "audios/2505.15192v1.mp3", "timestamp": "2025-05-22T23:34:14.825615"}
{"query": "Diffusion Model", "id": "2505.15313v1", "url": "http://arxiv.org/abs/2505.15313v1", "title": "FaceCrafter: Identity-Conditional Diffusion with Disentangled Control over Facial Pose, Expression, and Emotion", "summary": "Human facial images encode a rich spectrum of information, encompassing both\nstable identity-related traits and mutable attributes such as pose, expression,\nand emotion. While recent advances in image generation have enabled\nhigh-quality identity-conditional face synthesis, precise control over\nnon-identity attributes remains challenging, and disentangling identity from\nthese mutable factors is particularly difficult. To address these limitations,\nwe propose a novel identity-conditional diffusion model that introduces two\nlightweight control modules designed to independently manipulate facial pose,\nexpression, and emotion without compromising identity preservation. These\nmodules are embedded within the cross-attention layers of the base diffusion\nmodel, enabling precise attribute control with minimal parameter overhead.\nFurthermore, our tailored training strategy, which leverages cross-attention\nbetween the identity feature and each non-identity control feature, encourages\nidentity features to remain orthogonal to control signals, enhancing\ncontrollability and diversity. Quantitative and qualitative evaluations, along\nwith perceptual user studies, demonstrate that our method surpasses existing\napproaches in terms of control accuracy over pose, expression, and emotion,\nwhile also improving generative diversity under identity-only conditioning.", "authors": ["Kazuaki Mishima", "Antoni Bigata Casademunt", "Stavros Petridis", "Maja Pantic", "Kenji Suzuki"], "published_date": "2025-05-21", "title_zh": "人臉工匠：基於身份條件的擴散模型，實現對面部姿態、表情和情緒的解耦控制", "summary_zh": "人臉圖像包含豐富資訊，包括穩定身份特徵和可變屬性，如姿態、表情和情緒。雖然圖像生成技術在身份條件人臉合成方面取得進展，但精確控制非身份屬性仍然具挑戰性，且分離身份與這些可變因素尤其困難。為解決這些問題，我們提出一種新型身份條件擴散模型，引入兩個輕量級控制模塊，獨立操縱人臉姿態、表情和情緒，同時保持身份不變。這些模塊嵌入在基礎擴散模型的交叉注意力層中，以最小的參數開銷實現精確的屬性控制。此外，我們採用定制化訓練策略，利用身份特徵和每個非身份控制特徵之間的交叉注意力，促使身份特徵與控制信號保持正交，從而提高可控性和多樣性。定量、定性評估以及感知用戶研究表明，我們的方法在姿態、表情和情緒的控制精度上優於現有方法，同時提高了僅在身份條件下的生成多樣性。", "audio": "audios/2505.15313v1.mp3", "timestamp": "2025-05-22T23:34:22.160515"}
{"query": "AI", "id": "2505.15596v1", "url": "http://arxiv.org/abs/2505.15596v1", "title": "Exploring LLM-Generated Feedback for Economics Essays: How Teaching Assistants Evaluate and Envision Its Use", "summary": "This project examines the prospect of using AI-generated feedback as\nsuggestions to expedite and enhance human instructors' feedback provision. In\nparticular, we focus on understanding the teaching assistants' perspectives on\nthe quality of AI-generated feedback and how they may or may not utilize AI\nfeedback in their own workflows. We situate our work in a foundational college\nEconomics class, which has frequent short essay assignments. We developed an\nLLM-powered feedback engine that generates feedback on students' essays based\non grading rubrics used by the teaching assistants (TAs). To ensure that TAs\ncan meaningfully critique and engage with the AI feedback, we had them complete\ntheir regular grading jobs. For a randomly selected set of essays that they had\ngraded, we used our feedback engine to generate feedback and displayed the\nfeedback as in-text comments in a Word document. We then performed think-aloud\nstudies with 5 TAs over 20 1-hour sessions to have them evaluate the AI\nfeedback, contrast the AI feedback with their handwritten feedback, and share\nhow they envision using the AI feedback if they were offered as suggestions.\nThe study highlights the importance of providing detailed rubrics for AI to\ngenerate high-quality feedback for knowledge-intensive essays. TAs considered\nthat using AI feedback as suggestions during their grading could expedite\ngrading, enhance consistency, and improve overall feedback quality. We discuss\nthe importance of decomposing the feedback generation task into steps and\npresenting intermediate results, in order for TAs to use the AI feedback.", "authors": ["Xinyi Lu", "Aditya Mahesh", "Zejia Shen", "Mitchell Dudley", "Larissa Sano", "Xu Wang"], "published_date": "2025-05-21", "title_zh": "探索大型語言模型生成之經濟學論文回饋：教學助理如何評估並設想其應用", "summary_zh": "本研究探討利用人工智慧生成的回饋意見，加速並提升人工教師回饋品質的可能性。重點在於了解助教對AI生成回饋的品質觀感，以及他們如何運用AI回饋於工作流程中。研究以大學基礎經濟學課程為背景，該課程包含頻繁的短篇論文作業。開發了一款基於大型語言模型的回饋引擎，根據助教使用的評分標準為學生論文生成回饋。為確保助教有效評估和運用AI回饋，讓他們完成常規評分工作。針對隨機選取、已評分的論文，使用回饋引擎生成回饋，並以Word文檔內文評論形式呈現。與5位助教進行20次、每次1小時的思考發聲研究，評估AI回饋，並將其與手寫回饋進行對比，分享他們如何運用AI回饋作為建議。研究強調提供詳細評分標準，對AI生成高品質知識密集型論文回饋的重要性。助教認為，在評分過程中將AI回饋作為建議，可加速評分，增強一致性，並提高整體回饋品質。討論了將回饋生成任務分解為多個步驟並呈現中間結果，對於助教使用AI回饋的重要性。", "audio": "audios/2505.15596v1.mp3", "timestamp": "2025-05-22T16:24:40.489307"}
{"query": "Foundation Model", "id": "2505.15185v1", "url": "http://arxiv.org/abs/2505.15185v1", "title": "MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth Foundation Models", "summary": "Recent advances in generalizable 3D Gaussian Splatting have demonstrated\npromising results in real-time high-fidelity rendering without per-scene\noptimization, yet existing approaches still struggle to handle unfamiliar\nvisual content during inference on novel scenes due to limited\ngeneralizability. To address this challenge, we introduce MonoSplat, a novel\nframework that leverages rich visual priors from pre-trained monocular depth\nfoundation models for robust Gaussian reconstruction. Our approach consists of\ntwo key components: a Mono-Multi Feature Adapter that transforms monocular\nfeatures into multi-view representations, coupled with an Integrated Gaussian\nPrediction module that effectively fuses both feature types for precise\nGaussian generation. Through the Adapter's lightweight attention mechanism,\nfeatures are seamlessly aligned and aggregated across views while preserving\nvaluable monocular priors, enabling the Prediction module to generate Gaussian\nprimitives with accurate geometry and appearance. Through extensive experiments\non diverse real-world datasets, we convincingly demonstrate that MonoSplat\nachieves superior reconstruction quality and generalization capability compared\nto existing methods while maintaining computational efficiency with minimal\ntrainable parameters. Codes are available at\nhttps://github.com/CUHK-AIM-Group/MonoSplat.", "authors": ["Yifan Liu", "Keyu Fan", "Weihao Yu", "Chenxin Li", "Hao Lu", "Yixuan Yuan"], "published_date": "2025-05-21", "title_zh": "MonoSplat：基於單目深度基礎模型的可泛化三維高斯濺射", "summary_zh": "通用三維高斯濺射技術雖在即時高保真渲染上展現潛力，但現有方法在處理新場景時仍因泛化能力不足而難以應對不熟悉的視覺內容。為此，我們提出MonoSplat，一種利用預訓練單眼深度基礎模型的豐富視覺先驗進行穩健高斯重建的新框架。該方法包含：將單眼特徵轉換為多視圖表示的單-多特徵適配器，以及有效融合兩種特徵以精確生成高斯分佈的集成高斯預測模組。透過適配器的輕量級注意力機制，特徵在視圖間無縫對齊和聚合，同時保留寶貴的單眼先驗，使預測模組能生成具有準確幾何和外觀的高斯圖元。在多樣真實世界資料集上的實驗證明，相較於現有方法，MonoSplat在保持計算效率和極少可訓練參數的同時，實現了卓越的重建品質和泛化能力。程式碼可在https://github.com/CUHK-AIM-Group/MonoSplat取得。", "audio": "audios/2505.15185v1.mp3", "timestamp": "2025-05-22T16:25:09.297783"}
{"query": "Diffusion Model", "id": "2505.15157v1", "url": "http://arxiv.org/abs/2505.15157v1", "title": "Cascaded Diffusion Models for Neural Motion Planning", "summary": "Robots in the real world need to perceive and move to goals in complex\nenvironments without collisions. Avoiding collisions is especially difficult\nwhen relying on sensor perception and when goals are among clutter. Diffusion\npolicies and other generative models have shown strong performance in solving\nlocal planning problems, but often struggle at avoiding all of the subtle\nconstraint violations that characterize truly challenging global motion\nplanning problems. In this work, we propose an approach for learning global\nmotion planning using diffusion policies, allowing the robot to generate full\ntrajectories through complex scenes and reasoning about multiple obstacles\nalong the path. Our approach uses cascaded hierarchical models which unify\nglobal prediction and local refinement together with online plan repair to\nensure the trajectories are collision free. Our method outperforms (by ~5%) a\nwide variety of baselines on challenging tasks in multiple domains including\nnavigation and manipulation.", "authors": ["Mohit Sharma", "Adam Fishman", "Vikash Kumar", "Chris Paxton", "Oliver Kroemer"], "published_date": "2025-05-21", "title_zh": "用於神經運動規劃的級聯擴散模型", "summary_zh": "現實環境中的機器人需要在複雜場景中感知並移動至目標點，同時避免碰撞。仰賴感測器感知且目標位於雜亂環境時，避碰尤為困難。擴散策略及其他生成模型在解決局部規劃問題上表現出色，但在避免複雜全局運動規劃問題中細微的約束違規方面往往力不從心。本研究提出一種利用擴散策略學習全局運動規劃的方法，使機器人能夠生成穿越複雜場景的完整軌跡，並推論路徑上的多個障礙物。該方法採用級聯階層模型，結合全局預測與局部優化，並透過線上計畫修正確保軌跡無碰撞。在導航和操作等多個領域的挑戰性任務中，該方法優於多種基準方法約5%。", "audio": "audios/2505.15157v1.mp3", "timestamp": "2025-05-22T16:25:16.858980"}
{"query": "AI", "id": "2505.15590v1", "url": "http://arxiv.org/abs/2505.15590v1", "title": "Bridging the Gap: Physical PCI Device Integration Into SystemC-TLM Virtual Platforms", "summary": "In today's technology-driven world, early-stage software development and\ntesting are crucial. Virtual Platforms (VPs) have become indispensable tools\nfor this purpose as they serve as a platform to execute and debug the\nunmodified target software at an early design stage. With the increasing\ncomplexity of software, especially in areas like Artificial Intelligence (AI)\napplications, VPs need to provide high simulation speed to ensure the target\nsoftware executes within a reasonable time. Hybrid simulation, which combines\nvirtual models with real hardware, can improve the performance of VPs. This\npaper introduces a novel approach for integrating real Peripheral Component\nInterconnect (PCI) devices into SystemC-TLM-2.0-based VPs. The embedded PCI\ndevices enable high performance, easy integration, and allow introspection for\nanalysis and optimization. To illustrate the practical application of our\napproach, we present a case study where we integrate Google Coral's Edge Tensor\nProcessing Unit (TPU) into an ARM-based VP. The integration allows efficient\nexecution of AI workloads, accelerating simulation speeds by up to 480x while\neliminating the need for complex virtual device models. Beyond accelerating\nAI-workload execution, our framework enables driver development, regression\ntesting across architectures, and device communication analysis. Our findings\ndemonstrate that embedding PCI devices into SystemC simulations significantly\nenhances", "authors": ["Nils Bosbach", "Rebecca Pelke", "Niko Zurstraßen", "Jan Henrik Weinstock", "Lukas Jünger", "Rainer Leupers"], "published_date": "2025-05-21", "title_zh": "彌合鴻溝：將實體PCI裝置整合至SystemC-TLM虛擬平台", "summary_zh": "在當今科技驅動的世界中，早期軟體開發與測試至關重要。虛擬平台（VP）已成為不可或缺的工具，可在設計初期執行和除錯未修改的目標軟體。隨著軟體複雜性不斷增加，尤其是在人工智慧（AI）應用領域，VP需要提供高速模擬，以確保目標軟體在合理時間內執行。混合模擬結合虛擬模型與真實硬體，可提升VP效能。本文介紹一種將真實周邊組件互連（PCI）設備整合至基於SystemC-TLM-2.0的VP的新方法。嵌入式PCI設備實現了高性能、易於整合，並允許進行內省分析與優化。為說明該方法的實際應用，本文呈現了一個案例研究，將Google Coral Edge張量處理單元（TPU）整合到基於ARM的VP中。此整合可有效執行AI工作負載，加速模擬速度高達480倍，同時免除對複雜虛擬設備模型的需求。除了加速AI工作負載執行外，該框架還可實現驅動程式開發、跨架構回歸測試以及設備通訊分析。研究結果表明，將PCI設備嵌入SystemC模擬可顯著提高效能。", "audio": "audios/2505.15590v1.mp3", "timestamp": "2025-05-22T17:17:08.437912"}
{"query": "Foundation Model", "id": "2505.15151v1", "url": "http://arxiv.org/abs/2505.15151v1", "title": "Time Tracker: Mixture-of-Experts-Enhanced Foundation Time Series Forecasting Model with Decoupled Training Pipelines", "summary": "In the past few years, time series foundation models have achieved superior\npredicting accuracy. However, real-world time series often exhibit significant\ndiversity in their temporal patterns across different time spans and domains,\nmaking it challenging for a single model architecture to fit all complex\nscenarios. In addition, time series data may have multiple variables exhibiting\ncomplex correlations between each other. Recent mainstream works have focused\non modeling times series in a channel-independent manner in both pretraining\nand finetuning stages, overlooking the valuable inter-series dependencies. To\nthis end, we propose \\textbf{Time Tracker} for better predictions on\nmultivariate time series data. Firstly, we leverage sparse mixture of experts\n(MoE) within Transformers to handle the modeling of diverse time series\npatterns, thereby alleviating the learning difficulties of a single model while\nimproving its generalization. Besides, we propose Any-variate Attention,\nenabling a unified model structure to seamlessly handle both univariate and\nmultivariate time series, thereby supporting channel-independent modeling\nduring pretraining and channel-mixed modeling for finetuning. Furthermore, we\ndesign a graph learning module that constructs relations among sequences from\nfrequency-domain features, providing more precise guidance to capture\ninter-series dependencies in channel-mixed modeling. Based on these\nadvancements, Time Tracker achieves state-of-the-art performance in predicting\naccuracy, model generalization and adaptability.", "authors": ["Xiaohou Shi", "Ke Li", "Aobo Liang", "Yan Sun"], "published_date": "2025-05-21", "title_zh": "時間追蹤器：具解耦訓練管線之混合專家增強型基礎時間序列預測模型", "summary_zh": "近年來，時間序列基礎模型展現卓越的預測精度。然而，真實世界的時間序列在不同時間跨度和領域中呈現顯著多樣性，使單一模型架構難以適應所有複雜情境。此外，時間序列資料可能有多個變數，彼此間存在複雜關聯。目前主流研究多著重於預訓練和微調階段中通道獨立的時間序列建模，忽略了重要的序列間依賴關係。為此，我們提出Time Tracker，以提升多變數時間序列資料的預測效果。首先，我們在Transformer中使用稀疏混合專家模型(MoE)，處理多樣時間序列模式的建模，從而減輕單一模型的學習難度，並提高其泛化能力。其次，我們提出Any-variate Attention，使統一模型結構能無縫處理單變數和多變數時間序列，進而支援預訓練期間的通道獨立建模，以及微調期間的通道混合建模。此外，我們設計了一個圖學習模組，從頻域特徵中構建序列間關係，為通道混合建模中捕捉序列間依賴關係提供更精確的指導。基於這些進展，Time Tracker在預測精度、模型泛化能力和適應性方面均實現了最先進的性能。", "audio": "audios/2505.15151v1.mp3", "timestamp": "2025-05-22T17:17:18.583298"}
{"query": "Diffusion Model", "id": "2505.15152v1", "url": "http://arxiv.org/abs/2505.15152v1", "title": "Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation", "summary": "Feature Transformation (FT) crafts new features from original ones via\nmathematical operations to enhance dataset expressiveness for downstream\nmodels. However, existing FT methods exhibit critical limitations: discrete\nsearch struggles with enormous combinatorial spaces, impeding practical use;\nand continuous search, being highly sensitive to initialization and step sizes,\noften becomes trapped in local optima, restricting global exploration. To\novercome these limitations, DIFFT redefines FT as a reward-guided generative\ntask. It first learns a compact and expressive latent space for feature sets\nusing a Variational Auto-Encoder (VAE). A Latent Diffusion Model (LDM) then\nnavigates this space to generate high-quality feature embeddings, its\ntrajectory guided by a performance evaluator towards task-specific optima. This\nsynthesis of global distribution learning (from LDM) and targeted optimization\n(reward guidance) produces potent embeddings, which a novel semi-autoregressive\ndecoder efficiently converts into structured, discrete features, preserving\nintra-feature dependencies while allowing parallel inter-feature generation.\nExtensive experiments on 14 benchmark datasets show DIFFT consistently\noutperforms state-of-the-art baselines in predictive accuracy and robustness,\nwith significantly lower training and inference times.", "authors": ["Nanxu Gong", "Zijun Li", "Sixun Dong", "Haoyue Bai", "Wangyang Ying", "Xinyuan Wang", "Yanjie Fu"], "published_date": "2025-05-21", "title_zh": "從噪聲雕琢特徵：獎勵導向的分層擴散用於任務最佳特徵轉換", "summary_zh": "特徵轉換透過數學運算從原始特徵中創建新特徵，以增強資料集的表達能力。現有方法存在離散搜索組合空間龐大和連續搜索易陷入局部最佳解的局限。DIFFT將特徵轉換重新定義為獎勵引導的生成任務。首先，使用變分自編碼器學習特徵集的壓縮潛在空間。然後，潛在擴散模型在此空間中生成高品質特徵嵌入，其軌跡由性能評估器引導至特定任務的最佳狀態。這種全局分佈學習和目標優化的結合產生了強大的嵌入，新型半自迴歸解碼器將其高效轉換為結構化的離散特徵，在允許並行特徵間生成的同時，保留了特徵內部的依賴關係。在14個基準資料集上的實驗表明，DIFFT在預測準確性和穩健性方面始終優於現有方法，且訓練和推論時間顯著降低。", "audio": "audios/2505.15152v1.mp3", "timestamp": "2025-05-22T17:17:25.618844"}
{"query": "AI", "id": "2505.15571v1", "url": "http://arxiv.org/abs/2505.15571v1", "title": "Temporal Spectrum Cartography in Low-Altitude Economy Networks: A Generative AI Framework with Multi-Agent Learning", "summary": "This paper introduces a two-stage generative AI (GenAI) framework tailored\nfor temporal spectrum cartography in low-altitude economy networks (LAENets).\nLAENets, characterized by diverse aerial devices such as UAVs, rely heavily on\nwireless communication technologies while facing challenges, including spectrum\ncongestion and dynamic environmental interference. Traditional spectrum\ncartography methods have limitations in handling the temporal and spatial\ncomplexities inherent to these networks. Addressing these challenges, the\nproposed framework first employs a Reconstructive Masked Autoencoder (RecMAE)\ncapable of accurately reconstructing spectrum maps from sparse and temporally\nvarying sensor data using a novel dual-mask mechanism. This approach\nsignificantly enhances the precision of reconstructed radio frequency (RF)\npower maps. In the second stage, the Multi-agent Diffusion Policy (MADP) method\nintegrates diffusion-based reinforcement learning to optimize the trajectories\nof dynamic UAV sensors. By leveraging temporal-attention encoding, this method\neffectively manages spatial exploration and exploitation to minimize cumulative\nreconstruction errors. Extensive numerical experiments validate that this\nintegrated GenAI framework outperforms traditional interpolation methods and\ndeep learning baselines by achieving 57.35% and 88.68% reconstruction error\nreduction, respectively. The proposed trajectory planner substantially improves\nspectrum map accuracy, reconstruction stability, and sensor deployment\nefficiency in dynamically evolving low-altitude environments.", "authors": ["Changyuan Zhao", "Ruichen Zhang", "Jiacheng Wang", "Dusit Niyato", "Geng Sun", "Hongyang Du", "Zan Li", "Abbas Jamalipour", "Dong In Kim"], "published_date": "2025-05-21", "title_zh": "低空經濟網絡時序頻譜製圖：基於多代理學習的生成式人工智慧框架", "summary_zh": "本研究提出一個雙階段生成式AI框架，專為低空經濟網路中的時序頻譜地圖繪製設計。針對低空經濟網路中UAV等設備對無線通訊的依賴及其面臨的頻譜擁塞和動態干擾問題，傳統頻譜地圖繪製方法難以應對其時空複雜性。此框架首先採用重建式遮罩自編碼器（RecMAE），利用雙遮罩機制，從稀疏且隨時間變化的感測器數據中精確重建頻譜圖，顯著提升射頻功率地圖的精確度。其次，多智能體擴散策略（MADP）整合基於擴散的強化學習，優化動態UAV感測器的軌跡。透過時序注意力編碼，有效管理空間探索與利用，以最小化累積重建誤差。數值實驗驗證，此生成式AI框架優於傳統內插法和深度學習基準，分別降低57.35%和88.68%的重建誤差。此軌跡規劃器顯著提升動態低空環境中的頻譜圖準確性、重建穩定性和感測器部署效率。", "audio": "audios/2505.15571v1.mp3", "timestamp": "2025-05-22T20:20:33.666103"}
{"query": "Foundation Model", "id": "2505.15147v1", "url": "http://arxiv.org/abs/2505.15147v1", "title": "From Pixels to Images: Deep Learning Advances in Remote Sensing Image Semantic Segmentation", "summary": "Remote sensing images (RSIs) capture both natural and human-induced changes\non the Earth's surface, serving as essential data for environmental monitoring,\nurban planning, and resource management. Semantic segmentation (SS) of RSIs\nenables the fine-grained interpretation of surface features, making it a\ncritical task in remote sensing analysis. With the increasing diversity and\nvolume of RSIs collected by sensors on various platforms, traditional\nprocessing methods struggle to maintain efficiency and accuracy. In response,\ndeep learning (DL) has emerged as a transformative approach, enabling\nsubstantial advances in remote sensing image semantic segmentation (RSISS) by\nautomating feature extraction and improving segmentation accuracy across\ndiverse modalities. This paper revisits the evolution of DL-based RSISS by\ncategorizing existing approaches into four stages: the early pixel-based\nmethods, the prevailing patch-based and tile-based techniques, and the emerging\nimage-based strategies enabled by foundation models. We analyze these\ndevelopments from the perspective of feature extraction and learning\nstrategies, revealing the field's progression from pixel-level to tile-level\nand from unimodal to multimodal segmentation. Furthermore, we conduct a\ncomprehensive evaluation of nearly 40 advanced techniques on a unified dataset\nto quantitatively characterize their performance and applicability. This review\noffers a holistic view of DL-based SS for RS, highlighting key advancements,\ncomparative insights, and open challenges to guide future research.", "authors": ["Quanwei Liu", "Tao Huang", "Yanni Dong", "Jiaqi Yang", "Wei Xiang"], "published_date": "2025-05-21", "title_zh": "從像素到影像：遙感影像語義分割的深度學習進展", "summary_zh": "遙感影像記錄地球表面自然與人為變遷，是環境監測、都市規劃及資源管理的重要數據。遙感影像語義分割可精細解譯地表特徵，為遙感分析的關鍵任務。面對日益增多且多樣的遙感影像，傳統方法難以兼顧效率與準確性。深度學習通過自動化特徵提取和提升分割精度，已成為遙感影像語義分割的變革性方法。本文回顧基於深度學習的遙感影像語義分割發展歷程，將現有方法分為四個階段：早期基於像素的方法、主流的基於圖像塊/瓦片的方法，以及由基礎模型驅動的新興基於圖像的方法。我們從特徵提取和學習策略角度分析這些發展，揭示該領域從像素級到瓦片級、從單模態到多模態分割的演進。此外，我們在統一數據集上對近40種先進技術進行全面評估，量化其性能和適用性。本綜述全面展示基於深度學習的遙感影像語義分割，重點介紹關鍵進展、比較性見解和未決挑戰，以指導未來研究。", "audio": "audios/2505.15147v1.mp3", "timestamp": "2025-05-22T20:20:43.447800"}
{"query": "Diffusion Model", "id": "2505.15093v1", "url": "http://arxiv.org/abs/2505.15093v1", "title": "Steering Generative Models with Experimental Data for Protein Fitness Optimization", "summary": "Protein fitness optimization involves finding a protein sequence that\nmaximizes desired quantitative properties in a combinatorially large design\nspace of possible sequences. Recent developments in steering protein generative\nmodels (e.g diffusion models, language models) offer a promising approach.\nHowever, by and large, past studies have optimized surrogate rewards and/or\nutilized large amounts of labeled data for steering, making it unclear how well\nexisting methods perform and compare to each other in real-world optimization\ncampaigns where fitness is measured by low-throughput wet-lab assays. In this\nstudy, we explore fitness optimization using small amounts (hundreds) of\nlabeled sequence-fitness pairs and comprehensively evaluate strategies such as\nclassifier guidance and posterior sampling for guiding generation from\ndifferent discrete diffusion models of protein sequences. We also demonstrate\nhow guidance can be integrated into adaptive sequence selection akin to\nThompson sampling in Bayesian optimization, showing that plug-and-play guidance\nstrategies offer advantages compared to alternatives such as reinforcement\nlearning with protein language models.", "authors": ["Jason Yang", "Wenda Chu", "Daniel Khalil", "Raul Astudillo", "Bruce J. Wittmann", "Frances H. Arnold", "Yisong Yue"], "published_date": "2025-05-21", "title_zh": "利用實驗數據導引生成模型以優化蛋白質適應性", "summary_zh": "蛋白質適應性最佳化旨在廣大的序列空間中尋找能最大化所需定量性質的蛋白質序列。引導蛋白質生成模型（如擴散模型、語言模型）是個有潛力的途徑。然而，過去研究主要優化替代獎勵或依賴大量標記數據進行引導，難以評估現有方法在真實實驗中的表現和相互比較，因真實實驗通常以低通量濕實驗測定適應性。本研究探討使用少量（數百個）標記序列-適應性配對進行適應性最佳化，並全面評估分類器引導和後驗抽樣等策略，以引導不同蛋白質序列離散擴散模型的生成。我們也展示如何將引導整合到類似貝葉斯最佳化中湯普森抽樣的自適應序列選擇中，表明隨插即用引導策略優於使用蛋白質語言模型的強化學習等替代方案。", "audio": "audios/2505.15093v1.mp3", "timestamp": "2025-05-22T20:20:56.741332"}
{"query": "AI", "id": "2505.15528v1", "url": "http://arxiv.org/abs/2505.15528v1", "title": "PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided Gaussian Splatting", "summary": "Recent years have seen substantial improvements in the ability to generate\nsynthetic 3D objects using AI. However, generating complex 3D objects, such as\nplants, remains a considerable challenge. Current generative 3D models struggle\nwith plant generation compared to general objects, limiting their usability in\nplant analysis tools, which require fine detail and accurate geometry. We\nintroduce PlantDreamer, a novel approach to 3D synthetic plant generation,\nwhich can achieve greater levels of realism for complex plant geometry and\ntextures than available text-to-3D models. To achieve this, our new generation\npipeline leverages a depth ControlNet, fine-tuned Low-Rank Adaptation and an\nadaptable Gaussian culling algorithm, which directly improve textural realism\nand geometric integrity of generated 3D plant models. Additionally,\nPlantDreamer enables both purely synthetic plant generation, by leveraging\nL-System-generated meshes, and the enhancement of real-world plant point clouds\nby converting them into 3D Gaussian Splats. We evaluate our approach by\ncomparing its outputs with state-of-the-art text-to-3D models, demonstrating\nthat PlantDreamer outperforms existing methods in producing high-fidelity\nsynthetic plants. Our results indicate that our approach not only advances\nsynthetic plant generation, but also facilitates the upgrading of legacy point\ncloud datasets, making it a valuable tool for 3D phenotyping applications.", "authors": ["Zane K J Hartley", "Lewis A G Stuart", "Andrew P French", "Michael P Pound"], "published_date": "2025-05-21", "title_zh": "PlantDreamer：藉由擴散引導高斯潑濺實現逼真3D植物模型", "summary_zh": "近年人工智慧在3D物件生成能力上顯著提升，然複雜物件如植物之生成仍具挑戰。現有3D生成模型於植物生成方面表現遜於一般物件，限制其於植物分析工具之應用，因該等工具需精細細節與精確幾何。本研究提出PlantDreamer，一種新穎之3D合成植物生成方法，相較於現有文字轉3D模型，能實現更高層次之複雜植物幾何與紋理真實感。為此，本研究之生成流程採用深度ControlNet、微調之低秩適應及可調整之高斯剔除演算法，直接改善生成之3D植物模型之紋理真實感與幾何完整性。此外，PlantDreamer能藉由L系統生成之網格實現純合成植物生成，並透過將真實世界植物點雲轉換為3D高斯潑濺，進而強化該點雲。經由與最先進文字轉3D模型之比較評估，PlantDreamer在生成高保真合成植物方面優於現有方法。研究結果顯示，本方法不僅推進合成植物生成，亦有助於升級舊有點雲資料集，使其成為3D表型分析應用之寶貴工具。", "audio": "audios/2505.15528v1.mp3", "timestamp": "2025-05-22T21:17:16.678569"}
{"query": "Foundation Model", "id": "2505.15132v1", "url": "http://arxiv.org/abs/2505.15132v1", "title": "Multicrossmodal Automated Agent for Integrating Diverse Materials Science Data", "summary": "We introduce a multicrossmodal LLM-agent framework motivated by the growing\nvolume and diversity of materials-science data ranging from high-resolution\nmicroscopy and dynamic simulation videos to tabular experiment logs and\nsprawling literature archives. While recent AI efforts have accelerated\nindividual tasks such as property prediction or image classification, they\ntypically treat each modality in isolation, leaving rich cross-modal\ncorrelations unexplored and forcing researchers to perform laborious manual\nintegration. Moreover, existing multimodal foundation models often require\nexpensive retraining or fine-tuning on domain data, and current multi-agent\nsystems in materials informatics address only narrow subtasks. To overcome\nthese obstacles, we design a coordinated team of specialized LLM agents, each\nequipped with domain-adapted prompts and plugins that project their outputs\ninto a shared embedding space. A dynamic gating mechanism then weights and\nmerges these insights, enabling unified reasoning over heterogeneous inputs\nwithout ever modifying the underlying LLM weights. We validate our approach on\nchallenging case studies and demonstrate substantial gains in retrieval\naccuracy (85%), captioning fidelity, and integrated coverage (35%) compared to\nsingle-modality and zero-shot baselines. Our work paves the way for AI digital\nresearchers capable of bridging data silos and accelerating the\nmaterials-discovery cycle. The code is available at\nhttps://github.com/adibgpt/Multicrossmodal-Autonomous-Materials-Science-Agent.", "authors": ["Adib Bazgir", "Rama chandra Praneeth Madugula", "Yuwen Zhang"], "published_date": "2025-05-21", "title_zh": "用於整合多樣材料科學數據的多模態自動化代理", "summary_zh": "本研究提出一個多重跨模態大型語言模型代理框架，旨在應對材料科學領域日益增長且多樣化的數據，包含高解析度顯微鏡影像、動態模擬影片、表格實驗日誌以及大量的文獻檔案。現有AI研究雖加速了材料性質預測或影像分類等任務，但通常孤立地處理各模態數據，忽略了豐富的跨模態關聯性，導致研究人員需耗費大量精力進行人工整合。此外，現有的多模態基礎模型往往需要在領域數據上進行昂貴的重新訓練或微調，而目前的材料資訊學多代理系統僅解決狹窄的子任務。為克服這些障礙，我們設計了一個協調的專用大型語言模型代理團隊，每個代理都配備了領域特定的提示和插件，將其輸出投影到共享嵌入空間中。然後，動態門控機制對這些見解進行加權和合併，實現對異質輸入的統一推理，且無需修改底層大型語言模型的權重。我們在具挑戰性的案例研究中驗證了該方法，並證明與單模態和零樣本基準相比，檢索準確度（85%）、字幕保真度和整合覆蓋率（35%）顯著提升。本研究為能夠橋接數據孤島並加速材料發現週期的人工智慧數位研究人員奠定了基礎。", "audio": "audios/2505.15132v1.mp3", "timestamp": "2025-05-22T21:17:24.359867"}
{"query": "Diffusion Model", "id": "2505.15077v1", "url": "http://arxiv.org/abs/2505.15077v1", "title": "Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation", "summary": "Urban forests play a key role in enhancing environmental quality and\nsupporting biodiversity in cities. Mapping and monitoring these green spaces\nare crucial for urban planning and conservation, yet accurately detecting trees\nis challenging due to complex landscapes and the variability in image\nresolution caused by different satellite sensors or UAV flight altitudes. While\ndeep learning architectures have shown promise in addressing these challenges,\ntheir effectiveness remains strongly dependent on the availability of large and\nmanually labeled datasets, which are often expensive and difficult to obtain in\nsufficient quantity. In this work, we propose a novel pipeline that integrates\ndomain adaptation with GANs and Diffusion models to enhance the quality of\nlow-resolution aerial images. Our proposed pipeline enhances low-resolution\nimagery while preserving semantic content, enabling effective tree segmentation\nwithout requiring large volumes of manually annotated data. Leveraging models\nsuch as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we\ngenerate realistic and structurally consistent synthetic samples that expand\nthe training dataset and unify scale across domains. This approach not only\nimproves the robustness of segmentation models across different acquisition\nconditions but also provides a scalable and replicable solution for remote\nsensing scenarios with scarce annotation resources. Experimental results\ndemonstrated an improvement of over 50% in IoU for low-resolution images,\nhighlighting the effectiveness of our method compared to traditional pipelines.", "authors": ["Alessandro dos Santos Ferreira", "Ana Paula Marques Ramos", "José Marcato Junior", "Wesley Nunes Gonçalves"], "published_date": "2025-05-21", "title_zh": "基於GAN與擴散模型的數據增強和分辨率提升於樹木分割", "summary_zh": "都市森林對於提升城市環境品質和支持生物多樣性至關重要。繪製和監測這些綠地對於城市規劃和保育至關重要，然而，由於複雜的景觀以及不同衛星感測器或無人機飛行高度導致的影像解析度變化，準確檢測樹木極具挑戰性。深度學習架構已展現解決這些挑戰的潛力，但其有效性仍高度依賴於大量手動標記資料集，而這些資料集通常成本高昂且難以充分獲取。本研究提出一種新穎流程，整合領域自適應與GANs和擴散模型，以提高低解析度航拍影像的品質。此流程增強了低解析度影像，同時保留了語義內容，從而實現有效的樹木分割，而無需大量手動註釋資料。利用pix2pix、Real-ESRGAN、潛在擴散和穩定擴散等模型，生成逼真且結構一致的合成樣本，以擴展訓練資料集並統一跨領域的尺度。此方法不僅提高了分割模型在不同獲取條件下的穩健性，還為註釋資源稀缺的遙感場景提供了可擴展和可複製的解決方案。實驗結果表明，低解析度影像的IoU提高了50%以上，突顯了該方法相較於傳統流程的有效性。", "audio": "audios/2505.15077v1.mp3", "timestamp": "2025-05-22T21:17:32.045043"}
{"query": "AI", "id": "2505.15519v1", "url": "http://arxiv.org/abs/2505.15519v1", "title": "Exploiting Age of Information in Network Digital Twins for AI-driven Real-Time Link Blockage Detection", "summary": "The Line-of-Sight (LoS) identification is crucial to ensure reliable\nhigh-frequency communication links, especially those vulnerable to blockages.\nNetwork Digital Twins and Artificial Intelligence are key technologies enabling\nblockage detection (LoS identification) for high-frequency wireless systems,\ne.g., 6>GHz. In this work, we enhance Network Digital Twins by incorporating\nAge of Information (AoI) metrics, a quantification of status update freshness,\nenabling reliable real-time blockage detection (LoS identification) in dynamic\nwireless environments. By integrating raytracing techniques, we automate\nlarge-scale collection and labeling of channel data, specifically tailored to\nthe evolving conditions of the environment. The introduced AoI is integrated\nwith the loss function to prioritize more recent information to fine-tune deep\nlearning models in case of performance degradation (model drift). The\neffectiveness of the proposed solution is demonstrated in realistic urban\nsimulations, highlighting the trade-off between input resolution, computational\ncost, and model performance. A resolution reduction of 4x8 from an original\nchannel sample size of (32, 1024) along the angle and subcarrier dimension\nresults in a computational speedup of 32 times. The proposed fine-tuning\nsuccessfully mitigates performance degradation while requiring only 1% of the\navailable data samples, enabling automated and fast mitigation of model drifts.", "authors": ["Michele Zhu", "Francesco Linsalata", "Silvia Mura", "Lorenzo Cazzella", "Damiano Badini", "Umberto Spagnolini"], "published_date": "2025-05-21", "title_zh": "利用網絡數位孿生中的資訊年齡實現人工智慧驅動的即時鏈路阻塞偵測", "summary_zh": "視距(LoS)識別對於確保可靠的高頻通訊鏈路至關重要，特別是易受阻擋的鏈路。網絡數位雙生與人工智慧是實現高頻無線系統（如6GHz以上頻段）阻擋偵測（LoS識別）的關鍵技術。本研究通過納入資訊年齡(AoI)指標來強化網絡數位雙生，量化狀態更新的新鮮度，從而在動態無線環境中實現可靠的即時阻擋偵測（LoS識別）。通過整合射線追蹤技術，我們自動化大規模通道數據的收集與標記，專門針對不斷變化的環境條件。引入的AoI與損失函數整合，優先處理較新的資訊，以在性能下降（模型漂移）時微調深度學習模型。在真實的城市模擬中驗證了所提出解決方案的有效性，突顯了輸入分辨率、計算成本和模型性能之間的權衡。相較於(32, 1024)的原始通道樣本尺寸，角度和子載波維度的分辨率降低4x8倍，計算速度提升32倍。所提出的微調成功緩解了性能下降，同時僅需1%的可用數據樣本，實現了自動化且快速的模型漂移緩解。", "audio": "audios/2505.15519v1.mp3", "timestamp": "2025-05-22T23:17:30.158419"}
{"query": "Foundation Model", "id": "2505.15116v1", "url": "http://arxiv.org/abs/2505.15116v1", "title": "Graph Foundation Models: A Comprehensive Survey", "summary": "Graph-structured data pervades domains such as social networks, biological\nsystems, knowledge graphs, and recommender systems. While foundation models\nhave transformed natural language processing, vision, and multimodal learning\nthrough large-scale pretraining and generalization, extending these\ncapabilities to graphs -- characterized by non-Euclidean structures and complex\nrelational semantics -- poses unique challenges and opens new opportunities. To\nthis end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose\nintelligence to structured data, enabling broad transfer across graph-centric\ntasks and domains. This survey provides a comprehensive overview of GFMs,\nunifying diverse efforts under a modular framework comprising three key\ncomponents: backbone architectures, pretraining strategies, and adaptation\nmechanisms. We categorize GFMs by their generalization scope -- universal,\ntask-specific, and domain-specific -- and review representative methods, key\ninnovations, and theoretical insights within each category. Beyond methodology,\nwe examine theoretical foundations including transferability and emergent\ncapabilities, and highlight key challenges such as structural alignment,\nheterogeneity, scalability, and evaluation. Positioned at the intersection of\ngraph learning and general-purpose AI, GFMs are poised to become foundational\ninfrastructure for open-ended reasoning over structured data. This survey\nconsolidates current progress and outlines future directions to guide research\nin this rapidly evolving field. Resources are available at\nhttps://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.", "authors": ["Zehong Wang", "Zheyuan Liu", "Tianyi Ma", "Jiazheng Li", "Zheyuan Zhang", "Xingbo Fu", "Yiyang Li", "Zhengqing Yuan", "Wei Song", "Yijun Ma", "Qingkai Zeng", "Xiusi Chen", "Jianan Zhao", "Jundong Li", "Meng Jiang", "Pietro Lio", "Nitesh Chawla", "Chuxu Zhang", "Yanfang Ye"], "published_date": "2025-05-21", "title_zh": "圖基礎模型：一份綜合綜述", "summary_zh": "圖結構數據廣泛存在於社交網路、生物系統、知識圖譜及推薦系統等領域。大型預訓練模型已革新自然語言處理、視覺及多模態學習，然將此能力擴展至具非歐幾里德結構及複雜關係語義的圖數據，面臨獨特挑戰及機遇。圖基礎模型旨在為結構化數據帶來可擴展的通用智能，實現跨圖中心任務與領域的廣泛遷移。本綜述提供圖基礎模型的全面概述，透過包含主幹架構、預訓練策略和適應機制的三模塊框架整合各項研究。依據通用性範圍（通用、任務特定及領域特定）對圖基礎模型進行分類，並回顧各類別的代表性方法、創新及理論見解。除方法論外，亦檢視包含可遷移性和湧現能力在內的理論基礎，並強調結構對齊、異質性、可擴展性及評估等關鍵挑戰。圖基礎模型位於圖學習與通用人工智慧的交叉點，有望成為結構化數據開放式推理的基礎設施。本綜述總結當前進展並概述未來方向，以指導此快速發展領域的研究。資源位於https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs。", "audio": "audios/2505.15116v1.mp3", "timestamp": "2025-05-22T23:17:42.449436"}
{"query": "Diffusion Model", "id": "2505.15064v1", "url": "http://arxiv.org/abs/2505.15064v1", "title": "Generalization Through Growth: Hidden Dynamics Controls Depth Dependence", "summary": "Recent theory has reduced the depth dependence of generalization bounds from\nexponential to polynomial and even depth-independent rates, yet these results\nremain tied to specific architectures and Euclidean inputs. We present a\nunified framework for arbitrary \\blue{pseudo-metric} spaces in which a\ndepth-\\(k\\) network is the composition of continuous hidden maps\n\\(f:\\mathcal{X}\\to \\mathcal{X}\\) and an output map \\(h:\\mathcal{X}\\to\n\\mathbb{R}\\). The resulting bound $O(\\sqrt{(\\alpha + \\log \\beta(k))/n})$\nisolates the sole depth contribution in \\(\\beta(k)\\), the word-ball growth of\nthe semigroup generated by the hidden layers. By Gromov's theorem polynomial\n(resp. exponential) growth corresponds to virtually nilpotent (resp. expanding)\ndynamics, revealing a geometric dichotomy behind existing $O(\\sqrt{k})$\n(sublinear depth) and $\\tilde{O}(1)$ (depth-independent) rates. We further\nprovide covering-number estimates showing that expanding dynamics yield an\nexponential parameter saving via compositional expressivity. Our results\ndecouple specification from implementation, offering architecture-agnostic and\ndynamical-systems-aware guarantees applicable to modern deep-learning paradigms\nsuch as test-time inference and diffusion models.", "authors": ["Sho Sonoda", "Yuka Hashimoto", "Isao Ishikawa", "Masahiro Ikeda"], "published_date": "2025-05-21", "title_zh": "透過成長實現泛化：隱藏動態控制深度依賴性", "summary_zh": "近期理論已將泛化邊界的深度依賴性從指數級降低到多項式級甚至深度無關的速率，但這些結果仍與特定架構和歐幾里得輸入相關。我們提出了一個統一框架，適用於任意偽度量空間，其中深度為\\(k\\)的網路是連續隱藏映射\\(f:\\mathcal{X}\\to \\mathcal{X}\\)和輸出映射\\(h:\\mathcal{X}\\to \\mathbb{R}\\)的組合。產生的邊界\\(O(\\sqrt{(\\alpha + \\log \\beta(k))/n})\\)將唯一的深度貢獻隔離在\\(\\beta(k)\\)中，即隱藏層生成的半群的詞球增長。藉由Gromov定理，多項式（或指數）增長對應於幾乎冪零（或擴張）動力學，揭示了現有\\(O(\\sqrt{k})\\)（亞線性深度）和\\(\\tilde{O}(1)\\)（深度無關）速率背後的幾何二分法。我們進一步提供覆蓋數估計，表明擴張動力學透過組合表達性實現了指數級參數節省。我們的結果將規範與實現分離，提供架構無關且具有動態系統感知的保證，適用於現代深度學習範例，如測試時推論和擴散模型。", "audio": "audios/2505.15064v1.mp3", "timestamp": "2025-05-22T23:17:55.865232"}
{"query": "AI", "id": "2505.15516v1", "url": "http://arxiv.org/abs/2505.15516v1", "title": "Explainable embeddings with Distance Explainer", "summary": "While eXplainable AI (XAI) has advanced significantly, few methods address\ninterpretability in embedded vector spaces where dimensions represent complex\nabstractions. We introduce Distance Explainer, a novel method for generating\nlocal, post-hoc explanations of embedded spaces in machine learning models. Our\napproach adapts saliency-based techniques from RISE to explain the distance\nbetween two embedded data points by assigning attribution values through\nselective masking and distance-ranked mask filtering. We evaluate Distance\nExplainer on cross-modal embeddings (image-image and image-caption pairs) using\nestablished XAI metrics including Faithfulness, Sensitivity/Robustness, and\nRandomization. Experiments with ImageNet and CLIP models demonstrate that our\nmethod effectively identifies features contributing to similarity or\ndissimilarity between embedded data points while maintaining high robustness\nand consistency. We also explore how parameter tuning, particularly mask\nquantity and selection strategy, affects explanation quality. This work\naddresses a critical gap in XAI research and enhances transparency and\ntrustworthiness in deep learning applications utilizing embedded spaces.", "authors": ["Christiaan Meijer", "E. G. Patrick Bos"], "published_date": "2025-05-21", "title_zh": "基於距離解釋器的可解釋嵌入", "summary_zh": "可解釋人工智慧(XAI)雖有顯著進展，但針對嵌入向量空間(其維度代表複雜抽象概念)之可解釋性的方法仍然有限。本研究提出距離解釋器(Distance Explainer)，一種新穎的後設局部解釋方法，用於解釋機器學習模型中嵌入空間的距離。本方法改編自RISE的顯著性技術，透過選擇性遮罩和距離排序遮罩過濾來分配歸因值，以解釋兩個嵌入資料點之間的距離。我們使用既定的XAI指標(包括忠實度、敏感度/穩健性和隨機化)在跨模態嵌入(圖像-圖像和圖像-標題對)上評估距離解釋器。使用ImageNet和CLIP模型的實驗表明，該方法有效地識別了有助於嵌入資料點之間相似性或相異性的特徵，同時保持高度的穩健性和一致性。我們還探討了參數調整，特別是遮罩數量和選擇策略，如何影響解釋品質。本研究解決了XAI研究中的一個關鍵缺口，並提高了利用嵌入空間的深度學習應用程式的透明度和可信度。", "audio": "audios/2505.15516v1.mp3", "timestamp": "2025-05-23T01:27:16.595148"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略引導展平層級結構", "summary_zh": "離線目標條件強化學習(GCRL)有望於大規模無獎勵軌跡數據集上預訓練通用策略，類似於電腦視覺和自然語言處理中用於訓練基礎模型的自監督目標。然而，由於稀疏獎勵和折扣的組合，將GCRL擴展到更長的時間範圍仍然具有挑戰性，這模糊了原始動作相對於遠期目標的比較優勢。分層強化學習方法在長程目標達成任務上取得了良好的實證結果，但其對模組化、特定時間尺度策略和子目標生成的依賴引入了顯著的額外複雜性，並阻礙了在高維目標空間中的擴展。本研究提出一種演算法，透過優勢加權重要性抽樣，基於子目標條件策略進行自舉，以訓練扁平(非分層)目標條件策略。此方法無需目標空間上的生成模型，這對於在大狀態空間中擴展到高維控制至關重要。我們進一步表明，現有的分層和基於自舉的方法對應於我們推導中的特定設計選擇。在一系列全面的基於狀態和像素的運動和操控基準測試中，我們的方法與最先進的離線GCRL演算法相匹配或超越，並可擴展到先前方法失敗的複雜長程任務。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-23T01:27:26.087450"}
{"query": "Diffusion Model", "id": "2505.15057v1", "url": "http://arxiv.org/abs/2505.15057v1", "title": "Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine Diffusion Models", "summary": "Magnetic Resonance Imaging (MRI) is highly susceptible to motion artifacts\ndue to the extended acquisition times required for k-space sampling. These\nartifacts can compromise diagnostic utility, particularly for dynamic imaging.\nWe propose a novel alternating minimization framework that leverages a bespoke\ndiffusion model to jointly reconstruct and correct non-rigid motion-corrupted\nk-space data. The diffusion model uses a coarse-to-fine denoising strategy to\ncapture large overall motion and reconstruct the lower frequencies of the image\nfirst, providing a better inductive bias for motion estimation than that of\nstandard diffusion models. We demonstrate the performance of our approach on\nboth real-world cine cardiac MRI datasets and complex simulated rigid and\nnon-rigid deformations, even when each motion state is undersampled by a factor\nof 64x. Additionally, our method is agnostic to sampling patterns, anatomical\nvariations, and MRI scanning protocols, as long as some low frequency\ncomponents are sampled during each motion state.", "authors": ["Frederic Wang", "Jonathan I. Tamir"], "published_date": "2025-05-21", "title_zh": "基於粗細粒度擴散模型的磁共振成像非剛性運動校正重建", "summary_zh": "磁振造影易受運動偽影影響，因其k空間採樣需時較長，此偽影損害診斷效用，尤其於動態影像。本研究提出一種新型交替最小化框架，利用客製化擴散模型聯合重建並校正非剛性運動所汙染之k空間數據。此擴散模型採用由粗到細的去噪策略，先捕捉整體大範圍運動並重建影像低頻部分，為運動估計提供優於標準擴散模型的歸納偏置。實驗結果顯示，本方法於真實心臟電影磁振造影數據集以及複雜模擬的剛性和非剛性變形中皆表現良好，即使在每個運動狀態下欠採樣64倍時亦然。此外，本方法不受採樣模式、解剖結構變異和磁振造影掃描協議的限制，惟需確保在每個運動狀態下皆採樣部分低頻成分。", "audio": "audios/2505.15057v1.mp3", "timestamp": "2025-05-23T01:27:33.657456"}
{"query": "AI", "id": "2505.17021v1", "url": "http://arxiv.org/abs/2505.17021v1", "title": "ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark", "summary": "As Large Multimodal Models (LMMs) become more capable, there is growing\ninterest in evaluating their reasoning processes alongside their final outputs.\nHowever, most benchmarks remain focused on English, overlooking languages with\nrich linguistic and cultural contexts, such as Arabic. To address this gap, we\nintroduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the\nfirst benchmark designed to evaluate step-by-step reasoning in Arabic across\nboth textual and visual modalities. ARB spans 11 diverse domains, including\nvisual reasoning, document understanding, OCR, scientific analysis, and\ncultural interpretation. It comprises 1,356 multimodal samples paired with\n5,119 human-curated reasoning steps and corresponding actions. We evaluated 12\nstate-of-the-art open- and closed-source LMMs and found persistent challenges\nin coherence, faithfulness, and cultural grounding. ARB offers a structured\nframework for diagnosing multimodal reasoning in underrepresented languages and\nmarks a critical step toward inclusive, transparent, and culturally aware AI\nsystems. We release the benchmark, rubric, and evaluation suit to support\nfuture research and reproducibility. Code available at:\nhttps://github.com/mbzuai-oryx/ARB", "authors": ["Sara Ghaboura", "Ketan More", "Wafa Alghallabi", "Omkar Thawakar", "Jorma Laaksonen", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "published_date": "2025-05-22", "title_zh": "ARB：綜合性阿拉伯語多模態推理基準", "summary_zh": "大型多模態模型能力日漸提升，對其推理過程的評估日益重要。然而，現有評測多集中於英語，忽略了如阿拉伯語等具有豐富語言及文化背景的語種。為此，我們推出綜合阿拉伯語多模態推理基準（ARB），首個旨在評估阿拉伯語文本與視覺模態逐步推理的基準。ARB涵蓋視覺推理、文檔理解、光學字元識別、科學分析和文化詮釋等11個領域，包含1356個多模態樣本，並搭配5119個人工校正的推理步驟與相應行動。對12個頂尖開放及封閉源碼大型多模態模型的評估顯示，模型在連貫性、忠實性和文化基礎方面仍面臨挑戰。ARB為診斷代表性不足語言的多模態推理提供結構化框架，是邁向具包容性、透明化和文化意識的人工智慧系統的關鍵一步。我們公開基準、評分標準與評估套件，以支持未來研究和可重複性。程式碼見：https://github.com/mbzuai-oryx/ARB", "audio": "audios/2505.17021v1.mp3", "timestamp": "2025-05-23T03:10:34.878775"}
{"query": "Foundation Model", "id": "2505.16982v1", "url": "http://arxiv.org/abs/2505.16982v1", "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "summary": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "published_date": "2025-05-22", "title_zh": "超越相關性：邁向生物醫學領域的因果大型語言模型代理", "summary_zh": "大型語言模型在生物醫學領域展現潛力，但缺乏真正的因果理解，僅依賴關聯性。本文設想因果大型語言模型代理，整合多模態數據（文本、圖像、基因組等），並執行基於干預的推理以推斷因果關係。實現此目標需克服多項挑戰：設計安全、可控的代理框架；開發嚴格的因果評估基準；整合異質數據源；以及將大型語言模型與結構化知識圖譜和正式因果推論工具結合。此類代理可釋放變革性機遇，包含透過自動假設生成和模擬加速藥物發現，並透過特定患者的因果模型實現個人化醫療。此研究旨在促進跨學科合作，橋接因果概念與基礎模型，以開發可靠的人工智慧夥伴，促進生物醫學進展。", "audio": "audios/2505.16982v1.mp3", "timestamp": "2025-05-23T03:10:43.637960"}
{"query": "Diffusion Model", "id": "2505.17013v1", "url": "http://arxiv.org/abs/2505.17013v1", "title": "When Are Concepts Erased From Diffusion Models?", "summary": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models.", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "published_date": "2025-05-22", "title_zh": "擴散模型中概念的擦除時機", "summary_zh": "概念擦除，即選擇性阻止模型生成特定概念的能力，備受關注，並湧現多種方法應對此挑戰。然而，這些方法擦除目標概念的徹底程度仍不明確。本文首先提出擴散模型中擦除機制的兩種概念模型：(一)降低生成目標概念的可能性，(二)干擾模型的內部引導機制。為徹底評估概念是否已從模型中真正擦除，本文引入一套獨立評估方法，包含對抗性攻擊、新型探測技術，以及對模型替代生成的分析。研究結果闡明了最小化副作用與維持對抗性提示的穩健性之間的緊張關係。總體而言，本文強調了對擴散模型中擦除進行全面評估的重要性。", "audio": "audios/2505.17013v1.mp3", "timestamp": "2025-05-23T03:10:50.888157"}
{"query": "AI", "id": "2505.17019v1", "url": "http://arxiv.org/abs/2505.17019v1", "title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework", "summary": "Metaphorical comprehension in images remains a critical challenge for AI\nsystems, as existing models struggle to grasp the nuanced cultural, emotional,\nand contextual implications embedded in visual content. While multimodal large\nlanguage models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they\nstruggle with a fundamental limitation on image implication tasks: contextual\ngaps that obscure the relationships between different visual elements and their\nabstract meanings. Inspired by the human cognitive process, we propose Let\nAndroids Dream (LAD), a novel framework for image implication understanding and\nreasoning. LAD addresses contextual missing through the three-stage framework:\n(1) Perception: converting visual information into rich and multi-level textual\nrepresentations, (2) Search: iteratively searching and integrating cross-domain\nknowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment\nimage implication via explicit reasoning. Our framework with the lightweight\nGPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English\nimage implication benchmark and a huge improvement on Chinese benchmark,\nperforming comparable with the GPT-4o model on Multiple-Choice Question (MCQ)\nand outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work\nprovides new insights into how AI can more effectively interpret image\nimplications, advancing the field of vision-language reasoning and human-AI\ninteraction. Our project is publicly available at\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.", "authors": ["Chenhao Zhang", "Yazhe Niu"], "published_date": "2025-05-22", "title_zh": "安卓是否夢見電子羊：類人圖像意涵理解與推理框架", "summary_zh": "圖像隱喻理解對人工智慧系統構成重大挑戰，現有模型難以掌握視覺內容中細微的文化、情感與情境意涵。多模態大型語言模型雖擅長基本視覺問答，但在圖像意涵任務中受限於情境缺口，難以辨識視覺元素間的關係及其抽象意義。受人類認知啟發，我們提出Let Androids Dream (LAD)框架，用於圖像意涵理解與推理。LAD透過三階段解決情境缺失：(1)感知：將視覺資訊轉化為豐富的多層次文本表示；(2)搜尋：迭代搜尋並整合跨領域知識以消除歧義；(3)推理：透過顯式推理產生情境對齊的圖像意涵。搭載輕量級GPT-4o-mini模型，本框架在英語圖像意涵基準測試中達到最佳效能，並在中文基準測試中實現顯著提升，在選擇題上與GPT-4o模型相當，在開放式問題上超越36.7%。本研究為人工智慧如何更有效地解讀圖像意涵提供了新見解，推進了視覺語言推理與人機互動領域。專案已公開。", "audio": "audios/2505.17019v1.mp3", "timestamp": "2025-05-23T04:22:00.731004"}
{"query": "Foundation Model", "id": "2505.16941v1", "url": "http://arxiv.org/abs/2505.16941v1", "title": "FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records", "summary": "Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models.", "authors": ["Chao Pang", "Vincent Jeanselme", "Young Sang Choi", "Xinzhuo Jiang", "Zilin Jing", "Aparajita Kashyap", "Yuta Kobayashi", "Yanwei Li", "Florent Pollet", "Karthik Natarajan", "Shalmali Joshi"], "published_date": "2025-05-22", "title_zh": "FoMoH：結構化電子病歷臨床意義基礎模型評估", "summary_zh": "基於其獨立於下游任務提取有意義表徵的能力，基石模型在醫療保健領域展現出巨大潛力。此特性已促成結構化電子病歷數據訓練的多項臨床應用達到頂尖效能，即使在標記數據有限的環境中，這也是醫療保健領域常見的挑戰。然而，由於缺乏對全面且有意義任務的理想要求以及充分多樣化的評估以描述相較於傳統監督式學習的優勢，對於這些模型在臨床上的實用性鮮少共識。為了解決此差距，我們提出一系列具有臨床意義的任務，涵蓋患者預後、急慢性疾病的早期預測，並包含穩健評估的理想要求。我們使用來自紐約市哥倫比亞大學歐文醫學中心 (CUMC) 的五百萬患者的電子病歷數據，在 14 項臨床相關任務中評估了最先進的基石模型。我們測量了總體準確性、校準和亞群體效能，以揭示基於預訓練、標記化和數據表示策略選擇的權衡。本研究旨在推進結構化電子病歷基石模型的實證評估，並指導未來醫療保健基石模型的開發。", "audio": "audios/2505.16941v1.mp3", "timestamp": "2025-05-23T04:22:07.646757"}
{"query": "Diffusion Model", "id": "2505.17004v1", "url": "http://arxiv.org/abs/2505.17004v1", "title": "Guided Diffusion Sampling on Function Spaces with Applications to PDEs", "summary": "We propose a general framework for conditional sampling in PDE-based inverse\nproblems, targeting the recovery of whole solutions from extremely sparse or\nnoisy measurements. This is accomplished by a function-space diffusion model\nand plug-and-play guidance for conditioning. Our method first trains an\nunconditional discretization-agnostic denoising model using neural operator\narchitectures. At inference, we refine the samples to satisfy sparse\nobservation data via a gradient-based guidance mechanism. Through rigorous\nmathematical analysis, we extend Tweedie's formula to infinite-dimensional\nHilbert spaces, providing the theoretical foundation for our posterior sampling\napproach. Our method (FunDPS) accurately captures posterior distributions in\nfunction spaces under minimal supervision and severe data scarcity. Across five\nPDE tasks with only 3% observation, our method achieves an average 32% accuracy\nimprovement over state-of-the-art fixed-resolution diffusion baselines while\nreducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning\nensures strong cross-resolution generalizability. To the best of our knowledge,\nthis is the first diffusion-based framework to operate independently of\ndiscretization, offering a practical and flexible solution for forward and\ninverse problems in the context of PDEs. Code is available at\nhttps://github.com/neuraloperator/FunDPS", "authors": ["Jiachen Yao", "Abbas Mammadov", "Julius Berner", "Gavin Kerrigan", "Jong Chul Ye", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "published_date": "2025-05-22", "title_zh": "函數空間上的導引擴散採樣及其在偏微分方程中的應用", "summary_zh": "本研究提出一個基於偏微分方程逆問題的條件採樣通用框架，旨在從極度稀疏或含噪量測中復原完整解。該方法利用函數空間擴散模型與隨插即用引導實現條件化。首先，採用神經算子架構訓練一個無條件、與離散化無關的去噪模型。在推論階段，透過基於梯度的引導機制精煉樣本，以滿足稀疏觀測數據。透過嚴謹的數學分析，將Tweedie公式擴展到無限維希爾伯特空間，為後驗採樣方法提供理論基礎。此方法(FunDPS)能在極少的監督和嚴重的數據匱乏情況下，準確捕獲函數空間中的後驗分佈。在五項偏微分方程任務中，僅使用3%的觀測數據，此方法比最先進的固定解析度擴散模型平均提高了32%的準確性，同時減少了4倍的採樣步驟。此外，多解析度微調確保了強大的跨解析度泛化能力。據我們所知，這是第一個獨立於離散化運作的基於擴散的框架，為偏微分方程背景下的正向和逆向問題提供了一種實用且靈活的解決方案。代碼位於[https://github.com/neuraloperator/FunDPS](https://github.com/neuraloperator/FunDPS)。", "audio": "audios/2505.17004v1.mp3", "timestamp": "2025-05-23T04:22:15.738654"}
{"query": "AI", "id": "2505.16997v1", "url": "http://arxiv.org/abs/2505.16997v1", "title": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs", "summary": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems.", "authors": ["Rui Ye", "Xiangrui Liu", "Qimin Wu", "Xianghe Pang", "Zhenfei Yin", "Lei Bai", "Siheng Chen"], "published_date": "2025-05-22", "title_zh": "X-MAS：邁向構建具異質大型語言模型的多代理系統", "summary_zh": "基於大型語言模型的多代理系統透過促進多個專業代理之間的協作，擴展了單一大型語言模型的能力。 然而，現有框架大多依賴單一大型語言模型驅動所有代理，限制了系統智慧。 本研究探索異質大型語言模型驅動的多代理系統範式，其中代理由不同的大型語言模型提供支持，從而提升系統潛力。 我們引入了 X-MAS-Bench，一個用於評估不同大型語言模型在不同領域和多代理系統相關功能表現的綜合測試平台。 我們評估了跨越 5 個領域（包含 21 個測試集）和 5 個功能的 27 個大型語言模型，進行了超過 170 萬次評估，以識別每個領域-功能組合的最佳模型選擇。 研究表明，從同質到異質大型語言模型驅動的多代理系統的轉變可以顯著提高系統性能，且無需結構重新設計。 在僅包含聊天機器人的多代理系統情境中，異質配置在 MATH 數據集上產生了高達 8.4% 的性能提升。 在混合聊天機器人-推理器的情境中，異質多代理系統在 AIME 數據集上實現了顯著的 47% 性能提升。 我們的結果強調了異質大型語言模型在多代理系統中的變革潛力，為推進可擴展的協作式人工智慧系統提供了一個有前景的途徑。", "audio": "audios/2505.16997v1.mp3", "timestamp": "2025-05-23T07:18:26.794247"}
{"query": "Foundation Model", "id": "2505.16832v1", "url": "http://arxiv.org/abs/2505.16832v1", "title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization", "summary": "While foundation models (FMs), such as diffusion models and large\nvision-language models (LVLMs), have been widely applied in educational\ncontexts, their ability to generate pedagogically effective visual explanations\nremains limited. Most existing approaches focus primarily on textual reasoning,\noverlooking the critical role of structured and interpretable visualizations in\nsupporting conceptual understanding. To better assess the visual reasoning\ncapabilities of FMs in educational settings, we introduce EduVisBench, a\nmulti-domain, multi-level benchmark. EduVisBench features diverse STEM problem\nsets requiring visually grounded solutions, along with a fine-grained\nevaluation rubric informed by pedagogical theory. Our empirical analysis\nreveals that existing models frequently struggle with the inherent challenge of\ndecomposing complex reasoning and translating it into visual representations\naligned with human cognitive processes. To address these limitations, we\npropose EduVisAgent, a multi-agent collaborative framework that coordinates\nspecialized agents for instructional planning, reasoning decomposition,\nmetacognitive prompting, and visualization design. Experimental results show\nthat EduVisAgent substantially outperforms all baselines, achieving a 40.2%\nimprovement and delivering more educationally aligned visualizations.\nEduVisBench and EduVisAgent are available at\nhttps://github.com/aiming-lab/EduVisBench and\nhttps://github.com/aiming-lab/EduVisAgent.", "authors": ["Haonian Ji", "Shi Qiu", "Siyang Xin", "Siwei Han", "Zhaorun Chen", "Hongyi Wang", "Dake Zhang", "Huaxiu Yao"], "published_date": "2025-05-22", "title_zh": "從EduVisBench到EduVisAgent：教學視覺化之基準測試與多代理人框架", "summary_zh": "基礎模型如擴散模型與大型視覺語言模型已廣泛應用於教育，然其產生具教學效益之視覺解釋能力仍受限。現有方法多側重文字推理，忽略結構化且可解釋之視覺化於概念理解之關鍵作用。為評估基礎模型於教育情境之視覺推理能力，本研究提出EduVisBench，一多領域、多層級基準測試。EduVisBench包含需視覺化解答之STEM問題集，及基於教學理論之細緻評分標準。實證分析顯示，現有模型常難以分解複雜推理，並將其轉譯為符合人類認知歷程之視覺表徵。為解決此問題，本研究提出EduVisAgent，一多代理人協作框架，協調專業代理人進行教學規劃、推理分解、後設認知提示及視覺化設計。實驗結果顯示，EduVisAgent顯著優於所有基準模型，提升40.2%，並產出更符合教育目標之視覺化。EduVisBench與EduVisAgent已於https://github.com/aiming-lab/EduVisBench及https://github.com/aiming-lab/EduVisAgent公開。", "audio": "audios/2505.16832v1.mp3", "timestamp": "2025-05-23T07:18:36.135433"}
{"query": "Diffusion Model", "id": "2505.16980v1", "url": "http://arxiv.org/abs/2505.16980v1", "title": "Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose Interaction", "summary": "Video virtual try-on aims to seamlessly dress a subject in a video with a\nspecific garment. The primary challenge involves preserving the visual\nauthenticity of the garment while dynamically adapting to the pose and physique\nof the subject. While existing methods have predominantly focused on\nimage-based virtual try-on, extending these techniques directly to videos often\nresults in temporal inconsistencies. Most current video virtual try-on\napproaches alleviate this challenge by incorporating temporal modules, yet\nstill overlook the critical spatiotemporal pose interactions between human and\ngarment. Effective pose interactions in videos should not only consider spatial\nalignment between human and garment poses in each frame but also account for\nthe temporal dynamics of human poses throughout the entire video. With such\nmotivation, we propose a new framework, namely Dynamic Pose Interaction\nDiffusion Models (DPIDM), to leverage diffusion models to delve into dynamic\npose interactions for video virtual try-on. Technically, DPIDM introduces a\nskeleton-based pose adapter to integrate synchronized human and garment poses\ninto the denoising network. A hierarchical attention module is then exquisitely\ndesigned to model intra-frame human-garment pose interactions and long-term\nhuman pose dynamics across frames through pose-aware spatial and temporal\nattention mechanisms. Moreover, DPIDM capitalizes on a temporal regularized\nattention loss between consecutive frames to enhance temporal consistency.\nExtensive experiments conducted on VITON-HD, VVT and ViViD datasets demonstrate\nthe superiority of our DPIDM against the baseline methods. Notably, DPIDM\nachieves VFID score of 0.506 on VVT dataset, leading to 60.5% improvement over\nthe state-of-the-art GPD-VVTO approach.", "authors": ["Dong Li", "Wenqi Zhong", "Wei Yu", "Yingwei Pan", "Dingwen Zhang", "Ting Yao", "Junwei Han", "Tao Mei"], "published_date": "2025-05-22", "title_zh": "藉由動態姿態互動實現時序一致的影片虛擬試穿", "summary_zh": "影片虛擬試穿旨在將特定服裝無縫套用於影片中的人物。主要挑戰在於保持服裝視覺真實性的同時，動態適應人物的姿勢和體態。現有方法多側重於圖像虛擬試穿，直接將其延伸至影片常導致時間不一致。目前影片虛擬試穿方法雖加入時間模組以緩解此問題，但忽略了人體與服裝間的時空姿勢互動。有效的姿勢互動應考量每幀中人體與服裝姿勢的空間對齊，以及整段影片中人體姿勢的時間動態。為此，我們提出動態姿勢互動擴散模型（DPIDM），利用擴散模型深入研究影片虛擬試穿的動態姿勢互動。DPIDM採用基於骨架的姿勢適配器，將同步的人體與服裝姿勢整合至去噪網路。精心設計的分層注意力模組通過姿勢感知的空間和時間注意力機制，對幀內人體-服裝姿勢互動以及跨幀的長期人體姿勢動態進行建模。此外，DPIDM利用連續幀之間的時間正規化注意力損失來增強時間一致性。在VITON-HD、VVT和ViViD數據集上的大量實驗表明，我們的DPIDM優於基線方法。值得注意的是，DPIDM在VVT數據集上實現了0.506的VFID評分，相較於最先進的GPD-VVTO方法提升了60.5%。", "audio": "audios/2505.16980v1.mp3", "timestamp": "2025-05-23T07:18:44.910411"}
