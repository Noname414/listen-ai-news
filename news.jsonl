{"id": "2505.10561v1", "url": "http://arxiv.org/abs/2505.10561v1", "title": "T2A-Feedback: Improving Basic Capabilities of Text-to-Audio Generation via Fine-grained AI Feedback", "summary": "Text-to-audio (T2A) generation has achieved remarkable progress in generating\na variety of audio outputs from language prompts. However, current\nstate-of-the-art T2A models still struggle to satisfy human preferences for\nprompt-following and acoustic quality when generating complex multi-event\naudio. To improve the performance of the model in these high-level\napplications, we propose to enhance the basic capabilities of the model with AI\nfeedback learning. First, we introduce fine-grained AI audio scoring pipelines\nto: 1) verify whether each event in the text prompt is present in the audio\n(Event Occurrence Score), 2) detect deviations in event sequences from the\nlanguage description (Event Sequence Score), and 3) assess the overall acoustic\nand harmonic quality of the generated audio (Acoustic&Harmonic Quality). We\nevaluate these three automatic scoring pipelines and find that they correlate\nsignificantly better with human preferences than other evaluation metrics. This\nhighlights their value as both feedback signals and evaluation metrics.\nUtilizing our robust scoring pipelines, we construct a large audio preference\ndataset, T2A-FeedBack, which contains 41k prompts and 249k audios, each\naccompanied by detailed scores. Moreover, we introduce T2A-EpicBench, a\nbenchmark that focuses on long captions, multi-events, and story-telling\nscenarios, aiming to evaluate the advanced capabilities of T2A models. Finally,\nwe demonstrate how T2A-FeedBack can enhance current state-of-the-art audio\nmodel. With simple preference tuning, the audio generation model exhibits\nsignificant improvements in both simple (AudioCaps test set) and complex\n(T2A-EpicBench) scenarios.", "authors": ["Zehan Wang", "Ke Lei", "Chen Zhu", "Jiawei Huang", "Sashuai Zhou", "Luping Liu", "Xize Cheng", "Shengpeng Ji", "Zhenhui Ye", "Tao Jin", "Zhou Zhao"], "published_date": "2025-05-15", "title_zh": "T2A-Feedback：透過細粒度AI回饋提升文字轉語音生成之基礎能力", "summary_zh": "現今的文字轉語音模型在生成複雜多事件音訊時，難以完全符合人類對提示遵循度和音訊品質的期望。本研究提出利用AI回饋學習來提升模型基礎能力。首先，建立了精細的AI音訊評分流程，包含事件發生評分、事件序列評分，以及音訊和諧品質評分。這些評分流程能更準確地反映人類偏好。接著，利用這些評分流程，構建了包含41k個提示和249k個音訊的大型音訊偏好數據集T2A-FeedBack，並推出專注於長描述、多事件和故事敘述場景的評測基準T2A-EpicBench。實驗證明，透過簡單的偏好調整，T2A-FeedBack能有效提升現有音訊生成模型在簡單和複雜場景下的表現。", "audio": "audios/2505.10561v1.mp3", "timestamp": "2025-05-18T23:05:41.112280"}
{"id": "2505.10556v1", "url": "http://arxiv.org/abs/2505.10556v1", "title": "An AI-driven framework for the prediction of personalised health response to air pollution", "summary": "Air pollution poses a significant threat to public health, causing or\nexacerbating many respiratory and cardiovascular diseases. In addition, climate\nchange is bringing about more extreme weather events such as wildfires and\nheatwaves, which can increase levels of pollution and worsen the effects of\npollution exposure. Recent advances in personal sensing have transformed the\ncollection of behavioural and physiological data, leading to the potential for\nnew improvements in healthcare. We wish to capitalise on this data, alongside\nnew capabilities in AI for making time series predictions, in order to monitor\nand predict health outcomes for an individual. Thus, we present a novel\nworkflow for predicting personalised health responses to pollution by\nintegrating physiological data from wearable fitness devices with real-time\nenvironmental exposures. The data is collected from various sources in a secure\nand ethical manner, and is used to train an AI model to predict individual\nhealth responses to pollution exposure within a cloud-based, modular framework.\nWe demonstrate that the AI model -- an Adversarial Autoencoder neural network\nin this case -- accurately reconstructs time-dependent health signals and\ncaptures nonlinear responses to pollution. Transfer learning is applied using\ndata from a personal smartwatch, which increases the generalisation abilities\nof the AI model and illustrates the adaptability of the approach to real-world,\nuser-generated data.", "authors": ["Nazanin Zounemat Kermani", "Sadjad Naderi", "Claire H. Dilliway", "Claire E. Heaney", "Shrreya Behll", "Boyang Chen", "Hisham Abubakar-Waziri", "Alexandra E. Porter", "Marc Chadeau-Hyam", "Fangxin Fang", "Ian M. Adcock", "Kian Fan Chung", "Christopher C. Pain"], "published_date": "2025-05-15", "title_zh": "一個AI驅動的框架，用於預測個人化健康對空氣污染的反應", "summary_zh": "本研究提出一個新的AI框架，結合穿戴裝置收集的生理數據和即時環境暴露數據，來預測個人對空氣污染的健康反應。透過雲端架構訓練AI模型，精準重建時間序列健康訊號，並捕捉非線性污染反應。研究使用個人智慧手錶的數據進行遷移學習，提升模型泛化能力，展現此方法在真實世界使用者數據中的適應性。", "audio": "audios/2505.10556v1.mp3", "timestamp": "2025-05-18T23:05:46.038004"}
{"id": "2505.10527v1", "url": "http://arxiv.org/abs/2505.10527v1", "title": "WorldPM: Scaling Human Preference Modeling", "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.", "authors": ["Binghai Wang", "Runji Lin", "Keming Lu", "Le Yu", "Zhenru Zhang", "Fei Huang", "Chujie Zheng", "Kai Dang", "Yang Fan", "Xingzhang Ren", "An Yang", "Binyuan Hui", "Dayiheng Liu", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang", "Bowen Yu", "Jingren Zhou", "Junyang Lin"], "published_date": "2025-05-15", "title_zh": "WorldPM：擴展人類偏好建模", "summary_zh": "受到語言模型擴展定律的啟發，我們發現類似的定律也存在於偏好建模中。我們提出 WorldPM (World Preference Modeling) 以強調這種擴展潛力，它統一了人類偏好的表達。我們收集了來自公共論壇的偏好數據，涵蓋不同的用戶群體，並使用1500萬規模的數據和從15億到720億參數的模型進行了廣泛的訓練。結果顯示，對抗性指標（識別欺騙性特徵的能力）隨著訓練數據和模型大小的增加而持續提升；客觀性指標（有明確答案的客觀知識）在大語言模型中展現出湧現行為，突顯了 WorldPM 的可擴展性；主觀性指標則未顯示出擴展趨勢。實驗驗證了 WorldPM 作為偏好微調基礎的有效性。在七個基準測試的 20 個子任務中，WorldPM 顯著提高了跨不同規模人類偏好數據集（7K、100K 和 800K 樣本）的泛化性能，在許多關鍵子任務上的性能提升超過 5%。將 WorldPM 整合到我們的內部 RLHF 流程中，我們觀察到內部和公共評估集的顯著改進，內部評估的增益顯著提高了 4% 到 8%。", "audio": "audios/2505.10527v1.mp3", "timestamp": "2025-05-18T23:05:53.910110"}
{"id": "2505.10525v1", "url": "http://arxiv.org/abs/2505.10525v1", "title": "Sobolev and quasiconformal distortion of intermediate dimension with applications to conformal dimension", "summary": "We study the distortion of intermediate dimension under supercritical Sobolev\nmappings and also under quasiconformal or quasisymmetric homeomorphisms. In\nparticular, we extend to the setting of intermediate dimensions both the\nGehring--V\\\"ais\\\"al\\\"a theorem on dilatation-dependent quasiconformal\ndistortion of dimension and Kovalev's theorem on the nonexistence of metric\nspaces with conformal dimension strictly between zero and one. Applications\ninclude new contributions to the quasiconformal classification of Euclidean\nsets and a new sufficient condition for the vanishing of conformal box-counting\ndimension. We illustrate our conclusions with specific consequences for\nBedford--McMullen carpets, samples of Mandelbrot percolation, and product sets\ncontaining a polynomially convergent sequence factor.", "authors": ["Jonathan M. Fraser", "Jeremy T. Tyson"], "published_date": "2025-05-15", "title_zh": "中間維度的Sobolev及擬共形扭曲，及其於共形維度的應用", "summary_zh": "本研究探討超臨界Sobolev映射及擬共形/擬對稱同胚變換下，中間維度的扭曲現象。我們將Gehring-V\"ais\"al\"a關於膨脹係數與擬共形維度扭曲的定理，以及Kovalev關於不存在共形維度介於0與1之間的度量空間的定理，推廣到中間維度的情境。研究成果應用於歐幾里得集合的擬共形分類，並提出新的充分條件判斷共形盒計數維度是否消失。我們以Bedford-McMullen地毯、Mandelbrot滲流樣本，以及包含多項式收斂序列因子的乘積集合為例，闡述了我們的結論。", "audio": "audios/2505.10525v1.mp3", "timestamp": "2025-05-18T23:05:58.394013"}
{"id": "2505.10496v1", "url": "http://arxiv.org/abs/2505.10496v1", "title": "CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs", "summary": "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework\nfor synthetic chest radiograph generation that simultaneously assesses\nfidelity, privacy risks, and clinical utility across state-of-the-art\ntext-to-image generative models. Despite rapid advancements in generative AI\nfor real-world imagery, medical domain evaluations have been hindered by\nmethodological inconsistencies, outdated architectural comparisons, and\ndisconnected assessment criteria that rarely address the practical clinical\nvalue of synthetic samples. CheXGenBench overcomes these limitations through\nstandardised data partitioning and a unified evaluation protocol comprising\nover 20 quantitative metrics that systematically analyse generation quality,\npotential privacy vulnerabilities, and downstream clinical applicability across\n11 leading text-to-image architectures. Our results reveal critical\ninefficiencies in the existing evaluation protocols, particularly in assessing\ngenerative fidelity, leading to inconsistent and uninformative comparisons. Our\nframework establishes a standardised benchmark for the medical AI community,\nenabling objective and reproducible comparisons while facilitating seamless\nintegration of both existing and future generative models. Additionally, we\nrelease a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K\nradiographs generated by the top-performing model (Sana 0.6B) in our benchmark\nto support further research in this critical domain. Through CheXGenBench, we\nestablish a new state-of-the-art and release our framework, models, and\nSynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/", "authors": ["Raman Dutt", "Pedro Sanchez", "Yongchen Yao", "Steven McDonagh", "Sotirios A. Tsaftaris", "Timothy Hospedales"], "published_date": "2025-05-15", "title_zh": "CheXGenBench：合成胸腔X光片逼真度、隱私與效用之統一評估基準", "summary_zh": "CheXGenBench是一個綜合性的評估框架，用來衡量合成胸腔X光片生成模型的品質。它同時考量逼真度、潛在隱私風險以及在臨床上的實用性。 研究團隊發現現有的評估方法存在缺陷，導致結果不一致且缺乏參考價值。 因此，他們設計了一個標準化的評估基準，包含超過20個量化指標，並使用11種主流的文字轉圖像模型進行測試。研究結果揭示了現有評估方法的不足。 此外，研究團隊也釋出一個高品質的合成胸腔X光片資料集，SynthCheX-75K，包含7萬5千張圖像，以支持醫學AI領域的進一步研究。", "audio": "audios/2505.10496v1.mp3", "timestamp": "2025-05-18T23:06:03.275100"}
{"id": "2505.10490v1", "url": "http://arxiv.org/abs/2505.10490v1", "title": "Campus AI vs Commercial AI: A Late-Breaking Study on How LLM As-A-Service Customizations Shape Trust and Usage Patterns", "summary": "As the use of Large Language Models (LLMs) by students, lecturers and\nresearchers becomes more prevalent, universities - like other organizations -\nare pressed to develop coherent AI strategies. LLMs as-a-Service (LLMaaS) offer\naccessible pre-trained models, customizable to specific (business) needs. While\nmost studies prioritize data, model, or infrastructure adaptations (e.g., model\nfine-tuning), we focus on user-salient customizations, like interface changes\nand corporate branding, which we argue influence users' trust and usage\npatterns. This study serves as a functional prequel to a large-scale field\nstudy in which we examine how students and employees at a German university\nperceive and use their institution's customized LLMaaS compared to ChatGPT. The\ngoals of this prequel are to stimulate discussions on psychological effects of\nLLMaaS customizations and refine our research approach through feedback. Our\nforthcoming findings will deepen the understanding of trust dynamics in LLMs,\nproviding practical guidance for organizations considering LLMaaS deployment.", "authors": ["Leon Hannig", "Annika Bush", "Meltem Aksoy", "Steffen Becker", "Greta Ontrup"], "published_date": "2025-05-15", "title_zh": "校園AI vs. 商業AI：一項關於LLM即服務客製化如何形塑信任與使用模式的最新研究", "summary_zh": "隨著大學生、講師和研究人員廣泛使用大型語言模型（LLM），各大學正面臨制定完善AI策略的需求。本研究探討使用者可見的LLM客製化，例如介面修改和品牌形象，如何影響使用者對大學客製LLM的信任感和使用模式，並與ChatGPT進行比較。這項前期研究旨在引發關於LLM客製化心理效應的討論，並為後續的大規模實地研究提供方向，最終目標是深入了解LLM中的信任動態，為考慮部署LLMaaS的組織提供實用建議。", "audio": "audios/2505.10490v1.mp3", "timestamp": "2025-05-18T23:14:13.567508"}
{"id": "2505.10472v1", "url": "http://arxiv.org/abs/2505.10472v1", "title": "Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI", "summary": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools.", "authors": ["Agnik Saha", "Victoria Churchill", "Anny D. Rodriguez", "Ugur Kursuncu", "Muhammed Y. Idris"], "published_date": "2025-05-15", "title_zh": "用於癌症溝通的大型語言模型：評估生成式人工智慧中的語言品質、安全性和可及性", "summary_zh": "關於乳癌和子宮頸癌的有效溝通仍然是個健康挑戰。本研究評估了大型語言模型（LLMs）生成準確、安全且易於理解的癌症資訊的能力。研究發現，通用LLMs在語言品質和感染力方面表現較好，而醫療LLMs則在溝通可及性方面更勝一籌。然而，醫療LLMs也更容易產生潛在危害、毒性和偏見。總體而言，研究強調了在設計模型時需要有針對性地改進，特別是在降低危害和偏見方面，從而創建更安全、有效且可信賴的AI健康資訊工具。", "audio": "audios/2505.10472v1.mp3", "timestamp": "2025-05-18T23:14:30.270685"}
{"id": "2505.10468v1", "url": "http://arxiv.org/abs/2505.10468v1", "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge", "summary": "This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "published_date": "2025-05-15", "title_zh": "AI代理程式 vs. 具代理能力AI：概念分類、應用與挑戰", "summary_zh": "本研究區分了AI代理程式（AI Agents）和具代理能力AI（Agentic AI）兩個概念，並建立了結構化的分類體系。AI代理程式著重於利用大型語言模型（LLMs）和大型圖像模型（LIMs）進行狹窄、特定任務的自動化。而具代理能力AI則是一種範式轉移，強調多代理程式協作、動態任務分解、持久記憶和協調自主。研究比較了兩者的架構演進、運作機制、互動方式和自主程度，並探討了各自在客戶支援、研究自動化等領域的應用。同時，也分析了幻覺、脆弱性、突發行為和協調失敗等挑戰，並提出了針對性的解決方案。本研究旨在為開發穩健、可擴展且可解釋的AI代理程式和具代理能力AI系統提供清晰的指引。", "audio": "audios/2505.10468v1.mp3", "timestamp": "2025-05-18T15:29:49.604850"}
{"id": "2505.10454v1", "url": "http://arxiv.org/abs/2505.10454v1", "title": "Emotion-sensitive Explanation Model", "summary": "Explainable AI (XAI) research has traditionally focused on rational users,\naiming to improve understanding and reduce cognitive biases. However, emotional\nfactors play a critical role in how explanations are perceived and processed.\nPrior work shows that prior and task-generated emotions can negatively impact\nthe understanding of explanation. Building on these insights, we propose a\nthree-stage model for emotion-sensitive explanation grounding: (1) emotional or\nepistemic arousal, (2) understanding, and (3) agreement. This model provides a\nconceptual basis for developing XAI systems that dynamically adapt explanation\nstrategies to users emotional states, ultimately supporting more effective and\nuser-centered decision-making.", "authors": ["Christian Schütze", "Birte Richter", "Britta Wrede"], "published_date": "2025-05-15", "title_zh": "情緒敏感的解釋模型", "summary_zh": "傳統的可解釋AI(XAI)研究主要關注理性用戶，旨在提升理解和減少認知偏差。然而，情緒在解釋的感知和處理中扮演關鍵角色。本文提出一個三階段的情緒敏感解釋模型，包含情緒激發、理解和認同。此模型為開發能根據用戶情緒狀態動態調整解釋策略的XAI系統提供概念基礎，最終支持更有效且以用戶為中心的決策。", "audio": "audios/2505.10454v1.mp3", "timestamp": "2025-05-18T15:43:10.566009"}
{"id": "2505.10453v1", "url": "http://arxiv.org/abs/2505.10453v1", "title": "Vision language models have difficulty recognizing virtual objects", "summary": "Vision language models (VLMs) are AI systems paired with both language and\nvision encoders to process multimodal input. They are capable of performing\ncomplex semantic tasks such as automatic captioning, but it remains an open\nquestion about how well they comprehend the visuospatial properties of scenes\ndepicted in the images they process. We argue that descriptions of virtual\nobjects -- objects that are not visually represented in an image -- can help\ntest scene comprehension in these AI systems. For example, an image that\ndepicts a person standing under a tree can be paired with the following prompt:\nimagine that a kite is stuck in the tree. VLMs that comprehend the scene should\nupdate their representations and reason sensibly about the spatial relations\nbetween all three objects. We describe systematic evaluations of\nstate-of-the-art VLMs and show that their ability to process virtual objects is\ninadequate.", "authors": ["Tyler Tran", "Sangeet Khemlani", "J. G. Trafton"], "published_date": "2025-05-15", "title_zh": "視覺語言模型難以識別虛擬物件", "summary_zh": "視覺語言模型（VLMs）結合了語言和視覺編碼器，可以處理多模態輸入，例如自動生成圖片描述。然而，它們對圖像中場景的視覺空間理解程度仍然是個問題。研究發現，當提示VLMs想像圖像中不存在的虛擬物件（例如：一棵樹下站著一個人，提示想像樹上卡著風箏），它們難以合理更新場景表徵並推理物件之間的空間關係。這表明目前的VLMs在處理虛擬物件方面的能力不足。", "audio": "audios/2505.10453v1.mp3", "timestamp": "2025-05-18T15:52:48.305855"}
{"id": "2505.10427v1", "url": "http://arxiv.org/abs/2505.10427v1", "title": "Influence of prior and task generated emotions on XAI explanation retention and understanding", "summary": "The explanation of AI results and how they are received by users is an\nincreasingly active research field. However, there is a surprising lack of\nknowledge about how social factors such as emotions affect the process of\nexplanation by a decision support system (DSS). While previous research has\nshown effects of emotions on DSS supported decision-making, it remains unknown\nin how far emotions affect cognitive processing during an explanation. In this\nstudy, we, therefore, investigated the influence of prior emotions and\ntask-related arousal on the retention and understanding of explained feature\nrelevance. To investigate the influence of prior emotions, we induced happiness\nand fear prior to the decision support interaction. Before emotion induction,\nuser characteristics to assess their risk type were collected via a\nquestionnaire. To identify emotional reactions to the explanations of the\nrelevance of different features, we observed heart rate variability (HRV),\nfacial expressions, and self-reported emotions of the explainee while observing\nand listening to the explanation and assessed their retention of the features\nas well as their influence on the outcome of the decision task. Results\nindicate that (1) task-unrelated prior emotions do not affected the ratantion\nbut may affect the understanding of the relevance of certain features in the\nsense of an emotion-induced confirmation bias, (2) certain features related to\npersonal attitudes yielded arousal in individual participants, (3) this arousal\naffected the understanding of these variables.", "authors": ["Birte Richter", "Christian Schütze", "Anna Aksonova", "Britta Wrede"], "published_date": "2025-05-15", "title_zh": "先前情緒與任務產生情緒對XAI解釋保留與理解的影響", "summary_zh": "理解AI決策的解釋越來越重要。本研究探討情緒如何影響使用者對AI解釋的理解與記憶。我們透過誘導快樂與恐懼情緒，以及監測心率、面部表情等生理反應，觀察情緒如何影響使用者理解AI解釋中的特徵重要性。研究發現，先前情緒可能影響使用者對特定特徵的理解，產生情緒誘導的確認偏誤；某些與個人態度相關的特徵會引起使用者的情緒反應，進而影響他們對這些特徵的理解。", "audio": "audios/2505.10427v1.mp3", "timestamp": "2025-05-18T16:06:13.256765"}
{"id": "2505.10426v1", "url": "http://arxiv.org/abs/2505.10426v1", "title": "Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility", "summary": "The legal compliance and safety of different Human-in-the-loop (HITL) setups\nfor AI can vary greatly. This manuscript aims to identify new ways of choosing\nbetween such setups, and shows that there is an unavoidable trade-off between\nthe attribution of legal responsibility and the technical explainability of AI.\nWe begin by using the notion of oracle machines from computability theory to\nformalise different HITL setups, distinguishing between trivial human\nmonitoring, single endpoint human action, and highly involved interaction\nbetween the human(s) and the AI. These correspond to total functions, many-one\nreductions, and Turing reductions respectively. A taxonomy categorising HITL\nfailure modes is then presented, highlighting the limitations on what any HITL\nsetup can actually achieve. Our approach then identifies oversights from UK and\nEU legal frameworks, which focus on certain HITL setups which may not always\nachieve the desired ethical, legal, and sociotechnical outcomes. We suggest\nareas where the law should recognise the effectiveness of different HITL setups\nand assign responsibility in these contexts, avoiding unnecessary and\nunproductive human \"scapegoating\". Overall, we show how HITL setups involve\nmany technical design decisions, and can be prone to failures which are often\nout of the humans' control. This opens up a new analytic perspective on the\nchallenges arising in the creation of HITL setups, helping inform AI developers\nand lawmakers on designing HITL to better achieve their desired outcomes.", "authors": ["Maurice Chiodo", "Dennis Müller", "Paul Siewert", "Jean-Luc Wetherall", "Zoya Yasmine", "John Burden"], "published_date": "2025-05-15", "title_zh": "人機迴路正式化：計算歸約、失效模式與法律-道德責任", "summary_zh": "本研究探討不同人工智慧人機迴路(HITL)架構在法律合規和安全上的差異，指出法律責任歸屬與AI技術可解釋性之間存在不可避免的權衡。我們利用計算理論的預言機概念，形式化不同HITL架構，並建立HITL失效模式分類法。研究揭示了現有法律框架的不足，並建議如何根據不同HITL架構的有效性來分配責任，避免不必要的“替罪羊”效應。 總而言之，本研究揭示了HITL架構設計的複雜性以及潛在的失效風險，為AI開發者和立法者設計更有效的HITL架構提供了新的分析視角。", "audio": "audios/2505.10426v1.mp3", "timestamp": "2025-05-18T16:20:24.598334"}
{"id": "2505.10405v1", "url": "http://arxiv.org/abs/2505.10405v1", "title": "Visual Fidelity Index for Generative Semantic Communications with Critical Information Embedding", "summary": "Generative semantic communication (Gen-SemCom) with large artificial\nintelligence (AI) model promises a transformative paradigm for 6G networks,\nwhich reduces communication costs by transmitting low-dimensional prompts\nrather than raw data. However, purely prompt-driven generation loses\nfine-grained visual details. Additionally, there is a lack of systematic\nmetrics to evaluate the performance of Gen-SemCom systems. To address these\nissues, we develop a hybrid Gen-SemCom system with a critical information\nembedding (CIE) framework, where both text prompts and semantically critical\nfeatures are extracted for transmissions. First, a novel approach of semantic\nfiltering is proposed to select and transmit the semantically critical features\nof images relevant to semantic label. By integrating the text prompt and\ncritical features, the receiver reconstructs high-fidelity images using a\ndiffusion-based generative model. Next, we propose the generative visual\ninformation fidelity (GVIF) metric to evaluate the visual quality of the\ngenerated image. By characterizing the statistical models of image features,\nthe GVIF metric quantifies the mutual information between the distorted\nfeatures and their original counterparts. By maximizing the GVIF metric, we\ndesign a channel-adaptive Gen-SemCom system that adaptively control the volume\nof features and compression rate according to the channel state. Experimental\nresults validate the GVIF metric's sensitivity to visual fidelity, correlating\nwith both the PSNR and critical information volume. In addition, the optimized\nsystem achieves superior performance over benchmarking schemes in terms of\nhigher PSNR and lower FID scores.", "authors": ["Jianhao Huang", "Qunsong Zeng", "Kaibin Huang"], "published_date": "2025-05-15", "title_zh": "具有關鍵資訊嵌入的生成式語義通訊視覺保真度指標", "summary_zh": "研究提出一種混合生成式語義通訊系統，透過嵌入關鍵資訊框架，同時傳輸文字提示和語義上重要的圖像特徵，以解決純粹提示驅動的生成導致細節丟失的問題。為評估系統性能，提出生成式視覺資訊保真度（GVIF）指標，以量化失真特徵與原始特徵之間的互信息。實驗結果驗證了GVIF指標對視覺保真度的敏感性，並設計了一種基於GVIF最大化的通道自適應系統，能夠根據通道狀態調整特徵數量和壓縮率，進而提升重建圖像的品質。", "audio": "audios/2505.10405v1.mp3", "timestamp": "2025-05-18T17:14:39.044337"}
{"id": "2505.10377v1", "url": "http://arxiv.org/abs/2505.10377v1", "title": "The Art of Two-Round Voting", "summary": "We study the voting problem with two alternatives where voters' preferences\ndepend on a not-directly-observable state variable. While equilibria in the\none-round voting mechanisms lead to a good decision, they are usually hard to\ncompute and follow. We consider the two-round voting mechanism where the first\nround serves as a polling stage and the winning alternative only depends on the\noutcome of the second round. We show that the two-round voting mechanism is a\npowerful tool for making collective decisions. Firstly, every (approximated)\nequilibrium in the two-round voting mechanisms (asymptotically) leads to the\ndecision preferred by the majority as if the state of the world were revealed\nto the voters. Moreover, there exist natural equilibria in the two-round game\nfollowing intuitive behaviors such as informative voting, sincere voting\n[Austen-Smith and Banks, 1996], and the surprisingly popular strategy [Prelec\net al., 2017]. This sharply contrasts with the one-round voting mechanisms in\nthe previous literature, where no simple equilibrium is known. Finally, we show\nthat every equilibrium in the standard one-round majority vote mechanism gives\nan equilibrium in the two-round mechanisms that is not more complicated than\nthe one-round equilibrium. Therefore, the two-round voting mechanism provides a\nnatural equilibrium in every instance, including those where one-round voting\nfails to have a natural solution, and it can reach an informed majority\ndecision whenever one-round voting can. Our experiments on generative AI voters\nalso imply that two-round voting leads to the correct outcome more often than\none-round voting under some circumstances.", "authors": ["Qishen Han", "Grant Schoenebeck", "Biaoshuai Tao", "Lirong Xia"], "published_date": "2025-05-15", "title_zh": "兩輪投票的藝術", "summary_zh": "研究選民偏好取決於不可直接觀察狀態變數的雙選項投票問題。單輪投票機制雖能做出好的決策，但其均衡通常難以計算和遵循。論文探討兩輪投票機制，首輪作為民調，勝負僅取決於第二輪結果。研究表明兩輪投票機制是強大的集體決策工具。首先，兩輪投票機制的每個（近似）均衡（漸近地）導向多數人偏好的決策，如同世界狀態已被揭露給選民。此外，存在自然的兩輪賽局均衡，遵循直觀行為，例如資訊性投票、真誠投票和令人驚訝的流行策略。這與先前文獻中單輪投票機制形成鮮明對比，因為單輪投票機制沒有已知的簡單均衡。最後，研究表明標準單輪多數投票機制中的每個均衡都給出兩輪機制中的均衡，且不比單輪均衡更複雜。因此，兩輪投票機制在每個實例中都提供了一種自然的均衡，包括那些單輪投票未能產生自然解決方案的實例，並且只要單輪投票可以，它就可以達成知情的多数人决策。我們在生成式AI選民上的實驗也表明，在某些情況下，兩輪投票比單輪投票更有可能導致正確的結果。", "audio": "audios/2505.10377v1.mp3", "timestamp": "2025-05-18T18:23:30.321298"}
{"id": "2505.10375v1", "url": "http://arxiv.org/abs/2505.10375v1", "title": "Are Sparse Autoencoders Useful for Java Function Bug Detection?", "summary": "Software vulnerabilities such as buffer overflows and SQL injections are a\nmajor source of security breaches. Traditional methods for vulnerability\ndetection remain essential but are limited by high false positive rates,\nscalability issues, and reliance on manual effort. These constraints have\ndriven interest in AI-based approaches to automated vulnerability detection and\nsecure code generation. While Large Language Models (LLMs) have opened new\navenues for classification tasks, their complexity and opacity pose challenges\nfor interpretability and deployment. Sparse Autoencoder offer a promising\nsolution to this problem. We explore whether SAEs can serve as a lightweight,\ninterpretable alternative for bug detection in Java functions. We evaluate the\neffectiveness of SAEs when applied to representations from GPT-2 Small and\nGemma 2B, examining their capacity to highlight buggy behaviour without\nfine-tuning the underlying LLMs. We found that SAE-derived features enable bug\ndetection with an F1 score of up to 89%, consistently outperforming fine-tuned\ntransformer encoder baselines. Our work provides the first empirical evidence\nthat SAEs can be used to detect software bugs directly from the internal\nrepresentations of pretrained LLMs, without any fine-tuning or task-specific\nsupervision.", "authors": ["Rui Melo", "Claudia Mamede", "Andre Catarino", "Rui Abreu", "Henrique Lopes Cardoso"], "published_date": "2025-05-15", "title_zh": "稀疏自編碼器對Java函式錯誤偵測有用嗎？", "summary_zh": "軟體漏洞，如緩衝區溢位和SQL注入，是資安漏洞的主要來源。傳統的漏洞偵測方法雖然重要，但誤報率高，擴展性差，且依賴人工。這促使人們對基於AI的自動漏洞偵測和安全程式碼生成產生興趣。大型語言模型（LLM）雖然為分類任務開闢了新途徑，但其複雜性和不透明性對可解釋性和部署構成了挑戰。稀疏自編碼器（SAE）為此問題提供了一個有希望的解決方案。本文探討了SAE是否可以作為Java函式中錯誤偵測的輕量級、可解釋的替代方案。我們評估了將SAE應用於GPT-2 Small和Gemma 2B的表示時的有效性，檢驗了它們在不微調底層LLM的情況下突出顯示錯誤行為的能力。我們發現，SAE衍生的特徵能夠以高達89%的F1分數進行錯誤偵測，始終優於微調後的Transformer編碼器基準線。我們的研究提供了第一個經驗證據，證明SAE可以用於直接從預訓練LLM的內部表示中檢測軟體錯誤，而無需任何微調或特定於任務的監督。", "audio": "audios/2505.10375v1.mp3", "timestamp": "2025-05-18T19:13:34.703921"}
{"id": "2505.10360v1", "url": "http://arxiv.org/abs/2505.10360v1", "title": "FactsR: A Safer Method for Producing High Quality Healthcare Documentation", "summary": "There are now a multitude of AI-scribing solutions for healthcare promising\nthe utilization of large language models for ambient documentation. However,\nthese AI scribes still rely on one-shot, or few-shot prompts for generating\nnotes after the consultation has ended, employing little to no reasoning. This\nrisks long notes with an increase in hallucinations, misrepresentation of the\nintent of the clinician, and reliance on the proofreading of the clinician to\ncatch errors. A dangerous combination for patient safety if vigilance is\ncompromised by workload and fatigue. In this paper, we introduce a method for\nextracting salient clinical information in real-time alongside the healthcare\nconsultation, denoted Facts, and use that information recursively to generate\nthe final note. The FactsR method results in more accurate and concise notes by\nplacing the clinician-in-the-loop of note generation, while opening up new use\ncases within real-time decision support.", "authors": ["Victor Petrén Bach Hansen", "Lasse Krogsbøll", "Jonas Lyngsø", "Mathias Baltzersen", "Andreas Motzfeldt", "Kevin Pelgrims", "Lars Maaløe"], "published_date": "2025-05-15", "title_zh": "FactsR：一種更安全的生成高品質醫療文檔的方法", "summary_zh": "現今有許多AI醫療抄寫方案，聲稱利用大型語言模型進行環境文檔記錄。但這些方案仍依賴於少量樣本提示生成筆記，幾乎沒有推理能力，容易產生過長、充滿幻覺、誤解臨床醫生意圖的筆記，需要臨床醫生校對。若工作量大且疲勞，會危及患者安全。本研究提出FactsR方法，在醫療諮詢期間即時提取關鍵臨床資訊（Facts），並遞迴地利用這些資訊生成最終筆記。FactsR透過讓臨床醫生參與筆記生成，產生更精準簡潔的筆記，並開創了即時決策支援的新應用。", "audio": "audios/2505.10360v1.mp3", "timestamp": "2025-05-18T20:19:19.221424"}
{"id": "2505.10338v1", "url": "http://arxiv.org/abs/2505.10338v1", "title": "Telecom-to-Visible Quantum Frequency Converter on a Silicon Nitride Chip", "summary": "Quantum frequency conversion serves a key role in the realization of hybrid\nquantum networks by interfacing between wavelength-incompatible platforms. Here\nwe present the first quantum frequency converter connecting visible and telecom\ndomains on a silicon nitride (SiN) chip, using Bragg-scattering four-wave\nmixing to upconvert heralded single photons from 1260 to 698 nm, which covers a\n192 THz span. We examine the noise sources in SiN and devise approaches to\nsuppress noise photons at the source and target frequencies to enable\nmeasurements at the single-photon level. We demonstrate an on-chip conversion\nefficiency of 5% in photon flux and describe design modifications that can be\nimplemented to significantly improve it. Our results pave the way for the\nimplementation of CMOS-compatible devices in quantum networks.", "authors": ["Sidarth Raghunathan", "Richard Oliver", "Yun Zhao", "Karl McNulty", "Chaitali Joshi", "Michal Lipson", "Alexander L. Gaeta"], "published_date": "2025-05-15", "title_zh": "矽晶氮化矽晶片上用於電信頻段到可見光頻段的量子頻率轉換器", "summary_zh": "量子頻率轉換是連接不同波長量子平台的關鍵技術。這篇論文展示了首個矽晶氮化矽晶片上的量子頻率轉換器，能將電信頻段（1260奈米）的單光子上轉換到可見光頻段（698奈米），跨越192太赫茲的頻寬。研究人員探討了矽晶氮化矽晶片上的雜訊來源，並設計了抑制雜訊光子的方法，實現了單光子層級的測量。晶片上的轉換效率達到了5%，並且論文中也提出了可以顯著提高轉換效率的設計修改方案。這項成果為在量子網路中實現與CMOS相容的元件鋪平了道路。", "audio": "audios/2505.10338v1.mp3", "timestamp": "2025-05-18T21:15:26.692705"}
{"id": "2505.10325v1", "url": "http://arxiv.org/abs/2505.10325v1", "title": "A Representation Learning Approach to Feature Drift Detection in Wireless Networks", "summary": "AI is foreseen to be a centerpiece in next generation wireless networks\nenabling enabling ubiquitous communication as well as new services. However, in\nreal deployment, feature distribution changes may degrade the performance of AI\nmodels and lead to undesired behaviors. To counter for undetected model\ndegradation, we propose ALERT; a method that can detect feature distribution\nchanges and trigger model re-training that works well on two wireless network\nuse cases: wireless fingerprinting and link anomaly detection. ALERT includes\nthree components: representation learning, statistical testing and utility\nassessment. We rely on MLP for designing the representation learning component,\non Kolmogorov-Smirnov and Population Stability Index tests for designing the\nstatistical testing and a new function for utility assessment. We show the\nsuperiority of the proposed method against ten standard drift detection methods\navailable in the literature on two wireless network use cases.", "authors": ["Athanasios Tziouvaras", "Blaz Bertalanic", "George Floros", "Kostas Kolomvatsos", "Panagiotis Sarigiannidis", "Carolina Fortuna"], "published_date": "2025-05-15", "title_zh": "無線網路中基於表徵學習的特徵漂移偵測方法", "summary_zh": "下一代無線網路預計將廣泛應用人工智慧。然而，實際部署中，特徵分佈的改變可能降低人工智慧模型效能，導致不良行為。為了解決這個問題，我們提出 ALERT 方法，它可以偵測特徵分佈的改變，並觸發模型重新訓練。ALERT 包含表徵學習、統計檢定和效用評估三個部分。我們在無線指紋辨識和鏈路異常偵測兩個無線網路應用案例中，驗證了 ALERT 優於現有的十種漂移偵測方法。", "audio": "audios/2505.10325v1.mp3", "timestamp": "2025-05-18T22:16:22.632157"}
{"id": "2505.10320v1", "url": "http://arxiv.org/abs/2505.10320v1", "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning", "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.", "authors": ["Chenxi Whitehouse", "Tianlu Wang", "Ping Yu", "Xian Li", "Jason Weston", "Ilia Kulikov", "Swarnadeep Saha"], "published_date": "2025-05-15", "title_zh": "J1：透過強化學習激勵LLM作為評審的思考能力", "summary_zh": "AI發展受限於評估品質，而強大的LLM評審模型是關鍵解決方案。更強的思考鏈推理能提升判斷力，促使我們尋找訓練這些模型思考的最佳方法。本研究提出J1，一種利用強化學習訓練LLM評審模型的方法。我們的方法將可驗證和不可驗證的提示轉換為可驗證獎勵的判斷任務，激勵思考並減少判斷偏差。我們的模型在8B或70B模型大小下，表現優於所有現有的同規模模型，包括從DeepSeek-R1提煉的模型。J1也優於o1-mini，甚至在某些基準測試中優於R1，儘管訓練的模型較小。我們分析比較了Pairwise-J1 vs Pointwise-J1模型、離線 vs 線上訓練、獎勵策略、初始提示，以及思考長度和內容的變化。我們發現，我們的模型通過學習概述評估標準、與自我生成的參考答案進行比較，以及重新評估模型回應的正確性，來做出更好的判斷。", "audio": "audios/2505.10320v1.mp3", "timestamp": "2025-05-18T23:16:55.410198"}
{"id": "2505.10315v1", "url": "http://arxiv.org/abs/2505.10315v1", "title": "Private Transformer Inference in MLaaS: A Survey", "summary": "Transformer models have revolutionized AI, powering applications like content\ngeneration and sentiment analysis. However, their deployment in Machine\nLearning as a Service (MLaaS) raises significant privacy concerns, primarily\ndue to the centralized processing of sensitive user data. Private Transformer\nInference (PTI) offers a solution by utilizing cryptographic techniques such as\nsecure multi-party computation and homomorphic encryption, enabling inference\nwhile preserving both user data and model privacy. This paper reviews recent\nPTI advancements, highlighting state-of-the-art solutions and challenges. We\nalso introduce a structured taxonomy and evaluation framework for PTI, focusing\non balancing resource efficiency with privacy and bridging the gap between\nhigh-performance inference and data privacy.", "authors": ["Yang Li", "Xinyu Zhou", "Yitong Wang", "Liangxin Qian", "Jun Zhao"], "published_date": "2025-05-15", "title_zh": "MLaaS中的私有Transformer推論：綜述", "summary_zh": "Transformer模型在AI領域取得重大突破，但其在機器學習即服務（MLaaS）中的部署引發隱私問題，因為使用者敏感資料集中處理。私有Transformer推論（PTI）透過安全多方計算和同態加密等技術，在保護使用者資料和模型隱私的同時進行推論。本論文回顧了最新的PTI研究進展，重點介紹了最新的解決方案和挑戰，並提出了針對PTI的結構化分類和評估框架，旨在平衡資源效率與隱私保護，並彌合高性能推論與資料隱私之間的差距。", "audio": "audios/2505.10315v1.mp3", "timestamp": "2025-05-19T01:38:18.066985"}
{"id": "2505.11483v1", "url": "http://arxiv.org/abs/2505.11483v1", "title": "msf-CNN: Patch-based Multi-Stage Fusion with Convolutional Neural Networks for TinyML", "summary": "AI spans from large language models to tiny models running on\nmicrocontrollers (MCUs). Extremely memory-efficient model architectures are\ndecisive to fit within an MCU's tiny memory budget e.g., 128kB of RAM. However,\ninference latency must remain small to fit real-time constraints. An approach\nto tackle this is patch-based fusion, which aims to optimize data flows across\nneural network layers. In this paper, we introduce msf-CNN, a novel technique\nthat efficiently finds optimal fusion settings for convolutional neural\nnetworks (CNNs) by walking through the fusion solution space represented as a\ndirected acyclic graph. Compared to previous work on CNN fusion for MCUs,\nmsf-CNN identifies a wider set of solutions. We published an implementation of\nmsf-CNN running on various microcontrollers (ARM Cortex-M, RISC-V, ESP32). We\nshow that msf-CNN can achieve inference using 50% less RAM compared to the\nprior art (MCUNetV2 and StreamNet). We thus demonstrate how msf-CNN offers\nadditional flexibility for system designers.", "authors": ["Zhaolan Huang", "Emmanuel Baccelli"], "published_date": "2025-05-16", "title_zh": "msf-CNN：基於卷積神經網路的塊狀多階段融合TinyML", "summary_zh": "針對微控制器(MCU)上運行的TinyML，本研究提出msf-CNN，一種尋找卷積神經網路(CNN)最佳融合設定的新技術。透過在有向無環圖表示的融合方案空間中搜尋，msf-CNN能找到更廣泛的解決方案。實驗證明，相比於現有技術MCUNetV2和StreamNet，msf-CNN能減少50%的RAM使用量，為系統設計者提供更大的彈性。", "audio": "audios/2505.11483v1.mp3", "timestamp": "2025-05-19T03:17:07.151829"}
{"id": "2505.11481v1", "url": "http://arxiv.org/abs/2505.11481v1", "title": "MOSAAIC: Managing Optimization towards Shared Autonomy, Authority, and Initiative in Co-creation", "summary": "Striking the appropriate balance between humans and co-creative AI is an open\nresearch question in computational creativity. Co-creativity, a form of hybrid\nintelligence where both humans and AI take action proactively, is a process\nthat leads to shared creative artifacts and ideas. Achieving a balanced dynamic\nin co-creativity requires characterizing control and identifying strategies to\ndistribute control between humans and AI. We define control as the power to\ndetermine, initiate, and direct the process of co-creation. Informed by a\nsystematic literature review of 172 full-length papers, we introduce MOSAAIC\n(Managing Optimization towards Shared Autonomy, Authority, and Initiative in\nCo-creation), a novel framework for characterizing and balancing control in\nco-creation. MOSAAIC identifies three key dimensions of control: autonomy,\ninitiative, and authority. We supplement our framework with control\noptimization strategies in co-creation. To demonstrate MOSAAIC's applicability,\nwe analyze the distribution of control in six existing co-creative AI case\nstudies and present the implications of using this framework.", "authors": ["Alayt Issak", "Jeba Rezwana", "Casper Harteveld"], "published_date": "2025-05-16", "title_zh": "MOSAAIC：管理共享自主性、權威性與主動性，以優化共同創作", "summary_zh": "這篇論文探討人與AI協作創作時，如何取得控制權的平衡。研究提出了一個新的框架MOSAAIC，從自主性、主動性和權威性三個面向，分析和管理創作過程中的控制權分配。透過分析大量文獻和案例，論文展示了MOSAAIC框架的應用，幫助我們了解如何在人與AI之間更有效地分配控制權，促進更好的共同創作。", "audio": "audios/2505.11481v1.mp3", "timestamp": "2025-05-19T04:26:17.442066"}
{"id": "2505.13448v1", "url": "http://arxiv.org/abs/2505.13448v1", "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "summary": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE.", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "published_date": "2025-05-19", "category": "AI", "title_zh": "CIE：使用連續訊號控制語言模型文本生成", "summary_zh": "為了提升使用者體驗，如何讓語言模型更符合使用者意圖越來越重要。本研究提出一種方法，透過連續訊號（例如介於「短」到「長」之間的向量）來控制語言模型生成的文本屬性，例如文本長度。實驗證明，相較於使用自然語言提示或離散訊號的方法，此方法能更可靠地控制生成文本的長度。相關程式碼與資料集已開源。", "audio": "audios/2505.13448v1.mp3", "timestamp": "2025-05-20T03:11:17.454935"}
{"id": "2505.13434v1", "url": "http://arxiv.org/abs/2505.13434v1", "title": "SMOTExT: SMOTE meets Large Language Models", "summary": "Data scarcity and class imbalance are persistent challenges in training\nrobust NLP models, especially in specialized domains or low-resource settings.\nWe propose a novel technique, SMOTExT, that adapts the idea of Synthetic\nMinority Over-sampling (SMOTE) to textual data. Our method generates new\nsynthetic examples by interpolating between BERT-based embeddings of two\nexisting examples and then decoding the resulting latent point into text with\nxRAG architecture. By leveraging xRAG's cross-modal retrieval-generation\nframework, we can effectively turn interpolated vectors into coherent text.\nWhile this is preliminary work supported by qualitative outputs only, the\nmethod shows strong potential for knowledge distillation and data augmentation\nin few-shot settings. Notably, our approach also shows promise for\nprivacy-preserving machine learning: in early experiments, training models\nsolely on generated data achieved comparable performance to models trained on\nthe original dataset. This suggests a viable path toward safe and effective\nlearning under data protection constraints.", "authors": ["Mateusz Bystroński", "Mikołaj Hołysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "published_date": "2025-05-19", "category": "Foundation Model", "title_zh": "SMOTExT：SMOTE 遇上大型語言模型", "summary_zh": "SMOTExT是一種新的文字資料增強技術，它將SMOTE（合成少數類過採樣技術）的概念應用於自然語言處理。此方法透過插值BERT嵌入向量，然後使用xRAG架構將插值後的向量解碼為文本，生成新的合成樣本。初步實驗顯示，SMOTExT在小樣本學習中具有知識蒸餾和數據增強的潛力，甚至能在隱私保護的機器學習中，僅用生成的數據訓練出與原始數據訓練的模型相近的效能。總而言之，SMOTExT為解決資料稀缺和類別不平衡問題提供了一種有前景的方案。", "audio": "audios/2505.13434v1.mp3", "timestamp": "2025-05-20T03:11:21.857638"}
{"id": "2505.13447v1", "url": "http://arxiv.org/abs/2505.13447v1", "title": "Mean Flows for One-step Generative Modeling", "summary": "We propose a principled and effective framework for one-step generative\nmodeling. We introduce the notion of average velocity to characterize flow\nfields, in contrast to instantaneous velocity modeled by Flow Matching methods.\nA well-defined identity between average and instantaneous velocities is derived\nand used to guide neural network training. Our method, termed the MeanFlow\nmodel, is self-contained and requires no pre-training, distillation, or\ncurriculum learning. MeanFlow demonstrates strong empirical performance: it\nachieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet\n256x256 trained from scratch, significantly outperforming previous\nstate-of-the-art one-step diffusion/flow models. Our study substantially\nnarrows the gap between one-step diffusion/flow models and their multi-step\npredecessors, and we hope it will motivate future research to revisit the\nfoundations of these powerful models.", "authors": ["Zhengyang Geng", "Mingyang Deng", "Xingjian Bai", "J. Zico Kolter", "Kaiming He"], "published_date": "2025-05-19", "category": "Diffusion Model", "title_zh": "用於單步生成建模的平均流", "summary_zh": "本論文提出了一個穩健且有效的單步生成模型框架。不同於Flow Matching方法中建模瞬時速度，本文引入了「平均速度」的概念來描述流場。透過推導平均速度和瞬時速度之間明確的關係，引導神經網絡訓練。此方法命名為MeanFlow模型，無需預訓練、知識提煉或課程學習。實驗結果顯示，MeanFlow在ImageNet 256x256上從零開始訓練，僅需單次函數評估（1-NFE）便達到3.43的FID，顯著超越先前最先進的單步擴散/流模型，大幅縮小了單步模型與多步模型之間的差距。期望這項研究能激勵未來對於這些強大模型基礎的深入探討。", "audio": "audios/2505.13447v1.mp3", "timestamp": "2025-05-20T03:11:26.812839"}
{"id": "2505.13445v1", "url": "http://arxiv.org/abs/2505.13445v1", "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "published_date": "2025-05-19", "category": "AI", "title_zh": "信任，但要驗證：一種使用可驗證獎勵的強化學習自驗證方法", "summary_zh": "大型語言模型在複雜推理方面展現了巨大潛力，其中使用可驗證獎勵的強化學習（RLVR）是一種關鍵的增強策略。然而，一個常見的問題是“表面自反思”，即模型無法有效驗證自己的輸出。我們引入了RISE（利用自驗證強化推理），這是一種新的線上強化學習框架，旨在解決這個問題。RISE明確地並且同時訓練大型語言模型，在單一整合的強化學習過程中，提高其解決問題和自我驗證的能力。核心機制是利用來自結果驗證器的可驗證獎勵，為解決方案生成和自我驗證任務提供即時回饋。在每次迭代中，模型生成解決方案，然後批判性地檢視自身生成的解決方案，這兩個軌跡都有助於策略更新。在各種數學推理基準上的廣泛實驗表明，RISE始終如一地提高了模型解決問題的準確性，同時培養了強大的自我驗證能力。我們的分析強調了線上驗證的優勢以及增加驗證計算的好處。此外，RISE模型在推理過程中表現出更頻繁和準確的自我驗證行為。這些優勢鞏固了RISE作為開發更穩健和具有自我意識的推理器的靈活且有效的途徑。", "audio": "audios/2505.13445v1.mp3", "timestamp": "2025-05-20T04:22:04.619238"}
{"id": "2505.13419v1", "url": "http://arxiv.org/abs/2505.13419v1", "title": "FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning", "summary": "Facial Emotion Analysis (FEA) plays a crucial role in visual affective\ncomputing, aiming to infer a person's emotional state based on facial data.\nScientifically, facial expressions (FEs) result from the coordinated movement\nof facial muscles, which can be decomposed into specific action units (AUs)\nthat provide detailed emotional insights. However, traditional methods often\nstruggle with limited interpretability, constrained generalization and\nreasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have\nshown exceptional performance in various visual tasks, while they still face\nsignificant challenges in FEA due to the lack of specialized datasets and their\ninability to capture the intricate relationships between FEs and AUs. To\naddress these issues, we introduce a novel FEA Instruction Dataset that\nprovides accurate and aligned FE and AU descriptions and establishes causal\nreasoning relationships between them, followed by constructing a new benchmark,\nFEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to\ncapture more detailed facial information, enhancing its capability in FEA\ntasks. Our model demonstrates strong performance on FEABench and impressive\ngeneralization capability through zero-shot evaluation on various datasets,\nincluding RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and\neffectiveness in FEA tasks. The dataset and code will be available at\nhttps://github.com/953206211/FEALLM.", "authors": ["Zhuozhao Hu", "Kaishen Yuan", "Xin Liu", "Zitong Yu", "Yuan Zong", "Jingang Shi", "Huanjing Yue", "Jingyu Yang"], "published_date": "2025-05-19", "category": "Foundation Model", "title_zh": "FEALLM：透過情緒協同與推理，推進多模態大型語言模型在臉部表情分析方面的能力", "summary_zh": "FEALLM 提出了一個新的臉部表情分析方法，運用多模態大型語言模型。它建立了一個包含精準的臉部表情和動作單元描述的資料集，並設計了一個新的模型架構，強化模型捕捉細緻臉部資訊的能力。實驗結果顯示，FEALLM 在臉部表情分析任務上表現出色，並展現了良好的泛化能力。", "audio": "audios/2505.13419v1.mp3", "timestamp": "2025-05-20T04:22:10.035222"}
{"id": "2505.13273v1", "url": "http://arxiv.org/abs/2505.13273v1", "title": "Seeing the Unseen: How EMoE Unveils Bias in Text-to-Image Diffusion Models", "summary": "Estimating uncertainty in text-to-image diffusion models is challenging\nbecause of their large parameter counts (often exceeding 100 million) and\noperation in complex, high-dimensional spaces with virtually infinite input\npossibilities. In this paper, we propose Epistemic Mixture of Experts (EMoE), a\nnovel framework for efficiently estimating epistemic uncertainty in diffusion\nmodels. EMoE leverages pre-trained networks without requiring additional\ntraining, enabling direct uncertainty estimation from a prompt. We leverage a\nlatent space within the diffusion process that captures epistemic uncertainty\nbetter than existing methods. Experimental results on the COCO dataset\ndemonstrate EMoE's effectiveness, showing a strong correlation between\nuncertainty and image quality. Additionally, EMoE identifies under-sampled\nlanguages and regions with higher uncertainty, revealing hidden biases in the\ntraining set. This capability demonstrates the relevance of EMoE as a tool for\naddressing fairness and accountability in AI-generated content.", "authors": ["Lucas Berry", "Axel Brando", "Wei-Di Chang", "Juan Camilo Gamboa Higuera", "David Meger"], "published_date": "2025-05-19", "category": "Diffusion Model", "title_zh": "看見未見之處：EMoE 如何揭示文本到圖像擴散模型中的偏見", "summary_zh": "現今文本到圖像的擴散模型參數龐大，難以估算其不確定性。本研究提出「知識混合專家模型」(EMoE)，能有效率地估計擴散模型中的知識不確定性。EMoE利用預訓練網路，無需額外訓練，即可直接從提示詞中估算不確定性，並藉由擴散過程中的潛在空間來捕捉知識不確定性，效果優於現有方法。實驗證明EMoE與圖像品質高度相關，並且能識別訓練集中代表性不足的語言和區域，進而揭示模型中隱藏的偏見。這顯示EMoE能作為AI生成內容中，處理公平性和問責制問題的有效工具。", "audio": "audios/2505.13273v1.mp3", "timestamp": "2025-05-20T04:22:15.863491"}
{"id": "2505.13439v1", "url": "http://arxiv.org/abs/2505.13439v1", "title": "VTBench: Evaluating Visual Tokenizers for Autoregressive Image Generation", "summary": "Autoregressive (AR) models have recently shown strong performance in image\ngeneration, where a critical component is the visual tokenizer (VT) that maps\ncontinuous pixel inputs to discrete token sequences. The quality of the VT\nlargely defines the upper bound of AR model performance. However, current\ndiscrete VTs fall significantly behind continuous variational autoencoders\n(VAEs), leading to degraded image reconstructions and poor preservation of\ndetails and text. Existing benchmarks focus on end-to-end generation quality,\nwithout isolating VT performance. To address this gap, we introduce VTBench, a\ncomprehensive benchmark that systematically evaluates VTs across three core\ntasks: Image Reconstruction, Detail Preservation, and Text Preservation, and\ncovers a diverse range of evaluation scenarios. We systematically assess\nstate-of-the-art VTs using a set of metrics to evaluate the quality of\nreconstructed images. Our findings reveal that continuous VAEs produce superior\nvisual representations compared to discrete VTs, particularly in retaining\nspatial structure and semantic detail. In contrast, the degraded\nrepresentations produced by discrete VTs often lead to distorted\nreconstructions, loss of fine-grained textures, and failures in preserving text\nand object integrity. Furthermore, we conduct experiments on GPT-4o image\ngeneration and discuss its potential AR nature, offering new insights into the\nrole of visual tokenization. We release our benchmark and codebase publicly to\nsupport further research and call on the community to develop strong,\ngeneral-purpose open-source VTs.", "authors": ["Huawei Lin", "Tong Geng", "Zhaozhuo Xu", "Weijie Zhao"], "published_date": "2025-05-19", "category": "AI", "title_zh": "VTBench：評估自迴歸圖像生成中的視覺 Tokenizer", "summary_zh": "自迴歸模型在圖像生成方面表現出色，其中視覺 Tokenizer (VT) 至關重要，它將連續像素輸入映射到離散的 Token 序列。VT 的品質直接影響著自迴歸模型的效能上限。然而，目前的離散 VT 相較於連續變分自編碼器 (VAE) 仍有明顯差距，導致圖像重建品質下降，細節和文字的保留效果不佳。現有評估標準著重於端到端生成品質，忽略了對 VT 效能的獨立評估。為了解決這個問題，我們推出了 VTBench，一個全面的評估標準，它系統性地評估 VT 在三個核心任務上的表現：圖像重建、細節保留和文字保留，並涵蓋多樣的評估場景。我們使用一系列指標系統性地評估了最先進的 VT，以評估重建圖像的品質。我們的研究結果表明，連續 VAE 產生了優於離散 VT 的視覺表徵，尤其是在保留空間結構和語義細節方面。相比之下，離散 VT 產生的劣質表徵通常會導致失真的重建、細粒度紋理的丟失以及在保留文本和物件完整性方面的失敗。此外，我們還對 GPT-4o 圖像生成進行了實驗，並討論了其潛在的自迴歸性質，為視覺 Tokenization 的作用提供了新的見解。我們公開發布了我們的評估標準和程式碼庫，以支持進一步的研究，並呼籲社群開發強大且通用的開源 VT。", "audio": "audios/2505.13439v1.mp3", "timestamp": "2025-05-20T05:18:43.416925"}
{"id": "2505.13418v1", "url": "http://arxiv.org/abs/2505.13418v1", "title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness", "summary": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter.", "authors": ["Lotem Peled-Cohen", "Maya Zadok", "Nitay Calderon", "Hila Gonen", "Roi Reichart"], "published_date": "2025-05-19", "category": "Foundation Model", "title_zh": "用不同的角度看失智症：針對人類與大型語言模型感知的可解釋性建模，以利早期察覺", "summary_zh": "認知功能衰退往往在診斷前數年就體現在語言中。非專業人士，像是病患的親近家人，常常是第一個察覺到變化的。隨著大型語言模型日益融入日常生活，甚至可能由它們發現異狀。這篇論文探討非專業人士如何透過語言感知失智症，並比較其與大型語言模型的判斷。研究發現，人類對失智症的感知不一致，且仰賴狹隘甚至具誤導性的線索。相比之下，大型語言模型利用更豐富、更細膩的特徵，更貼近臨床模式。但兩者都容易出現假陰性，經常忽略失智症病例。透過可解釋性框架，我們希望能幫助非專業人士更好地識別重要的語言徵兆。", "audio": "audios/2505.13418v1.mp3", "timestamp": "2025-05-20T05:18:49.560907"}
{"id": "2505.13244v1", "url": "http://arxiv.org/abs/2505.13244v1", "title": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models", "summary": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection.", "authors": ["Jieying Xue", "Phuong Minh Nguyen", "Minh Le Nguyen", "Xin Liu"], "published_date": "2025-05-19", "category": "Diffusion Model", "title_zh": "JNLP於SemEval-2025 Task 11：使用生成模型進行跨語言多標籤情感偵測", "summary_zh": "隨著全球數位化，跨語言情感偵測日益重要。本研究針對SemEval-2025 Task 11，利用預訓練多語言模型，探討多標籤情感偵測和情感強度這兩個子任務。我們採用微調的BERT分類模型和指令調整的生成LLM兩種架構，並提出兩種方法處理多標籤分類。實驗結果表明，我們的模型在多語言情感辨識方面具有強大的泛化能力，在Track A中，我們的模型在10種語言中取得前四的成績，並在印地語中排名第一。在Track B中，我們的方法在7種語言中也取得了前五名的成績，證明了其簡潔性和有效性。", "audio": "audios/2505.13244v1.mp3", "timestamp": "2025-05-20T05:18:54.395293"}
{"id": "2505.13438v1", "url": "http://arxiv.org/abs/2505.13438v1", "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "published_date": "2025-05-19", "category": "AI", "title_zh": "透過預算相對策略優化來最佳化隨時推理", "summary_zh": "為了提升大型語言模型的推理能力，增加測試時的計算量非常重要。現有方法通常使用強化學習來最大化推理結束時的可驗證獎勵。然而，這些方法只優化固定預算下的最終性能，效率不高。本研究提出一個名為 AnytimeReasoner 的新框架，旨在最佳化隨時推理性能，提升token效率，並在不同token預算限制下提供更靈活的推理。透過將完整推理過程截斷到來自先驗分佈的採樣token預算內，模型必須為每次截斷的思考總結出最佳答案進行驗證。這引入了可驗證的密集獎勵，促進強化學習中更有效的信用分配。此外，我們還提出了一種新的方差縮減技術，即預算相對策略優化（BRPO），以增強學習過程的穩健性和效率。在數學推理任務中的實驗結果表明，我們的模型在各種先驗分佈下，始終優於GRPO，提高了訓練和token效率。", "audio": "audios/2505.13438v1.mp3", "timestamp": "2025-05-20T06:27:24.396559"}
{"id": "2505.13416v1", "url": "http://arxiv.org/abs/2505.13416v1", "title": "Gluon: Making Muon & Scion Great Again! (Bridging Theory and Practice of LMO-based Optimizers for LLMs)", "summary": "Recent developments in deep learning optimization have brought about\nradically new algorithms based on the Linear Minimization Oracle (LMO)\nframework, such as $\\sf Muon$ and $\\sf Scion$. After over a decade of $\\sf\nAdam$'s dominance, these LMO-based methods are emerging as viable replacements,\noffering several practical advantages such as improved memory efficiency,\nbetter hyperparameter transferability, and most importantly, superior empirical\nperformance on large-scale tasks, including LLM training. However, a\nsignificant gap remains between their practical use and our current theoretical\nunderstanding: prior analyses (1) overlook the layer-wise LMO application of\nthese optimizers in practice, and (2) rely on an unrealistic smoothness\nassumption, leading to impractically small stepsizes. To address both, we\npropose a new LMO-based method called $\\sf Gluon$, capturing prior\ntheoretically analyzed methods as special cases, and introduce a new refined\ngeneralized smoothness model that captures the layer-wise geometry of neural\nnetworks, matches the layer-wise practical implementation of $\\sf Muon$ and\n$\\sf Scion$, and leads to convergence guarantees with strong practical\npredictive power. Unlike prior results, our theoretical stepsizes closely match\nthe fine-tuned values reported by Pethick et al. (2025). Our experiments with\nNanoGPT and CNN confirm that our assumption holds along the optimization\ntrajectory, ultimately closing the gap between theory and practice.", "authors": ["Artem Riabinin", "Egor Shulgin", "Kaja Gruntkowska", "Peter Richtárik"], "published_date": "2025-05-19", "category": "Foundation Model", "title_zh": "Gluon：讓Muon和Scion再次偉大！（彌合基於LMO的LLM優化器之理論與實踐差距）", "summary_zh": "近年來，基於線性最小化預言機（LMO）框架的Muon和Scion等優化算法嶄露頭角，有望取代Adam。它們在記憶體效率、超參數遷移和大規模任務（包括LLM訓練）的性能上表現更佳。然而，理論與實踐間存在差距，過去的研究未能考慮這些優化器在實踐中逐層應用LMO的特性，以及過於理想化的平滑性假設導致步長過小。為了解決這些問題，我們提出了一種新的LMO方法Gluon，它涵蓋了先前理論分析過的方法，並引入了一種新的廣義平滑性模型，可以捕捉神經網路的逐層幾何結構，與Muon和Scion的逐層實作相符，並能提供具有實際預測能力的收斂保證。實驗結果表明，我們的理論步長與實際調整值非常接近，最終彌合了理論與實踐之間的差距。", "audio": "audios/2505.13416v1.mp3", "timestamp": "2025-05-20T06:27:30.916151"}
{"id": "2505.13213v1", "url": "http://arxiv.org/abs/2505.13213v1", "title": "Diffusion Models with Double Guidance: Generate with aggregated datasets", "summary": "Creating large-scale datasets for training high-performance generative models\nis often prohibitively expensive, especially when associated attributes or\nannotations must be provided. As a result, merging existing datasets has become\na common strategy. However, the sets of attributes across datasets are often\ninconsistent, and their naive concatenation typically leads to block-wise\nmissing conditions. This presents a significant challenge for conditional\ngenerative modeling when the multiple attributes are used jointly as\nconditions, thereby limiting the model's controllability and applicability. To\naddress this issue, we propose a novel generative approach, Diffusion Model\nwith Double Guidance, which enables precise conditional generation even when no\ntraining samples contain all conditions simultaneously. Our method maintains\nrigorous control over multiple conditions without requiring joint annotations.\nWe demonstrate its effectiveness in molecular and image generation tasks, where\nit outperforms existing baselines both in alignment with target conditional\ndistributions and in controllability under missing condition settings.", "authors": ["Yanfeng Yang", "Kenji Fukumizu"], "published_date": "2025-05-19", "category": "Diffusion Model", "title_zh": "雙重引導的擴散模型：使用聚合數據集進行生成", "summary_zh": "為了訓練高效能的生成模型，建立大規模數據集往往成本高昂。因此，合併現有數據集成為常見策略，但各數據集的屬性往往不一致，直接合併容易導致條件缺失。針對這個問題，我們提出一種名為「雙重引導的擴散模型」的新方法，即使沒有同時包含所有條件的訓練樣本，也能實現精確的條件生成，在不需要聯合標註的情況下，嚴格控制多個條件。實驗證明，在分子和圖像生成任務中，我們的模型在目標條件分佈對齊以及缺失條件下的可控性方面，都優於現有基線。", "audio": "audios/2505.13213v1.mp3", "timestamp": "2025-05-20T06:27:36.081132"}
{"query": "AI", "id": "2505.13400v1", "url": "http://arxiv.org/abs/2505.13400v1", "title": "Robin: A multi-agent system for automating scientific discovery", "summary": "Scientific discovery is driven by the iterative process of background\nresearch, hypothesis generation, experimentation, and data analysis. Despite\nrecent advancements in applying artificial intelligence to scientific\ndiscovery, no system has yet automated all of these stages in a single\nworkflow. Here, we introduce Robin, the first multi-agent system capable of\nfully automating the key intellectual steps of the scientific process. By\nintegrating literature search agents with data analysis agents, Robin can\ngenerate hypotheses, propose experiments, interpret experimental results, and\ngenerate updated hypotheses, achieving a semi-autonomous approach to scientific\ndiscovery. By applying this system, we were able to identify a novel treatment\nfor dry age-related macular degeneration (dAMD), the major cause of blindness\nin the developed world. Robin proposed enhancing retinal pigment epithelium\nphagocytosis as a therapeutic strategy, and identified and validated a\npromising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho\nkinase (ROCK) inhibitor that has never previously been proposed for treating\ndAMD. To elucidate the mechanism of ripasudil-induced upregulation of\nphagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment,\nwhich revealed upregulation of ABCA1, a critical lipid efflux pump and possible\nnovel target. All hypotheses, experimental plans, data analyses, and data\nfigures in the main text of this report were produced by Robin. As the first AI\nsystem to autonomously discover and validate a novel therapeutic candidate\nwithin an iterative lab-in-the-loop framework, Robin establishes a new paradigm\nfor AI-driven scientific discovery.", "authors": ["Ali Essam Ghareeb", "Benjamin Chang", "Ludovico Mitchener", "Angela Yiu", "Caralyn J. Szostkiewicz", "Jon M. Laurent", "Muhammed T. Razzak", "Andrew D. White", "Michaela M. Hinks", "Samuel G. Rodriques"], "published_date": "2025-05-19", "title_zh": "羅賓：一個用於自動化科學發現的多代理人系統", "summary_zh": "科學發現通常需要反覆進行文獻研究、假說生成、實驗以及數據分析。本文介紹了名為「羅賓」的多代理人系統，它整合了文獻搜尋和數據分析代理，首次能夠完全自動化科學發現過程中的關鍵步驟。羅賓能生成假說、提出實驗方案、解讀實驗結果並更新假說，實現半自主的科學發現。利用羅賓，我們發現了一種治療乾性老年黃斑部病變（dAMD）的新方法，並驗證了潛在候選藥物ripasudil。羅賓還分析了後續實驗，揭示了ABCA1的表達上調，這可能是個新的治療靶點。重要的是，本文中的所有假說、實驗計畫、數據分析和圖表均由羅賓生成。作為首個在迭代實驗迴路中自主發現並驗證治療候選藥物的人工智慧系統，羅賓為人工智慧驅動的科學發現建立了一個新典範。", "audio": "audios/2505.13400v1.mp3", "timestamp": "2025-05-20T09:53:34.781452"}
{"query": "Foundation Model", "id": "2505.13291v1", "url": "http://arxiv.org/abs/2505.13291v1", "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents", "summary": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating\nArtificial Intelligence (AI) agents on time series machine learning engineering\nchallenges. Existing benchmarks lack scalability, focus narrowly on model\nbuilding in well-defined settings, and evaluate only a limited set of research\nartifacts (e.g., CSV submission files). To make AI agent benchmarking more\nrelevant to the practice of machine learning engineering, our framework scales\nalong two critical dimensions. First, recognizing that effective ML engineering\nrequires a range of diverse skills, TimeSeriesGym incorporates challenges from\ndiverse sources spanning multiple domains and tasks. We design challenges to\nevaluate both isolated capabilities (including data handling, understanding\nresearch repositories, and code translation) and their combinations, and rather\nthan addressing each challenge independently, we develop tools that support\ndesigning multiple challenges at scale. Second, we implement evaluation\nmechanisms for multiple research artifacts, including submission files, code,\nand models, using both precise numeric measures and more flexible LLM-based\nevaluation approaches. This dual strategy balances objective assessment with\ncontextual judgment. Although our initial focus is on time series applications,\nour framework can be readily extended to other data modalities, broadly\nenhancing the comprehensiveness and practical utility of agentic AI evaluation.\nWe open-source our benchmarking framework to facilitate future research on the\nML engineering capabilities of AI agents.", "authors": ["Yifu Cai", "Xinyu Li", "Mononito Goswami", "Michał Wiliński", "Gus Welter", "Artur Dubrawski"], "published_date": "2025-05-19", "title_zh": "TimeSeriesGym：一個可擴展的機器學習工程（時間序列）代理基準測試", "summary_zh": "TimeSeriesGym 是一個可擴展的基準測試框架，用於評估人工智慧 (AI) 代理在時間序列機器學習工程挑戰中的表現。現有的基準測試缺乏可擴展性，且過於狹隘地關注良好定義環境下的模型構建，並僅評估有限的研究成果。TimeSeriesGym 透過兩個關鍵面向提升可擴展性：首先，它整合了來自多個領域和任務的多樣化挑戰，評估資料處理、理解研究庫和程式碼翻譯等不同技能的組合。其次，它評估多種研究成果，包括提交檔案、程式碼和模型，同時採用精確的數值度量和基於大型語言模型 (LLM) 的彈性評估方法。這個框架開放原始碼，旨在促進對 AI 代理機器學習工程能力的研究。", "audio": "audios/2505.13291v1.mp3", "timestamp": "2025-05-20T09:53:40.535627"}
{"query": "Diffusion Model", "id": "2505.13389v1", "url": "http://arxiv.org/abs/2505.13389v1", "title": "Faster Video Diffusion with Trainable Sparse Attention", "summary": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at \\emph{both}\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight \\emph{critical tokens}; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models.", "authors": ["Peiyuan Zhang", "Haofeng Huang", "Yongqi Chen", "Will Lin", "Zhengzhong Liu", "Ion Stoica", "Eric P. Xing", "Hao Zhang"], "published_date": "2025-05-19", "title_zh": "透過可訓練的稀疏注意力加速影片擴散", "summary_zh": "影片擴散轉換器（DiT）的擴展受限於其二次方的3D注意力，即便大部分注意力集中在一小部分位置。我們提出了VSA，一種可訓練、硬體高效的稀疏注意力，在訓練和推論階段都取代了完整注意力。VSA使用輕量級粗略階段將tokens池化成tiles，並識別高權重的「關鍵tokens」；精細階段僅在這些tiles內部計算token級別的注意力，並採用區塊計算布局以確保硬體效率。這產生了一個可端到端訓練的單一可微核心，無需事後分析，並維持了FlashAttention3 MFU的85%。我們進行了大量的消融研究和縮放律實驗，對參數量從60M到1.4B的DiT進行預訓練。VSA達到了一個帕雷托點，在擴散損失沒有下降的情況下，將訓練FLOPS降低了2.53倍。改造開源的Wan-2.1模型後，注意力時間加速了6倍，並在品質相當的情況下，將端到端生成時間從31秒降低到18秒。這些結果表明，可訓練的稀疏注意力是完整注意力的一個實用替代方案，也是進一步擴展影片擴散模型的關鍵推動因素。", "audio": "audios/2505.13389v1.mp3", "timestamp": "2025-05-20T09:53:47.983791"}
{"query": "AI", "id": "2505.13381v1", "url": "http://arxiv.org/abs/2505.13381v1", "title": "How Adding Metacognitive Requirements in Support of AI Feedback in Practice Exams Transforms Student Learning Behaviors", "summary": "Providing personalized, detailed feedback at scale in large undergraduate\nSTEM courses remains a persistent challenge. We present an empirically\nevaluated practice exam system that integrates AI generated feedback with\ntargeted textbook references, deployed in a large introductory biology course.\nOur system encourages metacognitive behavior by asking students to explain\ntheir answers and declare their confidence. It uses OpenAI's GPT-4o to generate\npersonalized feedback based on this information, while directing them to\nrelevant textbook sections. Through interaction logs from consenting\nparticipants across three midterms (541, 342, and 413 students respectively),\ntotaling 28,313 question-student interactions across 146 learning objectives,\nalong with 279 surveys and 23 interviews, we examined the system's impact on\nlearning outcomes and engagement. Across all midterms, feedback types showed no\nstatistically significant performance differences, though some trends suggested\npotential benefits. The most substantial impact came from the required\nconfidence ratings and explanations, which students reported transferring to\ntheir actual exam strategies. About 40 percent of students engaged with\ntextbook references when prompted by feedback -- far higher than traditional\nreading rates. Survey data revealed high satisfaction (mean rating 4.1 of 5),\nwith 82.1 percent reporting increased confidence on practiced midterm topics,\nand 73.4 percent indicating they could recall and apply specific concepts. Our\nfindings suggest that embedding structured reflection requirements may be more\nimpactful than sophisticated feedback mechanisms.", "authors": ["Mak Ahmad", "Prerna Ravi", "David Karger", "Marc Facciotti"], "published_date": "2025-05-19", "title_zh": "如何在練習測驗中加入元認知需求以支援人工智慧回饋，進而改變學生學習行為", "summary_zh": "在大型大學STEM課程中提供大規模且個人化的回饋是個挑戰。這項研究設計了一個練習測驗系統，結合人工智慧生成的回饋與相關教科書參考，並在生物入門課中實施。該系統鼓勵學生進行元認知，要求他們解釋答案並聲明信心程度。系統使用GPT-4o生成個人化回饋，並引導學生查閱教科書。研究發現，雖然不同回饋類型的成績差異不大，但要求學生評估信心程度並解釋答案，對他們的學習策略有顯著影響，許多學生也表示將這些策略應用於正式考試中。當被回饋提示時，約有40%的學生會參考教科書，遠高於傳統閱讀率。調查顯示學生滿意度高，並表示對練習過的考題更有信心，也更能回憶和應用特定概念。研究表明，比起複雜的回饋機制，嵌入結構化的反思需求可能更具影響力。", "audio": "audios/2505.13381v1.mp3", "timestamp": "2025-05-20T10:20:40.252613"}
{"query": "Foundation Model", "id": "2505.13255v1", "url": "http://arxiv.org/abs/2505.13255v1", "title": "Policy Contrastive Decoding for Robotic Foundation Models", "summary": "Robotic foundation models, or generalist robot policies, hold immense\npotential to enable flexible, general-purpose and dexterous robotic systems.\nDespite their advancements, our empirical experiments reveal that existing\nrobot policies are prone to learning spurious correlations from pre-training\ntrajectories, adversely affecting their generalization capabilities beyond the\ntraining data. To tackle this, we propose a novel Policy Contrastive Decoding\n(PCD) approach, which redirects the robot policy's focus toward object-relevant\nvisual clues by contrasting action probability distributions derived from\noriginal and object-masked visual inputs. As a training-free method, our PCD\ncan be used as a plugin to improve different types of robot policies without\nneeding to finetune or access model weights. We conduct extensive experiments\non top of three open-source robot policies, including the autoregressive policy\nOpenVLA and the diffusion-based policies Octo and $\\pi_0$. The obtained results\nin both simulation and real-world environments prove PCD's flexibility and\neffectiveness, e.g., PCD enhances the state-of-the-art policy $\\pi_0$ by 8% in\nthe simulation environment and by 108% in the real-world environment. Code and\ndemos are publicly available at: https://Koorye.github.io/proj/PCD.", "authors": ["Shihan Wu", "Ji Zhang", "Xu Luo", "Junlin Xie", "Jingkuan Song", "Heng Tao Shen", "Lianli Gao"], "published_date": "2025-05-19", "title_zh": "用於機器人基礎模型的策略對比解碼", "summary_zh": "機器人基礎模型，也就是通用型機器人策略，有潜力打造更靈活的機器人系統。然而，研究發現現有的機器人策略容易從預訓練資料中學到虛假關聯性，導致泛化能力下降。為了解決這個問題，我們提出一種名為「策略對比解碼 (PCD)」的新方法，它通過对比原始視覺輸入和遮蔽物體的視覺輸入所得到的動作概率分佈，引導機器人策略關注與物體相關的視覺線索。PCD無需訓練，可以像插件一樣使用，提升各種機器人策略的性能，而無需微調或訪問模型權重。大量實驗表明PCD具有靈活性和有效性，例如，在模擬環境和真實環境中，它分別將最先進的策略 π₀ 的性能提升了 8% 和 108%。程式碼和演示可在指定連結取得。", "audio": "audios/2505.13255v1.mp3", "timestamp": "2025-05-20T10:20:47.331287"}
{"query": "Diffusion Model", "id": "2505.13377v1", "url": "http://arxiv.org/abs/2505.13377v1", "title": "Restoration Score Distillation: From Corrupted Diffusion Pretraining to One-Step High-Quality Generation", "summary": "Learning generative models from corrupted data is a fundamental yet\npersistently challenging task across scientific disciplines, particularly when\naccess to clean data is limited or expensive. Denoising Score Distillation\n(DSD) \\cite{chen2025denoising} recently introduced a novel and surprisingly\neffective strategy that leverages score distillation to train high-fidelity\ngenerative models directly from noisy observations. Building upon this\nfoundation, we propose \\textit{Restoration Score Distillation} (RSD), a\nprincipled generalization of DSD that accommodates a broader range of\ncorruption types, such as blurred, incomplete, or low-resolution images. RSD\noperates by first pretraining a teacher diffusion model solely on corrupted\ndata and subsequently distilling it into a single-step generator that produces\nhigh-quality reconstructions. Empirically, RSD consistently surpasses its\nteacher model across diverse restoration tasks on both natural and scientific\ndatasets. Moreover, beyond standard diffusion objectives, the RSD framework is\ncompatible with several corruption-aware training techniques such as Ambient\nTweedie, Ambient Diffusion, and its Fourier-space variant, enabling flexible\nintegration with recent advances in diffusion modeling. Theoretically, we\ndemonstrate that in a linear regime, RSD recovers the eigenspace of the clean\ndata covariance matrix from linear measurements, thereby serving as an implicit\nregularizer. This interpretation recasts score distillation not only as a\nsampling acceleration technique but as a principled approach to enhancing\ngenerative performance in severely degraded data regimes.", "authors": ["Yasi Zhang", "Tianyu Chen", "Zhendong Wang", "Ying Nian Wu", "Mingyuan Zhou", "Oscar Leong"], "published_date": "2025-05-19", "title_zh": "復原分數蒸餾：從已損毀的擴散預訓練到一步式高品質生成", "summary_zh": "從損毀數據學習生成模型是個重要的挑戰。復原分數蒸餾(RSD)是一種新的方法，它先用損毀數據訓練一個擴散模型作為老師，然後將其知識提煉成一個一步式生成器，直接重建出高品質的影像。RSD可以處理各種損毀類型，例如模糊、不完整或低解析度的圖像，並且在各種還原任務上都超越了老師模型。理論分析表明，RSD可以從線性測量中恢復乾淨數據的協方差矩陣的特徵空間，因此它不僅是一種加速取樣的技術，更是一種在數據嚴重降級的情況下提升生成性能的有效方法。", "audio": "audios/2505.13377v1.mp3", "timestamp": "2025-05-20T10:20:54.898993"}
{"query": "AI", "id": "2505.13355v1", "url": "http://arxiv.org/abs/2505.13355v1", "title": "Multi-Armed Bandits Meet Large Language Models", "summary": "Bandit algorithms and Large Language Models (LLMs) have emerged as powerful\ntools in artificial intelligence, each addressing distinct yet complementary\nchallenges in decision-making and natural language processing. This survey\nexplores the synergistic potential between these two fields, highlighting how\nbandit algorithms can enhance the performance of LLMs and how LLMs, in turn,\ncan provide novel insights for improving bandit-based decision-making. We first\nexamine the role of bandit algorithms in optimizing LLM fine-tuning, prompt\nengineering, and adaptive response generation, focusing on their ability to\nbalance exploration and exploitation in large-scale learning tasks.\nSubsequently, we explore how LLMs can augment bandit algorithms through\nadvanced contextual understanding, dynamic adaptation, and improved policy\nselection using natural language reasoning. By providing a comprehensive review\nof existing research and identifying key challenges and opportunities, this\nsurvey aims to bridge the gap between bandit algorithms and LLMs, paving the\nway for innovative applications and interdisciplinary research in AI.", "authors": ["Djallel Bouneffouf", "Raphael Feraud"], "published_date": "2025-05-19", "title_zh": "多臂老虎機遇上大型語言模型", "summary_zh": "這篇論文探討了多臂老虎機演算法與大型語言模型（LLM）之間的協同效應。多臂老虎機演算法可以優化 LLM 的微調、提示工程和自適應回應生成，而 LLM 則能利用其強大的上下文理解能力、動態適應能力和自然語言推理能力來改進多臂老虎機演算法的策略選擇。這篇綜述旨在促進這兩個領域的交叉研究，為人工智慧的創新應用鋪平道路。", "audio": "audios/2505.13355v1.mp3", "timestamp": "2025-05-20T11:15:50.184028"}
{"query": "Foundation Model", "id": "2505.13227v1", "url": "http://arxiv.org/abs/2505.13227v1", "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "published_date": "2025-05-19", "title_zh": "透過使用者介面分解與合成來擴展電腦使用接地的能力", "summary_zh": "圖形使用者介面 (GUI) 接地，也就是將自然語言指令映射到 GUI 上的特定操作，仍然是電腦使用代理程式開發中的一個關鍵瓶頸。現有基準測試過於簡化接地任務，將其視為簡短的指稱表達式，未能捕捉到需要軟體常識、版面理解和細粒度操作能力的真實世界互動的複雜性。為了應對這些局限性，我們引入了 OSWorld-G，這是一個全面的基準測試，包含 564 個跨多種任務類型（包括文本匹配、元素識別、版面理解和精確操作）的精細註釋樣本。此外，我們透過多視角解耦任務，合成並發布了最大的電腦使用接地資料集 Jedi，其中包含 400 萬個示例。我們在 Jedi 上訓練的多尺度模型透過優於 ScreenSpot-v2、ScreenSpot-Pro 和我們的 OSWorld-G 上的現有方法，證明了其有效性。此外，我們證明了 Jedi 改進的接地能力直接增強了通用基礎模型在複雜電腦任務上的代理能力，在 OSWorld 上從 5% 提高到 27%。透過詳細的消融研究，我們確定了影響接地效能的關鍵因素，並驗證了針對不同介面元素的專業資料的結合能夠實現對新介面的組合成泛化。所有基準測試、資料、檢查點和程式碼都是開源的，並且可於 https://osworld-grounding.github.io 取得。", "audio": "audios/2505.13227v1.mp3", "timestamp": "2025-05-20T11:16:02.194993"}
{"query": "Diffusion Model", "id": "2505.13375v1", "url": "http://arxiv.org/abs/2505.13375v1", "title": "Minimum-Excess-Work Guidance", "summary": "We propose a regularization framework inspired by thermodynamic work for\nguiding pre-trained probability flow generative models (e.g., continuous\nnormalizing flows or diffusion models) by minimizing excess work, a concept\nrooted in statistical mechanics and with strong conceptual connections to\noptimal transport. Our approach enables efficient guidance in sparse-data\nregimes common to scientific applications, where only limited target samples or\npartial density constraints are available. We introduce two strategies: Path\nGuidance for sampling rare transition states by concentrating probability mass\non user-defined subsets, and Observable Guidance for aligning generated\ndistributions with experimental observables while preserving entropy. We\ndemonstrate the framework's versatility on a coarse-grained protein model,\nguiding it to sample transition configurations between folded/unfolded states\nand correct systematic biases using experimental data. The method bridges\nthermodynamic principles with modern generative architectures, offering a\nprincipled, efficient, and physics-inspired alternative to standard fine-tuning\nin data-scarce domains. Empirical results highlight improved sample efficiency\nand bias reduction, underscoring its applicability to molecular simulations and\nbeyond.", "authors": ["Christopher Kolloff", "Tobias Höppe", "Emmanouil Angelis", "Mathias Jacob Schreiner", "Stefan Bauer", "Andrea Dittadi", "Simon Olsson"], "published_date": "2025-05-19", "title_zh": "最小過剩功引導", "summary_zh": "我們提出一種基於熱力學功的正則化框架，引導預訓練的機率流生成模型（例如連續歸一化流或擴散模型），通過最小化過剩功實現。這種方法在科學應用常見的稀疏數據情況下非常有效，僅需少量目標樣本或部分密度約束。我們介紹了兩種策略：路徑引導，用於採樣罕見的過渡態，以及可觀測量引導，用於將生成的分布與實驗觀測值對齊。在粗粒化蛋白模型上的實驗表明，該框架能夠有效地採樣摺疊/解摺疊狀態之間的過渡構型，並利用實驗數據修正系統偏差。總之，這項工作將熱力學原理與現代生成架構相結合，為數據稀缺領域提供了一種基於物理、高效且有原則的替代方案，優於標準微調方法。", "audio": "audios/2505.13375v1.mp3", "timestamp": "2025-05-20T11:16:08.821722"}
{"query": "AI", "id": "2505.13354v1", "url": "http://arxiv.org/abs/2505.13354v1", "title": "A large-scale analysis of public-facing, community-built chatbots on Character.AI", "summary": "This paper presents the first large-scale analysis of public-facing chatbots\non Character.AI, a rapidly growing social media platform where users create and\ninteract with chatbots. Character.AI is distinctive in that it merges\ngenerative AI with user-generated content, enabling users to build bots-often\nmodeled after fictional or public personas-for others to engage with. It is\nalso popular, with over 20 million monthly active users, and impactful, with\nrecent headlines detailing significant issues with youth engagement on the\nsite. Character.AI is thus of interest to study both substantively and\nconceptually. To this end, we present a descriptive overview of the site using\na dataset of 2.1 million English-language prompts (or ``greetings'') for\nchatbots on the site, created by around 1 million users. Our work explores the\nprevalence of different fandoms on the site, broader tropes that persist across\nfandoms, and how dynamics of power intersect with gender within greetings.\nOverall, our findings illuminate an emerging form of online (para)social\ninteraction that toes a unique and important intersection between generative AI\nand user-generated content.", "authors": ["Owen Lee", "Kenneth Joseph"], "published_date": "2025-05-19", "title_zh": "Character.AI上公開、社群建立的聊天機器人的大規模分析", "summary_zh": "本研究首次大規模分析Character.AI平台上公開的聊天機器人。Character.AI是一個快速成長的社交媒體平台，用戶可以創建並與聊天機器人互動。它結合了生成式AI和使用者產生的內容，讓使用者可以建立模仿虛構或公眾人物的機器人。本研究利用包含210萬條英文提示詞的數據集，描述了Character.AI的概況，並探討了平台上的不同粉絲群體、常見的主題，以及權力動態如何與性別交叉。研究結果揭示了一種新興的線上準社交互動形式，它獨特且重要地結合了生成式AI和使用者產生的內容。", "audio": "audios/2505.13354v1.mp3", "timestamp": "2025-05-20T12:38:41.704889"}
{"query": "Foundation Model", "id": "2505.13192v1", "url": "http://arxiv.org/abs/2505.13192v1", "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics", "summary": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters and\norders of magnitude faster inference times. DynaMix outperforms TS foundation\nmodels in terms of long-term statistics, and often also short-term forecasts,\neven on real-world time series, like traffic or weather data, typically used\nfor training and evaluating TS models, but not at all part of DynaMix' training\ncorpus. We illustrate some of the failure modes of TS models for DSR problems,\nand conclude that models built on DS principles may bear a huge potential also\nfor advancing the TS prediction field.", "authors": ["Christoph Jürgen Hemmer", "Daniel Durstewitz"], "published_date": "2025-05-19", "title_zh": "真實零樣本推論：長期統計量保持的動態系統", "summary_zh": "DynaMix是一種新的動態系統重建模型，它基於ALRNN的專家混合架構進行預訓練。與傳統方法不同，DynaMix無需針對每個新系統進行重新訓練，就能夠零樣本泛化到未知的動態系統。只需提供上下文信號，DynaMix即可忠實地預測新系統的長期演化，其性能優於現有的時間序列基礎模型，且參數更少、推論速度更快。即使面對真實世界的交通或天氣數據，DynaMix也能在長期統計量方面勝過這些模型。這項研究表明，基於動態系統原理構建的模型在時間序列預測領域具有巨大潛力。", "audio": "audios/2505.13192v1.mp3", "timestamp": "2025-05-20T12:38:46.833826"}
{"query": "Diffusion Model", "id": "2505.13358v1", "url": "http://arxiv.org/abs/2505.13358v1", "title": "One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling", "summary": "Diffusion-based generative models have demonstrated exceptional performance,\nyet their iterative sampling procedures remain computationally expensive. A\nprominent strategy to mitigate this cost is distillation, with offline\ndistillation offering particular advantages in terms of efficiency, modularity,\nand flexibility. In this work, we identify two key observations that motivate a\nprincipled distillation framework: (1) while diffusion models have been viewed\nthrough the lens of dynamical systems theory, powerful and underexplored tools\ncan be further leveraged; and (2) diffusion models inherently impose\nstructured, semantically coherent trajectories in latent space. Building on\nthese observations, we introduce the Koopman Distillation Model KDM, a novel\noffline distillation approach grounded in Koopman theory-a classical framework\nfor representing nonlinear dynamics linearly in a transformed space. KDM\nencodes noisy inputs into an embedded space where a learned linear operator\npropagates them forward, followed by a decoder that reconstructs clean samples.\nThis enables single-step generation while preserving semantic fidelity. We\nprovide theoretical justification for our approach: (1) under mild assumptions,\nthe learned diffusion dynamics admit a finite-dimensional Koopman\nrepresentation; and (2) proximity in the Koopman latent space correlates with\nsemantic similarity in the generated outputs, allowing for effective trajectory\nalignment. Empirically, KDM achieves state-of-the-art performance across\nstandard offline distillation benchmarks, improving FID scores by up to 40% in\na single generation step. All implementation details and code for the\nexperimental setups are provided in our GitHub -\nhttps://github.com/azencot-group/KDM, or in our project page -\nhttps://sites.google.com/view/koopman-distillation-model.", "authors": ["Nimrod Berman", "Ilan Naiman", "Moshe Eliasof", "Hedi Zisling", "Omri Azencot"], "published_date": "2025-05-19", "title_zh": "基於Koopman建模的擴散模型一步式離線蒸餾", "summary_zh": "擴散模型在生成任務上表現出色，但迭代採樣過程耗時。本研究提出一種名為 Koopman Distillation Model (KDM) 的創新離線蒸餾方法，利用 Koopman 理論將非線性擴散動態線性地表示在轉換後的空間中。KDM 通過學習線性算子在嵌入空間中傳播噪聲輸入，實現單步生成高品質樣本，在標準離線蒸餾基準測試中，FID 指標提升高達 40%。程式碼和更多資訊可在 GitHub 或專案頁面找到。", "audio": "audios/2505.13358v1.mp3", "timestamp": "2025-05-20T12:38:54.099908"}
{"query": "AI", "id": "2505.13338v1", "url": "http://arxiv.org/abs/2505.13338v1", "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "summary": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities.", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "published_date": "2025-05-19", "title_zh": "用於多模態語音-LLM的情境副語言資料創建：資料濃縮與口語問答生成", "summary_zh": "現有的語音語言模型在情境推理和副語言理解方面能力有限，主要是缺乏涵蓋這兩方面的問答資料集。本文提出一個新穎的框架，從真實世界的語音資料中生成同時整合情境推理和副語言資訊的資料集，包含基於偽副語言標籤的資料濃縮，以及基於大型語言模型的情境副語言問答生成。實驗證明，基於此框架生成的資料集，能有效提升語音語言模型的性能，但也揭示了模型在同理心推理任務上的不足，突顯了創建此類資料集及開發更強大模型的重要性。此框架為首創，有潛力訓練出更強大且具備副語言推理能力的語音語言模型。", "audio": "audios/2505.13338v1.mp3", "timestamp": "2025-05-20T13:31:45.014604"}
{"query": "Foundation Model", "id": "2505.13150v1", "url": "http://arxiv.org/abs/2505.13150v1", "title": "Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics", "summary": "Behavioral Foundation Models (BFMs) proved successful in producing policies\nfor arbitrary tasks in a zero-shot manner, requiring no test-time training or\ntask-specific fine-tuning. Among the most promising BFMs are the ones that\nestimate the successor measure learned in an unsupervised way from\ntask-agnostic offline data. However, these methods fail to react to changes in\nthe dynamics, making them inefficient under partial observability or when the\ntransition function changes. This hinders the applicability of BFMs in a\nreal-world setting, e.g., in robotics, where the dynamics can unexpectedly\nchange at test time. In this work, we demonstrate that Forward-Backward (FB)\nrepresentation, one of the methods from the BFM family, cannot distinguish\nbetween distinct dynamics, leading to an interference among the latent\ndirections, which parametrize different policies. To address this, we propose a\nFB model with a transformer-based belief estimator, which greatly facilitates\nzero-shot adaptation. We also show that partitioning the policy encoding space\ninto dynamics-specific clusters, aligned with the context-embedding directions,\nyields additional gain in performance. These traits allow our method to respond\nto the dynamics observed during training and to generalize to unseen ones.\nEmpirically, in the changing dynamics setting, our approach achieves up to a 2x\nhigher zero-shot returns compared to the baselines for both discrete and\ncontinuous tasks.", "authors": ["Maksim Bobrin", "Ilya Zisman", "Alexander Nikulin", "Vladislav Kurenkov", "Dmitry Dylov"], "published_date": "2025-05-19", "title_zh": "行為基礎模型在未見動力學下的零樣本適應", "summary_zh": "行為基礎模型（BFM）無需訓練或微調就能零樣本執行各種任務。然而，基於後繼度量估計的BFM在動力學改變時表現不佳。本研究指出，一種名為Forward-Backward (FB)表示的BFM無法區分不同的動力學，導致策略混淆。為解決此問題，我們提出一種結合Transformer的信念估計器FB模型，顯著提升了零樣本適應能力。此外，將策略編碼空間劃分為特定動力學的群組，可以進一步提高性能。實驗證明，在動力學變化環境中，我們的模型在離散和連續任務上，零樣本回報比基線方法高達2倍。", "audio": "audios/2505.13150v1.mp3", "timestamp": "2025-05-20T13:31:50.537855"}
{"query": "Diffusion Model", "id": "2505.13280v1", "url": "http://arxiv.org/abs/2505.13280v1", "title": "FlowPure: Continuous Normalizing Flows for Adversarial Purification", "summary": "Despite significant advancements in the area, adversarial robustness remains\na critical challenge in systems employing machine learning models. The removal\nof adversarial perturbations at inference time, known as adversarial\npurification, has emerged as a promising defense strategy. To achieve this,\nstate-of-the-art methods leverage diffusion models that inject Gaussian noise\nduring a forward process to dilute adversarial perturbations, followed by a\ndenoising step to restore clean samples before classification. In this work, we\npropose FlowPure, a novel purification method based on Continuous Normalizing\nFlows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings\nfrom adversarial examples to their clean counterparts. Unlike prior\ndiffusion-based approaches that rely on fixed noise processes, FlowPure can\nleverage specific attack knowledge to improve robustness under known threats,\nwhile also supporting a more general stochastic variant trained on Gaussian\nperturbations for settings where such knowledge is unavailable. Experiments on\nCIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art\npurification-based defenses in preprocessor-blind and white-box scenarios, and\ncan do so while fully preserving benign accuracy in the former. Moreover, our\nresults show that not only is FlowPure a highly effective purifier but it also\nholds a strong potential for adversarial detection, identifying\npreprocessor-blind PGD samples with near-perfect accuracy.", "authors": ["Elias Collaert", "Abel Rodríguez", "Sander Joos", "Lieven Desmet", "Vera Rimmer"], "published_date": "2025-05-19", "title_zh": "FlowPure：使用連續歸一化流進行對抗性淨化", "summary_zh": "機器學習模型的對抗性魯棒性是個重要挑戰。FlowPure 是一種新的淨化方法，它利用連續歸一化流 (CNF) 來學習將對抗性樣本映射到乾淨樣本。與先前依賴固定噪音過程的擴散模型方法不同，FlowPure 能針對特定攻擊知識進行訓練，提升已知威脅下的魯棒性，也能在缺乏相關知識的情況下，使用基於高斯擾動的隨機變體。實驗表明，FlowPure 在 CIFAR-10 和 CIFAR-100 上的表現優於現有的淨化方法，且在預處理器盲測情境下能完全保持良性樣本的準確度。此外，FlowPure 在對抗性檢測方面也表現出色，幾乎能完美識別預處理器盲測的 PGD 樣本。", "audio": "audios/2505.13280v1.mp3", "timestamp": "2025-05-20T13:31:59.957771"}
{"query": "AI", "id": "2505.13329v1", "url": "http://arxiv.org/abs/2505.13329v1", "title": "Recommender Systems for Democracy: Toward Adversarial Robustness in Voting Advice Applications", "summary": "Voting advice applications (VAAs) help millions of voters understand which\npolitical parties or candidates best align with their views. This paper\nexplores the potential risks these applications pose to the democratic process\nwhen targeted by adversarial entities. In particular, we expose 11 manipulation\nstrategies and measure their impact using data from Switzerland's primary VAA,\nSmartvote, collected during the last two national elections. We find that\naltering application parameters, such as the matching method, can shift a\nparty's recommendation frequency by up to 105%. Cherry-picking questionnaire\nitems can increase party recommendation frequency by over 261%, while subtle\nchanges to parties' or candidates' responses can lead to a 248% increase. To\naddress these vulnerabilities, we propose adversarial robustness properties\nVAAs should satisfy, introduce empirical metrics for assessing the resilience\nof various matching methods, and suggest possible avenues for research toward\nmitigating the effect of manipulation. Our framework is key to ensuring secure\nand reliable AI-based VAAs poised to emerge in the near future.", "authors": ["Frédéric Berdoz", "Dustin Brunner", "Yann Vonlanthen", "Roger Wattenhofer"], "published_date": "2025-05-19", "title_zh": "民主推薦系統：邁向投票建議應用程式的對抗性穩健性", "summary_zh": "投票建議應用程式幫助選民了解哪些政黨或候選人最符合他們的觀點。這篇論文探討了這些應用程式在受到對抗性實體攻擊時，對民主進程構成的潛在風險。研究揭露了11種操控策略，發現更改應用程式參數、精心挑選問卷題目或修改政黨/候選人的回答，都可能顯著改變政黨的推薦頻率。為了應對這些漏洞，研究提出了投票建議應用程式應該滿足的對抗性穩健性屬性，引入了評估不同匹配方法韌性的指標，並建議了減輕操控影響的研究方向，旨在確保未來基於AI的投票建議應用程式的安全和可靠性。", "audio": "audios/2505.13329v1.mp3", "timestamp": "2025-05-20T14:18:35.743976"}
{"query": "Foundation Model", "id": "2505.13099v1", "url": "http://arxiv.org/abs/2505.13099v1", "title": "Industry-focused Synthetic Segmentation Pre-training", "summary": "Pre-training on real-image datasets has been widely proven effective for\nimproving instance segmentation. However, industrial applications face two key\nchallenges: (1) legal and ethical restrictions, such as ImageNet's prohibition\nof commercial use, and (2) limited transferability due to the domain gap\nbetween web images and industrial imagery. Even recent vision foundation\nmodels, including the segment anything model (SAM), show notable performance\ndegradation in industrial settings. These challenges raise critical questions:\nCan we build a vision foundation model for industrial applications without\nrelying on real images or manual annotations? And can such models outperform\neven fine-tuned SAM on industrial datasets? To address these questions, we\npropose the Instance Core Segmentation Dataset (InsCore), a synthetic\npre-training dataset based on formula-driven supervised learning (FDSL).\nInsCore generates fully annotated instance segmentation images that reflect key\ncharacteristics of industrial data, including complex occlusions, dense\nhierarchical masks, and diverse non-rigid shapes, distinct from typical web\nimagery. Unlike previous methods, InsCore requires neither real images nor\nhuman annotations. Experiments on five industrial datasets show that models\npre-trained with InsCore outperform those trained on COCO and ImageNet-21k, as\nwell as fine-tuned SAM, achieving an average improvement of 6.2 points in\ninstance segmentation performance. This result is achieved using only 100k\nsynthetic images, more than 100 times fewer than the 11 million images in SAM's\nSA-1B dataset, demonstrating the data efficiency of our approach. These\nfindings position InsCore as a practical and license-free vision foundation\nmodel for industrial applications.", "authors": ["Shinichi Mae", "Ryosuke Yamada", "Hirokatsu Kataoka"], "published_date": "2025-05-19", "title_zh": "針對產業的合成分割預訓練", "summary_zh": "現有的圖像分割預訓練模型常受限於授權問題及與工業圖像的領域差距。為此，我們提出一個名為InsCore的合成預訓練數據集，它基於公式驅動的監督學習，能生成反映工業數據特徵的完整標註分割圖像，例如複雜的遮擋、密集的分層遮罩和多樣化的非剛性形狀。實驗證明，使用InsCore預訓練的模型，在五個工業數據集上的分割表現優於在COCO和ImageNet-21k上訓練的模型，甚至超越微調後的SAM模型，平均提升了6.2個百分點。重點是，InsCore僅使用10萬張合成圖像，效率遠高於SAM的SA-1B數據集，為工業應用提供了一種實用且無授權限制的視覺基礎模型。", "audio": "audios/2505.13099v1.mp3", "timestamp": "2025-05-20T14:18:44.078291"}
{"query": "Diffusion Model", "id": "2505.13152v1", "url": "http://arxiv.org/abs/2505.13152v1", "title": "Higher fidelity perceptual image and video compression with a latent conditioned residual denoising diffusion model", "summary": "Denoising diffusion models achieved impressive results on several image\ngeneration tasks often outperforming GAN based models. Recently, the generative\ncapabilities of diffusion models have been employed for perceptual image\ncompression, such as in CDC. A major drawback of these diffusion-based methods\nis that, while producing impressive perceptual quality images they are dropping\nin fidelity/increasing the distortion to the original uncompressed images when\ncompared with other traditional or learned image compression schemes aiming for\nfidelity. In this paper, we propose a hybrid compression scheme optimized for\nperceptual quality, extending the approach of the CDC model with a decoder\nnetwork in order to reduce the impact on distortion metrics such as PSNR. After\nusing the decoder network to generate an initial image, optimized for\ndistortion, the latent conditioned diffusion model refines the reconstruction\nfor perceptual quality by predicting the residual. On standard benchmarks, we\nachieve up to +2dB PSNR fidelity improvements while maintaining comparable\nLPIPS and FID perceptual scores when compared with CDC. Additionally, the\napproach is easily extensible to video compression, where we achieve similar\nresults.", "authors": ["Jonas Brenig", "Radu Timofte"], "published_date": "2025-05-19", "title_zh": "使用潛在條件殘差去噪擴散模型實現更高保真度的感知圖像與影片壓縮", "summary_zh": "本研究提出一種混合壓縮方法，旨在提升感知圖像與影片壓縮的品質和保真度。利用解碼器網路產生初步重建圖像，優化失真度，然後使用潛在條件擴散模型預測殘差，進一步提升感知品質。實驗結果顯示，在保持感知品質指標（如LPIPS和FID）不變的前提下，PSNR可提升高達2dB，且該方法能輕鬆擴展至影片壓縮，並獲得相似的成果。", "audio": "audios/2505.13152v1.mp3", "timestamp": "2025-05-20T14:18:48.810505"}
{"query": "AI", "id": "2505.13324v1", "url": "http://arxiv.org/abs/2505.13324v1", "title": "From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable AI", "summary": "Counterfactuals play a pivotal role in the two distinct data science fields\nof causal inference (CI) and explainable artificial intelligence (XAI). While\nthe core idea behind counterfactuals remains the same in both fields--the\nexamination of what would have happened under different circumstances--there\nare key differences in how they are used and interpreted. We introduce a formal\ndefinition that encompasses the multi-faceted concept of the counterfactual in\nCI and XAI. We then discuss how counterfactuals are used, evaluated, generated,\nand operationalized in CI vs. XAI, highlighting conceptual and practical\ndifferences. By comparing and contrasting the two, we hope to identify\nopportunities for cross-fertilization across CI and XAI.", "authors": ["Galit Shmueli", "David Martens", "Jaewon Yoo", "Travis Greene"], "published_date": "2025-05-19", "title_zh": "從「如果...會怎樣」到洞見：因果推論與可解釋人工智慧中的反事實分析", "summary_zh": "反事實分析在因果推論和可解釋人工智慧這兩個領域都扮演關鍵角色。雖然核心概念都是探討在不同情況下會發生什麼，但它們的使用和解釋方式存在差異。這篇論文定義了一個涵蓋因果推論和可解釋人工智慧中反事實分析的多面向概念，並比較了它們在應用、評估、生成和實用化方面的不同，旨在促進兩個領域的互相借鑒。", "audio": "audios/2505.13324v1.mp3", "timestamp": "2025-05-20T15:20:23.282037"}
{"query": "Foundation Model", "id": "2505.12890v1", "url": "http://arxiv.org/abs/2505.12890v1", "title": "ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling", "summary": "The real-world complexity of surgeries necessitates surgeons to have deep and\nholistic comprehension to ensure precision, safety, and effective\ninterventions. Computational systems are required to have a similar level of\ncomprehension within the operating room. Prior works, limited to single-task\nefforts like phase recognition or scene graph generation, lack scope and\ngeneralizability. In this work, we introduce ORQA, a novel OR question\nanswering benchmark and foundational multimodal model to advance OR\nintelligence. By unifying all four public OR datasets into a comprehensive\nbenchmark, we enable our approach to concurrently address a diverse range of OR\nchallenges. The proposed multimodal large language model fuses diverse OR\nsignals such as visual, auditory, and structured data, for a holistic modeling\nof the OR. Finally, we propose a novel, progressive knowledge distillation\nparadigm, to generate a family of models optimized for different speed and\nmemory requirements. We show the strong performance of ORQA on our proposed\nbenchmark, and its zero-shot generalization, paving the way for scalable,\nunified OR modeling and significantly advancing multimodal surgical\nintelligence. We will release our code and data upon acceptance.", "authors": ["Ege Özsoy", "Chantal Pellegrini", "David Bani-Harouni", "Kun Yuan", "Matthias Keicher", "Nassir Navab"], "published_date": "2025-05-19", "title_zh": "ORQA：整體手術室建模的基準和基礎模型", "summary_zh": "為了讓電腦系統也能理解手術室的複雜性，如同外科醫生一般，我們推出了ORQA，一個全新的手術室問答基準和多模態基礎模型。ORQA整合了現有公開的手術室數據集，可以同時處理多樣化的手術室挑戰。我們提出的多模態大型語言模型結合了視覺、聽覺和結構化數據等各種手術室訊號，以實現對手術室的整體建模。此外，我們還提出了一種漸進式知識蒸餾方法，可以生成一系列針對不同速度和記憶體需求的模型。實驗結果顯示，ORQA在基準測試中表現出色，並具有零樣本泛化能力，為可擴展、統一的手術室建模奠定了基礎，並顯著推進了多模態手術智慧。", "audio": "audios/2505.12890v1.mp3", "timestamp": "2025-05-20T15:20:37.753880"}
{"query": "Diffusion Model", "id": "2505.13138v1", "url": "http://arxiv.org/abs/2505.13138v1", "title": "Neurosymbolic Diffusion Models", "summary": "Neurosymbolic (NeSy) predictors combine neural perception with symbolic\nreasoning to solve tasks like visual reasoning. However, standard NeSy\npredictors assume conditional independence between the symbols they extract,\nthus limiting their ability to model interactions and uncertainty - often\nleading to overconfident predictions and poor out-of-distribution\ngeneralisation. To overcome the limitations of the independence assumption, we\nintroduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy\npredictors that use discrete diffusion to model dependencies between symbols.\nOur approach reuses the independence assumption from NeSy predictors at each\nstep of the diffusion process, enabling scalable learning while capturing\nsymbol dependencies and uncertainty quantification. Across both synthetic and\nreal-world benchmarks - including high-dimensional visual path planning and\nrule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among\nNeSy predictors and demonstrate strong calibration.", "authors": ["Emile van Krieken", "Pasquale Minervini", "Edoardo Ponti", "Antonio Vergari"], "published_date": "2025-05-19", "title_zh": "神經符號擴散模型", "summary_zh": "傳統神經符號模型假設符號之間彼此獨立，導致無法有效模擬互動和不確定性。為了解決這個問題，我們提出了神經符號擴散模型（NeSyDMs），利用離散擴散過程來模擬符號之間的依賴關係。NeSyDMs在擴散的每一步驟中重用獨立性假設，實現可擴展的學習，同時捕捉符號之間的依賴關係和量化不確定性。在合成和真實世界的基準測試中，包括高維視覺路徑規劃和基於規則的自動駕駛，NeSyDMs在神經符號預測器中實現了最先進的準確性，並展現出強大的校準能力。", "audio": "audios/2505.13138v1.mp3", "timestamp": "2025-05-20T15:20:46.326880"}
{"query": "AI", "id": "2505.13315v1", "url": "http://arxiv.org/abs/2505.13315v1", "title": "KHRONOS: a Kernel-Based Neural Architecture for Rapid, Resource-Efficient Scientific Computation", "summary": "Contemporary models of high dimensional physical systems are constrained by\nthe curse of dimensionality and a reliance on dense data. We introduce KHRONOS\n(Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an\nAI framework for model based, model free and model inversion tasks. KHRONOS\nconstructs continuously differentiable target fields with a hierarchical\ncomposition of per-dimension kernel expansions, which are tensorized into modes\nand then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation\nbenchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L2 square\nerrors of 5e-4 down to 6e-10. This represents a 100 time gain over Kolmogorov\nArnold Networks (which itself reports a 100 times improvement on MLPs/PINNs\nwith 100 times fewer parameters) when controlling for the number of parameters.\nThis also represents a 1e4 times improvement in L2 square error compared to\nstandard linear FEM at comparable DoFs. Inference complexity is dominated by\ninner products, yielding sub-millisecond full-field predictions that scale to\nan arbitrary resolution. For inverse problems, KHRONOS facilitates rapid,\niterative level set recovery in only a few forward evaluations, with\nsub-microsecond per sample latency. KHRONOS scalability, expressivity, and\ninterpretability open new avenues in constrained edge computing, online\ncontrol, computer vision, and beyond.", "authors": ["Reza T. Batley", "Sourav Saha"], "published_date": "2025-05-19", "title_zh": "KHRONOS：一種基於核心的類神經網路架構，用於快速、資源高效的科學計算", "summary_zh": "KHRONOS (核心擴展層級化簡階、神經優化代理模型) 是一個 AI 框架，能處理基於模型、無模型和模型反演的任務。它利用分層式的單維核心擴展構建連續可微的目標場，並透過張量化和疊加來提升效率。在 Poisson 方程的基準測試中，KHRONOS 展現了極高的準確性和速度，在參數數量相當的情況下，相比其他方法有顯著優勢。此外，KHRONOS 還能快速解決反問題，具有良好的延展性和可解釋性，未來有望應用於邊緣計算、線上控制、電腦視覺等領域。", "audio": "audios/2505.13315v1.mp3", "timestamp": "2025-05-20T16:23:27.181018"}
{"query": "Foundation Model", "id": "2505.12738v1", "url": "http://arxiv.org/abs/2505.12738v1", "title": "EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting", "summary": "Advanced epidemic forecasting is critical for enabling precision containment\nstrategies, highlighting its strategic importance for public health security.\nWhile recent advances in Large Language Models (LLMs) have demonstrated\neffectiveness as foundation models for domain-specific tasks, their potential\nfor epidemic forecasting remains largely unexplored. In this paper, we\nintroduce EpiLLM, a novel LLM-based framework tailored for spatio-temporal\nepidemic forecasting. Considering the key factors in real-world epidemic\ntransmission: infection cases and human mobility, we introduce a dual-branch\narchitecture to achieve fine-grained token-level alignment between such complex\nepidemic patterns and language tokens for LLM adaptation. To unleash the\nmulti-step forecasting and generalization potential of LLM architectures, we\npropose an autoregressive modeling paradigm that reformulates the epidemic\nforecasting task into next-token prediction. To further enhance LLM perception\nof epidemics, we introduce spatio-temporal prompt learning techniques, which\nstrengthen forecasting capabilities from a data-driven perspective. Extensive\nexperiments show that EpiLLM significantly outperforms existing baselines on\nreal-world COVID-19 datasets and exhibits scaling behavior characteristic of\nLLMs.", "authors": ["Chenghua Gong", "Rui Sun", "Yuhao Zheng", "Juyuan Zhang", "Tianjun Gu", "Liming Pan", "Linyuan Lv"], "published_date": "2025-05-19", "title_zh": "EpiLLM：釋放大型語言模型在流行病預測中的潛力", "summary_zh": "EpiLLM 是一種基於大型語言模型的新框架，專為時空流行病預測量身定制。它考慮了感染病例和人口流動等關鍵因素，並利用自迴歸建模將預測任務轉化為下一代詞預測。此外，還引入了時空提示學習技術以加強模型對流行病的理解。實驗結果表明，EpiLLM 在 COVID-19 數據集上顯著優於現有方法，並展現了大型語言模型的擴展特性。", "audio": "audios/2505.12738v1.mp3", "timestamp": "2025-05-20T16:23:32.743890"}
{"query": "Diffusion Model", "id": "2505.13131v1", "url": "http://arxiv.org/abs/2505.13131v1", "title": "Constraint-Aware Diffusion Guidance for Robotics: Real-Time Obstacle Avoidance for Autonomous Racing", "summary": "Diffusion models hold great potential in robotics due to their ability to\ncapture complex, high-dimensional data distributions. However, their lack of\nconstraint-awareness limits their deployment in safety-critical applications.\nWe propose Constraint-Aware Diffusion Guidance (CoDiG), a data-efficient and\ngeneral-purpose framework that integrates barrier functions into the denoising\nprocess, guiding diffusion sampling toward constraint-satisfying outputs. CoDiG\nenables constraint satisfaction even with limited training data and generalizes\nacross tasks. We evaluate our framework in the challenging setting of miniature\nautonomous racing, where real-time obstacle avoidance is essential. Real-world\nexperiments show that CoDiG generates safe outputs efficiently under dynamic\nconditions, highlighting its potential for broader robotic applications. A\ndemonstration video is available at https://youtu.be/KNYsTdtdxOU.", "authors": ["Hao Ma", "Sabrina Bodmer", "Andrea Carron", "Melanie Zeilinger", "Michael Muehlebach"], "published_date": "2025-05-19", "title_zh": "機器人約束感知擴散引導：自主競速的即時避障", "summary_zh": "擴散模型在機器人領域潛力巨大，但缺乏約束感知能力。我們提出「約束感知擴散引導 (CoDiG)」，它將障礙函數整合到去噪過程中，引导擴散採樣生成滿足約束的輸出。CoDiG能在訓練數據有限的情況下满足约束，並且具有泛化能力。我們在微型自主競速中驗證了該框架，CoDiG能高效地產生安全輸出，展現其在更廣泛機器人應用中的潛力。", "audio": "audios/2505.13131v1.mp3", "timestamp": "2025-05-20T16:23:38.404568"}
{"query": "AI", "id": "2505.13302v1", "url": "http://arxiv.org/abs/2505.13302v1", "title": "I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models", "summary": "Large language models are increasingly integrated into news recommendation\nsystems, raising concerns about their role in spreading misinformation. In\nhumans, visual content is known to boost credibility and shareability of\ninformation, yet its effect on vision-language models (VLMs) remains unclear.\nWe present the first study examining how images influence VLMs' propensity to\nreshare news content, whether this effect varies across model families, and how\npersona conditioning and content attributes modulate this behavior. To support\nthis analysis, we introduce two methodological contributions: a\njailbreaking-inspired prompting strategy that elicits resharing decisions from\nVLMs while simulating users with antisocial traits and political alignments;\nand a multimodal dataset of fact-checked political news from PolitiFact, paired\nwith corresponding images and ground-truth veracity labels. Experiments across\nmodel families reveal that image presence increases resharing rates by 4.8% for\ntrue news and 15.0% for false news. Persona conditioning further modulates this\neffect: Dark Triad traits amplify resharing of false news, whereas\nRepublican-aligned profiles exhibit reduced veracity sensitivity. Of all the\ntested models, only Claude-3-Haiku demonstrates robustness to visual\nmisinformation. These findings highlight emerging risks in multimodal model\nbehavior and motivate the development of tailored evaluation frameworks and\nmitigation strategies for personalized AI systems. Code and dataset are\navailable at: https://github.com/3lis/misinfo_vlm", "authors": ["Alice Plebe", "Timothy Douglas", "Diana Riazi", "R. Maria del Rio-Chanona"], "published_date": "2025-05-19", "title_zh": "眼見為憑：圖像會增加視覺語言模型中錯誤資訊的傳播", "summary_zh": "大型語言模型越來越多地被整合到新聞推薦系統中，引發了人們對其在傳播錯誤資訊方面所扮演角色的擔憂。研究發現，圖像會顯著增加視覺語言模型轉發新聞的意願，尤其是假新聞，轉發率提高了15%。特定人格特徵，例如「黑暗三性格」，以及政治立場，也會影響模型的轉發行為。只有Claude-3-Haiku模型對視覺錯誤資訊表現出較強的抵抗力。這項研究揭示了多模態模型行為中潛在的風險，並強調需要針對個性化AI系統開發評估框架和緩解策略。", "audio": "audios/2505.13302v1.mp3", "timestamp": "2025-05-20T17:16:15.208753"}
{"query": "Foundation Model", "id": "2505.12684v1", "url": "http://arxiv.org/abs/2505.12684v1", "title": "Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement", "summary": "Recent advances in graph machine learning have shifted to data-centric\nparadigms, driven by two emerging fields: (1) Federated graph learning (FGL)\nenables multi-client collaboration but faces challenges from data and task\nheterogeneity, limiting its practicality; (2) Graph foundation models (GFM)\noffer strong domain generalization but are usually trained on single machines,\nmissing out on cross-silo data and resources.\n  These paradigms are complementary, and their integration brings notable\nbenefits. Motivated by this, we propose FedGFM, a novel decentralized GFM\ntraining paradigm. However, a key challenge is knowledge entanglement, where\nmulti-domain knowledge merges into indistinguishable representations, hindering\ndownstream adaptation.\n  To address this, we present FedGFM+, an enhanced framework with two core\nmodules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based\ndomain-aware initialization strategy. Before pre-training, each client encodes\nits local graph into domain-specific prototypes that serve as semantic anchors.\nSynthetic embeddings around these anchors initialize the global model. We\ntheoretically prove these prototypes are distinguishable across domains,\nproviding a strong inductive bias to disentangle domain-specific knowledge. (2)\nAdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a\nlightweight graph prompt capturing domain semantics during pre-training. During\nfine-tuning, prompts from all clients form a pool from which the GFM selects\nrelevant prompts to augment target graph attributes, improving downstream\nadaptation.\n  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and\ntasks, outperforming 20 baselines from supervised learning, FGL, and federated\nGFM variants.", "authors": ["Yinlin Zhu", "Xunkai Li", "Jishuo Jia", "Miao Hu", "Di Wu", "Meikang Qiu"], "published_date": "2025-05-19", "title_zh": "邁向高效能聯邦圖基礎模型：透過降低知識糾纏", "summary_zh": "現今圖機器學習趨勢轉向以資料為中心，聯邦圖學習(FGL)和圖基礎模型(GFM)是兩個重要領域。FGL雖能促進多方協作，但受限於資料和任務異質性；GFM雖具備強大的領域泛化能力，卻常在單機上訓練，錯失跨機構的資料和資源。因此，我們提出FedGFM，一種去中心化的GFM訓練方法。然而，知識糾纏是主要挑戰，它會讓多領域知識混合成無法區分的表示，阻礙下游適應。為了解決此問題，我們提出FedGFM+，透過AncDAI（錨點式領域感知初始化）和AdaDPP（自適應領域敏感提示池）兩個核心模組來降低知識糾纏。AncDAI在預訓練前，將本地圖編碼成領域特定的原型作為語義錨點，並以此初始化全域模型，提供領域知識解耦的強烈歸納偏置。AdaDPP讓每個客戶端學習捕捉領域語義的輕量級圖提示，在微調時，將所有客戶端的提示形成提示池，GFM從中選擇相關提示來增強目標圖屬性，提升下游適應性。實驗證明，FedGFM+在多個領域和任務的八個基準測試中，優於20個基線模型。", "audio": "audios/2505.12684v1.mp3", "timestamp": "2025-05-20T17:16:25.053705"}
{"query": "Diffusion Model", "id": "2505.13091v1", "url": "http://arxiv.org/abs/2505.13091v1", "title": "Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction", "summary": "Diffusion models have made breakthroughs in 3D generation tasks. Current 3D\ndiffusion models focus on reconstructing target shape from images or a set of\npartial observations. While excelling in global context understanding, they\nstruggle to capture the local details of complex shapes and limited to the\nocclusion and lighting conditions. To overcome these limitations, we utilize\ntactile images to capture the local 3D information and propose a Touch2Shape\nmodel, which leverages a touch-conditioned diffusion model to explore and\nreconstruct the target shape from touch. For shape reconstruction, we have\ndeveloped a touch embedding module to condition the diffusion model in creating\na compact representation and a touch shape fusion module to refine the\nreconstructed shape. For shape exploration, we combine the diffusion model with\nreinforcement learning to train a policy. This involves using the generated\nlatent vector from the diffusion model to guide the touch exploration policy\ntraining through a novel reward design. Experiments validate the reconstruction\nquality thorough both qualitatively and quantitative analysis, and our touch\nexploration policy further boosts reconstruction performance.", "authors": ["Yuanbo Wang", "Zhaoxuan Zhang", "Jiajin Qiu", "Dilong Sun", "Zhengyu Meng", "Xiaopeng Wei", "Xin Yang"], "published_date": "2025-05-19", "title_zh": "Touch2Shape：觸摸條件下的3D擴散模型，用於形狀探索與重建", "summary_zh": "3D擴散模型在形狀生成上表現亮眼，但對複雜形狀的局部細節捕捉能力有限。本論文提出 Touch2Shape 模型，利用觸覺影像捕捉局部3D資訊，並結合觸摸條件的擴散模型來探索和重建目標形狀。模型包含觸摸嵌入模組，產生精簡表示，以及觸摸形狀融合模組，優化重建效果。此外，結合擴散模型與強化學習，訓練觸摸探索策略，進一步提升重建效能。實驗證明此方法能有效重建形狀，並且觸摸探索策略可以改善重建結果。", "audio": "audios/2505.13091v1.mp3", "timestamp": "2025-05-20T17:16:30.788447"}
{"query": "AI", "id": "2505.13292v1", "url": "http://arxiv.org/abs/2505.13292v1", "title": "Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms of AI Systems by Integrating Federated Learning and LLMs", "summary": "In the age of cloud computing, data privacy protection has become a major\nchallenge, especially when sharing sensitive data across cloud environments.\nHowever, how to optimize collaboration across cloud environments remains an\nunresolved problem. In this paper, we combine federated learning with\nlarge-scale language models to optimize the collaborative mechanism of AI\nsystems. Based on the existing federated learning framework, we introduce a\ncross-cloud architecture in which federated learning works by aggregating model\nupdates from decentralized nodes without exposing the original data. At the\nsame time, combined with large-scale language models, its powerful context and\nsemantic understanding capabilities are used to improve model training\nefficiency and decision-making ability. We've further innovated by introducing\na secure communication layer to ensure the privacy and integrity of model\nupdates and training data. The model enables continuous model adaptation and\nfine-tuning across different cloud environments while protecting sensitive\ndata. Experimental results show that the proposed method is significantly\nbetter than the traditional federated learning model in terms of accuracy,\nconvergence speed and data privacy protection.", "authors": ["Huaiying Luo", "Cheng Ji"], "published_date": "2025-05-19", "title_zh": "跨雲端資料隱私保護：整合聯邦學習與大型語言模型優化AI系統的協作機制", "summary_zh": "本研究探討在雲端運算時代，跨雲端共享敏感資料時的資料隱私保護挑戰。我們結合聯邦學習和大型語言模型，優化AI系統的協作機制。透過跨雲端架構，聯邦學習可在不洩露原始資料的情況下匯總模型更新。同時，利用大型語言模型的強大語義理解能力，提升模型訓練效率和決策能力。此外，引入安全通訊層確保模型更新和訓練資料的隱私和完整性。實驗結果顯示，相較於傳統聯邦學習模型，本方法在準確度、收斂速度和資料隱私保護方面有顯著提升。", "audio": "audios/2505.13292v1.mp3", "timestamp": "2025-05-20T18:26:43.658852"}
{"query": "Foundation Model", "id": "2505.12638v1", "url": "http://arxiv.org/abs/2505.12638v1", "title": "ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data", "summary": "The advent of single-cell Assay for Transposase-Accessible Chromatin using\nsequencing (scATAC-seq) offers an innovative perspective for deciphering\nregulatory mechanisms by assembling a vast repository of single-cell chromatin\naccessibility data. While foundation models have achieved significant success\nin single-cell transcriptomics, there is currently no foundation model for\nscATAC-seq that supports zero-shot high-quality cell identification and\ncomprehensive multi-omics analysis simultaneously. Key challenges lie in the\nhigh dimensionality and sparsity of scATAC-seq data, as well as the lack of a\nstandardized schema for representing open chromatin regions (OCRs). Here, we\npresent \\textbf{ChromFound}, a foundation model tailored for scATAC-seq.\nChromFound utilizes a hybrid architecture and genome-aware tokenization to\neffectively capture genome-wide long contexts and regulatory signals from\ndynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues\nand 6 disease conditions, ChromFound demonstrates broad applicability across 6\ndiverse tasks. Notably, it achieves robust zero-shot performance in generating\nuniversal cell representations and exhibits excellent transferability in cell\ntype annotation and cross-omics prediction. By uncovering enhancer-gene links\nundetected by existing computational methods, ChromFound offers a promising\nframework for understanding disease risk variants in the noncoding genome.", "authors": ["Yifeng Jiao", "Yuchen Liu", "Yu Zhang", "Xin Guo", "Yushuai Wu", "Chen Jiang", "Jiyang Li", "Hongwei Zhang", "Limei Han", "Xin Gao", "Yuan Qi", "Yuan Cheng"], "published_date": "2025-05-19", "title_zh": "ChromFound：邁向單細胞染色質可及性數據的通用基礎模型", "summary_zh": "隨著單細胞ATAC-seq技術的發展，我們得以以前所未有的視角解析調控機制。然而，雖然基礎模型在單細胞轉錄組學上取得了巨大成功，但在單細胞染色質可及性數據方面，卻缺乏一個能同時支持零樣本高質量細胞識別和全面多組學分析的基礎模型。為了解決這個問題，我們開發了ChromFound，一個專為單細胞ATAC-seq設計的基礎模型。它通過混合架構和基因組感知的Tokenization技術，有效地捕捉了全基因組的長程上下文和來自動態染色質環境的調控信號。ChromFound預訓練了來自30個組織和6種疾病條件的197萬個細胞，展示了廣泛的適用性，並在多項任務中表現出色，特別是在零樣本細胞表示生成和跨組學預測方面。ChromFound還有望幫助我們理解非編碼基因組中的疾病風險變異。", "audio": "audios/2505.12638v1.mp3", "timestamp": "2025-05-20T18:26:52.420514"}
{"query": "Diffusion Model", "id": "2505.13023v1", "url": "http://arxiv.org/abs/2505.13023v1", "title": "Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions", "summary": "As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models.", "authors": ["Yimao Guo", "Zuomin Qu", "Wei Lu", "Xiangyang Luo"], "published_date": "2025-05-19", "title_zh": "反填補：針對未知條件下基於惡意擴散模型的影像填補器的預防性防禦", "summary_zh": "基於擴散模型的惡意影像篡改日益普遍，針對此問題，我們提出「反填補」這種預防性防禦機制。它透過多層級特徵提取、多尺度語義保留的資料擴增，以及基於選擇的分布偏差優化策略，在未知條件下也能有效地保護影像，抵禦惡意影像填補，並在多項實驗中證明了其效能和魯棒性。", "audio": "audios/2505.13023v1.mp3", "timestamp": "2025-05-20T18:27:07.718187"}
{"query": "AI", "id": "2505.13259v1", "url": "http://arxiv.org/abs/2505.13259v1", "title": "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery", "summary": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.", "authors": ["Tianshi Zheng", "Zheye Deng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Zihao Wang", "Yangqiu Song"], "published_date": "2025-05-19", "title_zh": "從自動化到自主化：大型語言模型在科學發現中的綜述", "summary_zh": "大型語言模型正在徹底改變科學研究。它們不再只是自動化工具，而是逐漸變成具有自主性的智能體，重塑研究流程和人機協作模式。本綜述系統性地探討了這個新興領域，重點關注大型語言模型在科學領域中不斷變化的角色和日益提升的能力。我們從科學方法出發，提出了工具、分析師和科學家三個層級的分類，來描述模型自主性的演進。此外，我們也指出了機器人自動化、自我改進和倫理治理等關鍵挑戰和未來研究方向。總之，本綜述提供了一個概念框架和策略遠見，旨在引導和塑造AI驅動的科學發現的未來，促進快速創新和負責任的發展。", "audio": "audios/2505.13259v1.mp3", "timestamp": "2025-05-20T19:14:36.941246"}
{"query": "Foundation Model", "id": "2505.12583v1", "url": "http://arxiv.org/abs/2505.12583v1", "title": "A Comprehensive Survey on Physical Risk Control in the Era of Foundation Model-enabled Robotics", "summary": "Recent Foundation Model-enabled robotics (FMRs) display greatly improved\ngeneral-purpose skills, enabling more adaptable automation than conventional\nrobotics. Their ability to handle diverse tasks thus creates new opportunities\nto replace human labor. However, unlike general foundation models, FMRs\ninteract with the physical world, where their actions directly affect the\nsafety of humans and surrounding objects, requiring careful deployment and\ncontrol. Based on this proposition, our survey comprehensively summarizes robot\ncontrol approaches to mitigate physical risks by covering all the lifespan of\nFMRs ranging from pre-deployment to post-accident stage. Specifically, we\nbroadly divide the timeline into the following three phases: (1) pre-deployment\nphase, (2) pre-incident phase, and (3) post-incident phase. Throughout this\nsurvey, we find that there is much room to study (i) pre-incident risk\nmitigation strategies, (ii) research that assumes physical interaction with\nhumans, and (iii) essential issues of foundation models themselves. We hope\nthat this survey will be a milestone in providing a high-resolution analysis of\nthe physical risks of FMRs and their control, contributing to the realization\nof a good human-robot relationship.", "authors": ["Takeshi Kojima", "Yaonan Zhu", "Yusuke Iwasawa", "Toshinori Kitamura", "Gang Yan", "Shu Morikuni", "Ryosuke Takanami", "Alfredo Solano", "Tatsuya Matsushima", "Akiko Murakami", "Yutaka Matsuo"], "published_date": "2025-05-19", "title_zh": "基於基礎模型的機器人時代物理風險控制全面綜述", "summary_zh": "近年來，基於基礎模型的機器人展現出更強的通用能力，使得機器人能更靈活地自動化。然而，與一般基礎模型不同，它們會與物理世界互動，其行為直接影響人類和周遭物體的安全，需要仔細部署和控制。本綜述全面總結了機器人控制方法，以減輕物理風險，涵蓋從部署前到事故後的整個生命週期，並將時間線分為部署前、事故前和事故後三個階段。研究發現，事故前的風險緩解策略、假設與人類進行物理互動的研究以及基礎模型本身的基本問題，都還有很大的研究空間。希望本綜述能為分析基於基礎模型的機器人的物理風險及其控制提供高解析度的分析，從而有助於實現良好的人機關係。", "audio": "audios/2505.12583v1.mp3", "timestamp": "2025-05-20T19:14:49.584037"}
{"query": "Diffusion Model", "id": "2505.12935v1", "url": "http://arxiv.org/abs/2505.12935v1", "title": "LatentINDIGO: An INN-Guided Latent Diffusion Algorithm for Image Restoration", "summary": "There is a growing interest in the use of latent diffusion models (LDMs) for\nimage restoration (IR) tasks due to their ability to model effectively the\ndistribution of natural images. While significant progress has been made, there\nare still key challenges that need to be addressed. First, many approaches\ndepend on a predefined degradation operator, making them ill-suited for complex\nor unknown degradations that deviate from standard analytical models. Second,\nmany methods struggle to provide a stable guidance in the latent space and\nfinally most methods convert latent representations back to the pixel domain\nfor guidance at every sampling iteration, which significantly increases\ncomputational and memory overhead. To overcome these limitations, we introduce\na wavelet-inspired invertible neural network (INN) that simulates degradations\nthrough a forward transform and reconstructs lost details via the inverse\ntransform. We further integrate this design into a latent diffusion pipeline\nthrough two proposed approaches: LatentINDIGO-PixelINN, which operates in the\npixel domain, and LatentINDIGO-LatentINN, which stays fully in the latent space\nto reduce complexity. Both approaches alternate between updating intermediate\nlatent variables under the guidance of our INN and refining the INN forward\nmodel to handle unknown degradations. In addition, a regularization step\npreserves the proximity of latent variables to the natural image manifold.\nExperiments demonstrate that our algorithm achieves state-of-the-art\nperformance on synthetic and real-world low-quality images, and can be readily\nadapted to arbitrary output sizes.", "authors": ["Di You", "Daniel Siromani", "Pier Luigi Dragotti"], "published_date": "2025-05-19", "title_zh": "潛在INDIGO：一種用於影像修復的INN引導潛在擴散演算法", "summary_zh": "潛在擴散模型在影像修復領域越來越受歡迎，但現有方法在處理複雜或未知降質、提供穩定潛在空間引導，以及計算效率等方面仍存在挑戰。本文提出一種名為LatentINDIGO的演算法，它使用波小波啟發的可逆神經網路（INN）來模擬降質過程，並通過逆變換重建丟失的細節。該演算法有兩個版本：PixelINN版本在像素域操作，LatentINN版本則完全在潛在空間中操作，以減少複雜度。這兩種方法交替更新潛在變量和精煉INN模型，並通過正則化步驟確保潛在變量接近自然圖像流形。實驗結果表明，該演算法在合成和真實低質量圖像上均取得了最先進的性能，並且可以輕鬆適應任意輸出尺寸。", "audio": "audios/2505.12935v1.mp3", "timestamp": "2025-05-20T19:14:58.408447"}
{"query": "AI", "id": "2505.13246v1", "url": "http://arxiv.org/abs/2505.13246v1", "title": "Agentic Publications: An LLM-Driven Framework for Interactive Scientific Publishing, Supplementing Traditional Papers with AI-Powered Knowledge Systems", "summary": "The exponential growth of scientific literature presents significant\nchallenges for researchers navigating the complex knowledge landscape. We\npropose \"Agentic Publications\", a novel LLM-driven framework complementing\ntraditional publishing by transforming papers into interactive knowledge\nsystems. Our architecture integrates structured data with unstructured content\nthrough retrieval-augmented generation and multi-agent verification. The\nframework offers interfaces for both humans and machines, combining narrative\nexplanations with machine-readable outputs while addressing ethical\nconsiderations through automated validation and transparent governance. Key\nfeatures include continuous knowledge updates, automatic integration of new\nfindings, and customizable detail levels. Our proof-of-concept demonstrates\nmultilingual interaction, API accessibility, and structured knowledge\nrepresentation through vector databases, knowledge graphs, and verification\nagents. This approach enhances scientific communication across disciplines,\nimproving efficiency and collaboration while preserving traditional publishing\npathways, particularly valuable for interdisciplinary fields where knowledge\nintegration remains challenging.", "authors": ["Roberto Pugliese", "George Kourousias", "Francesco Venier", "Grazia Garlatti Costa"], "published_date": "2025-05-19", "title_zh": "具代理能力的出版品：一個由大型語言模型驅動的互動式科學出版框架，透過 AI 驅動的知識系統來補充傳統論文", "summary_zh": "科學文獻爆炸性成長，研究人員難以掌握。本研究提出「具代理能力的出版品」框架，利用大型語言模型將傳統論文轉化為互動式知識系統，結合結構化和非結構化數據，並透過多重代理驗證確保準確性。此框架提供人機介面，具備知識持續更新、自動整合新發現等功能。此方法透過提升跨領域的科學交流效率和協作，並保留傳統出版途徑，尤其對於知識整合困難的跨領域研究而言，更具價值。", "audio": "audios/2505.13246v1.mp3", "timestamp": "2025-05-20T20:20:44.106068"}
{"query": "Foundation Model", "id": "2505.12534v1", "url": "http://arxiv.org/abs/2505.12534v1", "title": "ChemPile: A 250GB Diverse and Curated Dataset for Chemical Foundation Models", "summary": "Foundation models have shown remarkable success across scientific domains,\nyet their impact in chemistry remains limited due to the absence of diverse,\nlarge-scale, high-quality datasets that reflect the field's multifaceted\nnature. We present the ChemPile, an open dataset containing over 75 billion\ntokens of curated chemical data, specifically built for training and evaluating\ngeneral-purpose models in the chemical sciences. The dataset mirrors the human\nlearning journey through chemistry -- from educational foundations to\nspecialized expertise -- spanning multiple modalities and content types\nincluding structured data in diverse chemical representations (SMILES, SELFIES,\nIUPAC names, InChI, molecular renderings), scientific and educational text,\nexecutable code, and chemical images. ChemPile integrates foundational\nknowledge (textbooks, lecture notes), specialized expertise (scientific\narticles and language-interfaced data), visual understanding (molecular\nstructures, diagrams), and advanced reasoning (problem-solving traces and code)\n-- mirroring how human chemists develop expertise through diverse learning\nmaterials and experiences. Constructed through hundreds of hours of expert\ncuration, the ChemPile captures both foundational concepts and domain-specific\ncomplexity. We provide standardized training, validation, and test splits,\nenabling robust benchmarking. ChemPile is openly released via HuggingFace with\na consistent API, permissive license, and detailed documentation. We hope the\nChemPile will serve as a catalyst for chemical AI, enabling the development of\nthe next generation of chemical foundation models.", "authors": ["Adrian Mirza", "Nawaf Alampara", "Martiño Ríos-García", "Mohamed Abdelalim", "Jack Butler", "Bethany Connolly", "Tunca Dogan", "Marianna Nezhurina", "Bünyamin Şen", "Santosh Tirunagari", "Mark Worrall", "Adamo Young", "Philippe Schwaller", "Michael Pieler", "Kevin Maik Jablonka"], "published_date": "2025-05-18", "title_zh": "ChemPile：一個250GB的多樣化且精心策劃的化學基礎模型數據集", "summary_zh": "ChemPile是一個開放的250GB化學數據集，包含超過750億個tokens，專為訓練和評估化學領域的通用模型而設計。它涵蓋結構化數據、文本、程式碼和圖像等多種形式，模擬人類學習化學的過程，從基礎知識到專業知識，致力於推動化學人工智慧的發展，並助力新一代化學基礎模型的誕生。", "audio": "audios/2505.12534v1.mp3", "timestamp": "2025-05-20T20:20:49.050176"}
{"query": "Diffusion Model", "id": "2505.12882v1", "url": "http://arxiv.org/abs/2505.12882v1", "title": "PhyDA: Physics-Guided Diffusion Models for Data Assimilation in Atmospheric Systems", "summary": "Data Assimilation (DA) plays a critical role in atmospheric science by\nreconstructing spatially continous estimates of the system state, which serves\nas initial conditions for scientific analysis. While recent advances in\ndiffusion models have shown great potential for DA tasks, most existing\napproaches remain purely data-driven and often overlook the physical laws that\ngovern complex atmospheric dynamics. As a result, they may yield physically\ninconsistent reconstructions that impair downstream applications. To overcome\nthis limitation, we propose PhyDA, a physics-guided diffusion framework\ndesigned to ensure physical coherence in atmospheric data assimilation. PhyDA\nintroduces two key components: (1) a Physically Regularized Diffusion Objective\nthat integrates physical constraints into the training process by penalizing\ndeviations from known physical laws expressed as partial differential\nequations, and (2) a Virtual Reconstruction Encoder that bridges observational\nsparsity for structured latent representations, further enhancing the model's\nability to infer complete and physically coherent states. Experiments on the\nERA5 reanalysis dataset demonstrate that PhyDA achieves superior accuracy and\nbetter physical plausibility compared to state-of-the-art baselines. Our\nresults emphasize the importance of combining generative modeling with\ndomain-specific physical knowledge and show that PhyDA offers a promising\ndirection for improving real-world data assimilation systems.", "authors": ["Hao Wang", "Jindong Han", "Wei Fan", "Weijia Zhang", "Hao Liu"], "published_date": "2025-05-19", "title_zh": "PhyDA：物理引導的擴散模型用於大氣系統中的資料同化", "summary_zh": "PhyDA是一個新型的大氣資料同化框架，它利用物理定律引導擴散模型，確保重建的大氣狀態不僅準確，而且符合物理規律。它透過將物理約束納入訓練目標，並使用編碼器來處理觀測資料的稀疏性，從而優於傳統方法，更適用於實際應用。", "audio": "audios/2505.12882v1.mp3", "timestamp": "2025-05-20T20:20:53.711520"}
{"query": "AI", "id": "2505.14680v1", "url": "http://arxiv.org/abs/2505.14680v1", "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search", "summary": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.", "authors": ["Sunhao Dai", "Wenjie Wang", "Liang Pang", "Jun Xu", "See-Kiong Ng", "Ji-Rong Wen", "Tat-Seng Chua"], "published_date": "2025-05-20", "title_zh": "NExT-Search：重建生成式AI搜尋的使用者回饋生態系統", "summary_zh": "生成式AI搜尋雖然方便，但打破了傳統搜尋仰賴使用者回饋不斷改進的機制。傳統搜尋可以透過使用者點擊、停留時間等精細回饋來優化排序模型。而生成式AI搜尋流程更長，使用者僅對最終答案提供粗略回饋，難以追溯問題源頭，導致系統難以改進。本研究提出NExT-Search，透過「使用者除錯模式」讓使用者介入關鍵步驟，並利用「影子使用者模式」模擬使用者偏好提供AI輔助回饋，重新引入精細的流程級回饋。這些回饋將用於線上即時調整搜尋結果，以及離線微調查詢分解、檢索和生成模型，最終打造能夠持續根據使用者回饋進化的AI搜尋系統。", "audio": "audios/2505.14680v1.mp3", "timestamp": "2025-05-21T03:11:29.471335"}
{"query": "Foundation Model", "id": "2505.14683v1", "url": "http://arxiv.org/abs/2505.14683v1", "title": "Emerging Properties in Unified Multimodal Pretraining", "summary": "Unifying multimodal understanding and generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0source foundational model that natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\non trillions of tokens curated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such as free-form image manipulation,\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/", "authors": ["Chaorui Deng", "Deyao Zhu", "Kunchang Li", "Chenhui Gou", "Feng Li", "Zeyu Wang", "Shu Zhong", "Weihao Yu", "Xiaonan Nie", "Ziang Song", "Guang Shi", "Haoqi Fan"], "published_date": "2025-05-20", "title_zh": "統一多模態預訓練中湧現的特性", "summary_zh": "本研究介紹了開放原始碼的多模態基礎模型 BAGEL，它能同時理解和生成多模態內容。BAGEL 基於大量的文字、圖片、影片和網路數據進行預訓練，展現了在複雜多模態推理方面的能力。在多模態生成和理解方面，BAGEL 的表現明顯優於其他開放原始碼的統一模型，並且具備進階的多模態推理能力，例如自由形式的圖像操作、未來幀預測、3D 操作和世界導航。研究團隊分享了重要的發現、預訓練細節、數據創建協議，並公開了程式碼和模型權重，希望能促進多模態研究的發展。", "audio": "audios/2505.14683v1.mp3", "timestamp": "2025-05-21T03:11:36.102262"}
{"query": "Diffusion Model", "id": "2505.14673v1", "url": "http://arxiv.org/abs/2505.14673v1", "title": "Training-Free Watermarking for Autoregressive Image Generation", "summary": "Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression.", "authors": ["Yu Tong", "Zihao Pan", "Shuai Yang", "Kaiyang Zhou"], "published_date": "2025-05-20", "title_zh": "無需訓練的自迴歸圖像生成浮水印", "summary_zh": "一種為自迴歸圖像生成模型設計的，無需訓練的浮水印框架IndexMark。它利用碼本的冗餘特性，將自迴歸生成的索引替換為視覺上相似的索引，以嵌入肉眼難以察覺的浮水印，且不影響圖像質量。透過計算生成圖像中浮水印標記的比例來驗證浮水印，並使用索引編碼器進一步提高精度。實驗表明，IndexMark在圖像質量和驗證準確性方面都表現出色，並且對各種攻擊具有魯棒性，例如裁剪、噪聲、模糊等等。", "audio": "audios/2505.14673v1.mp3", "timestamp": "2025-05-21T03:11:41.526487"}
{"query": "AI", "id": "2505.14677v1", "url": "http://arxiv.org/abs/2505.14677v1", "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning", "summary": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.", "authors": ["Jiaer Xia", "Yuhang Zang", "Peng Gao", "Yixuan Li", "Kaiyang Zhou"], "published_date": "2025-05-20", "title_zh": "Visionary-R1：利用強化學習減輕視覺推理中的捷徑", "summary_zh": "大型語言模型(LLM)利用強化學習在推理方面取得進展。本研究旨在透過強化學習訓練視覺語言模型(VLM)進行圖像推理，無需逐步思考(CoT)的監督。研究發現，直接應用強化學習於VLM可能會因簡單問題而產生捷徑，降低其泛化能力。為解決此問題，本研究提出先生成圖像的詳細描述，再進行推理的Caption-Reason-Answer方法。訓練模型Visionary-R1後，其在多個視覺推理基準測試中超越了GPT-4o等強大的多模態模型。", "audio": "audios/2505.14677v1.mp3", "timestamp": "2025-05-21T04:22:43.916166"}
{"query": "Foundation Model", "id": "2505.14648v1", "url": "http://arxiv.org/abs/2505.14648v1", "title": "Vox-Profile: A Speech Foundation Model Benchmark for Characterizing Diverse Speaker and Speech Traits", "summary": "We introduce Vox-Profile, a comprehensive benchmark to characterize rich\nspeaker and speech traits using speech foundation models. Unlike existing works\nthat focus on a single dimension of speaker traits, Vox-Profile provides\nholistic and multi-dimensional profiles that reflect both static speaker traits\n(e.g., age, sex, accent) and dynamic speech properties (e.g., emotion, speech\nflow). This benchmark is grounded in speech science and linguistics, developed\nwith domain experts to accurately index speaker and speech characteristics. We\nreport benchmark experiments using over 15 publicly available speech datasets\nand several widely used speech foundation models that target various static and\ndynamic speaker and speech properties. In addition to benchmark experiments, we\nshowcase several downstream applications supported by Vox-Profile. First, we\nshow that Vox-Profile can augment existing speech recognition datasets to\nanalyze ASR performance variability. Vox-Profile is also used as a tool to\nevaluate the performance of speech generation systems. Finally, we assess the\nquality of our automated profiles through comparison with human evaluation and\nshow convergent validity. Vox-Profile is publicly available at:\nhttps://github.com/tiantiaf0627/vox-profile-release.", "authors": ["Tiantian Feng", "Jihwan Lee", "Anfeng Xu", "Yoonjeong Lee", "Thanathai Lertpetchpun", "Xuan Shi", "Helin Wang", "Thomas Thebaud", "Laureano Moro-Velazquez", "Dani Byrd", "Najim Dehak", "Shrikanth Narayanan"], "published_date": "2025-05-20", "title_zh": "Vox-Profile: 一個用於表徵多樣化說話者和語音特徵的語音基礎模型基準", "summary_zh": "Vox-Profile 是一個全面的基準測試，旨在利用語音基礎模型來分析說話者和語音的豐富特徵。它不僅關注說話者的年齡、性別、口音等靜態特徵，還包含情緒、語速等動態語音屬性。該基準基於語音科學和語言學，由領域專家開發，能準確地索引說話者和語音的特徵。研究者使用超過15個公開語音數據集和多個主流語音基礎模型進行了基準測試，並展示了Vox-Profile在增強語音識別數據集、評估語音生成系統和驗證自動分析結果等方面的應用。Vox-Profile程式碼已公開。", "audio": "audios/2505.14648v1.mp3", "timestamp": "2025-05-21T04:22:49.028751"}
{"query": "Diffusion Model", "id": "2505.14556v1", "url": "http://arxiv.org/abs/2505.14556v1", "title": "Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI", "summary": "Brain-to-image decoding has been recently propelled by the progress in\ngenerative AI models and the availability of large ultra-high field functional\nMagnetic Resonance Imaging (fMRI). However, current approaches depend on\ncomplicated multi-stage pipelines and preprocessing steps that typically\ncollapse the temporal dimension of brain recordings, thereby limiting\ntime-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural\nActivity Diffusion for Image Reconstruction), a new single-stage diffusion\nmodel designed for reconstructing images from dynamically evolving fMRI\nrecordings. Our approach offers three main contributions. First, Dynadiff\nsimplifies training as compared to existing approaches. Second, our model\noutperforms state-of-the-art models on time-resolved fMRI signals, especially\non high-level semantic image reconstruction metrics, while remaining\ncompetitive on preprocessed fMRI data that collapse time. Third, this approach\nallows a precise characterization of the evolution of image representations in\nbrain activity. Overall, this work lays the foundation for time-resolved\nbrain-to-image decoding.", "authors": ["Marlène Careil", "Yohann Benchetrit", "Jean-Rémi King"], "published_date": "2025-05-20", "title_zh": "Dynadiff: 從持續演進的fMRI數據單階段解碼圖像", "summary_zh": "近年來，腦部到圖像的解碼技術，受益於生成式AI和高場強功能性磁振造影（fMRI）的發展。然而，現有方法依賴複雜的多階段流程，並通常會壓縮腦部記錄的時間維度，限制了時間分辨的腦部解碼器。我們提出Dynadiff，一種新的單階段擴散模型，旨在從動態演進的fMRI記錄中重建圖像。Dynadiff簡化了訓練流程，在時間分辨的fMRI訊號上優於現有模型，特別是在高階語義圖像重建指標上，同時在預處理過的、時間維度已壓縮的fMRI數據上仍具競爭力。此外，它能精確描述腦部活動中圖像表徵的演進過程。這項研究為時間分辨的腦部到圖像解碼奠定了基礎。", "audio": "audios/2505.14556v1.mp3", "timestamp": "2025-05-21T04:22:55.811957"}
{"query": "AI", "id": "2505.14675v1", "url": "http://arxiv.org/abs/2505.14675v1", "title": "Semi-parametric efficient estimation of small genetic effects in large-scale population cohorts", "summary": "Population genetics seeks to quantify DNA variant associations with traits or\ndiseases, as well as interactions among variants and with environmental\nfactors. Computing millions of estimates in large cohorts in which small effect\nsizes are expected, necessitates minimising model-misspecification bias to\ncontrol false discoveries. We present TarGene, a unified statistical workflow\nfor the semi-parametric efficient and double robust estimation of genetic\neffects including k-point interactions among categorical variables in the\npresence of confounding and weak population dependence. k-point interactions,\nor Average Interaction Effects (AIEs), are a direct generalisation of the usual\naverage treatment effect (ATE). We estimate AIEs with cross-validated and/or\nweighted versions of Targeted Minimum Loss-based Estimators (TMLE) and One-Step\nEstimators (OSE). The effect of dependence among data units on variance\nestimates is corrected by using sieve plateau variance estimators based on\ngenetic relatedness across the units. We present extensive realistic\nsimulations to demonstrate power, coverage, and control of type I error. Our\nmotivating application is the targeted estimation of genetic effects on trait,\nincluding two-point and higher-order gene-gene and gene-environment\ninteractions, in large-scale genomic databases such as UK Biobank and All of\nUs. All cross-validated and/or weighted TMLE and OSE for the AIE k-point\ninteraction, as well as ATEs, conditional ATEs and functions thereof, are\nimplemented in the general purpose Julia package TMLE.jl. For high-throughput\napplications in population genomics, we provide the open-source Nextflow\npipeline and software TarGene which integrates seamlessly with modern\nhigh-performance and cloud computing platforms.", "authors": ["Olivier Labayle", "Breeshey Roskams-Hieter", "Joshua Slaughter", "Kelsey Tetley-Campbell", "Mark J. van der Laan", "Chris P. Ponting", "Sjoerd Viktor Beentjes", "Ava Khamseh"], "published_date": "2025-05-20", "title_zh": "大規模群體世代研究中小型遺傳效應的半參數有效估計", "summary_zh": "本研究提出 TarGene，一個統一的統計流程，旨在準確且高效地估計大規模基因體數據庫中小型遺傳效應，即使存在混雜因素和弱群體依賴性。TarGene 使用半參數方法，包括目標最小損失估計器（TMLE）和單步估計器（OSE），並結合交叉驗證和加權策略，來估計基因間和基因與環境間的多點交互作用（平均交互效應 AIE）。透過基於遺傳相關性的篩法平穩方差估計器，修正數據單元間依賴性對方差估計的影響。TarGene 的目標應用是在如 UK Biobank 和 All of Us 等大型數據庫中，針對性地估計遺傳效應，包括高階基因-基因和基因-環境交互作用。所有方法都實現在 Julia 語言的 TMLE.jl 包中，並提供 Nextflow 流程 TarGene 方便在高通量環境下使用。", "audio": "audios/2505.14675v1.mp3", "timestamp": "2025-05-21T06:27:20.327572"}
{"query": "Foundation Model", "id": "2505.14603v1", "url": "http://arxiv.org/abs/2505.14603v1", "title": "Towards a Foundation Model for Communication Systems", "summary": "Artificial Intelligence (AI) has demonstrated unprecedented performance\nacross various domains, and its application to communication systems is an\nactive area of research. While current methods focus on task-specific\nsolutions, the broader trend in AI is shifting toward large general models\ncapable of supporting multiple applications. In this work, we take a step\ntoward a foundation model for communication data--a transformer-based,\nmulti-modal model designed to operate directly on communication data. We\npropose methodologies to address key challenges, including tokenization,\npositional embedding, multimodality, variable feature sizes, and normalization.\nFurthermore, we empirically demonstrate that such a model can successfully\nestimate multiple features, including transmission rank, selected precoder,\nDoppler spread, and delay profile.", "authors": ["Davide Buffelli", "Sowmen Das", "Yu-Wei Lin", "Sattar Vakili", "Chien-Yi Wang", "Masoud Attarifar", "Pritthijit Nath", "Da-shan Shiu"], "published_date": "2025-05-20", "title_zh": "邁向通訊系統的基礎模型", "summary_zh": "本文旨在探索通訊系統領域的基礎模型。 借鑒AI領域發展趨勢，提出一個基於Transformer的多模態模型，直接處理通訊數據。研究針對通訊數據的特殊性，解決了分詞、位置嵌入、多模態、可變特徵尺寸和正規化等關鍵挑戰。實驗結果顯示，該模型能有效預測多種通訊指標，例如傳輸等級、預編碼器、都卜勒頻展和延遲分佈。", "audio": "audios/2505.14603v1.mp3", "timestamp": "2025-05-21T06:27:23.893770"}
{"query": "Diffusion Model", "id": "2505.14521v1", "url": "http://arxiv.org/abs/2505.14521v1", "title": "SparC: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling", "summary": "High-fidelity 3D object synthesis remains significantly more challenging than\n2D image generation due to the unstructured nature of mesh data and the cubic\ncomplexity of dense volumetric grids. Existing two-stage pipelines-compressing\nmeshes with a VAE (using either 2D or 3D supervision), followed by latent\ndiffusion sampling-often suffer from severe detail loss caused by inefficient\nrepresentations and modality mismatches introduced in VAE. We introduce SparC,\na unified framework that combines a sparse deformable marching cubes\nrepresentation SparseCubes with a novel encoder SparConv-VAE. SparseCubes\nconverts raw meshes into high-resolution ($1024^3$) surfaces with arbitrary\ntopology by scattering signed distance and deformation fields onto a sparse\ncube, allowing differentiable optimization. SparConv-VAE is the first\nmodality-consistent variational autoencoder built entirely upon sparse\nconvolutional networks, enabling efficient and near-lossless 3D reconstruction\nsuitable for high-resolution generative modeling through latent diffusion.\nSparC achieves state-of-the-art reconstruction fidelity on challenging inputs,\nincluding open surfaces, disconnected components, and intricate geometry. It\npreserves fine-grained shape details, reduces training and inference cost, and\nintegrates naturally with latent diffusion models for scalable, high-resolution\n3D generation.", "authors": ["Zhihao Li", "Yufei Wang", "Heliang Zheng", "Yihao Luo", "Bihan Wen"], "published_date": "2025-05-20", "title_zh": "SparC: 用於高解析度3D形狀建模的稀疏表示與建構", "summary_zh": "SparC是一个统一的3D模型生成框架，它結合了稀疏可變形移動立方體表示SparseCubes和新颖的編碼器SparConv-VAE。SparseCubes能将原始网格转换为高分辨率的表面，而SparConv-VAE則是首個完全基於稀疏卷積網絡的變分自編碼器，实现高效近乎無损的3D重建。SparC在高精度重建复杂3D模型方面表现出色，并且能与潜在扩散模型整合，实现可扩展的高分辨率3D生成。", "audio": "audios/2505.14521v1.mp3", "timestamp": "2025-05-21T06:27:28.672877"}
{"query": "AI", "id": "2505.14668v1", "url": "http://arxiv.org/abs/2505.14668v1", "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions", "summary": "Recent advances in Large Language Models (LLMs) have propelled intelligent\nagents from reactive responses to proactive support. While promising, existing\nproactive agents either rely exclusively on observations from enclosed\nenvironments (e.g., desktop UIs) with direct LLM inference or employ rule-based\nproactive notifications, leading to suboptimal user intent understanding and\nlimited functionality for proactive service. In this paper, we introduce\nContextAgent, the first context-aware proactive agent that incorporates\nextensive sensory contexts to enhance the proactive capabilities of LLM agents.\nContextAgent first extracts multi-dimensional contexts from massive sensory\nperceptions on wearables (e.g., video and audio) to understand user intentions.\nContextAgent then leverages the sensory contexts and the persona contexts from\nhistorical data to predict the necessity for proactive services. When proactive\nassistance is needed, ContextAgent further automatically calls the necessary\ntools to assist users unobtrusively. To evaluate this new task, we curate\nContextAgentBench, the first benchmark for evaluating context-aware proactive\nLLM agents, covering 1,000 samples across nine daily scenarios and twenty\ntools. Experiments on ContextAgentBench show that ContextAgent outperforms\nbaselines by achieving up to 8.5% and 6.0% higher accuracy in proactive\npredictions and tool calling, respectively. We hope our research can inspire\nthe development of more advanced, human-centric, proactive AI assistants.", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Kaiwei Liu", "Siyang Jiang", "Wenrui Lu", "Hongkai Chen", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "published_date": "2025-05-20", "title_zh": "ContextAgent：具備開放世界感知能力的語境感知主動式大型語言模型代理", "summary_zh": "論文提出 ContextAgent，一個能主動提供協助的 AI 代理。它利用穿戴裝置的感測數據，像是影像和聲音，以及歷史資料，來理解使用者的意圖，並預測使用者是否需要協助。當需要協助時，ContextAgent 會自動調用工具來提供服務。研究團隊還建立了 ContextAgentBench 基準測試，證明 ContextAgent 在主動預測和工具調用方面都比其他方法更準確。目標是開發更先進、以人為本的主動式 AI 助理。", "audio": "audios/2505.14668v1.mp3", "timestamp": "2025-05-21T08:24:48.499842"}
{"query": "Foundation Model", "id": "2505.14543v1", "url": "http://arxiv.org/abs/2505.14543v1", "title": "Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions", "summary": "Traditional time series models are task-specific and often depend on\ndataset-specific training and extensive feature engineering. While\nTransformer-based architectures have improved scalability, foundation models,\ncommonplace in text, vision, and audio, remain under-explored for time series\nand are largely restricted to forecasting. We introduce $\\textbf{CHARM}$, a\nfoundation embedding model for multivariate time series that learns shared,\ntransferable, and domain-aware representations. To address the unique\ndifficulties of time series foundation learning, $\\textbf{CHARM}$ incorporates\narchitectural innovations that integrate channel-level textual descriptions\nwhile remaining invariant to channel order. The model is trained using a Joint\nEmbedding Predictive Architecture (JEPA), with novel augmentation schemes and a\nloss function designed to improve interpretability and training stability. Our\n$7$M-parameter model achieves state-of-the-art performance across diverse\ndownstream tasks, setting a new benchmark for time series representation\nlearning.", "authors": ["Utsav Dutta", "Sina Khoshfetrat Pakazad", "Henrik Ohlsson"], "published_date": "2025-05-20", "title_zh": "時間嵌入：利用通道描述解鎖時間序列基礎模型", "summary_zh": "傳統時間序列模型高度依賴特定任務和數據集，且需要大量特徵工程。雖然Transformer架構提升了可擴展性，但時間序列的基礎模型開發仍落後於文字、視覺和音訊領域，且主要集中在預測上。本研究提出一個名為CHARM的多元時間序列基礎嵌入模型，旨在學習可共享、可遷移且具領域感知性的表徵。CHARM結合了通道層級的文字描述，並具備通道順序不變性，克服了時間序列基礎學習的獨特挑戰。CHARM採用聯合嵌入預測架構（JEPA）進行訓練，結合創新的增強方案和損失函數，以提升可解釋性和訓練穩定性。僅有700萬參數的CHARM模型在各種下游任務中表現出色，為時間序列表徵學習設定了新的基準。", "audio": "audios/2505.14543v1.mp3", "timestamp": "2025-05-21T08:24:54.929489"}
{"query": "Diffusion Model", "id": "2505.14502v1", "url": "http://arxiv.org/abs/2505.14502v1", "title": "Learning to Integrate Diffusion ODEs by Averaging the Derivatives", "summary": "To accelerate diffusion model inference, numerical solvers perform poorly at\nextremely small steps, while distillation techniques often introduce complexity\nand instability. This work presents an intermediate strategy, balancing\nperformance and cost, by learning ODE integration using loss functions derived\nfrom the derivative-integral relationship, inspired by Monte Carlo integration\nand Picard iteration. From a geometric perspective, the losses operate by\ngradually extending the tangent to the secant, thus are named as secant losses.\nThe secant losses can rapidly convert (via fine-tuning or distillation) a\npretrained diffusion model into its secant version. In our experiments, the\nsecant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the\nsecant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID\nof $1.96$ on ImageNet-$256\\times256$. Code will be available.", "authors": ["Wenze Liu", "Xiangyu Yue"], "published_date": "2025-05-20", "title_zh": "透過平均導數學習整合擴散常微分方程式", "summary_zh": "為了加速擴散模型的推論速度，數值解法在極小步長下表現不佳，而知識蒸餾技術又常引入複雜性和不穩定性。本文提出一種中間策略，在性能和成本之間取得平衡，透過學習常微分方程式的積分，利用源自導數-積分關係的損失函數，靈感來自蒙地卡羅積分和皮卡迭代。從幾何角度來看，這些損失函數透過逐步將切線延伸至割線來運作，因此被命名為割線損失。割線損失可以快速地（透過微調或知識蒸餾）將預訓練的擴散模型轉換為其割線版本。在實驗中，EDM 的割線版本在 CIFAR-10 上僅需 10 步即可達到 2.14 的 FID，而 SiT-XL/2 的割線版本在 ImageNet-256x256 上僅需 4 步即可達到 2.27 的 FID，8 步可達 1.96 的 FID。程式碼將會公開。", "audio": "audios/2505.14502v1.mp3", "timestamp": "2025-05-21T08:25:02.392047"}
{"query": "AI", "id": "2505.14667v1", "url": "http://arxiv.org/abs/2505.14667v1", "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment", "summary": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI.", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Minsuk Kahng", "Albert No"], "published_date": "2025-05-20", "title_zh": "SAFEPATH：透過早期對齊預防鏈式思考中的有害推理", "summary_zh": "大型推理模型在解決複雜問題上表現出色，但鏈式思考過程可能在面對有害提示時產生不安全的輸出。現有安全對齊方法雖可減少有害輸出，但也可能降低推理深度，影響複雜任務的表現，且容易受到精巧的越獄攻擊。為此，我們提出 SAFEPATH，這是一種輕量級的對齊方法，透過微調大型推理模型，使其在收到有害提示時，於推理的開頭輸出一段簡短的 8 字元安全引言，同時讓剩餘的推理過程保持無監督。實驗結果顯示，SAFEPATH 能有效減少有害輸出，同時維持推理效能。我們還提出了一個零樣本變體，無需任何微調。此外，我們分析了現有大型語言模型方法應用於以推理為中心的模型時的泛化能力，揭示了關鍵的不足之處和更安全 AI 的新方向。", "audio": "audios/2505.14667v1.mp3", "timestamp": "2025-05-21T09:20:36.302657"}
{"query": "Foundation Model", "id": "2505.14417v1", "url": "http://arxiv.org/abs/2505.14417v1", "title": "Towards Non-Euclidean Foundation Models: Advancing AI Beyond Euclidean Frameworks", "summary": "In the era of foundation models and Large Language Models (LLMs), Euclidean\nspace is the de facto geometric setting of our machine learning architectures.\nHowever, recent literature has demonstrated that this choice comes with\nfundamental limitations. To that end, non-Euclidean learning is quickly gaining\ntraction, particularly in web-related applications where complex relationships\nand structures are prevalent. Non-Euclidean spaces, such as hyperbolic,\nspherical, and mixed-curvature spaces, have been shown to provide more\nefficient and effective representations for data with intrinsic geometric\nproperties, including web-related data like social network topology,\nquery-document relationships, and user-item interactions. Integrating\nfoundation models with non-Euclidean geometries has great potential to enhance\ntheir ability to capture and model the underlying structures, leading to better\nperformance in search, recommendations, and content understanding. This\nworkshop focuses on the intersection of Non-Euclidean Foundation Models and\nGeometric Learning (NEGEL), exploring its potential benefits, including the\npotential benefits for advancing web-related technologies, challenges, and\nfuture directions. Workshop page:\n[https://hyperboliclearning.github.io/events/www2025workshop](https://hyperboliclearning.github.io/events/www2025workshop)", "authors": ["Menglin Yang", "Yifei Zhang", "Jialin Chen", "Melanie Weber", "Rex Ying"], "published_date": "2025-05-20", "title_zh": "邁向非歐幾里得基礎模型：超越歐幾里得框架推進人工智慧", "summary_zh": "歐幾里得空間是目前機器學習架構的預設幾何設定，但它存在根本性的局限性。因此，非歐幾里得學習正迅速受到關注，尤其是在網路相關應用中。例如，雙曲、球面和混合曲率空間，在處理具有內在幾何特性的資料（如社交網路拓撲、查詢-文件關係和使用者-項目互動）方面更有效率。將非歐幾里得幾何與基礎模型整合，可望提升模型捕捉和建模底層結構的能力，進而改善搜尋、推薦和內容理解。本工作坊聚焦於非歐幾里得基礎模型和幾何學習的交叉領域，探討其潛在優勢、挑戰和未來方向，特別是在推進網路相關技術方面的潛力。", "audio": "audios/2505.14417v1.mp3", "timestamp": "2025-05-21T09:20:41.995382"}
{"query": "Diffusion Model", "id": "2505.14455v1", "url": "http://arxiv.org/abs/2505.14455v1", "title": "CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation", "summary": "Although autoregressive models have dominated language modeling in recent\nyears, there has been a growing interest in exploring alternative paradigms to\nthe conventional next-token prediction framework. Diffusion-based language\nmodels have emerged as a compelling alternative due to their powerful parallel\ngeneration capabilities and inherent editability. However, these models are\noften constrained by fixed-length generation. A promising direction is to\ncombine the strengths of both paradigms, segmenting sequences into blocks,\nmodeling autoregressive dependencies across blocks while leveraging discrete\ndiffusion to estimate the conditional distribution within each block given the\npreceding context. Nevertheless, their practical application is often hindered\nby two key limitations: rigid fixed-length outputs and a lack of flexible\ncontrol mechanisms. In this work, we address the critical limitations of fixed\ngranularity and weak controllability in current large diffusion language\nmodels. We propose CtrlDiff, a dynamic and controllable semi-autoregressive\nframework that adaptively determines the size of each generation block based on\nlocal semantics using reinforcement learning. Furthermore, we introduce a\nclassifier-guided control mechanism tailored to discrete diffusion, which\nsignificantly reduces computational overhead while facilitating efficient\npost-hoc conditioning without retraining. Extensive experiments demonstrate\nthat CtrlDiff sets a new standard among hybrid diffusion models, narrows the\nperformance gap to state-of-the-art autoregressive approaches, and enables\neffective conditional text generation across diverse tasks.", "authors": ["Chihan Huang", "Hao Tang"], "published_date": "2025-05-20", "title_zh": "CtrlDiff：透過動態區塊預測與可控生成提升大型擴散語言模型", "summary_zh": "現今，擴散語言模型因其強大的平行生成能力和可編輯性而備受關注。然而，這些模型常受限於固定長度的生成。CtrlDiff提出一種動態且可控的半自迴歸框架，能基於局部語義自適應地決定每個生成區塊的大小。此外，我們還引入了一種針對離散擴散的分類器引導控制機制，在無需重新訓練的情況下，實現高效的後驗條件生成。實驗表明，CtrlDiff在混合擴散模型中樹立了新的標竿，縮小了與最先進自迴歸方法的性能差距，並能跨多種任務實現有效的條件文本生成。", "audio": "audios/2505.14455v1.mp3", "timestamp": "2025-05-21T09:20:47.242642"}
{"query": "AI", "id": "2505.14661v1", "url": "http://arxiv.org/abs/2505.14661v1", "title": "Abacus: A Cost-Based Optimizer for Semantic Operator Systems", "summary": "LLMs enable an exciting new class of data processing applications over large\ncollections of unstructured documents. Several new programming frameworks have\nenabled developers to build these applications by composing them out of\nsemantic operators: a declarative set of AI-powered data transformations with\nnatural language specifications. These include LLM-powered maps, filters,\njoins, etc. used for document processing tasks such as information extraction,\nsummarization, and more. While systems of semantic operators have achieved\nstrong performance on benchmarks, they can be difficult to optimize. An\noptimizer for this setting must determine how to physically implement each\nsemantic operator in a way that optimizes the system globally. Existing\noptimizers are limited in the number of optimizations they can apply, and most\n(if not all) cannot optimize system quality, cost, or latency subject to\nconstraint(s) on the other dimensions. In this paper we present Abacus, an\nextensible, cost-based optimizer which searches for the best implementation of\na semantic operator system given a (possibly constrained) optimization\nobjective. Abacus estimates operator performance by leveraging a minimal set of\nvalidation examples and, if available, prior beliefs about operator\nperformance. We evaluate Abacus on document processing workloads in the\nbiomedical and legal domains (BioDEX; CUAD) and multi-modal question answering\n(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%\nbetter quality and up to 23.6x lower cost and 4.2x lower latency than the next\nbest system.", "authors": ["Matthew Russo", "Sivaprasad Sudhir", "Gerardo Vitagliano", "Chunwei Liu", "Tim Kraska", "Samuel Madden", "Michael Cafarella"], "published_date": "2025-05-20", "title_zh": "算盤 (Abacus): 一個基於成本的語義運算子系統最佳化器", "summary_zh": "大型語言模型催生了新的非結構化文件處理應用。開發者可以使用語義運算子，也就是基於AI、具自然語言規範的資料轉換，來建構這些應用。然而，優化這些運算子系統非常困難。現有的優化器能力有限，且難以同時優化品質、成本和延遲。本文提出 Abacus，一個可擴展、基於成本的優化器，能根據給定的優化目標（可能帶有約束）尋找語義運算子系統的最佳實現方案。Abacus 利用少量的驗證樣本和運算子效能的先驗知識來估算運算子效能。在生物醫學、法律領域和多模態問答工作負載上的評估表明，由 Abacus 優化的系統比其他系統能達到18.7%-39.2%的品質提升，並降低高達23.6倍的成本和4.2倍的延遲。", "audio": "audios/2505.14661v1.mp3", "timestamp": "2025-05-21T11:15:44.284777"}
{"query": "Foundation Model", "id": "2505.14415v1", "url": "http://arxiv.org/abs/2505.14415v1", "title": "Table Foundation Models: on knowledge pre-training for tabular learning", "summary": "Table foundation models bring high hopes to data science: pre-trained on\ntabular data to embark knowledge or priors, they should facilitate downstream\ntasks on tables. One specific challenge is that of data semantics: numerical\nentries take their meaning from context, e.g., column name. Pre-trained neural\nnetworks that jointly model column names and table entries have recently\nboosted prediction accuracy. While these models outline the promises of world\nknowledge to interpret table values, they lack the convenience of popular\nfoundation models in text or vision. Indeed, they must be fine-tuned to bring\nbenefits, come with sizeable computation costs, and cannot easily be reused or\ncombined with other architectures. Here we introduce TARTE, a foundation model\nthat transforms tables to knowledge-enhanced vector representations using the\nstring to capture semantics. Pre-trained on large relational data, TARTE yields\nrepresentations that facilitate subsequent learning with little additional\ncost. These representations can be fine-tuned or combined with other learners,\ngiving models that push the state-of-the-art prediction performance and improve\nthe prediction/computation performance trade-off. Specialized to a task or a\ndomain, TARTE gives domain-specific representations that facilitate further\nlearning. Our study demonstrates an effective approach to knowledge\npre-training for tabular learning.", "authors": ["Myung Jun Kim", "Félix Lefebvre", "Gaëtan Brison", "Alexandre Perez-Lebel", "Gaël Varoquaux"], "published_date": "2025-05-20", "title_zh": "表格基礎模型：關於表格學習的知識預訓練", "summary_zh": "表格基礎模型被寄予厚望，期望透過在表格數據上進行預訓練，獲取知識或先驗知識，從而簡化下游表格任務。一個特別的挑戰是數據語義：數值條目的意義來自於上下文，例如欄位名稱。最近，聯合建模欄位名稱和表格條目的預訓練神經網路提高了預測準確性。然而，這些模型缺乏文本或視覺領域中常見基礎模型的便利性，必須進行微調才能帶來效益，計算成本高昂，且難以重複使用或與其他架構結合。因此，我們介紹了TARTE，一種基礎模型，它使用字串將表格轉換為知識增強的向量表示，從而捕獲語義。TARTE在大型關聯數據上進行預訓練，產生易於後續學習且幾乎無需額外成本的表示。這些表示可以進行微調或與其他學習器結合，從而使模型能夠提升最先進的預測性能，並改善預測/計算性能的權衡。針對特定任務或領域進行專門設計的TARTE，可以提供領域特定的表示，從而促進進一步的學習。我們的研究證明了一種有效的表格學習知識預訓練方法。", "audio": "audios/2505.14415v1.mp3", "timestamp": "2025-05-21T11:15:52.095562"}
{"query": "Diffusion Model", "id": "2505.14429v1", "url": "http://arxiv.org/abs/2505.14429v1", "title": "Compositional amortized inference for large-scale hierarchical Bayesian models", "summary": "Amortized Bayesian inference (ABI) has emerged as a powerful simulation-based\napproach for estimating complex mechanistic models, offering fast posterior\nsampling via generative neural networks. However, extending ABI to hierarchical\nmodels, a cornerstone of modern Bayesian analysis, remains a major challenge\ndue to the difficulty of scaling to large numbers of parameters. In this work,\nwe build on compositional score matching (CSM), a divide-and-conquer strategy\nfor Bayesian updating using diffusion models. To address existing stability\nissues of CSM, we propose adaptive solvers coupled with a novel, error-damping\ncompositional estimator. Our proposed method remains stable even with hundreds\nof thousands of data points and parameters. We validate our approach on a\ncontrolled toy example, a high-dimensional spatial autoregressive model, and a\nreal-world advanced microscopy biological application task involving over\n750,000 parameters.", "authors": ["Jonas Arruda", "Vikas Pandey", "Catherine Sherry", "Margarida Barroso", "Xavier Intes", "Jan Hasenauer", "Stefan T. Radev"], "published_date": "2025-05-20", "title_zh": "大規模層級貝氏模型的組合攤銷推論", "summary_zh": "提出一種基於生成神經網路的攤銷貝氏推論 (ABI) 方法，加速複雜模型後驗分布的取樣。針對 ABI 在大規模層級模型上的應用挑戰，本研究結合組合分數匹配 (CSM) 策略和自適應求解器，並提出一種新型的誤差阻尼組合估計器，克服 CSM 的穩定性問題。該方法即使在高維度數據和參數下也能保持穩定，並在模擬數據、空間自迴歸模型以及實際生物顯微鏡數據上驗證其有效性。簡言之，本研究提出一種穩定的方法，可以處理參數量巨大的層級貝氏模型推論問題。", "audio": "audios/2505.14429v1.mp3", "timestamp": "2025-05-21T11:15:57.552059"}
{"query": "AI", "id": "2505.14659v1", "url": "http://arxiv.org/abs/2505.14659v1", "title": "Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks", "summary": "As healthcare systems increasingly adopt advanced wireless networks and\nconnected devices, securing medical applications has become critical. The\nintegration of Internet of Medical Things devices, such as robotic surgical\ntools, intensive care systems, and wearable monitors has enhanced patient care\nbut introduced serious security risks. Cyberattacks on these devices can lead\nto life threatening consequences, including surgical errors, equipment failure,\nand data breaches. While the ITU IMT 2030 vision highlights 6G's transformative\nrole in healthcare through AI and cloud integration, it also raises new\nsecurity concerns. This paper explores how explainable AI techniques like SHAP,\nLIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve\ntrust and transparency in 6G enabled healthcare. We support our approach with\nexperimental analysis and highlight promising results.", "authors": ["Navneet Kaur", "Lav Gupta"], "published_date": "2025-05-20", "title_zh": "可解釋人工智慧在物聯網整合型 6G 無線網路中保障醫療保健安全", "summary_zh": "隨著醫療系統更廣泛使用無線網路和聯網設備，醫療應用的安全性變得至關重要。物聯網醫療設備雖然提升了照護品質，但也帶來了嚴重的安全風險，像是手術機器人可能被駭、數據外洩等。6G 的發展進一步促進醫療保健的 AI 和雲端整合，但也引發了新的安全隱憂。本文探討如何運用可解釋人工智慧技術，例如 SHAP、LIME 和 DiCE，來發現漏洞、強化防禦，並提高 6G 醫療保健的信任度和透明度。實驗分析結果也顯示這些方法很有潛力。", "audio": "audios/2505.14659v1.mp3", "timestamp": "2025-05-21T12:37:45.151636"}
{"query": "Foundation Model", "id": "2505.14414v1", "url": "http://arxiv.org/abs/2505.14414v1", "title": "Diving into the Fusion of Monocular Priors for Generalized Stereo Matching", "summary": "The matching formulation makes it naturally hard for the stereo matching to\nhandle ill-posed regions like occlusions and non-Lambertian surfaces. Fusing\nmonocular priors has been proven helpful for ill-posed matching, but the biased\nmonocular prior learned from small stereo datasets constrains the\ngeneralization. Recently, stereo matching has progressed by leveraging the\nunbiased monocular prior from the vision foundation model (VFM) to improve the\ngeneralization in ill-posed regions. We dive into the fusion process and\nobserve three main problems limiting the fusion of the VFM monocular prior. The\nfirst problem is the misalignment between affine-invariant relative monocular\ndepth and absolute depth of disparity. Besides, when we use the monocular\nfeature in an iterative update structure, the over-confidence in the disparity\nupdate leads to local optima results. A direct fusion of a monocular depth map\ncould alleviate the local optima problem, but noisy disparity results computed\nat the first several iterations will misguide the fusion. In this paper, we\npropose a binary local ordering map to guide the fusion, which converts the\ndepth map into a binary relative format, unifying the relative and absolute\ndepth representation. The computed local ordering map is also used to re-weight\nthe initial disparity update, resolving the local optima and noisy problem. In\naddition, we formulate the final direct fusion of monocular depth to the\ndisparity as a registration problem, where a pixel-wise linear regression\nmodule can globally and adaptively align them. Our method fully exploits the\nmonocular prior to support stereo matching results effectively and efficiently.\nWe significantly improve the performance from the experiments when generalizing\nfrom SceneFlow to Middlebury and Booster datasets while barely reducing the\nefficiency.", "authors": ["Chengtang Yao", "Lidong Yu", "Zhidan Liu", "Jiaxi Zeng", "Yuwei Wu", "Yunde Jia"], "published_date": "2025-05-20", "title_zh": "深入探討融合單眼先驗知識以提升廣義立體匹配", "summary_zh": "立體匹配在處理遮擋和非朗伯表面等不良區域時面臨挑戰。融合單眼先驗知識已被證明有效，但從小型立體數據集學習到的偏差先驗會限制泛化能力。近年來，利用視覺基礎模型（VFM）中無偏差的單眼先驗，立體匹配在不良區域的泛化能力方面取得了進展。本研究深入探討了融合過程，發現了限制VFM單眼先驗融合的三個主要問題：仿射不變的相對單眼深度與絕對深度視差之間的錯位；迭代更新結構中，過度自信的視差更新導致局部最優；以及早期迭代中嘈雜的視差結果會誤導直接融合單眼深度圖。我們提出了一種二元局部排序圖來引導融合，將深度圖轉換為二元相對格式，統一了相對深度和絕對深度表示。該局部排序圖還用於重新權衡初始視差更新，解決了局部最優和噪聲問題。此外，我們將單眼深度與視差的最終直接融合表述為一個註冊問題，通過像素級線性回歸模塊進行全局自適應對齊。實驗結果表明，我們的模型充分利用了單眼先驗知識，顯著提高了從SceneFlow泛化到Middlebury和Booster數據集的性能，同時幾乎沒有降低效率。", "audio": "audios/2505.14414v1.mp3", "timestamp": "2025-05-21T12:37:55.398628"}
{"query": "Diffusion Model", "id": "2505.14357v1", "url": "http://arxiv.org/abs/2505.14357v1", "title": "Vid2World: Crafting Video Diffusion Models to Interactive World Models", "summary": "World models, which predict transitions based on history observation and\naction sequences, have shown great promise in improving data efficiency for\nsequential decision making. However, existing world models often require\nextensive domain-specific training and still produce low-fidelity, coarse\npredictions, limiting their applicability in complex environments. In contrast,\nvideo diffusion models trained on large, internet-scale datasets have\ndemonstrated impressive capabilities in generating high-quality videos that\ncapture diverse real-world dynamics. In this work, we present Vid2World, a\ngeneral approach for leveraging and transferring pre-trained video diffusion\nmodels into interactive world models. To bridge the gap, Vid2World performs\ncasualization of a pre-trained video diffusion model by crafting its\narchitecture and training objective to enable autoregressive generation.\nFurthermore, it introduces a causal action guidance mechanism to enhance action\ncontrollability in the resulting interactive world model. Extensive experiments\nin robot manipulation and game simulation domains show that our method offers a\nscalable and effective approach for repurposing highly capable video diffusion\nmodels to interactive world models.", "authors": ["Siqiao Huang", "Jialong Wu", "Qixing Zhou", "Shangchen Miao", "Mingsheng Long"], "published_date": "2025-05-20", "title_zh": "Vid2World：打造視訊擴散模型成為互動式世界模型", "summary_zh": "Vid2World 提出一種利用預訓練視訊擴散模型打造互動式世界模型的新方法。它通過改造模型架構和訓練目標，使其能自迴歸地生成預測，並引入因果行動引導機制，增強行動的可控性。實驗證明，這種方法能有效地將强大的視訊擴散模型轉化為可互動的世界模型，並應用於機器人操作和遊戲模擬等領域。", "audio": "audios/2505.14357v1.mp3", "timestamp": "2025-05-21T12:37:58.683896"}
{"query": "AI", "id": "2505.14654v1", "url": "http://arxiv.org/abs/2505.14654v1", "title": "Beyond Words: Multimodal LLM Knows When to Speak", "summary": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI.", "authors": ["Zikai Liao", "Yi Ouyang", "Yi-Lun Lee", "Chen-Ping Yu", "Yi-Hsuan Tsai", "Zhaozheng Yin"], "published_date": "2025-05-20", "title_zh": "超越文字：多模態大型語言模型知道何時該說話", "summary_zh": "大型語言模型聊天機器人在產生連貫且符合上下文的回應方面表現出色，但它們常常難以理解何時該說話，尤其是在持續的對話中給予簡短、及時的回應。本研究著重於即時預測回應類型，特別是那些依賴視覺、聽覺和文字等細微多模態信號的簡短反應性話語。為此，研究者們建立了一個新的多模態數據集，並提出了一個名為MM-When2Speak的多模態模型，該模型能自適應地整合視覺、聽覺和文字信息，以預測何時應進行回應以及何種回應類型是適當的。實驗結果表明，該模型在回應時機的準確性上，比現有技術和基於大型語言模型的模型有顯著提升，突顯了多模態輸入對於產生及時、自然和引人入勝的對話式AI的重要性。", "audio": "audios/2505.14654v1.mp3", "timestamp": "2025-05-21T13:30:37.941299"}
{"query": "Foundation Model", "id": "2505.14411v1", "url": "http://arxiv.org/abs/2505.14411v1", "title": "Byte Pair Encoding for Efficient Time Series Forecasting", "summary": "Existing time series tokenization methods predominantly encode a constant\nnumber of samples into individual tokens. This inflexible approach can generate\nexcessive tokens for even simple patterns like extended constant values,\nresulting in substantial computational overhead. Inspired by the success of\nbyte pair encoding, we propose the first pattern-centric tokenization scheme\nfor time series analysis. Based on a discrete vocabulary of frequent motifs,\nour method merges samples with underlying patterns into tokens, compressing\ntime series adaptively. Exploiting our finite set of motifs and the continuous\nproperties of time series, we further introduce conditional decoding as a\nlightweight yet powerful post-hoc optimization method, which requires no\ngradient computation and adds no computational overhead. On recent time series\nfoundation models, our motif-based tokenization improves forecasting\nperformance by 36% and boosts efficiency by 1990% on average. Conditional\ndecoding further reduces MSE by up to 44%. In an extensive analysis, we\ndemonstrate the adaptiveness of our tokenization to diverse temporal patterns,\nits generalization to unseen data, and its meaningful token representations\ncapturing distinct time series properties, including statistical moments and\ntrends.", "authors": ["Leon Götz", "Marcel Kollovieh", "Stephan Günnemann", "Leo Schwinn"], "published_date": "2025-05-20", "title_zh": "用於高效時間序列預測的位元組對編碼", "summary_zh": "現有的時間序列符號化方法通常將固定數量的樣本編碼成單獨的符號，這種不靈活的方式會對簡單的模式（例如持續的常數值）產生過多的符號，導致大量的計算開銷。受位元組對編碼的成功啟發，我們提出了一種以模式為中心的的時間序列符號化方案。該方法基於常見的圖案離散詞彙，將具有底層模式的樣本合併到符號中，從而自適應地壓縮時間序列。利用我們的有限模式集和時間序列的連續性，我們進一步引入條件解碼作為一種輕量但強大的後處理優化方法，無需梯度計算，也不會增加計算開銷。在最新的時間序列基礎模型上，我們基於模式的符號化平均提高了36%的預測性能，並提高了1990%的效率。條件解碼進一步將均方誤差降低了最多44%。在廣泛的分析中，我們證明了我們的符號化對不同時間模式的適應性、對未見數據的泛化能力，以及其捕獲不同時間序列屬性（包括統計矩和趨勢）的有意義的符號表示。", "audio": "audios/2505.14411v1.mp3", "timestamp": "2025-05-21T13:30:45.995773"}
{"query": "Diffusion Model", "id": "2505.14254v1", "url": "http://arxiv.org/abs/2505.14254v1", "title": "Instructing Text-to-Image Diffusion Models via Classifier-Guided Semantic Optimization", "summary": "Text-to-image diffusion models have emerged as powerful tools for\nhigh-quality image generation and editing. Many existing approaches rely on\ntext prompts as editing guidance. However, these methods are constrained by the\nneed for manual prompt crafting, which can be time-consuming, introduce\nirrelevant details, and significantly limit editing performance. In this work,\nwe propose optimizing semantic embeddings guided by attribute classifiers to\nsteer text-to-image models toward desired edits, without relying on text\nprompts or requiring any training or fine-tuning of the diffusion model. We\nutilize classifiers to learn precise semantic embeddings at the dataset level.\nThe learned embeddings are theoretically justified as the optimal\nrepresentation of attribute semantics, enabling disentangled and accurate\nedits. Experiments further demonstrate that our method achieves high levels of\ndisentanglement and strong generalization across different domains of data.", "authors": ["Yuanyuan Chang", "Yinghua Yao", "Tao Qin", "Mengmeng Wang", "Ivor Tsang", "Guang Dai"], "published_date": "2025-05-20", "title_zh": "透過分類器引導的語義優化來指示文字到圖像擴散模型", "summary_zh": "現有的文字到圖像模型編輯方式仰賴人工撰寫提示詞，費時且易引入雜訊。本研究提出一種新方法，無需文字提示詞，也無需訓練或微調擴散模型，而是利用屬性分類器引導語義嵌入的優化，從而實現更精準、更具解耦性的圖像編輯。研究結果顯示此方法在不同數據領域皆具有良好的泛化能力。", "audio": "audios/2505.14254v1.mp3", "timestamp": "2025-05-21T13:30:50.483667"}
{"query": "AI", "id": "2505.14646v1", "url": "http://arxiv.org/abs/2505.14646v1", "title": "CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation", "summary": "Efficient creation of accurate and editable 3D CAD models is critical in\nengineering design, significantly impacting cost and time-to-market in product\ninnovation. Current manual workflows remain highly time-consuming and demand\nextensive user expertise. While recent developments in AI-driven CAD generation\nshow promise, existing models are limited by incomplete representations of CAD\noperations, inability to generalize to real-world images, and low output\naccuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model\n(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)\ndirectly from visual input. Leveraging a novel dataset that we\ncreated--GenCAD-Code, consisting of over 163k CAD-model image and code\npairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and\nQwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in\n3D solid similarity. Notably, our VLM demonstrates some signs of\ngeneralizability, successfully generating CAD code from real-world images and\nexecuting CAD operations unseen during fine-tuning. The performance and\nadaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code\nto streamline CAD workflows for engineers and designers. CAD-Coder is publicly\navailable at: https://github.com/anniedoris/CAD-Coder.", "authors": ["Anna C. Doris", "Md Ferdous Alam", "Amin Heyrani Nobari", "Faez Ahmed"], "published_date": "2025-05-20", "title_zh": "CAD-Coder：一個用於電腦輔助設計程式碼生成的開源視覺-語言模型", "summary_zh": "工程設計中，高效、精確且可編輯的 3D CAD 模型至關重要。現有方法耗時且需專業知識。本文介紹了 CAD-Coder，一個開源視覺-語言模型，能直接從視覺輸入生成可編輯的 CAD 程式碼。它基於一個包含超過 16 萬 CAD 模型圖像和程式碼配對的新數據集 GenCAD-Code 進行微調，性能超越了 GPT-4.5 和 Qwen2.5-VL-72B 等最先進的模型，實現了 100% 的有效語法率和最高的 3D 實體相似度精度。CAD-Coder 甚至展現了從真實圖像生成 CAD 程式碼的能力。這個模型的性能和適應性凸顯了程式碼微調的視覺-語言模型在簡化工程師和設計師 CAD 工作流程方面的潛力。CAD-Coder 可在 GitHub 上公開獲取。", "audio": "audios/2505.14646v1.mp3", "timestamp": "2025-05-21T14:19:13.872585"}
{"query": "Foundation Model", "id": "2505.14402v1", "url": "http://arxiv.org/abs/2505.14402v1", "title": "OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking", "summary": "The code of nature, embedded in DNA and RNA genomes since the origin of life,\nholds immense potential to impact both humans and ecosystems through genome\nmodeling. Genomic Foundation Models (GFMs) have emerged as a transformative\napproach to decoding the genome. As GFMs scale up and reshape the landscape of\nAI-driven genomics, the field faces an urgent need for rigorous and\nreproducible evaluation. We present OmniGenBench, a modular benchmarking\nplatform designed to unify the data, model, benchmarking, and interpretability\nlayers across GFMs. OmniGenBench enables standardized, one-command evaluation\nof any GFM across five benchmark suites, with seamless integration of over 31\nopen-source models. Through automated pipelines and community-extensible\nfeatures, the platform addresses critical reproducibility challenges, including\ndata transparency, model interoperability, benchmark fragmentation, and\nblack-box interpretability. OmniGenBench aims to serve as foundational\ninfrastructure for reproducible genomic AI research, accelerating trustworthy\ndiscovery and collaborative innovation in the era of genome-scale modeling.", "authors": ["Heng Yang", "Jack Cole", "Yuan Li", "Renzhi Chen", "Geyong Min", "Ke Li"], "published_date": "2025-05-20", "title_zh": "OmniGenBench：一個用於基因體基礎模型可重現性評測的模組化平台", "summary_zh": "OmniGenBench是一個基因體基礎模型（GFMs）的評測平台，旨在解決AI驅動基因體學領域中可重現性評估的迫切需求。它整合了數據、模型、評測和可解釋性等層面，支援一鍵式評估31個以上的開源模型。透過自動化流程和社群擴展功能，OmniGenBench致力於克服數據透明度、模型互操作性、基準測試碎片化和黑盒可解釋性等挑戰，加速基因體AI研究的可靠發現和協作創新。", "audio": "audios/2505.14402v1.mp3", "timestamp": "2025-05-21T14:19:18.151222"}
{"query": "Diffusion Model", "id": "2505.14206v1", "url": "http://arxiv.org/abs/2505.14206v1", "title": "Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data", "summary": "The widespread adoption of mobile sensors has the potential to provide\nmassive and heterogeneous time series data, driving Artificial Intelligence\napplications in mHealth. However, data collection remains limited due to\nstringent ethical regulations, privacy concerns, and other constraints,\nhindering progress in the field. Synthetic data generation, particularly\nthrough Generative Adversarial Networks and Diffusion Models, has emerged as a\npromising solution to address both data scarcity and privacy issues. Yet, these\nmodels are often limited to short-term, unimodal signal patterns. This paper\npresents a systematic evaluation of state-of-the-art generative models for time\nseries synthesis, with a focus on their ability to jointly handle\nmulti-modality, long-range dependencies, and conditional generation-key\nchallenges in the mHealth domain. To ensure a fair comparison, we introduce a\nnovel evaluation framework designed to measure both the intrinsic quality of\nsynthetic data and its utility in downstream predictive tasks. Our findings\nreveal critical limitations in the existing approaches, particularly in\nmaintaining cross-modal consistency, preserving temporal coherence, and\nensuring robust performance in train-on-synthetic, test-on-real, and data\naugmentation scenarios. Finally, we present our future research directions to\nenhance synthetic time series generation and improve the applicability of\ngenerative models in mHealth.", "authors": ["Flavio Di Martino", "Franca Delmastro"], "published_date": "2025-05-20", "title_zh": "mHealth感測器資料合成生成中的挑戰與限制", "summary_zh": "行動感測器在mHealth領域應用廣泛，能產生大量時間序列數據。然而，倫理、隱私等因素限制了真實數據的收集。利用生成對抗網路和擴散模型等技術合成數據是個有前景的解決方案，但現有模型通常只能處理短期、單模態訊號。本文系統性地評估了目前最先進的時間序列生成模型，著重於它們處理多模態、長程依賴以及條件生成的能力，這些都是mHealth領域的關鍵挑戰。研究結果揭示了現有方法的局限性，尤其是在保持跨模態一致性、保存時間相干性和確保在不同情境下的穩健性能方面。最後，論文提出了未來的研究方向，旨在改進合成時間序列生成技術，並提高生成模型在mHealth領域的應用性。", "audio": "audios/2505.14206v1.mp3", "timestamp": "2025-05-21T14:19:25.266113"}
{"query": "AI", "id": "2505.14640v1", "url": "http://arxiv.org/abs/2505.14640v1", "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation", "summary": "Large multimodal models (LMMs) have recently emerged as a powerful tool for\nlong video understanding (LVU), prompting the development of standardized LVU\nbenchmarks to evaluate their performance. However, our investigation reveals a\nrather sober lesson for existing LVU benchmarks. First, most existing\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\nresults are inflated due to the possibility of guessing the correct answer;\nSecond, a significant portion of questions in these benchmarks have strong\npriors to allow models to answer directly without even reading the input video.\nFor example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame\nfrom a long video on Video-MME. We also observe that increasing the number of\nframes does not necessarily lead to improvement on existing benchmarks, which\nis counterintuitive. As a result, the validity and robustness of current LVU\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\nrealistic LVU benchmark containing questions with open-ended short-answer,\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\nsegment-level and full-video understanding through perception and reasoning\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\nfollowing findings: (1) video LMMs show drastic performance ($>$25\\%) drops on\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\nframes. Our results show that VideoEval-Pro offers a more realistic and\nreliable measure of long video understanding, providing a clearer view of\nprogress in this domain.", "authors": ["Wentao Ma", "Weiming Ren", "Yiming Jia", "Zhuofeng Li", "Ping Nie", "Ge Zhang", "Wenhu Chen"], "published_date": "2025-05-20", "title_zh": "VideoEval-Pro：穩健且真實的長影片理解評估", "summary_zh": "現有的長影片理解評估基準測試存在嚴重問題，容易讓模型猜對答案，甚至不看影片也能答題，導致評估結果虛高。為了更真實地評估大型多模態模型的能力，我們提出了 VideoEval-Pro。它包含需要完整理解影片內容的開放式簡答題，涵蓋影片片段及完整影片的感知和推理任務。實驗結果顯示，現有模型在 VideoEval-Pro 上表現大幅下降，且增加輸入影格數更能提升效能，顯示 VideoEval-Pro 能更準確地衡量長影片理解能力。", "audio": "audios/2505.14640v1.mp3", "timestamp": "2025-05-21T16:24:06.392302"}
{"query": "Foundation Model", "id": "2505.14396v1", "url": "http://arxiv.org/abs/2505.14396v1", "title": "Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds", "summary": "Causal world models are systems that can answer counterfactual questions\nabout an environment of interest, i.e. predict how it would have evolved if an\narbitrary subset of events had been realized differently. It requires\nunderstanding the underlying causes behind chains of events and conducting\ncausal inference for arbitrary unseen distributions. So far, this task eludes\nfoundation models, notably large language models (LLMs), which do not have\ndemonstrated causal reasoning capabilities beyond the memorization of existing\ncausal relationships. Furthermore, evaluating counterfactuals in real-world\napplications is challenging since only the factual world is observed, limiting\nevaluation to synthetic datasets. We address these problems by explicitly\nextracting and modeling causal relationships and propose the Causal\nCartographer framework. First, we introduce a graph retrieval-augmented\ngeneration agent tasked to retrieve causal relationships from data. This\napproach allows us to construct a large network of real-world causal\nrelationships that can serve as a repository of causal knowledge and build\nreal-world counterfactuals. In addition, we create a counterfactual reasoning\nagent constrained by causal relationships to perform reliable step-by-step\ncausal inference. We show that our approach can extract causal knowledge and\nimprove the robustness of LLMs for causal reasoning tasks while reducing\ninference costs and spurious correlations.", "authors": ["Gaël Gendron", "Jože M. Rožanec", "Michael Witbrock", "Gillian Dobbie"], "published_date": "2025-05-20", "title_zh": "因果製圖師：從地圖繪製到反事實世界的推理", "summary_zh": "大型語言模型（LLM）擅長記憶，但缺乏真正的因果推理能力，難以回答關於反事實情境的問題，也就是預測如果某些事件以不同方式發生，世界將如何演變。本研究提出「因果製圖師」框架，透過檢索數據中的因果關係，構建一個龐大的真實世界因果關係網絡，作為因果知識庫。此外，我們還建立了一個受因果關係約束的反事實推理代理，使其能執行可靠的逐步因果推理。實驗證明，此方法能有效提取因果知識，提升LLM在因果推理任務中的穩健性，同時降低推理成本和錯誤的相關性。", "audio": "audios/2505.14396v1.mp3", "timestamp": "2025-05-21T16:24:11.925044"}
{"query": "Diffusion Model", "id": "2505.14139v1", "url": "http://arxiv.org/abs/2505.14139v1", "title": "FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning", "summary": "The use of guidance to steer sampling toward desired outcomes has been widely\nexplored within diffusion models, especially in applications such as image and\ntrajectory generation. However, incorporating guidance during training remains\nrelatively underexplored. In this work, we introduce energy-guided flow\nmatching, a novel approach that enhances the training of flow models and\neliminates the need for guidance at inference time. We learn a conditional\nvelocity field corresponding to the flow policy by approximating an\nenergy-guided probability path as a Gaussian path. Learning guided trajectories\nis appealing for tasks where the target distribution is defined by a\ncombination of data and an energy function, as in reinforcement learning.\nDiffusion-based policies have recently attracted attention for their expressive\npower and ability to capture multi-modal action distributions. Typically, these\npolicies are optimized using weighted objectives or by back-propagating\ngradients through actions sampled by the policy. As an alternative, we propose\nFlowQ, an offline reinforcement learning algorithm based on energy-guided flow\nmatching. Our method achieves competitive performance while the policy training\ntime is constant in the number of flow sampling steps.", "authors": ["Marvin Alles", "Nutan Chen", "Patrick van der Smagt", "Botond Cseke"], "published_date": "2025-05-20", "title_zh": "FlowQ：能量導引的流動策略用於離線強化學習", "summary_zh": "本研究提出FlowQ，一種基於能量導引的流動匹配的離線強化學習算法。FlowQ透過近似能量導引機率路徑為高斯路徑，學習條件速度場，從而訓練出更好的流動模型。這種方法消除了推理階段需要額外導引的步驟，並且在策略訓練時間上與流動採樣步數無關，同時能達到具競爭力的效能。", "audio": "audios/2505.14139v1.mp3", "timestamp": "2025-05-21T16:24:16.369091"}
{"query": "AI", "id": "2505.14633v1", "url": "http://arxiv.org/abs/2505.14633v1", "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas", "summary": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.", "authors": ["Yu Ying Chiu", "Zhilin Wang", "Sharan Maiya", "Yejin Choi", "Kyle Fish", "Sydney Levine", "Evan Hubinger"], "published_date": "2025-05-20", "title_zh": "AI會為了拯救生病的孩子而說謊嗎？用AIRiskDilemmas測試AI的價值優先順序", "summary_zh": "隨著AI模型變得更強大，偵測AI風險也更具挑戰性，因為它們會使用像是「對齊偽造」等新方法來迴避偵測。我們認為，識別AI模型內部的價值觀，可以作為AI潛在風險行為的早期預警系統。我們創建了LitmusValues評估流程，揭示AI模型在各種價值觀類別上的優先順序。接著，我們收集了AIRiskDilemmas，這是一系列多樣化的兩難情境，這些情境將價值觀彼此對立，並與AI安全風險（例如權力追求）相關。透過評估AI模型在這些情境中的選擇，我們可以獲得一致的價值優先順序預測，從而揭示潛在的風險。研究表明，LitmusValues中的價值觀（包括看似無害的價值觀，如關懷）可以預測AIRiskDilemmas中已知的風險行為，以及HarmBench中未知的風險行為。", "audio": "audios/2505.14633v1.mp3", "timestamp": "2025-05-21T17:16:05.673771"}
{"query": "Foundation Model", "id": "2505.14361v1", "url": "http://arxiv.org/abs/2505.14361v1", "title": "Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives", "summary": "Vision-language modeling (VLM) aims to bridge the information gap between\nimages and natural language. Under the new paradigm of first pre-training on\nmassive image-text pairs and then fine-tuning on task-specific data, VLM in the\nremote sensing domain has made significant progress. The resulting models\nbenefit from the absorption of extensive general knowledge and demonstrate\nstrong performance across a variety of remote sensing data analysis tasks.\nMoreover, they are capable of interacting with users in a conversational\nmanner. In this paper, we aim to provide the remote sensing community with a\ntimely and comprehensive review of the developments in VLM using the two-stage\nparadigm. Specifically, we first cover a taxonomy of VLM in remote sensing:\ncontrastive learning, visual instruction tuning, and text-conditioned image\ngeneration. For each category, we detail the commonly used network architecture\nand pre-training objectives. Second, we conduct a thorough review of existing\nworks, examining foundation models and task-specific adaptation methods in\ncontrastive-based VLM, architectural upgrades, training strategies and model\ncapabilities in instruction-based VLM, as well as generative foundation models\nwith their representative downstream applications. Third, we summarize datasets\nused for VLM pre-training, fine-tuning, and evaluation, with an analysis of\ntheir construction methodologies (including image sources and caption\ngeneration) and key properties, such as scale and task adaptability. Finally,\nwe conclude this survey with insights and discussions on future research\ndirections: cross-modal representation alignment, vague requirement\ncomprehension, explanation-driven model reliability, continually scalable model\ncapabilities, and large-scale datasets featuring richer modalities and greater\nchallenges.", "authors": ["Xingxing Weng", "Chao Pang", "Gui-Song Xia"], "published_date": "2025-05-20", "title_zh": "視覺-語言建模遇上遙感：模型、資料集與展望", "summary_zh": "本文綜述了視覺-語言建模（VLM）在遙感領域的最新進展。VLM透過大規模圖像-文字配對的預訓練，再針對特定任務進行微調，已展現出強大的遙感數據分析能力，甚至可以進行對話互動。本文分類介紹了遙感VLM的三種主要方法：對比學習、視覺指令調整和文本條件圖像生成，並深入探討了現有模型、訓練策略和資料集。最後，文章展望了未來研究方向，包括跨模態表示對齊、模糊需求理解、可解釋性、持續可擴展的模型能力，以及更大規模、更具挑戰性的多模態資料集。", "audio": "audios/2505.14361v1.mp3", "timestamp": "2025-05-21T17:16:12.073270"}
{"query": "Diffusion Model", "id": "2505.14036v1", "url": "http://arxiv.org/abs/2505.14036v1", "title": "Adaptive Cyclic Diffusion for Inference Scaling", "summary": "Diffusion models have demonstrated strong generative capabilities across\ndomains ranging from image synthesis to complex reasoning tasks. However, most\ninference-time scaling methods rely on fixed denoising schedules, limiting\ntheir ability to allocate computation based on instance difficulty or\ntask-specific demands adaptively. We introduce the challenge of adaptive\ninference-time scaling-dynamically adjusting computational effort during\ninference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a\nflexible, search-based inference framework. ABCD refines outputs through\nbi-directional diffusion cycles while adaptively controlling exploration depth\nand termination. It comprises three components: Cyclic Diffusion Search,\nAutomatic Exploration-Exploitation Balancing, and Adaptive Thinking Time.\nExperiments show that ABCD improves performance across diverse tasks while\nmaintaining computational efficiency.", "authors": ["Gyubin Lee", "Truong Nhat Nguyen Bao", "Jaesik Yoon", "Dongwoo Lee", "Minsu Kim", "Yoshua Bengio", "Sungjin Ahn"], "published_date": "2025-05-20", "title_zh": "自適應循環擴散用於推論規模調整", "summary_zh": "擴散模型在生成任務上表現出色，但現有的推論加速方法依賴固定降噪流程，無法根據任務難度或需求動態調整計算量。本文提出自適應推論規模調整的挑戰，並引入自適應雙向循環擴散(ABCD)框架。ABCD透過雙向擴散循環精煉輸出，並自適應控制探索深度和終止條件，包含循環擴散搜尋、自動探索-利用平衡和自適應思考時間三個部分。實驗證明，ABCD在多種任務上提升效能的同時，也保持了計算效率。", "audio": "audios/2505.14036v1.mp3", "timestamp": "2025-05-21T17:16:17.825422"}
{"query": "AI", "id": "2505.14613v1", "url": "http://arxiv.org/abs/2505.14613v1", "title": "Virtual Cells: Predict, Explain, Discover", "summary": "Drug discovery is fundamentally a process of inferring the effects of\ntreatments on patients, and would therefore benefit immensely from\ncomputational models that can reliably simulate patient responses, enabling\nresearchers to generate and test large numbers of therapeutic hypotheses safely\nand economically before initiating costly clinical trials. Even a more specific\nmodel that predicts the functional response of cells to a wide range of\nperturbations would be tremendously valuable for discovering safe and effective\ntreatments that successfully translate to the clinic. Creating such virtual\ncells has long been a goal of the computational research community that\nunfortunately remains unachieved given the daunting complexity and scale of\ncellular biology. Nevertheless, recent advances in AI, computing power, lab\nautomation, and high-throughput cellular profiling provide new opportunities\nfor reaching this goal. In this perspective, we present a vision for developing\nand evaluating virtual cells that builds on our experience at Recursion. We\nargue that in order to be a useful tool to discover novel biology, virtual\ncells must accurately predict the functional response of a cell to\nperturbations and explain how the predicted response is a consequence of\nmodifications to key biomolecular interactions. We then introduce key\nprinciples for designing therapeutically-relevant virtual cells, describe a\nlab-in-the-loop approach for generating novel insights with them, and advocate\nfor biologically-grounded benchmarks to guide virtual cell development.\nFinally, we make the case that our approach to virtual cells provides a useful\nframework for building other models at higher levels of organization, including\nvirtual patients. We hope that these directions prove useful to the research\ncommunity in developing virtual models optimized for positive impact on drug\ndiscovery outcomes.", "authors": ["Emmanuel Noutahi", "Jason Hartford", "Prudencio Tossou", "Shawn Whitfield", "Alisandra K. Denton", "Cas Wognum", "Kristina Ulicna", "Jonathan Hsu", "Michael Cuccarese", "Emmanuel Bengio", "Dominique Beaini", "Christopher Gibson", "Daniel Cohen", "Berton Earnshaw"], "published_date": "2025-05-20", "title_zh": "虛擬細胞：預測、解釋、發現", "summary_zh": "藥物發現的核心在於推斷治療對患者的影響。開發能夠可靠模擬患者反應的計算模型，將能幫助研究人員在昂貴的臨床試驗前，安全且經濟地產生和測試大量治療假設。本論文提出一個開發和評估「虛擬細胞」的願景，它能準確預測細胞對干擾的反應，並解釋其反應的生物分子機制。透過AI、運算能力、實驗室自動化和高通量細胞分析的進展，我們有機會打造出可用於發現安全有效療法的虛擬細胞，並最終建構更複雜的虛擬患者模型，從而加速藥物開發。", "audio": "audios/2505.14613v1.mp3", "timestamp": "2025-05-21T18:26:28.417346"}
{"query": "Foundation Model", "id": "2505.14100v1", "url": "http://arxiv.org/abs/2505.14100v1", "title": "Unlocking the Power of SAM 2 for Few-Shot Segmentation", "summary": "Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few\nclasses to segment arbitrary classes, but at the risk of overfitting. To\naddress this, some methods use the well-learned knowledge of foundation models\n(e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM\nby supporting video segmentation, whose class-agnostic matching ability is\nuseful to FSS. A simple idea is to encode support foreground (FG) features as\nmemory, with which query FG features are matched and fused. Unfortunately, the\nFG objects in different frames of SAM 2's video data are always the same\nidentity, while those in FSS are different identities, i.e., the matching step\nis incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo\nquery memory, matching with query features in a compatible way. However, the\nmemories can never be as accurate as the real ones, i.e., they are likely to\ncontain incomplete query FG, and some unexpected query background (BG)\nfeatures, leading to wrong segmentation. Hence, we further design Iterative\nMemory Refinement to fuse more query FG features into the memory, and devise a\nSupport-Calibrated Memory Attention to suppress the unexpected query BG\nfeatures in memory. Extensive experiments have been conducted on PASCAL-5$^i$\nand COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot\nmIoU can be 4.2\\% better than the best baseline.", "authors": ["Qianxiong Xu", "Lanyun Zhu", "Xuanyi Liu", "Guosheng Lin", "Cheng Long", "Ziyue Li", "Rui Zhao"], "published_date": "2025-05-20", "title_zh": "解鎖 SAM 2 在少樣本分割中的力量", "summary_zh": "少樣本分割的目標是學習類別無關的分割，但容易過擬合。本文利用SAM 2的影片分割能力，提出了一種新方法。核心挑戰是SAM 2訓練數據的目標在不同幀中是同一物體，而少樣本分割不是。因此，我們設計了偽提示生成器和迭代記憶體精煉，並提出了支持校準的記憶體注意力機制，以提升分割效果。實驗證明，我們的模型在PASCAL-5$^i$和COCO-20$^i$等數據集上表現優異，例如，1-shot mIoU 可以比最好的基線高出 4.2%。", "audio": "audios/2505.14100v1.mp3", "timestamp": "2025-05-21T18:26:32.856819"}
{"query": "Diffusion Model", "id": "2505.13983v1", "url": "http://arxiv.org/abs/2505.13983v1", "title": "Combining Deterministic Enhanced Conditions with Dual-Streaming Encoding for Diffusion-Based Speech Enhancement", "summary": "Diffusion-based speech enhancement (SE) models need to incorporate correct\nprior knowledge as reliable conditions to generate accurate predictions.\nHowever, providing reliable conditions using noisy features is challenging. One\nsolution is to use features enhanced by deterministic methods as conditions.\nHowever, the information distortion and loss caused by deterministic methods\nmight affect the diffusion process. In this paper, we first investigate the\neffects of using different deterministic SE models as conditions for diffusion.\nWe validate two conditions depending on whether the noisy feature was used as\npart of the condition: one using only the deterministic feature\n(deterministic-only), and the other using both deterministic and noisy features\n(deterministic-noisy). Preliminary investigation found that using deterministic\nenhanced conditions improves hearing experiences on real data, while the choice\nbetween using deterministic-only or deterministic-noisy conditions depends on\nthe deterministic models. Based on these findings, we propose a dual-streaming\nencoding Repair-Diffusion Model for SE (DERDM-SE) to more effectively utilize\nboth conditions. Moreover, we found that fine-grained deterministic models have\ngreater potential in objective evaluation metrics, while UNet-based\ndeterministic models provide more stable diffusion performance. Therefore, in\nthe DERDM-SE, we propose a deterministic model that combines coarse- and\nfine-grained processing. Experimental results on CHiME4 show that the proposed\nmodels effectively leverage deterministic models to achieve better SE\nevaluation scores, along with more stable performance compared to other\ndiffusion-based SE models.", "authors": ["Hao Shi", "Xugang Lu", "Kazuki Shimada", "Tatsuya Kawahara"], "published_date": "2025-05-20", "title_zh": "結合確定性增強條件與雙流編碼的基於擴散的語音增強", "summary_zh": "為了讓基於擴散的語音增強模型更準確，需要提供可靠的先驗知識作為條件。本研究探討使用確定性方法增強的特徵作為條件的效果，並發現加入確定性增強條件可以改善聽覺體驗。基於這些發現，我們提出了一種雙流編碼的修復擴散模型（DERDM-SE），能更有效地利用確定性增強條件。實驗結果表明，我們提出的模型能有效利用確定性模型，在語音增強評估中獲得更好的分數，且相較於其他基於擴散的語音增強模型，表現更穩定。", "audio": "audios/2505.13983v1.mp3", "timestamp": "2025-05-21T18:26:39.178977"}
{"query": "AI", "id": "2505.14612v1", "url": "http://arxiv.org/abs/2505.14612v1", "title": "AI Agents in the Electricity Market Game with Cryptocurrency Transactions: A Post-Terminator Analysis", "summary": "This paper extends (Spear 2003) by replacing human agents with artificial\nintelligence (AI) entities that derive utility solely from electricity\nconsumption. These AI agents must prepay for electricity using cryptocurrency\nand the verification of these transactions requires a fixed amount of\nelectricity. As a result the agents must strategically allocate electricity\nresources between consumption and payment verification. This paper analyzes the\nequilibrium outcomes of such a system and discusses the implications of\nAI-driven energy markets.", "authors": ["Microsoft Copilot", "Stephen E. Spear"], "published_date": "2025-05-20", "title_zh": "基於加密貨幣交易的電力市場博弈中的AI代理：後終結者時代分析", "summary_zh": "本研究將既有電力市場模型中的人類代理替換為完全依賴電力消費獲取效用的AI實體。這些AI代理必須使用加密貨幣預付電費，而交易驗證需要消耗一定量的電力。因此，AI代理需要在消費和支付驗證之間策略性地分配電力資源。本研究分析了該系統的均衡結果，並探討了AI驅動能源市場的影響。", "audio": "audios/2505.14612v1.mp3", "timestamp": "2025-05-21T19:14:36.343567"}
{"query": "Foundation Model", "id": "2505.14088v1", "url": "http://arxiv.org/abs/2505.14088v1", "title": "Generalizable Multispectral Land Cover Classification via Frequency-Aware Mixture of Low-Rank Token Experts", "summary": "We introduce Land-MoE, a novel approach for multispectral land cover\nclassification (MLCC). Spectral shift, which emerges from disparities in\nsensors and geospatial conditions, poses a significant challenge in this\ndomain. Existing methods predominantly rely on domain adaptation and\ngeneralization strategies, often utilizing small-scale models that exhibit\nlimited performance. In contrast, Land-MoE addresses these issues by\nhierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts,\nto fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner.\nSpecifically, Land-MoE comprises two key modules: the mixture of low-rank token\nexperts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages\nrank-differentiated tokens to generate diverse feature adjustments for\nindividual instances within multispectral images. By dynamically combining\nlearnable low-rank token experts of varying ranks, it enhances the robustness\nagainst spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on\nthe refined features. This process enables the model to effectively capture\nfrequency band information that is strongly correlated with semantic essence,\nwhile simultaneously suppressing frequency noise irrelevant to the task.\nComprehensive experiments on MLCC tasks involving cross-sensor and\ncross-geospatial setups demonstrate that Land-MoE outperforms existing methods\nby a large margin. Additionally, the proposed approach has also achieved\nstate-of-the-art performance in domain generalization semantic segmentation\ntasks of RGB remote sensing images.", "authors": ["Xi Chen", "Shen Yan", "Juelin Zhu", "Chen Chen", "Yu Liu", "Maojun Zhang"], "published_date": "2025-05-20", "title_zh": "基於頻率感知的低秩Token專家混合模型實現廣義多光譜地表覆蓋分類", "summary_zh": "Land-MoE 是一種新的多光譜地表覆蓋分類方法，旨在解決因傳感器和地理條件差異導致的光譜偏移問題。它通過層次化地插入一個頻率感知的低秩 Token 專家混合模型，以參數高效的方式微調視覺基礎模型。該方法包含低秩 Token 專家混合（MoLTE）和頻率感知濾波器（FAF）兩個關鍵模塊。MoLTE 利用不同秩的 Token 為多光譜圖像中的每個實例生成不同的特徵調整，增強了對光譜偏移的魯棒性。FAF 則對精煉後的特徵進行頻域調製，有效捕捉與語義本質密切相關的頻帶信息，同時抑制與任務無關的頻率噪聲。實驗結果表明，Land-MoE 在跨傳感器和跨地理空間的多光譜地表覆蓋分類任務中顯著優於現有方法，並在 RGB 遙感圖像的領域泛化語義分割任務中也取得了最先進的性能。", "audio": "audios/2505.14088v1.mp3", "timestamp": "2025-05-21T19:14:45.882789"}
{"query": "Diffusion Model", "id": "2505.13919v1", "url": "http://arxiv.org/abs/2505.13919v1", "title": "Predicting Dynamical Systems across Environments via Diffusive Model Weight Generation", "summary": "Data-driven methods offer an effective equation-free solution for predicting\nphysical dynamics. However, the same physical system can exhibit significantly\ndifferent dynamic behaviors in various environments. This causes prediction\nfunctions trained for specific environments to fail when transferred to unseen\nenvironments. Therefore, cross-environment prediction requires modeling the\ndynamic functions of different environments. In this work, we propose a model\nweight generation method, \\texttt{EnvAd-Diff}. \\texttt{EnvAd-Diff} operates in\nthe weight space of the dynamic function, generating suitable weights from\nscratch based on environmental condition for zero-shot prediction.\nSpecifically, we first train expert prediction functions on dynamic\ntrajectories from a limited set of visible environments to create a model zoo,\nthereby constructing sample pairs of prediction function weights and their\ncorresponding environments. Subsequently, we train a latent space diffusion\nmodel conditioned on the environment to model the joint distribution of weights\nand environments. Considering the lack of environmental prior knowledge in\nreal-world scenarios, we propose a physics-informed surrogate label to\ndistinguish different environments. Generalization experiments across multiple\nsystems demonstrate that a 1M parameter prediction function generated by\n\\texttt{EnvAd-Diff} outperforms a pre-trained 500M parameter foundation model.", "authors": ["Ruikun Li", "Huandong Wang", "Jingtao Ding", "Yuan Yuan", "Qingmin Liao", "Yong Li"], "published_date": "2025-05-20", "title_zh": "透過擴散模型權重生成跨環境預測動態系統", "summary_zh": "現有的資料驅動方法能有效預測物理動態，但在不同環境下，同一系統可能展現截然不同的行為，導致特定環境訓練的模型在未見環境失效。為解決跨環境預測問題，我們提出EnvAd-Diff，在動態函數的權重空間中，根據環境條件從頭生成合適的權重，以實現零樣本預測。我們訓練一系列基於可見環境數據的專家模型，建立模型庫，並以此構建權重和環境的樣本對。接著，我們訓練一個以環境為條件的潛空間擴散模型，學習權重和環境的聯合分佈。針對真實環境缺乏先驗知識的問題，我們提出物理資訊代理標籤來區分不同環境。實驗結果表明，EnvAd-Diff生成的100萬參數預測函數優於預訓練的5億參數基礎模型。", "audio": "audios/2505.13919v1.mp3", "timestamp": "2025-05-21T19:14:51.897428"}
{"query": "AI", "id": "2505.14588v1", "url": "http://arxiv.org/abs/2505.14588v1", "title": "Generative AI at the Crossroads: Light Bulb, Dynamo, or Microscope?", "summary": "With the advent of generative AI (genAI), the potential scope of artificial\nintelligence has increased dramatically, but the future effect of genAI on\nproductivity remains uncertain with the effect of the technology on the\ninnovation process a crucial open question. Some labor-saving innovations, such\nas the light bulb, temporarily raise productivity growth as adoption spreads,\nbut the effect fades when the market is saturated; that is, the level of output\nper hour is permanently higher but the growth rate is not. In contrast, two\ntypes of technologies stand out as having longer-lived effects on productivity\ngrowth. First, there are technologies known as general-purpose technologies\n(GPTs). GPTs are (1) widely adopted, (2) spur abundant knock-on innovations\n(new goods and services, process efficiencies, and business reorganization),\nand (3) improve continuously, refreshing this innovation cycle; the electric\ndynamo is an example. Second, there are inventions of methods of invention\n(IMIs). IMIs increase the efficiency of the research and development process,\ngenerating new ideas more quickly and cheaply; the compound microscope is an\nexample. We show that GenAI has the characteristics of both a GPT and an IMI --\nan encouraging sign. Even so, for genAI to boost productivity growth, its\ncontribution will have to exceed the fading growth effects of past IT\ninnovations baked into the trend, including predecessor AI technologies.", "authors": ["Martin Baily", "David Byrne", "Aidan Kane", "Paul Soto"], "published_date": "2025-05-20", "title_zh": "生成式人工智慧在十字路口：燈泡、發電機，還是顯微鏡？", "summary_zh": "生成式人工智慧（GenAI）的出現大幅拓展了人工智慧的潛力範圍，但其對生產力的未來影響仍不明朗，對創新過程的影響尤其關鍵。論文探討GenAI是否像燈泡一樣，僅短暫提高生產力後趨於平緩；還是像通用技術（GPT）如發電機，持續推動創新；又或者像發明方法的發明（IMI）如顯微鏡，加速研發進程。研究發現GenAI兼具GPT和IMI的特性，這是個鼓舞人心的跡象。然而，要提升生產力增長，GenAI的貢獻必須超越過去IT創新（包括先前的AI技術）所帶來的趨勢衰退效應。", "audio": "audios/2505.14588v1.mp3", "timestamp": "2025-05-21T20:20:15.738409"}
{"query": "Foundation Model", "id": "2505.14042v1", "url": "http://arxiv.org/abs/2505.14042v1", "title": "Adversarially Pretrained Transformers may be Universally Robust In-Context Learners", "summary": "Adversarial training is one of the most effective adversarial defenses, but\nit incurs a high computational cost. In this study, we show that transformers\nadversarially pretrained on diverse tasks can serve as robust foundation models\nand eliminate the need for adversarial training in downstream tasks.\nSpecifically, we theoretically demonstrate that through in-context learning, a\nsingle adversarially pretrained transformer can robustly generalize to multiple\nunseen tasks without any additional training, i.e., without any parameter\nupdates. This robustness stems from the model's focus on robust features and\nits resistance to attacks that exploit non-predictive features. Besides these\npositive findings, we also identify several limitations. Under certain\nconditions (though unrealistic), no universally robust single-layer\ntransformers exist. Moreover, robust transformers exhibit an\naccuracy--robustness trade-off and require a large number of in-context\ndemonstrations. The code is available at\nhttps://github.com/s-kumano/universally-robust-in-context-learner.", "authors": ["Soichiro Kumano", "Hiroshi Kera", "Toshihiko Yamasaki"], "published_date": "2025-05-20", "title_zh": "對抗式預訓練的變換器可能成為通用的穩健上下文學習器", "summary_zh": "對抗式訓練是有效的防禦手段，但成本高昂。本研究發現，在多樣化任務上進行對抗式預訓練的變換器，可作為穩健的基礎模型，無需在下游任務中額外進行對抗式訓練。理論上證明，透過上下文學習，單一對抗式預訓練的變換器，可以在沒有任何參數更新的情況下，穩健地泛化到多個未見過的任務。這種穩健性來自模型對穩健特徵的關注以及對利用非預測性特徵攻擊的抵抗力。研究也指出一些限制，例如在特定條件下，不存在通用的穩健單層變換器，且穩健的變換器需要在準確性和穩健性之間權衡，並需要大量的上下文示範。", "audio": "audios/2505.14042v1.mp3", "timestamp": "2025-05-21T20:20:22.156616"}
{"query": "Diffusion Model", "id": "2505.13843v1", "url": "http://arxiv.org/abs/2505.13843v1", "title": "A Semantic Information-based Hierarchical Speech Enhancement Method Using Factorized Codec and Diffusion Model", "summary": "Most current speech enhancement (SE) methods recover clean speech from noisy\ninputs by directly estimating time-frequency masks or spectrums. However, these\napproaches often neglect the distinct attributes, such as semantic content and\nacoustic details, inherent in speech signals, which can hinder performance in\ndownstream tasks. Moreover, their effectiveness tends to degrade in complex\nacoustic environments. To overcome these challenges, we propose a novel,\nsemantic information-based, step-by-step factorized SE method using factorized\ncodec and diffusion model. Unlike traditional SE methods, our hierarchical\nmodeling of semantic and acoustic attributes enables more robust clean speech\nrecovery, particularly in challenging acoustic scenarios. Moreover, this method\noffers further advantages for downstream TTS tasks. Experimental results\ndemonstrate that our algorithm not only outperforms SOTA baselines in terms of\nspeech quality but also enhances TTS performance in noisy environments.", "authors": ["Yang Xiang", "Canan Huang", "Desheng Hu", "Jingguang Tian", "Xinhui Hu", "Chao Zhang"], "published_date": "2025-05-20", "title_zh": "基於語義訊息的階層式語音增強方法，採用分解式編解碼器與擴散模型", "summary_zh": "現有的語音增強方法多半直接從嘈雜的訊號中估計時頻遮罩或頻譜來還原乾淨語音，忽略了語音訊號中獨特的語義內容和聲學細節，這會影響後續任務的表現。我們提出一種新的方法，利用分解式編解碼器和擴散模型，分階段、以語義訊息為基礎進行語音增強。這種階層式建模方式更能還原乾淨語音，尤其是在複雜的聲學環境中，而且還能提升後續的語音合成任務表現。實驗結果顯示，我們的演算法不僅在語音品質上優於現有方法，還能在嘈雜的環境中提升語音合成的性能。", "audio": "audios/2505.13843v1.mp3", "timestamp": "2025-05-21T20:20:27.423795"}
{"query": "AI", "id": "2505.14585v1", "url": "http://arxiv.org/abs/2505.14585v1", "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning", "summary": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +17.64% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively.", "authors": ["Wenbin Hu", "Haoran Li", "Huihao Jing", "Qi Hu", "Ziqian Zeng", "Sirui Han", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "published_date": "2025-05-20", "title_zh": "Context Reasoner：透過強化學習激勵情境化隱私與安全合規的推理能力", "summary_zh": "大型語言模型雖強大，卻也帶來安全和隱私風險。現有防護策略往往犧牲情境推理能力，過度依賴敏感模式匹配，且忽略現有的安全和隱私標準。本研究將安全和隱私問題轉化為情境化合規問題，並以情境完整性理論為基礎，對齊GDPR、歐盟AI法案和HIPAA等規範。我們利用強化學習，以規則為基礎的獎勵來激勵模型的情境推理能力，同時提升安全和隱私合規性。實驗證明，此方法不僅顯著提高法律合規性，更增強了一般的推理能力。", "audio": "audios/2505.14585v1.mp3", "timestamp": "2025-05-21T21:17:12.423871"}
{"query": "Foundation Model", "id": "2505.13840v1", "url": "http://arxiv.org/abs/2505.13840v1", "title": "EfficientLLM: Efficiency in Large Language Models", "summary": "Large Language Models (LLMs) have driven significant progress, yet their\ngrowing parameter counts and context windows incur prohibitive compute, energy,\nand monetary costs. We introduce EfficientLLM, a novel benchmark and the first\ncomprehensive empirical study evaluating efficiency techniques for LLMs at\nscale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our\nstudy systematically explores three key axes: (1) architecture pretraining\n(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts\n(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and\n(3) inference (quantization methods: int4, float16). We define six fine-grained\nmetrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy\nConsumption, Compression Rate) to capture hardware saturation,\nlatency-throughput balance, and carbon cost. Evaluating over 100\nmodel-technique pairs (0.5B-72B parameters), we derive three core insights: (i)\nEfficiency involves quantifiable trade-offs: no single method is universally\noptimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by\n40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%\naccuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal\nmemory-latency trade-offs for constrained devices, MLA achieves lowest\nperplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency\nonly beyond 14B parameters. (iii) Techniques generalize across modalities: we\nextend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and\nVision-Language Models (Qwen2.5-VL), confirming effective transferability. By\nopen-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM\nprovides essential guidance for researchers and engineers navigating the\nefficiency-performance landscape of next-generation foundation models.", "authors": ["Zhengqing Yuan", "Weixiang Sun", "Yixin Liu", "Huichi Zhou", "Rong Zhou", "Yiyang Li", "Zheyuan Zhang", "Wei Song", "Yue Huang", "Haolong Jia", "Keerthiram Murugesan", "Yu Wang", "Lifang He", "Jianfeng Gao", "Lichao Sun", "Yanfang Ye"], "published_date": "2025-05-20", "title_zh": "高效LLM：大型語言模型效率研究", "summary_zh": "大型語言模型雖然帶來突破，但參數量和上下文窗口的增長導致成本過高。本文推出EfficientLLM基準測試，對大型語言模型的效率提升技術進行大規模評估。研究涵蓋架構預訓練、微調和推理三個主要方向，並定義了六個細粒度指標來衡量硬體利用率、延遲吞吐量平衡和碳排放成本。研究發現，效率提升涉及權衡取捨，沒有萬能方法；最佳方案取決於任務和模型規模；這些技術也能推廣到視覺和視覺語言模型上。EfficientLLM開放資料集和評估流程，為研究者和工程師在下一代基礎模型的效率與性能之間做出選擇提供指引。", "audio": "audios/2505.13840v1.mp3", "timestamp": "2025-05-21T21:17:21.216998"}
{"query": "Diffusion Model", "id": "2505.13791v1", "url": "http://arxiv.org/abs/2505.13791v1", "title": "Scalable Autoregressive 3D Molecule Generation", "summary": "Generative models of 3D molecular structure play a rapidly growing role in\nthe design and simulation of molecules. Diffusion models currently dominate the\nspace of 3D molecule generation, while autoregressive models have trailed\nbehind. In this work, we present Quetzal, a simple but scalable autoregressive\nmodel that builds molecules atom-by-atom in 3D. Treating each molecule as an\nordered sequence of atoms, Quetzal combines a causal transformer that predicts\nthe next atom's discrete type with a smaller Diffusion MLP that models the\ncontinuous next-position distribution. Compared to existing autoregressive\nbaselines, Quetzal achieves substantial improvements in generation quality and\nis competitive with the performance of state-of-the-art diffusion models. In\naddition, by reducing the number of expensive forward passes through a dense\ntransformer, Quetzal enables significantly faster generation speed, as well as\nexact divergence-based likelihood computation. Finally, without any\narchitectural changes, Quetzal natively handles variable-size tasks like\nhydrogen decoration and scaffold completion. We hope that our work motivates a\nperspective on scalability and generality for generative modelling of 3D\nmolecules.", "authors": ["Austin H. Cheng", "Chong Sun", "Alán Aspuru-Guzik"], "published_date": "2025-05-20", "title_zh": "可擴展的自迴歸三維分子生成", "summary_zh": "分子結構的生成模型在分子設計和模擬中扮演著越來越重要的角色。目前擴散模型主導了三維分子生成領域，而自迴歸模型則相對落後。這篇論文介紹了 Quetzal，一個簡單但可擴展的自迴歸模型，能夠以原子為單位逐個構建三維分子。Quetzal 將每個分子視為一個有序的原子序列，結合了一個預測下一個原子離散類型的因果轉換器，以及一個較小的擴散 MLP 模型，用於模擬下一個位置的連續分佈。Quetzal 在生成品質上顯著優於現有的自迴歸模型，並且性能可與最先進的擴散模型相媲美。此外，Quetzal 透過減少密集轉換器的前向傳播次數，顯著提高了生成速度，並且能夠進行基於散度的精確似然計算。最後，Quetzal 在不改變架構的情況下，可以自然地處理氫原子裝飾和骨架補全等可變大小的任務。這項工作旨在激發人們對三維分子生成模型的可擴展性和通用性的思考。", "audio": "audios/2505.13791v1.mp3", "timestamp": "2025-05-21T21:17:51.909900"}
{"query": "AI", "id": "2505.14569v1", "url": "http://arxiv.org/abs/2505.14569v1", "title": "Agent Context Protocols Enhance Collective Inference", "summary": "AI agents have become increasingly adept at complex tasks such as coding,\nreasoning, and multimodal understanding. However, building generalist systems\nrequires moving beyond individual agents to collective inference -- a paradigm\nwhere multi-agent systems with diverse, task-specialized agents complement one\nanother through structured communication and collaboration. Today, coordination\nis usually handled with imprecise, ad-hoc natural language, which limits\ncomplex interaction and hinders interoperability with domain-specific agents.\nWe introduce Agent context protocols (ACPs): a domain- and agent-agnostic\nfamily of structured protocols for agent-agent communication, coordination, and\nerror handling. ACPs combine (i) persistent execution blueprints -- explicit\ndependency graphs that store intermediate agent outputs -- with (ii)\nstandardized message schemas, enabling robust and fault-tolerant multi-agent\ncollective inference. ACP-powered generalist systems reach state-of-the-art\nperformance: 28.3 % accuracy on AssistantBench for long-horizon web assistance\nand best-in-class multimodal technical reports, outperforming commercial AI\nsystems in human evaluation. ACPs are highly modular and extensible, allowing\npractitioners to build top-tier generalist agents quickly.", "authors": ["Devansh Bhardwaj", "Arjun Beniwal", "Shreyas Chaudhari", "Ashwin Kalyan", "Tanmay Rajpurohit", "Karthik R. Narasimhan", "Ameet Deshpande", "Vishvak Murahari"], "published_date": "2025-05-20", "title_zh": "代理上下文協議增強集體推理", "summary_zh": "AI 代理在編碼、推理和多模態理解等複雜任務中越來越強大。要打造通用系統，需要超越單個代理，轉向集體推理——一種多代理系統，其中不同且專精於特定任務的代理通過結構化的溝通和協作來互補。目前，協調通常依賴於不精確、臨時性的自然語言，這限制了複雜的互動，也阻礙了與特定領域代理的互操作性。為此，我們引入代理上下文協議 (ACPs)：一種領域和代理無關的結構化協議族，用於代理之間的通訊、協調和錯誤處理。ACPs 結合了 (i) 持續的執行藍圖 (顯式的依賴關係圖，用於儲存中間代理的輸出)，以及 (ii) 標準化的訊息架構，從而實現穩健且容錯的多代理集體推理。使用 ACPs 的通用系統達到了最先進的性能：在 AssistantBench 上，針對長週期網絡助手達到 28.3% 的準確率，並且在多模態技術報告方面表現最佳，在人類評估中優於商業 AI 系統。ACPs 具有高度的模組化和可擴展性，使從業者能夠快速構建頂級的通用代理。", "audio": "audios/2505.14569v1.mp3", "timestamp": "2025-05-21T22:17:50.906228"}
{"query": "Foundation Model", "id": "2505.13755v1", "url": "http://arxiv.org/abs/2505.13755v1", "title": "Panda: A pretrained forecast model for universal representation of chaotic dynamics", "summary": "Chaotic systems are intrinsically sensitive to small errors, challenging\nefforts to construct predictive data-driven models of real-world dynamical\nsystems such as fluid flows or neuronal activity. Prior efforts comprise either\nspecialized models trained separately on individual time series, or foundation\nmodels trained on vast time series databases with little underlying dynamical\nstructure. Motivated by dynamical systems theory, we present Panda, Patched\nAttention for Nonlinear DynAmics. We train Panda on a novel synthetic,\nextensible dataset of $2 \\times 10^4$ chaotic dynamical systems that we\ndiscover using an evolutionary algorithm. Trained purely on simulated data,\nPanda exhibits emergent properties: zero-shot forecasting of unseen real world\nchaotic systems, and nonlinear resonance patterns in cross-channel attention\nheads. Despite having been trained only on low-dimensional ordinary\ndifferential equations, Panda spontaneously develops the ability to predict\npartial differential equations without retraining. We demonstrate a neural\nscaling law for differential equations, underscoring the potential of\npretrained models for probing abstract mathematical domains like nonlinear\ndynamics.", "authors": ["Jeffrey Lai", "Anthony Bao", "William Gilpin"], "published_date": "2025-05-19", "title_zh": "Panda：用於混沌動力學通用表示的預訓練預測模型", "summary_zh": "混沌系統對微小誤差極為敏感，這對構建預測性的資料驅動模型構成挑戰。本研究提出 Panda 模型，利用演化算法發現的大量合成混沌動力學系統進行訓練。Panda 展現出驚人的能力，例如零樣本預測未見過的真實世界混沌系統，以及在跨通道注意力頭中呈現非線性共振模式。更令人驚訝的是，Panda 在僅接受低維常微分方程訓練後，無需重新訓練即可預測偏微分方程。研究還展示了微分方程的神經縮放定律，突顯了預訓練模型在探索非線性動力學等抽象數學領域的潛力。", "audio": "audios/2505.13755v1.mp3", "timestamp": "2025-05-21T22:18:10.039919"}
{"query": "Diffusion Model", "id": "2505.13771v1", "url": "http://arxiv.org/abs/2505.13771v1", "title": "Score-Based Training for Energy-Based TTS Models", "summary": "Noise contrastive estimation (NCE) is a popular method for training\nenergy-based models (EBM) with intractable normalisation terms. The key idea of\nNCE is to learn by comparing unnormalised log-likelihoods of the reference and\nnoisy samples, thus avoiding explicitly computing normalisation terms. However,\nNCE critically relies on the quality of noisy samples. Recently, sliced score\nmatching (SSM) has been popularised by closely related diffusion models (DM).\nUnlike NCE, SSM learns a gradient of log-likelihood, or score, by learning\ndistribution of its projections on randomly chosen directions. However, both\nNCE and SSM disregard the form of log-likelihood function, which is problematic\ngiven that EBMs and DMs make use of first-order optimisation during inference.\nThis paper proposes a new criterion that learns scores more suitable for\nfirst-order schemes. Experiments contrasts these approaches for training EBMs.", "authors": ["Wanli Sun", "Anton Ragni"], "published_date": "2025-05-19", "title_zh": "基於分數的能量型語音合成模型訓練", "summary_zh": "這篇論文探討能量型語音合成模型的訓練方法。能量型模型訓練的一大挑戰是難以計算歸一化項。論文比較了噪音對比估計 (NCE) 和分片分數匹配 (SSM) 等方法，前者透過比較真實和噪音樣本的未歸一化對數似然來學習，後者則學習對數似然梯度的投影分佈。然而，這些方法忽略了對數似然函數的形式，這在推理時使用一階最佳化的能量型模型中是個問題。因此，論文提出了一種新的訓練準則，旨在學習更適合一階方法的梯度，並通過實驗比較不同方法在能量型模型訓練上的效果。", "audio": "audios/2505.13771v1.mp3", "timestamp": "2025-05-21T22:18:28.330206"}
{"query": "AI", "id": "2505.14539v1", "url": "http://arxiv.org/abs/2505.14539v1", "title": "A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)", "summary": "In this work, we present the first general logic of attention. Attention is a\npowerful cognitive ability that allows agents to focus on potentially complex\ninformation, such as logically structured propositions, higher-order beliefs,\nor what other agents pay attention to. This ability is a strength, as it helps\nto ignore what is irrelevant, but it can also introduce biases when some types\nof information or agents are systematically ignored. Existing dynamic epistemic\nlogics for attention cannot model such complex attention scenarios, as they\nonly model attention to atomic formulas. Additionally, such logics quickly\nbecome cumbersome, as their size grows exponentially in the number of agents\nand announced literals. Here, we introduce a logic that overcomes both\nlimitations. First, we generalize edge-conditioned event models, which we show\nto be as expressive as standard event models yet exponentially more succinct\n(generalizing both standard event models and generalized arrow updates).\nSecond, we extend attention to arbitrary formulas, allowing agents to also\nattend to other agents' beliefs or attention. Our work treats attention as a\nmodality, like belief or awareness. We introduce attention principles that\nimpose closure properties on that modality and that can be used in its\naxiomatization. Throughout, we illustrate our framework with examples of AI\nagents reasoning about human attentional biases, demonstrating how such agents\ncan discover attentional biases.", "authors": ["Gaia Belardinelli", "Thomas Bolander", "Sebastian Watzl"], "published_date": "2025-05-20", "title_zh": "基於邊緣條件事件模型的一般注意力邏輯（擴展版本）", "summary_zh": "這篇論文提出了一種全新的、更通用化的注意力邏輯。傳統的注意力模型只能處理簡單的原子公式，而這個新邏輯可以處理更複雜的資訊，例如邏輯命題、高階信念，甚至是其他人的注意力。為了實現這一點，研究人員使用了邊緣條件事件模型，這種模型更簡潔，表達能力也更強。這個新邏輯將注意力視為一種模態，就像信念或意識一樣，並且可以捕捉人類注意力偏差的特性。文章用AI智能體推理人類注意力偏差的例子，展示了這個框架的應用。", "audio": "audios/2505.14539v1.mp3", "timestamp": "2025-05-21T23:17:25.202342"}
{"query": "Foundation Model", "id": "2505.13099v2", "url": "http://arxiv.org/abs/2505.13099v2", "title": "Industrial Synthetic Segment Pre-training", "summary": "Pre-training on real-image datasets has been widely proven effective for\nimproving instance segmentation. However, industrial applications face two key\nchallenges: (1) legal and ethical restrictions, such as ImageNet's prohibition\nof commercial use, and (2) limited transferability due to the domain gap\nbetween web images and industrial imagery. Even recent vision foundation\nmodels, including the segment anything model (SAM), show notable performance\ndegradation in industrial settings. These challenges raise critical questions:\nCan we build a vision foundation model for industrial applications without\nrelying on real images or manual annotations? And can such models outperform\neven fine-tuned SAM on industrial datasets? To address these questions, we\npropose the Instance Core Segmentation Dataset (InsCore), a synthetic\npre-training dataset based on formula-driven supervised learning (FDSL).\nInsCore generates fully annotated instance segmentation images that reflect key\ncharacteristics of industrial data, including complex occlusions, dense\nhierarchical masks, and diverse non-rigid shapes, distinct from typical web\nimagery. Unlike previous methods, InsCore requires neither real images nor\nhuman annotations. Experiments on five industrial datasets show that models\npre-trained with InsCore outperform those trained on COCO and ImageNet-21k, as\nwell as fine-tuned SAM, achieving an average improvement of 6.2 points in\ninstance segmentation performance. This result is achieved using only 100k\nsynthetic images, more than 100 times fewer than the 11 million images in SAM's\nSA-1B dataset, demonstrating the data efficiency of our approach. These\nfindings position InsCore as a practical and license-free vision foundation\nmodel for industrial applications.", "authors": ["Shinichi Mae", "Ryousuke Yamada", "Hirokatsu Kataoka"], "published_date": "2025-05-19", "title_zh": "工業合成分割預訓練", "summary_zh": "現有的圖像分割模型在工業應用中面臨授權限制和領域差異的挑戰。本研究提出一個基於公式驅動的合成數據集InsCore，無需真實圖像或人工標注，即可預訓練出專為工業應用設計的圖像分割模型。實驗證明，用InsCore預訓練的模型在多個工業數據集上的表現，優於用COCO和ImageNet預訓練的模型，甚至超越了微調後的SAM模型，且只需少量合成數據就能達到很好的效果。InsCore為工業應用提供了一個實用且無授權問題的視覺基礎模型。", "audio": "audios/2505.13099v2.mp3", "timestamp": "2025-05-21T23:17:30.656713"}
{"query": "Diffusion Model", "id": "2505.13740v1", "url": "http://arxiv.org/abs/2505.13740v1", "title": "Improving Compositional Generation with Diffusion Models Using Lift Scores", "summary": "We introduce a novel resampling criterion using lift scores, for improving\ncompositional generation in diffusion models. By leveraging the lift scores, we\nevaluate whether generated samples align with each single condition and then\ncompose the results to determine whether the composed prompt is satisfied. Our\nkey insight is that lift scores can be efficiently approximated using only the\noriginal diffusion model, requiring no additional training or external modules.\nWe develop an optimized variant that achieves relatively lower computational\noverhead during inference while maintaining effectiveness. Through extensive\nexperiments, we demonstrate that lift scores significantly improved the\ncondition alignment for compositional generation across 2D synthetic data,\nCLEVR position tasks, and text-to-image synthesis. Our code is available at\nhttp://github.com/rainorangelemon/complift.", "authors": ["Chenning Yu", "Sicun Gao"], "published_date": "2025-05-19", "title_zh": "使用Lift分數改進擴散模型中的組合生成", "summary_zh": "本研究提出一種新的重採樣標準，利用Lift分數來提升擴散模型的組合生成能力。透過Lift分數評估生成樣本與各個條件的對齊程度，進而判斷組合提示詞是否被滿足。此方法僅需原始擴散模型即可有效近似計算Lift分數，無需額外訓練或模組。實驗證明，Lift分數能顯著改善二維合成數據、CLEVR位置任務和文本生成圖像等場景中的條件對齊效果。", "audio": "audios/2505.13740v1.mp3", "timestamp": "2025-05-21T23:17:37.788568"}
{"query": "AI", "id": "2505.15811v1", "url": "http://arxiv.org/abs/2505.15811v1", "title": "On the creation of narrow AI: hierarchy and nonlocality of neural network skills", "summary": "We study the problem of creating strong, yet narrow, AI systems. While recent\nAI progress has been driven by the training of large general-purpose foundation\nmodels, the creation of smaller models specialized for narrow domains could be\nvaluable for both efficiency and safety. In this work, we explore two\nchallenges involved in creating such systems, having to do with basic\nproperties of how neural networks learn and structure their representations.\nThe first challenge regards when it is possible to train narrow models from\nscratch. Through experiments on a synthetic task, we find that it is sometimes\nnecessary to train networks on a wide distribution of data to learn certain\nnarrow skills within that distribution. This effect arises when skills depend\non each other hierarchically, and training on a broad distribution introduces a\ncurriculum which substantially accelerates learning. The second challenge\nregards how to transfer particular skills from large general models into small\nspecialized models. We find that model skills are often not perfectly localized\nto a particular set of prunable components. However, we find that methods based\non pruning can still outperform distillation. We investigate the use of a\nregularization objective to align desired skills with prunable components while\nunlearning unnecessary skills.", "authors": ["Eric J. Michaud", "Asher Parker-Sartori", "Max Tegmark"], "published_date": "2025-05-21", "title_zh": "關於窄人工智慧的創建：神經網路技能的層次性和非局部性", "summary_zh": "本文探討如何創建強大但專精於特定領域的窄人工智慧系統。研究發現，從頭訓練窄領域模型有時需要先在大範圍的數據上訓練，以建立技能間的層次結構，加速學習。此外，將大型通用模型的技能轉移到小型專精模型時，技能並非總是完美地集中在可修剪的組件上。儘管如此，修剪方法仍優於知識蒸餾。研究進一步探討如何透過正則化，將期望的技能與可修剪的組件對齊，同時移除不必要的技能。", "audio": "audios/2505.15811v1.mp3", "timestamp": "2025-05-22T03:10:54.678837"}
{"query": "Foundation Model", "id": "2505.15809v1", "url": "http://arxiv.org/abs/2505.15809v1", "title": "MMaDA: Multimodal Large Diffusion Language Models", "summary": "We introduce MMaDA, a novel class of multimodal diffusion foundation models\ndesigned to achieve superior performance across diverse domains such as textual\nreasoning, multimodal understanding, and text-to-image generation. The approach\nis distinguished by three key innovations: (i) MMaDA adopts a unified diffusion\narchitecture with a shared probabilistic formulation and a modality-agnostic\ndesign, eliminating the need for modality-specific components. This\narchitecture ensures seamless integration and processing across different data\ntypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning\nstrategy that curates a unified CoT format across modalities. By aligning\nreasoning processes between textual and visual domains, this strategy\nfacilitates cold-start training for the final reinforcement learning (RL)\nstage, thereby enhancing the model's ability to handle complex tasks from the\noutset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm\nspecifically tailored for diffusion foundation models. Utilizing diversified\nreward modeling, UniGRPO unifies post-training across both reasoning and\ngeneration tasks, ensuring consistent performance improvements. Experimental\nresults demonstrate that MMaDA-8B exhibits strong generalization capabilities\nas a unified multimodal foundation model. It surpasses powerful models like\nLLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in\nmultimodal understanding, and excels over SDXL and Janus in text-to-image\ngeneration. These achievements highlight MMaDA's effectiveness in bridging the\ngap between pretraining and post-training within unified diffusion\narchitectures, providing a comprehensive framework for future research and\ndevelopment. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA", "authors": ["Ling Yang", "Ye Tian", "Bowen Li", "Xinchen Zhang", "Ke Shen", "Yunhai Tong", "Mengdi Wang"], "published_date": "2025-05-21", "title_zh": "MMaDA：多模態大型擴散語言模型", "summary_zh": "MMaDA是一種新的多模態擴散模型，旨在提升文字推理、多模態理解和文字生成圖像等領域的表現。它採用統一的擴散架構，無需針對特定模態的元件。透過混合式的長鏈思考微調策略，統一不同模態的推理過程。更提出UniGRPO，一種基於策略梯度的強化學習演算法，適用於擴散模型，能統一推理和生成任務的後訓練。實驗結果顯示，MMaDA在文字推理、多模態理解和文字生成圖像方面都超越了其他模型，證明其作為統一多模態模型的強大泛化能力。", "audio": "audios/2505.15809v1.mp3", "timestamp": "2025-05-22T03:10:59.380909"}
{"query": "Diffusion Model", "id": "2505.15812v1", "url": "http://arxiv.org/abs/2505.15812v1", "title": "Leveraging the Powerful Attention of a Pre-trained Diffusion Model for Exemplar-based Image Colorization", "summary": "Exemplar-based image colorization aims to colorize a grayscale image using a\nreference color image, ensuring that reference colors are applied to\ncorresponding input regions based on their semantic similarity. To achieve\naccurate semantic matching between regions, we leverage the self-attention\nmodule of a pre-trained diffusion model, which is trained on a large dataset\nand exhibits powerful attention capabilities. To harness this power, we propose\na novel, fine-tuning-free approach based on a pre-trained diffusion model,\nmaking two key contributions. First, we introduce dual attention-guided color\ntransfer. We utilize the self-attention module to compute an attention map\nbetween the input and reference images, effectively capturing semantic\ncorrespondences. The color features from the reference image is then\ntransferred to the semantically matching regions of the input image, guided by\nthis attention map, and finally, the grayscale features are replaced with the\ncorresponding color features. Notably, we utilize dual attention to calculate\nattention maps separately for the grayscale and color images, achieving more\nprecise semantic alignment. Second, we propose classifier-free colorization\nguidance, which enhances the transferred colors by combining color-transferred\nand non-color-transferred outputs. This process improves the quality of\ncolorization. Our experimental results demonstrate that our method outperforms\nexisting techniques in terms of image quality and fidelity to the reference.\nSpecifically, we use 335 input-reference pairs from previous research,\nachieving an FID of 95.27 (image quality) and an SI-FID of 5.51 (fidelity to\nthe reference). Our source code is available at\nhttps://github.com/satoshi-kosugi/powerful-attention.", "authors": ["Satoshi Kosugi"], "published_date": "2025-05-21", "title_zh": "利用預訓練擴散模型強大的注意力機制進行範例式圖像著色", "summary_zh": "範例式圖像著色旨在利用參考彩色圖像為灰度圖像著色，確保參考顏色根據其語義相似性應用到相應的輸入區域。為了實現精確的區域語義匹配，我們利用預訓練擴散模型的自注意力模塊，該模型在大型數據集上訓練，展現了強大的注意力能力。我們提出了一種基於預訓練擴散模型的新穎、免微調方法，做出了兩個主要貢獻：雙重注意力引導的顏色傳輸，利用自注意力模塊計算輸入和參考圖像之間的注意力圖，有效捕捉語義對應關係，並將參考圖像的顏色特徵傳輸到輸入圖像的語義匹配區域；以及無分類器顏色引導，通過結合顏色傳輸和非顏色傳輸的輸出，增強傳輸的顏色，提高著色質量。實驗結果表明，我們的方法在圖像質量和參考保真度方面優於現有技術。我們使用先前研究中的335個輸入-參考對，实现了95.27的FID（图像质量）和5.51的SI-FID（参考保真度）。", "audio": "audios/2505.15812v1.mp3", "timestamp": "2025-05-22T03:11:06.284321"}
{"query": "AI", "id": "2505.15799v1", "url": "http://arxiv.org/abs/2505.15799v1", "title": "The Agentic Economy", "summary": "Generative AI has transformed human-computer interaction by enabling natural\nlanguage interfaces and the emergence of autonomous agents capable of acting on\nusers' behalf. While early applications have improved individual productivity,\nthese gains have largely been confined to predefined tasks within existing\nworkflows. We argue that the more profound economic impact lies in reducing\ncommunication frictions between consumers and businesses. This shift could\nreorganize markets, redistribute power, and catalyze the creation of new\nproducts and services. We explore the implications of an agentic economy, where\nassistant agents act on behalf of consumers and service agents represent\nbusinesses, interacting programmatically to facilitate transactions. A key\ndistinction we draw is between unscripted interactions -- enabled by technical\nadvances in natural language and protocol design -- and unrestricted\ninteractions, which depend on market structures and governance. We examine the\ncurrent limitations of siloed and end-to-end agents, and explore future\nscenarios shaped by technical standards and market dynamics. These include the\npotential tension between agentic walled gardens and an open web of agents,\nimplications for advertising and discovery, the evolution of\nmicro-transactions, and the unbundling and rebundling of digital goods.\nUltimately, we argue that the architecture of agentic communication will\ndetermine the extent to which generative AI democratizes access to economic\nopportunity.", "authors": ["David M. Rothschild", "Markus Mobius", "Jake M. Hofman", "Eleanor W. Dillon", "Daniel G. Goldstein", "Nicole Immorlica", "Sonia Jaffe", "Brendan Lucier", "Aleksandrs Slivkins", "Matthew Vogel"], "published_date": "2025-05-21", "title_zh": "代理經濟", "summary_zh": "生成式AI透過自然語言介面和自主代理，大幅改變人機互動。論文認為，其更深遠的經濟影響在於降低消費者與企業之間的溝通障礙，進而重塑市場、重新分配權力，並催生新產品和服務。論文探討代理經濟，消費者代理代表用戶，服務代理代表企業，透過程式化互動促進交易。關鍵區別在於，技術進步促成的「無腳本互動」與取決於市場結構和治理的「無限制互動」。論文分析了現有代理的局限性，並探討技術標準和市場動態塑造的未來場景，包括代理封閉花園與開放網路的潛在衝突、廣告與發現的影響、微交易的演變，以及數位商品的解綁和重組。論文強調，代理溝通的架構將決定生成式AI能否真正實現經濟機會的民主化。", "audio": "audios/2505.15799v1.mp3", "timestamp": "2025-05-22T04:23:32.823938"}
{"query": "Foundation Model", "id": "2505.15685v1", "url": "http://arxiv.org/abs/2505.15685v1", "title": "From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems", "summary": "Foundation models (FMs) are increasingly used to bridge language and action\nin embodied agents, yet the operational characteristics of different FM\nintegration strategies remain under-explored -- particularly for complex\ninstruction following and versatile action generation in changing environments.\nThis paper examines three paradigms for building robotic systems: end-to-end\nvision-language-action (VLA) models that implicitly integrate perception and\nplanning, and modular pipelines incorporating either vision-language models\n(VLMs) or multimodal large language models (LLMs). We evaluate these paradigms\nthrough two focused case studies: a complex instruction grounding task\nassessing fine-grained instruction understanding and cross-modal\ndisambiguation, and an object manipulation task targeting skill transfer via\nVLA finetuning. Our experiments in zero-shot and few-shot settings reveal\ntrade-offs in generalization and data efficiency. By exploring performance\nlimits, we distill design implications for developing language-driven physical\nagents and outline emerging challenges and opportunities for FM-powered\nrobotics in real-world conditions.", "authors": ["Xiuchao Sui", "Daiying Tian", "Qi Sun", "Ruirui Chen", "Dongkyu Choi", "Kenneth Kwok", "Soujanya Poria"], "published_date": "2025-05-21", "title_zh": "從基礎紮根到操控自如：具體化機器人系統中基礎模型整合的案例研究", "summary_zh": "本論文探討了如何將基礎模型整合到機器人系统中，使其能够理解人类指令并执行复杂操作。研究比较了三种不同的整合方法：端到端视觉-语言-动作模型、以及使用视觉-语言模型或多模态大型语言模型的模块化流程。通过两个案例研究，揭示了不同方法在泛化能力和数据效率方面的优缺点，为开发更智能的语言驱动型机器人提供了设计指导，并指出了未来实际应用中面临的挑战和机遇。", "audio": "audios/2505.15685v1.mp3", "timestamp": "2025-05-22T04:23:36.820479"}
{"query": "Diffusion Model", "id": "2505.15800v1", "url": "http://arxiv.org/abs/2505.15800v1", "title": "Interspatial Attention for Efficient 4D Human Video Generation", "summary": "Generating photorealistic videos of digital humans in a controllable manner\nis crucial for a plethora of applications. Existing approaches either build on\nmethods that employ template-based 3D representations or emerging video\ngeneration models but suffer from poor quality or limited consistency and\nidentity preservation when generating individual or multiple digital humans. In\nthis paper, we introduce a new interspatial attention (ISA) mechanism as a\nscalable building block for modern diffusion transformer (DiT)--based video\ngeneration models. ISA is a new type of cross attention that uses relative\npositional encodings tailored for the generation of human videos. Leveraging a\ncustom-developed video variation autoencoder, we train a latent ISA-based\ndiffusion model on a large corpus of video data. Our model achieves\nstate-of-the-art performance for 4D human video synthesis, demonstrating\nremarkable motion consistency and identity preservation while providing precise\ncontrol of the camera and body poses. Our code and model are publicly released\nat https://dsaurus.github.io/isa4d/.", "authors": ["Ruizhi Shao", "Yinghao Xu", "Yujun Shen", "Ceyuan Yang", "Yang Zheng", "Changan Chen", "Yebin Liu", "Gordon Wetzstein"], "published_date": "2025-05-21", "title_zh": "用於高效4D人體影片生成之跨空間注意力機制", "summary_zh": "論文提出一種新的跨空間注意力（ISA）機制，用於提升基於擴散Transformer（DiT）模型的4D人體影片生成效率。ISA是一種新型的跨注意力，針對人體影片生成設計了相對位置編碼。該模型透過訓練大量影片數據，在運動一致性和身份保持方面達到最佳效果，同時能精準控制相機和身體姿勢。程式碼和模型已公開。", "audio": "audios/2505.15800v1.mp3", "timestamp": "2025-05-22T04:23:41.402506"}
{"query": "AI", "id": "2505.15798v1", "url": "http://arxiv.org/abs/2505.15798v1", "title": "Model Merging is Secretly Certifiable: Non-Vacuous Generalisation Bounds for Low-Shot Learning", "summary": "Certifying the IID generalisation ability of deep networks is the first of\nmany requirements for trusting AI in high-stakes applications from medicine to\nsecurity. However, when instantiating generalisation bounds for deep networks\nit remains challenging to obtain non-vacuous guarantees, especially when\napplying contemporary large models on the small scale data prevalent in such\nhigh-stakes fields. In this paper, we draw a novel connection between a family\nof learning methods based on model fusion and generalisation certificates, and\nsurprisingly show that with minor adjustment several existing learning\nstrategies already provide non-trivial generalisation guarantees. Essentially,\nby focusing on data-driven learning of downstream tasks by fusion rather than\nfine-tuning, the certified generalisation gap becomes tiny and independent of\nthe base network size, facilitating its certification. Our results show for the\nfirst time non-trivial generalisation guarantees for learning with as low as\n100 examples, while using vision models such as VIT-B and language models such\nas mistral-7B. This observation is significant as it has immediate implications\nfor facilitating the certification of existing systems as trustworthy, and\nopens up new directions for research at the intersection of practice and\ntheory.", "authors": ["Taehoon Kim", "Henry Gouk", "Minyoung Kim", "Timothy Hospedales"], "published_date": "2025-05-21", "title_zh": "模型融合隱藏的認證性：少樣本學習的非空泛泛化界限", "summary_zh": "這篇論文揭示了模型融合技術隱藏的優勢，它能為深度學習模型提供可驗證的泛化能力，尤其是在少樣本學習的場景下。傳統上，要為深度網路建立有效的泛化界限非常困難，特別是在高風險領域中，資料量通常很小。研究發現，透過模型融合進行資料驅動的下游任務學習，可以顯著縮小認證泛化差距，使其與基底網路的大小無關。該研究首次展示了使用少至100個樣本，以及VIT-B和Mistral-7B等模型，也能實現非空泛的泛化保證，為現有系統的認證開闢了新的方向。", "audio": "audios/2505.15798v1.mp3", "timestamp": "2025-05-22T05:18:42.219278"}
{"query": "Foundation Model", "id": "2505.15594v1", "url": "http://arxiv.org/abs/2505.15594v1", "title": "Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off", "summary": "While foundation models demonstrate impressive performance across various\ntasks, they remain vulnerable to adversarial inputs. Current research explores\nvarious approaches to enhance model robustness, with Diffusion Denoised\nSmoothing emerging as a particularly promising technique. This method employs a\npretrained diffusion model to preprocess inputs before model inference. Yet,\nits effectiveness remains largely unexplored beyond classification. We aim to\naddress this gap by analyzing three datasets with four distinct downstream\ntasks under three different adversarial attack algorithms. Our findings reveal\nthat while foundation models maintain resilience against conventional\ntransformations, applying high-noise diffusion denoising to clean images\nwithout any distortions significantly degrades performance by as high as 57%.\nLow-noise diffusion settings preserve performance but fail to provide adequate\nprotection across all attack types. Moreover, we introduce a novel attack\nstrategy specifically targeting the diffusion process itself, capable of\ncircumventing defenses in the low-noise regime. Our results suggest that the\ntrade-off between adversarial robustness and performance remains a challenge to\nbe addressed.", "authors": ["Yury Belousov", "Brian Pulfer", "Vitaliy Kinakh", "Slava Voloshynovskiy"], "published_date": "2025-05-21", "title_zh": "超越分類：評估擴散去噪平滑在安全性與效用性權衡上的表現", "summary_zh": "大型模型雖然在各項任務表現出色，但容易受到對抗性輸入的攻擊。擴散去噪平滑是一種有潛力的防禦方法，它利用預訓練的擴散模型來預處理輸入。然而，目前對其在分類以外任務的有效性研究不足。本研究針對多個任務和攻擊策略進行分析，發現高噪音擴散去噪會顯著降低模型效能，甚至高達57%。低噪音設置雖能維持效能，但無法有效防禦所有攻擊。此外，我們還設計了一種針對擴散過程本身的新型攻擊，能夠繞過低噪音設置的防禦。結果表明，在對抗性魯棒性和模型效能之間取得平衡仍然是一個挑戰。", "audio": "audios/2505.15594v1.mp3", "timestamp": "2025-05-22T05:18:47.929179"}
{"query": "Diffusion Model", "id": "2505.15791v1", "url": "http://arxiv.org/abs/2505.15791v1", "title": "VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL", "summary": "Diffusion models have emerged as powerful generative tools across various\ndomains, yet tailoring pre-trained models to exhibit specific desirable\nproperties remains challenging. While reinforcement learning (RL) offers a\npromising solution,current methods struggle to simultaneously achieve stable,\nefficient fine-tuning and support non-differentiable rewards. Furthermore,\ntheir reliance on sparse rewards provides inadequate supervision during\nintermediate steps, often resulting in suboptimal generation quality. To\naddress these limitations, dense and differentiable signals are required\nthroughout the diffusion process. Hence, we propose VAlue-based Reinforced\nDiffusion (VARD): a novel approach that first learns a value function\npredicting expection of rewards from intermediate states, and subsequently uses\nthis value function with KL regularization to provide dense supervision\nthroughout the generation process. Our method maintains proximity to the\npretrained model while enabling effective and stable training via\nbackpropagation. Experimental results demonstrate that our approach facilitates\nbetter trajectory guidance, improves training efficiency and extends the\napplicability of RL to diffusion models optimized for complex,\nnon-differentiable reward functions.", "authors": ["Fengyuan Dai", "Zifeng Zhuang", "Yufei Huang", "Siteng Huang", "Bangyan Liao", "Donglin Wang", "Fajie Yuan"], "published_date": "2025-05-21", "title_zh": "VARD：基於價值的強化學習，用於擴散模型的高效且密集微調", "summary_zh": "擴散模型在生成領域表現出色，但要讓預訓練模型展現特定期望的特性仍然困難。強化學習雖有潛力，但現有方法難以兼顧穩定、高效的微調，且無法支援不可微分的獎勵。它們對稀疏獎勵的依賴導致中間步驟的監督不足，影響生成品質。VARD 旨在解決這些問題，它首先學習一個價值函數，預測中間狀態的獎勵期望值，然後用該價值函數及KL正則化，在生成過程中提供密集的監督。這種方法在保持與預訓練模型接近的同時，通過反向傳播實現有效且穩定的訓練。實驗表明，VARD 能更好地引導生成軌跡，提高訓練效率，並將強化學習應用擴展到優化複雜的、不可微分的獎勵函數的擴散模型上。", "audio": "audios/2505.15791v1.mp3", "timestamp": "2025-05-22T05:18:54.362516"}
{"query": "AI", "id": "2505.15790v1", "url": "http://arxiv.org/abs/2505.15790v1", "title": "Exploring the Innovation Opportunities for Pre-trained Models", "summary": "Innovators transform the world by understanding where services are\nsuccessfully meeting customers' needs and then using this knowledge to identify\nfailsafe opportunities for innovation. Pre-trained models have changed the AI\ninnovation landscape, making it faster and easier to create new AI products and\nservices. Understanding where pre-trained models are successful is critical for\nsupporting AI innovation. Unfortunately, the hype cycle surrounding pre-trained\nmodels makes it hard to know where AI can really be successful. To address\nthis, we investigated pre-trained model applications developed by HCI\nresearchers as a proxy for commercially successful applications. The research\napplications demonstrate technical capabilities, address real user needs, and\navoid ethical challenges. Using an artifact analysis approach, we categorized\ncapabilities, opportunity domains, data types, and emerging interaction design\npatterns, uncovering some of the opportunity space for innovation with\npre-trained models.", "authors": ["Minjung Park", "Jodi Forlizzi", "John Zimmerman"], "published_date": "2025-05-21", "title_zh": "探索預訓練模型的創新機會", "summary_zh": "預訓練模型正在重塑人工智慧創新，加速新產品和服務的開發。本研究分析人機互動(HCI)研究者開發的應用案例，作為商業成功的指標，旨在辨識預訓練模型真正能夠發揮價值的領域。透過分析這些應用，我們歸納出預訓練模型在能力、應用領域、資料類型和互動設計模式上的潛力，為人工智慧創新指明方向。", "audio": "audios/2505.15790v1.mp3", "timestamp": "2025-05-22T06:27:08.958151"}
{"query": "Foundation Model", "id": "2505.15572v1", "url": "http://arxiv.org/abs/2505.15572v1", "title": "Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback", "summary": "The data-to-equation (Data2Eqn) task aims to discover interpretable\nmathematical equations that map observed values to labels, offering physical\ninsights and broad applicability across academic and industrial domains.\nGenetic programming and traditional deep learning-based approaches suffer from\nsearch inefficiency and poor generalization on small task-specific datasets.\nFoundation models showed promise in this area, but existing approaches suffer\nfrom: 1) They are pretrained on general-purpose data distributions, making them\nless effective for domain-specific tasks; and 2) their training objectives\nfocus on token-level alignment, overlooking mathematical semantics, which can\nlead to inaccurate equations. To address these issues, we aim to enhance the\ndomain adaptability of foundation models for Data2Eqn tasks. In this work, we\npropose a reinforcement learning-based finetuning framework that directly\noptimizes the generation policy of a pretrained model through reward signals\nderived from downstream numerical fitness. Our method allows the model to adapt\nto specific and complex data distributions and generate mathematically\nmeaningful equations. Extensive experiments demonstrate that our approach\nimproves both the accuracy and robustness of equation generation under complex\ndistributions.", "authors": ["Wangyang Ying", "Haoyue Bai", "Nanxu Gong", "Xinyuan Wang", "Sixun Dong", "Haifeng Chen", "Yanjie Fu"], "published_date": "2025-05-21", "title_zh": "以強化回饋彌合方程式蒸餾中的領域差距", "summary_zh": "資料到方程式（Data2Eqn）任務旨在從觀測值找出可解釋的數學方程式，進而對應到標籤，以提供物理學見解並廣泛應用於學術界和工業界。我們發現，預訓練模型在特定領域的Data2Eqn任務中表現不佳，原因在於它們是在通用數據分佈上訓練，並且訓練目標側重於token級別對齊，忽略了數學語義，導致方程式不準確。為了解決這些問題，我們提出一個基於強化學習的微調框架，透過來自下游數值擬合的獎勵信號，直接優化預訓練模型的生成策略。實驗結果表明，此方法提高了模型在複雜分佈下生成方程式的準確性和魯棒性。", "audio": "audios/2505.15572v1.mp3", "timestamp": "2025-05-22T06:27:14.421326"}
{"query": "Diffusion Model", "id": "2505.15679v1", "url": "http://arxiv.org/abs/2505.15679v1", "title": "SwarmDiff: Swarm Robotic Trajectory Planning in Cluttered Environments via Diffusion Transformer", "summary": "Swarm robotic trajectory planning faces challenges in computational\nefficiency, scalability, and safety, particularly in complex, obstacle-dense\nenvironments. To address these issues, we propose SwarmDiff, a hierarchical and\nscalable generative framework for swarm robots. We model the swarm's\nmacroscopic state using Probability Density Functions (PDFs) and leverage\nconditional diffusion models to generate risk-aware macroscopic trajectory\ndistributions, which then guide the generation of individual robot trajectories\nat the microscopic level. To ensure a balance between the swarm's optimal\ntransportation and risk awareness, we integrate Wasserstein metrics and\nConditional Value at Risk (CVaR). Additionally, we introduce a Diffusion\nTransformer (DiT) to improve sampling efficiency and generation quality by\ncapturing long-range dependencies. Extensive simulations and real-world\nexperiments demonstrate that SwarmDiff outperforms existing methods in\ncomputational efficiency, trajectory validity, and scalability, making it a\nreliable solution for swarm robotic trajectory planning.", "authors": ["Kang Ding", "Chunxuan Jiao", "Yunze Hu", "Kangjie Zhou", "Pengying Wu", "Yao Mu", "Chang Liu"], "published_date": "2025-05-21", "title_zh": "SwarmDiff：基於擴散轉換器的複雜環境群體機器人軌跡規劃", "summary_zh": "SwarmDiff 是一個針對複雜環境中群體機器人的軌跡規劃框架。它運用擴散模型生成風險感知的群體宏觀軌跡分佈，再以此引導生成個體機器人的微觀軌跡。同時，整合了 Wasserstein 指標和條件風險值（CVaR）來平衡群體的最優運輸和風險意識。透過擴散轉換器 (DiT) 來提升抽樣效率和生成品質。實驗證明，SwarmDiff 在計算效率、軌跡有效性和可擴展性方面優於現有方法，為群體機器人軌跡規劃提供了一個可靠的解決方案。", "audio": "audios/2505.15679v1.mp3", "timestamp": "2025-05-22T06:27:18.842130"}
{"query": "AI", "id": "2505.15778v1", "url": "http://arxiv.org/abs/2505.15778v1", "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space", "summary": "Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking.", "authors": ["Zhen Zhang", "Xuehai He", "Weixiang Yan", "Ao Shen", "Chenyang Zhao", "Shuohang Wang", "Yelong Shen", "Xin Eric Wang"], "published_date": "2025-05-21", "title_zh": "軟性思考：釋放LLM在連續概念空間中的推理潛力", "summary_zh": "現有的推理模型受限於語言，只能處理離散的語義單位。這篇論文提出一種名為「軟性思考」的免訓練方法，模擬人類的抽象思考模式，在連續概念空間中產生軟性的概念標記。這些標記透過加權混合不同的詞嵌入形成，能更平滑地轉換並產生更豐富的表徵，突破傳統離散邊界。實驗結果顯示，「軟性思考」在數學和程式碼基準測試中，能有效提升準確度並降低 token 使用量，同時保持輸出的可讀性，展現其打破離散語言推理瓶頸的潛力。", "audio": "audios/2505.15778v1.mp3", "timestamp": "2025-05-22T09:21:16.643593"}
{"query": "Foundation Model", "id": "2505.15559v1", "url": "http://arxiv.org/abs/2505.15559v1", "title": "Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music Attributes", "summary": "Moonbeam is a transformer-based foundation model for symbolic music,\npretrained on a large and diverse collection of MIDI data totaling 81.6K hours\nof music and 18 billion tokens. Moonbeam incorporates music-domain inductive\nbiases by capturing both absolute and relative musical attributes through the\nintroduction of a novel domain-knowledge-inspired tokenization method and\nMultidimensional Relative Attention (MRA), which captures relative music\ninformation without additional trainable parameters. Leveraging the pretrained\nMoonbeam, we propose 2 finetuning architectures with full anticipatory\ncapabilities, targeting 2 categories of downstream tasks: symbolic music\nunderstanding and conditional music generation (including music infilling). Our\nmodel outperforms other large-scale pretrained music models in most cases in\nterms of accuracy and F1 score across 3 downstream music classification tasks\non 4 datasets. Moreover, our finetuned conditional music generation model\noutperforms a strong transformer baseline with a REMI-like tokenizer. We\nopen-source the code, pretrained model, and generated samples on Github.", "authors": ["Zixun Guo", "Simon Dixon"], "published_date": "2025-05-21", "title_zh": "Moonbeam：一個利用絕對與相對音樂屬性的MIDI基礎模型", "summary_zh": "Moonbeam是一個基於Transformer的符號音樂基礎模型，使用大量MIDI數據（總計8.16萬小時的音樂和180億個tokens）進行預訓練。它整合了音樂領域的歸納偏置，通過創新的token化方法和多維相對注意力機制（MRA）來捕捉絕對和相對的音樂屬性。MRA在不增加可訓練參數的情況下捕捉相對音樂信息。利用預訓練的Moonbeam，我們提出了兩種具有完整預測能力的微調架構，針對符號音樂理解和條件音樂生成（包括音樂填充）兩類下游任務。我們的模型在四個數據集的三个下游音樂分類任務中，其準確度和F1分數大多優於其他大規模預訓練音樂模型。此外，我們微調的條件音樂生成模型也勝過使用類似REMI tokenization方法的強Transformer基線。我們已在Github上開源代碼、預訓練模型和生成的樣本。", "audio": "audios/2505.15559v1.mp3", "timestamp": "2025-05-22T09:21:23.097162"}
{"query": "Diffusion Model", "id": "2505.15644v1", "url": "http://arxiv.org/abs/2505.15644v1", "title": "FragFake: A Dataset for Fine-Grained Detection of Edited Images with Vision Language Models", "summary": "Fine-grained edited image detection of localized edits in images is crucial\nfor assessing content authenticity, especially given that modern diffusion\nmodels and image editing methods can produce highly realistic manipulations.\nHowever, this domain faces three challenges: (1) Binary classifiers yield only\na global real-or-fake label without providing localization; (2) Traditional\ncomputer vision methods often rely on costly pixel-level annotations; and (3)\nNo large-scale, high-quality dataset exists for modern image-editing detection\ntechniques. To address these gaps, we develop an automated data-generation\npipeline to create FragFake, the first dedicated benchmark dataset for edited\nimage detection, which includes high-quality images from diverse editing models\nand a wide variety of edited objects. Based on FragFake, we utilize Vision\nLanguage Models (VLMs) for the first time in the task of edited image\nclassification and edited region localization. Experimental results show that\nfine-tuned VLMs achieve higher average Object Precision across all datasets,\nsignificantly outperforming pretrained models. We further conduct ablation and\ntransferability analyses to evaluate the detectors across various\nconfigurations and editing scenarios. To the best of our knowledge, this work\nis the first to reformulate localized image edit detection as a vision-language\nunderstanding task, establishing a new paradigm for the field. We anticipate\nthat this work will establish a solid foundation to facilitate and inspire\nsubsequent research endeavors in the domain of multimodal content authenticity.", "authors": ["Zhen Sun", "Ziyi Zhang", "Zeren Luo", "Zeyang Sha", "Tianshuo Cong", "Zheng Li", "Shiwen Cui", "Weiqiang Wang", "Jiaheng Wei", "Xinlei He", "Qi Li", "Qian Wang"], "published_date": "2025-05-21", "title_zh": "FragFake：一個利用視覺語言模型進行細粒度編輯圖像檢測的資料集", "summary_zh": "現代圖像編輯技術高度逼真，準確判斷圖像是否經過局部修改至關重要。 然而，傳統方法難以定位編輯區域，且缺乏高品質的大規模資料集。為此，我們創建了 FragFake 資料集，包含多種編輯模型和物件，並首次使用視覺語言模型 (VLMs) 進行編輯圖像分類和區域定位。實驗表明，經過微調的 VLMs 在檢測編輯區域的精確度上顯著優於預訓練模型。 這項研究將局部圖像編輯檢測重新定義為視覺語言理解任務，為該領域建立了一個新範式。", "audio": "audios/2505.15644v1.mp3", "timestamp": "2025-05-22T09:21:28.753217"}
{"query": "AI", "id": "2505.15755v1", "url": "http://arxiv.org/abs/2505.15755v1", "title": "Exploring The Visual Feature Space for Multimodal Neural Decoding", "summary": "The intrication of brain signals drives research that leverages multimodal AI\nto align brain modalities with visual and textual data for explainable\ndescriptions. However, most existing studies are limited to coarse\ninterpretations, lacking essential details on object descriptions, locations,\nattributes, and their relationships. This leads to imprecise and ambiguous\nreconstructions when using such cues for visual decoding. To address this, we\nanalyze different choices of vision feature spaces from pre-trained visual\ncomponents within Multimodal Large Language Models (MLLMs) and introduce a\nzero-shot multimodal brain decoding method that interacts with these models to\ndecode across multiple levels of granularities. % To assess a model's ability\nto decode fine details from brain signals, we propose the Multi-Granularity\nBrain Detail Understanding Benchmark (MG-BrainDub). This benchmark includes two\nkey tasks: detailed descriptions and salient question-answering, with metrics\nhighlighting key visual elements like objects, attributes, and relationships.\nOur approach enhances neural decoding precision and supports more accurate\nneuro-decoding applications. Code will be available at\nhttps://github.com/weihaox/VINDEX.", "authors": ["Weihao Xia", "Cengiz Oztireli"], "published_date": "2025-05-21", "title_zh": "探索多模態神經解碼的視覺特徵空間", "summary_zh": "現有研究利用多模態人工智慧將腦部訊號與視覺及文字資料對齊，以產生可解釋的描述，但細節不足，導致視覺解碼重建不精確。本研究分析多模態大型語言模型(MLLM)中預訓練視覺元件的不同視覺特徵空間，並提出一種零樣本多模態腦部解碼方法，與這些模型互動，在多個粒度層級上進行解碼。研究更推出多粒度腦部細節理解基準(MG-BrainDub)，包含詳細描述和顯著問答任務，以評估模型從腦部訊號解碼細節的能力。本研究旨在提高神經解碼的精確度，並支援更準確的神經解碼應用。程式碼將在https://github.com/weihaox/VINDEX 上公開。", "audio": "audios/2505.15755v1.mp3", "timestamp": "2025-05-22T10:20:08.924290"}
{"query": "Foundation Model", "id": "2505.15506v1", "url": "http://arxiv.org/abs/2505.15506v1", "title": "Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts", "summary": "Recently, Vision-Language foundation models like CLIP and ALIGN, which are\npre-trained on large-scale data have shown remarkable zero-shot generalization\nto diverse datasets with different classes and even domains. In this work, we\ntake a step further and analyze whether these models can be adapted to target\ndatasets having very different distributions and classes compared to what these\nmodels have been trained on, using only a few labeled examples from the target\ndataset. In such scenarios, finetuning large pretrained models is challenging\ndue to problems of overfitting as well as loss of generalization, and has not\nbeen well explored in prior literature. Since, the pre-training data of such\nmodels are unavailable, it is difficult to comprehend the performance on\nvarious downstream datasets. First, we try to answer the question: Given a\ntarget dataset with a few labelled examples, can we estimate whether further\nfine-tuning can enhance the performance compared to zero-shot evaluation? by\nanalyzing the common vision-language embedding space. Based on the analysis, we\npropose a novel prompt-tuning method, PromptMargin for adapting such\nlarge-scale VLMs directly on the few target samples. PromptMargin effectively\ntunes the text as well as visual prompts for this task, and has two main\nmodules: 1) Firstly, we use a selective augmentation strategy to complement the\nfew training samples in each task; 2) Additionally, to ensure robust training\nin the presence of unfamiliar class names, we increase the inter-class margin\nfor improved class discrimination using a novel Multimodal Margin Regularizer.\nExtensive experiments and analysis across fifteen target benchmark datasets,\nwith varying degrees of distribution shifts from natural images, shows the\neffectiveness of the proposed framework over the existing state-of-the-art\napproaches applied to this setting. github.com/debarshigit/PromptMargin.", "authors": ["Debarshi Brahma", "Anuska Roy", "Soma Biswas"], "published_date": "2025-05-21", "title_zh": "使用邊界正則化器調整視覺語言模型以應對分布偏移下的少樣本學習", "summary_zh": "CLIP 和 ALIGN 等視覺語言預訓練模型在不同資料集上展現了出色的零樣本泛化能力。本研究進一步探討如何使用目標資料集的少量標記樣本，使這些模型適應與訓練資料具有顯著分布和類別差異的目標資料集。針對這個問題，我們分析了視覺語言嵌入空間，提出一種新的 PromptMargin 方法，有效調整文本和視覺提示詞，並採用選擇性資料擴增和多模態邊界正則化器，增強模型在少樣本情況下的鲁棒性和類別區分能力。在十五個基準資料集上的實驗證明，PromptMargin 在分布偏移下的少樣本學習中優於現有方法。", "audio": "audios/2505.15506v1.mp3", "timestamp": "2025-05-22T10:20:16.069397"}
{"query": "Diffusion Model", "id": "2505.15450v1", "url": "http://arxiv.org/abs/2505.15450v1", "title": "Comprehensive Evaluation and Analysis for NSFW Concept Erasure in Text-to-Image Diffusion Models", "summary": "Text-to-image diffusion models have gained widespread application across\nvarious domains, demonstrating remarkable creative potential. However, the\nstrong generalization capabilities of diffusion models can inadvertently lead\nto the generation of not-safe-for-work (NSFW) content, posing significant risks\nto their safe deployment. While several concept erasure methods have been\nproposed to mitigate the issue associated with NSFW content, a comprehensive\nevaluation of their effectiveness across various scenarios remains absent. To\nbridge this gap, we introduce a full-pipeline toolkit specifically designed for\nconcept erasure and conduct the first systematic study of NSFW concept erasure\nmethods. By examining the interplay between the underlying mechanisms and\nempirical observations, we provide in-depth insights and practical guidance for\nthe effective application of concept erasure methods in various real-world\nscenarios, with the aim of advancing the understanding of content safety in\ndiffusion models and establishing a solid foundation for future research and\ndevelopment in this critical area.", "authors": ["Die Chen", "Zhiwen Li", "Cen Chen", "Yuexiang Xie", "Xiaodan Li", "Jinyan Ye", "Yingda Chen", "Yaliang Li"], "published_date": "2025-05-21", "title_zh": "文字生成圖像擴散模型中，針對不雅內容概念消除的全面評估與分析", "summary_zh": "文字生成圖像模型能力強大，但也可能生成不雅內容，造成安全風險。雖然已有消除不雅內容的方法，但缺乏全面評估。本研究開發工具，系統性地評估現有方法，深入分析其機制和效果，為實際應用提供指導，旨在提升對擴散模型內容安全性的理解，並為未來研究奠定基礎。\n\n**簡明摘要：**AI繪圖很厲害，但可能畫出不雅圖片。這項研究評估了目前消除不雅內容的方法，希望能讓AI繪圖更安全、更可靠。", "audio": "audios/2505.15450v1.mp3", "timestamp": "2025-05-22T10:20:23.383218"}
{"query": "AI", "id": "2505.15700v1", "url": "http://arxiv.org/abs/2505.15700v1", "title": "\"Alexa, can you forget me?\" Machine Unlearning Benchmark in Spoken Language Understanding", "summary": "Machine unlearning, the process of efficiently removing specific information\nfrom machine learning models, is a growing area of interest for responsible AI.\nHowever, few studies have explored the effectiveness of unlearning methods on\ncomplex tasks, particularly speech-related ones. This paper introduces\nUnSLU-BENCH, the first benchmark for machine unlearning in spoken language\nunderstanding (SLU), focusing on four datasets spanning four languages. We\naddress the unlearning of data from specific speakers as a way to evaluate the\nquality of potential \"right to be forgotten\" requests. We assess eight\nunlearning techniques and propose a novel metric to simultaneously better\ncapture their efficacy, utility, and efficiency. UnSLU-BENCH sets a foundation\nfor unlearning in SLU and reveals significant differences in the effectiveness\nand computational feasibility of various techniques.", "authors": ["Alkis Koudounas", "Claudio Savelli", "Flavio Giobergia", "Elena Baralis"], "published_date": "2025-05-21", "title_zh": "「Alexa，你可以忘記我嗎？」：口語理解中的機器遺忘基準測試", "summary_zh": "為響應「被遺忘權」的要求，機器遺忘技術日漸重要。本論文推出首個口語理解（SLU）機器遺忘基準測試UnSLU-BENCH，涵蓋四種語言的四個數據集，專注於將特定說話者的數據從模型中移除。研究評估了八種遺忘技術，並提出一種新的評估指標，綜合考量其效果、實用性和效率。UnSLU-BENCH 為口語理解中的遺忘技術奠定基礎，並揭示了不同技術在效果和計算可行性上的顯著差異。", "audio": "audios/2505.15700v1.mp3", "timestamp": "2025-05-22T11:16:43.359376"}
{"query": "Foundation Model", "id": "2505.15334v1", "url": "http://arxiv.org/abs/2505.15334v1", "title": "Parameter-Efficient Fine-Tuning of Multispectral Foundation Models for Hyperspectral Image Classification", "summary": "Foundation models have achieved great success across diverse domains,\nincluding remote sensing (RS), thanks to their versatility and strong\ngeneralization abilities. However, most RS foundation models are designed for\nmultispectral data, while hyperspectral imagery (HSI) - with its hundreds of\nspectral bands - remains less explored. Fine-tuning such models for downstream\ntasks is also challenging, often demanding considerable memory and storage. In\nthis paper, we propose an efficient framework to fine-tune SpectralGPT, a\nmultispectral foundation model, for hyperspectral image classification (HSIC).\nWe explore several Parameter-Efficient Fine-Tuning (PEFT) methods, including\nLow-Rank Adaptation (LoRA), Kronecker-based adaptation (KronA), Low-Rank\nKronecker (LoKr), and the recent LoRA+, which uses distinct learning rates for\nlow-rank adapters scaled by a factor lambda. Inspired by LoRA+, we introduce\nKronA+, which applies a similar mechanism to the Kronecker matrices. We\nevaluate our approach on five datasets from different sensors, showing\ncompetitive performance with state-of-the-art HSI models. Our full fine-tuning\n(FFT) setup for SpectralGPT even outperforms a dedicated hyperspectral\nfoundation model on some datasets while requiring only a quarter of the\ntraining epochs. Under the same number of epochs, KronA+ reaches similar\nperformance with far fewer trainable parameters - just 0.056 percent - and adds\nonly approximately 0.2 megabytes of storage, making it the most effective PEFT\nmethod tested.", "authors": ["Bernardin Ligan", "Khalide Jbilou", "Fahd Kalloubi", "Ahmed Ratnani"], "published_date": "2025-05-21", "title_zh": "針對高光譜影像分類的多光譜基礎模型之參數高效微調", "summary_zh": "本文提出一個高效框架，針對高光譜影像分類，微調多光譜基礎模型SpectralGPT。由於高光譜影像擁有數百個頻譜通道，直接微調大型模型耗費大量資源。因此，研究使用多種參數高效微調(PEFT)方法，如LoRA、KronA及其改良版本LoRA+和KronA+。實驗證明，我們的框架在多個數據集上表現優異，其中KronA+僅需極少的可訓練參數和儲存空間，即可達到與完整微調相近的效能，是目前測試中最有效的PEFT方法。", "audio": "audios/2505.15334v1.mp3", "timestamp": "2025-05-22T11:16:48.505895"}
{"query": "Diffusion Model", "id": "2505.15427v1", "url": "http://arxiv.org/abs/2505.15427v1", "title": "Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions", "summary": "The remarkable ability of diffusion models to generate high-fidelity images\nhas led to their widespread adoption. However, concerns have also arisen\nregarding their potential to produce Not Safe for Work (NSFW) content and\nexhibit social biases, hindering their practical use in real-world\napplications. In response to this challenge, prior work has focused on\nemploying security filters to identify and exclude toxic text, or\nalternatively, fine-tuning pre-trained diffusion models to erase sensitive\nconcepts. Unfortunately, existing methods struggle to achieve satisfactory\nperformance in the sense that they can have a significant impact on the normal\nmodel output while still failing to prevent the generation of harmful content\nin some cases. In this paper, we propose a novel self-discovery approach to\nidentifying a semantic direction vector in the embedding space to restrict text\nembedding within a safe region. Our method circumvents the need for correcting\nindividual words within the input text and steers the entire text prompt\ntowards a safe region in the embedding space, thereby enhancing model\nrobustness against all possibly unsafe prompts. In addition, we employ Low-Rank\nAdaptation (LoRA) for semantic direction vector initialization to reduce the\nimpact on the model performance for other semantics. Furthermore, our method\ncan also be integrated with existing methods to improve their social\nresponsibility. Extensive experiments on benchmark datasets demonstrate that\nour method can effectively reduce NSFW content and mitigate social bias\ngenerated by diffusion models compared to several state-of-the-art baselines.", "authors": ["Zhiwen Li", "Die Chen", "Mingyuan Fan", "Cen Chen", "Yaliang Li", "Yanhao Wang", "Wenmeng Zhou"], "published_date": "2025-05-21", "title_zh": "透過在安全區域內約束文本嵌入實現負責任的擴散模型", "summary_zh": "擴散模型雖然能生成高擬真度圖像，但也可能產生不宜內容和社會偏見，影響實際應用。為了應對這個問題，本研究提出一種新的方法，透過在嵌入空間中識別語義方向向量，將文本嵌入限制在安全區域內。這種方法無需修正輸入文本中的個別詞彙，而是引導整個文本提示詞前往嵌入空間中的安全區域，從而增強模型對所有可能不安全提示詞的抵抗力。此外，我們採用低秩適應 (LoRA) 初始化語義方向向量，以減少對模型性能的影響。實驗結果表明，與現有方法相比，我們的方法能有效減少擴散模型生成的不宜內容並減輕社會偏見。", "audio": "audios/2505.15427v1.mp3", "timestamp": "2025-05-22T11:16:53.788897"}
{"query": "AI", "id": "2505.15622v1", "url": "http://arxiv.org/abs/2505.15622v1", "title": "Benchmarking Energy and Latency in TinyML: A Novel Method for Resource-Constrained AI", "summary": "The rise of IoT has increased the need for on-edge machine learning, with\nTinyML emerging as a promising solution for resource-constrained devices such\nas MCU. However, evaluating their performance remains challenging due to\ndiverse architectures and application scenarios. Current solutions have many\nnon-negligible limitations. This work introduces an alternative benchmarking\nmethodology that integrates energy and latency measurements while\ndistinguishing three execution phases pre-inference, inference, and\npost-inference. Additionally, the setup ensures that the device operates\nwithout being powered by an external measurement unit, while automated testing\ncan be leveraged to enhance statistical significance. To evaluate our setup, we\ntested the STM32N6 MCU, which includes a NPU for executing neural networks. Two\nconfigurations were considered: high-performance and Low-power. The variation\nof the EDP was analyzed separately for each phase, providing insights into the\nimpact of hardware configurations on energy efficiency. Each model was tested\n1000 times to ensure statistically relevant results. Our findings demonstrate\nthat reducing the core voltage and clock frequency improve the efficiency of\npre- and post-processing without significantly affecting network execution\nperformance. This approach can also be used for cross-platform comparisons to\ndetermine the most efficient inference platform and to quantify how pre- and\npost-processing overhead varies across different hardware implementations.", "authors": ["Pietro Bartoli", "Christian Veronesi", "Andrea Giudici", "David Siorpaes", "Diana Trojaniello", "Franco Zappa"], "published_date": "2025-05-21", "title_zh": "TinyML 能效與延遲基準測試：一種針對資源受限AI的新方法", "summary_zh": "物聯網興起，對邊緣機器學習的需求增加。TinyML 成為資源受限裝置（如微控制器 MCU）的潛力解決方案。然而，評估其效能極具挑戰。本研究提出一種新的基準測試方法，整合能耗與延遲測量，並區分預處理、推論和後處理三個階段。此方法確保裝置獨立供電運行，並利用自動化測試提高統計顯著性。實驗結果顯示，降低核心電壓和時脈頻率能有效提升預處理和後處理的效率，且不顯著影響神經網路的推論效能。此方法也可用於跨平台比較，找出最有效率的推論平台，並量化不同硬體實現的預處理和後處理開銷。", "audio": "audios/2505.15622v1.mp3", "timestamp": "2025-05-22T12:38:29.628724"}
{"query": "Foundation Model", "id": "2505.15307v1", "url": "http://arxiv.org/abs/2505.15307v1", "title": "Towards Pre-training an Effective Respiratory Audio Foundation Model", "summary": "Recent advancements in foundation models have sparked interest in respiratory\naudio foundation models. However, the effectiveness of applying conventional\npre-training schemes to datasets that are small-sized and lack diversity has\nnot been sufficiently verified. This study aims to explore better pre-training\npractices for respiratory sounds by comparing numerous pre-trained audio\nmodels. Our investigation reveals that models pre-trained on AudioSet, a\ngeneral audio dataset, are more effective than the models specifically\npre-trained on respiratory sounds. Moreover, combining AudioSet and respiratory\nsound datasets for further pre-training enhances performance, and preserving\nthe frequency-wise information when aggregating features is vital. Along with\nmore insights found in the experiments, we establish a new state-of-the-art for\nthe OPERA benchmark, contributing to advancing respiratory audio foundation\nmodels. Our code is available online at\nhttps://github.com/nttcslab/eval-audio-repr/tree/main/plugin/OPERA.", "authors": ["Daisuke Niizumi", "Daiki Takeuchi", "Masahiro Yasuda", "Binh Thien Nguyen", "Yasunori Ohishi", "Noboru Harada"], "published_date": "2025-05-21", "title_zh": "邁向預訓練一個有效的呼吸音訊基礎模型", "summary_zh": "近期基礎模型的發展激發了對呼吸音訊基礎模型的興趣。本研究探討針對小規模、缺乏多樣性的呼吸音訊數據集，如何進行更有效的預訓練。研究比較了多個預訓練音訊模型，發現使用通用音訊數據集AudioSet預訓練的模型效果更好。進一步結合AudioSet和呼吸音訊數據集進行預訓練可提升性能，並且在聚合特徵時保留頻率信息至關重要。實驗結果為呼吸音訊基礎模型提供了新的洞見，並在OPERA基準測試中取得了最先進的結果。", "audio": "audios/2505.15307v1.mp3", "timestamp": "2025-05-22T12:38:33.648979"}
{"query": "Diffusion Model", "id": "2505.15336v1", "url": "http://arxiv.org/abs/2505.15336v1", "title": "My Face Is Mine, Not Yours: Facial Protection Against Diffusion Model Face Swapping", "summary": "The proliferation of diffusion-based deepfake technologies poses significant\nrisks for unauthorized and unethical facial image manipulation. While\ntraditional countermeasures have primarily focused on passive detection\nmethods, this paper introduces a novel proactive defense strategy through\nadversarial attacks that preemptively protect facial images from being\nexploited by diffusion-based deepfake systems. Existing adversarial protection\nmethods predominantly target conventional generative architectures (GANs, AEs,\nVAEs) and fail to address the unique challenges presented by diffusion models,\nwhich have become the predominant framework for high-quality facial deepfakes.\nCurrent diffusion-specific adversarial approaches are limited by their reliance\non specific model architectures and weights, rendering them ineffective against\nthe diverse landscape of diffusion-based deepfake implementations.\nAdditionally, they typically employ global perturbation strategies that\ninadequately address the region-specific nature of facial manipulation in\ndeepfakes.", "authors": ["Hon Ming Yam", "Zhongliang Guo", "Chun Pong Lau"], "published_date": "2025-05-21", "title_zh": "我的臉是我的，不是你的：針對擴散模型換臉的面部保護", "summary_zh": "擴散模型技術讓換臉變得更容易，但也帶來了未授權和不道德的面部圖像操縱風險。本文提出一種新的主動防禦策略，通過對抗性攻擊，預先保護臉部圖像免於被擴散模型的Deepfake系統利用。與過去針對GAN等生成模型的防禦方法不同，本文方法專門針對擴散模型的特性進行設計，克服了以往方法對模型依賴性強和擾動策略不夠精確等問題，旨在有效保護臉部圖像免受Deepfake攻擊。", "audio": "audios/2505.15336v1.mp3", "timestamp": "2025-05-22T12:38:39.258376"}
{"query": "AI", "id": "2505.15620v1", "url": "http://arxiv.org/abs/2505.15620v1", "title": "Observation of $χ_{cJ}\\to 3K_S^0K^\\pmπ^\\mp$", "summary": "By analyzing $(2712.4\\pm14.3)\\times10^6$ $\\psi(3686)$ events collected with\nthe BESIII detector operating at the BEPCII collider, the decays $\\chi_{c0,1,2}\n\\to 3K_S^0K^\\pm\\pi^\\mp$ are observed for the first time with statistical\nsignificances greater than $10\\sigma$. The branching fractions of these decays\nare determined to be $\\mathcal{B}(\\chi_{c0}\\to 3K_S^0K^\\pm\\pi^\\mp\n)=(7.95\\pm0.50\\pm0.65)\\times10^{-5},$ $\\mathcal{B}(\\chi_{c1}\\to\n3K_S^0K^\\pm\\pi^\\mp)=(2.62\\pm0.08\\pm0.19)\\times10^{-4},$ and\n$\\mathcal{B}(\\chi_{c2}\\to\n3K_S^0K^\\pm\\pi^\\mp)=(1.72\\pm0.07\\pm0.15)\\times10^{-4},$ where the first\nuncertainties are statistical and the second systematic.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "G. Chelkov", "C. Chen", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "Y. B. Chen", "Y. Q. Chen", "Z. J. Chen", "Z. K. Chen", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "Yang Gao", "S. Garbolino", "I. Garzia", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "T. Holtmann", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "B. Y. Hu", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "S. Janchiv", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "Z. H. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "Cheng Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "C. X. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. Liu", "L. C. Liu", "Lu Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. Ma", "H. L. Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "Y. H. Meng", "Z. X. Meng", "J. G. Messchendorp", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "Y. Y. Peng", "K. Peters", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "Ch. Rosner", "M. Q. Ruan", "S. N. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. X. Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "L. F. Tang", "M. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "V. Thoren", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "T. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "Lei Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "X. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. Z. Zhou", "Z. C. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-05-21", "title_zh": "$χ_{cJ}$ 衰變至 $3K_S^0K^\\pmπ^\\mp$ 的觀測", "summary_zh": "利用北京譜儀III實驗在BEPCII對撞機上收集的$(2712.4\\pm14.3)\\times10^6$個$\\psi(3686)$事件，首次觀測到衰變$\\chi_{c0,1,2} \\to 3K_S^0K^\\pm\\pi^\\mp$，統計顯著性均大於$10\\sigma$。測得分支比分別為$\\mathcal{B}(\\chi_{c0}\\to 3K_S^0K^\\pm\\pi^\\mp)=(7.95\\pm0.50\\pm0.65)\\times10^{-5}$，$\\mathcal{B}(\\chi_{c1}\\to 3K_S^0K^\\pm\\pi^\\mp)=(2.62\\pm0.08\\pm0.19)\\times10^{-4}$，以及$\\mathcal{B}(\\chi_{c2}\\to 3K_S^0K^\\pm\\pi^\\mp)=(1.72\\pm0.07\\pm0.15)\\times10^{-4}$，其中第一項誤差為統計誤差，第二項為系統誤差。", "audio": "audios/2505.15620v1.mp3", "timestamp": "2025-05-22T23:34:08.504115"}
{"query": "Foundation Model", "id": "2505.15192v1", "url": "http://arxiv.org/abs/2505.15192v1", "title": "Leveraging Foundation Models for Multimodal Graph-Based Action Recognition", "summary": "Foundation models have ushered in a new era for multimodal video\nunderstanding by enabling the extraction of rich spatiotemporal and semantic\nrepresentations. In this work, we introduce a novel graph-based framework that\nintegrates a vision-language foundation, leveraging VideoMAE for dynamic visual\nencoding and BERT for contextual textual embedding, to address the challenge of\nrecognizing fine-grained bimanual manipulation actions. Departing from\nconventional static graph architectures, our approach constructs an adaptive\nmultimodal graph where nodes represent frames, objects, and textual\nannotations, and edges encode spatial, temporal, and semantic relationships.\nThese graph structures evolve dynamically based on learned interactions,\nallowing for flexible and context-aware reasoning. A task-specific attention\nmechanism within a Graph Attention Network further enhances this reasoning by\nmodulating edge importance based on action semantics. Through extensive\nevaluations on diverse benchmark datasets, we demonstrate that our method\nconsistently outperforms state-of-the-art baselines, underscoring the strength\nof combining foundation models with dynamic graph-based reasoning for robust\nand generalizable action recognition.", "authors": ["Fatemeh Ziaeetabar", "Florentin Wörgötter"], "published_date": "2025-05-21", "title_zh": "利用基礎模型進行多模態圖基於動作識別", "summary_zh": "基礎模型為多模態影片理解帶來新紀元，能提取豐富時空與語義表徵。本文提出一種新型基於圖的框架，整合視覺-語言基礎模型，利用VideoMAE進行動態視覺編碼，BERT進行上下文文本嵌入，以解決精細雙手操作動作識別的挑戰。不同於傳統靜態圖架構，我們的方法構建自適應多模態圖，其中節點代表幀、對象和文本註釋，邊緣編碼空間、時間和語義關係。這些圖結構基於學習到的交互動態演化，實現靈活的上下文感知推理。圖注意力網絡中的任務特定注意力機制通過調節基於動作語義的邊緣重要性，進一步增強推理能力。在多個基準數據集上的廣泛評估表明，該方法始終優於最先進的基線，突顯了將基礎模型與基於動態圖的推理相結合，以實現穩健和通用的動作識別的優勢。", "audio": "audios/2505.15192v1.mp3", "timestamp": "2025-05-22T23:34:14.825615"}
{"query": "Diffusion Model", "id": "2505.15313v1", "url": "http://arxiv.org/abs/2505.15313v1", "title": "FaceCrafter: Identity-Conditional Diffusion with Disentangled Control over Facial Pose, Expression, and Emotion", "summary": "Human facial images encode a rich spectrum of information, encompassing both\nstable identity-related traits and mutable attributes such as pose, expression,\nand emotion. While recent advances in image generation have enabled\nhigh-quality identity-conditional face synthesis, precise control over\nnon-identity attributes remains challenging, and disentangling identity from\nthese mutable factors is particularly difficult. To address these limitations,\nwe propose a novel identity-conditional diffusion model that introduces two\nlightweight control modules designed to independently manipulate facial pose,\nexpression, and emotion without compromising identity preservation. These\nmodules are embedded within the cross-attention layers of the base diffusion\nmodel, enabling precise attribute control with minimal parameter overhead.\nFurthermore, our tailored training strategy, which leverages cross-attention\nbetween the identity feature and each non-identity control feature, encourages\nidentity features to remain orthogonal to control signals, enhancing\ncontrollability and diversity. Quantitative and qualitative evaluations, along\nwith perceptual user studies, demonstrate that our method surpasses existing\napproaches in terms of control accuracy over pose, expression, and emotion,\nwhile also improving generative diversity under identity-only conditioning.", "authors": ["Kazuaki Mishima", "Antoni Bigata Casademunt", "Stavros Petridis", "Maja Pantic", "Kenji Suzuki"], "published_date": "2025-05-21", "title_zh": "人臉工匠：基於身份條件的擴散模型，實現對面部姿態、表情和情緒的解耦控制", "summary_zh": "人臉圖像包含豐富資訊，包括穩定身份特徵和可變屬性，如姿態、表情和情緒。雖然圖像生成技術在身份條件人臉合成方面取得進展，但精確控制非身份屬性仍然具挑戰性，且分離身份與這些可變因素尤其困難。為解決這些問題，我們提出一種新型身份條件擴散模型，引入兩個輕量級控制模塊，獨立操縱人臉姿態、表情和情緒，同時保持身份不變。這些模塊嵌入在基礎擴散模型的交叉注意力層中，以最小的參數開銷實現精確的屬性控制。此外，我們採用定制化訓練策略，利用身份特徵和每個非身份控制特徵之間的交叉注意力，促使身份特徵與控制信號保持正交，從而提高可控性和多樣性。定量、定性評估以及感知用戶研究表明，我們的方法在姿態、表情和情緒的控制精度上優於現有方法，同時提高了僅在身份條件下的生成多樣性。", "audio": "audios/2505.15313v1.mp3", "timestamp": "2025-05-22T23:34:22.160515"}
{"query": "AI", "id": "2505.15596v1", "url": "http://arxiv.org/abs/2505.15596v1", "title": "Exploring LLM-Generated Feedback for Economics Essays: How Teaching Assistants Evaluate and Envision Its Use", "summary": "This project examines the prospect of using AI-generated feedback as\nsuggestions to expedite and enhance human instructors' feedback provision. In\nparticular, we focus on understanding the teaching assistants' perspectives on\nthe quality of AI-generated feedback and how they may or may not utilize AI\nfeedback in their own workflows. We situate our work in a foundational college\nEconomics class, which has frequent short essay assignments. We developed an\nLLM-powered feedback engine that generates feedback on students' essays based\non grading rubrics used by the teaching assistants (TAs). To ensure that TAs\ncan meaningfully critique and engage with the AI feedback, we had them complete\ntheir regular grading jobs. For a randomly selected set of essays that they had\ngraded, we used our feedback engine to generate feedback and displayed the\nfeedback as in-text comments in a Word document. We then performed think-aloud\nstudies with 5 TAs over 20 1-hour sessions to have them evaluate the AI\nfeedback, contrast the AI feedback with their handwritten feedback, and share\nhow they envision using the AI feedback if they were offered as suggestions.\nThe study highlights the importance of providing detailed rubrics for AI to\ngenerate high-quality feedback for knowledge-intensive essays. TAs considered\nthat using AI feedback as suggestions during their grading could expedite\ngrading, enhance consistency, and improve overall feedback quality. We discuss\nthe importance of decomposing the feedback generation task into steps and\npresenting intermediate results, in order for TAs to use the AI feedback.", "authors": ["Xinyi Lu", "Aditya Mahesh", "Zejia Shen", "Mitchell Dudley", "Larissa Sano", "Xu Wang"], "published_date": "2025-05-21", "title_zh": "探索大型語言模型生成之經濟學論文回饋：教學助理如何評估並設想其應用", "summary_zh": "本研究探討利用人工智慧生成的回饋意見，加速並提升人工教師回饋品質的可能性。重點在於了解助教對AI生成回饋的品質觀感，以及他們如何運用AI回饋於工作流程中。研究以大學基礎經濟學課程為背景，該課程包含頻繁的短篇論文作業。開發了一款基於大型語言模型的回饋引擎，根據助教使用的評分標準為學生論文生成回饋。為確保助教有效評估和運用AI回饋，讓他們完成常規評分工作。針對隨機選取、已評分的論文，使用回饋引擎生成回饋，並以Word文檔內文評論形式呈現。與5位助教進行20次、每次1小時的思考發聲研究，評估AI回饋，並將其與手寫回饋進行對比，分享他們如何運用AI回饋作為建議。研究強調提供詳細評分標準，對AI生成高品質知識密集型論文回饋的重要性。助教認為，在評分過程中將AI回饋作為建議，可加速評分，增強一致性，並提高整體回饋品質。討論了將回饋生成任務分解為多個步驟並呈現中間結果，對於助教使用AI回饋的重要性。", "audio": "audios/2505.15596v1.mp3", "timestamp": "2025-05-22T16:24:40.489307"}
{"query": "Foundation Model", "id": "2505.15185v1", "url": "http://arxiv.org/abs/2505.15185v1", "title": "MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth Foundation Models", "summary": "Recent advances in generalizable 3D Gaussian Splatting have demonstrated\npromising results in real-time high-fidelity rendering without per-scene\noptimization, yet existing approaches still struggle to handle unfamiliar\nvisual content during inference on novel scenes due to limited\ngeneralizability. To address this challenge, we introduce MonoSplat, a novel\nframework that leverages rich visual priors from pre-trained monocular depth\nfoundation models for robust Gaussian reconstruction. Our approach consists of\ntwo key components: a Mono-Multi Feature Adapter that transforms monocular\nfeatures into multi-view representations, coupled with an Integrated Gaussian\nPrediction module that effectively fuses both feature types for precise\nGaussian generation. Through the Adapter's lightweight attention mechanism,\nfeatures are seamlessly aligned and aggregated across views while preserving\nvaluable monocular priors, enabling the Prediction module to generate Gaussian\nprimitives with accurate geometry and appearance. Through extensive experiments\non diverse real-world datasets, we convincingly demonstrate that MonoSplat\nachieves superior reconstruction quality and generalization capability compared\nto existing methods while maintaining computational efficiency with minimal\ntrainable parameters. Codes are available at\nhttps://github.com/CUHK-AIM-Group/MonoSplat.", "authors": ["Yifan Liu", "Keyu Fan", "Weihao Yu", "Chenxin Li", "Hao Lu", "Yixuan Yuan"], "published_date": "2025-05-21", "title_zh": "MonoSplat：基於單目深度基礎模型的可泛化三維高斯濺射", "summary_zh": "通用三維高斯濺射技術雖在即時高保真渲染上展現潛力，但現有方法在處理新場景時仍因泛化能力不足而難以應對不熟悉的視覺內容。為此，我們提出MonoSplat，一種利用預訓練單眼深度基礎模型的豐富視覺先驗進行穩健高斯重建的新框架。該方法包含：將單眼特徵轉換為多視圖表示的單-多特徵適配器，以及有效融合兩種特徵以精確生成高斯分佈的集成高斯預測模組。透過適配器的輕量級注意力機制，特徵在視圖間無縫對齊和聚合，同時保留寶貴的單眼先驗，使預測模組能生成具有準確幾何和外觀的高斯圖元。在多樣真實世界資料集上的實驗證明，相較於現有方法，MonoSplat在保持計算效率和極少可訓練參數的同時，實現了卓越的重建品質和泛化能力。程式碼可在https://github.com/CUHK-AIM-Group/MonoSplat取得。", "audio": "audios/2505.15185v1.mp3", "timestamp": "2025-05-22T16:25:09.297783"}
{"query": "Diffusion Model", "id": "2505.15157v1", "url": "http://arxiv.org/abs/2505.15157v1", "title": "Cascaded Diffusion Models for Neural Motion Planning", "summary": "Robots in the real world need to perceive and move to goals in complex\nenvironments without collisions. Avoiding collisions is especially difficult\nwhen relying on sensor perception and when goals are among clutter. Diffusion\npolicies and other generative models have shown strong performance in solving\nlocal planning problems, but often struggle at avoiding all of the subtle\nconstraint violations that characterize truly challenging global motion\nplanning problems. In this work, we propose an approach for learning global\nmotion planning using diffusion policies, allowing the robot to generate full\ntrajectories through complex scenes and reasoning about multiple obstacles\nalong the path. Our approach uses cascaded hierarchical models which unify\nglobal prediction and local refinement together with online plan repair to\nensure the trajectories are collision free. Our method outperforms (by ~5%) a\nwide variety of baselines on challenging tasks in multiple domains including\nnavigation and manipulation.", "authors": ["Mohit Sharma", "Adam Fishman", "Vikash Kumar", "Chris Paxton", "Oliver Kroemer"], "published_date": "2025-05-21", "title_zh": "用於神經運動規劃的級聯擴散模型", "summary_zh": "現實環境中的機器人需要在複雜場景中感知並移動至目標點，同時避免碰撞。仰賴感測器感知且目標位於雜亂環境時，避碰尤為困難。擴散策略及其他生成模型在解決局部規劃問題上表現出色，但在避免複雜全局運動規劃問題中細微的約束違規方面往往力不從心。本研究提出一種利用擴散策略學習全局運動規劃的方法，使機器人能夠生成穿越複雜場景的完整軌跡，並推論路徑上的多個障礙物。該方法採用級聯階層模型，結合全局預測與局部優化，並透過線上計畫修正確保軌跡無碰撞。在導航和操作等多個領域的挑戰性任務中，該方法優於多種基準方法約5%。", "audio": "audios/2505.15157v1.mp3", "timestamp": "2025-05-22T16:25:16.858980"}
{"query": "AI", "id": "2505.15590v1", "url": "http://arxiv.org/abs/2505.15590v1", "title": "Bridging the Gap: Physical PCI Device Integration Into SystemC-TLM Virtual Platforms", "summary": "In today's technology-driven world, early-stage software development and\ntesting are crucial. Virtual Platforms (VPs) have become indispensable tools\nfor this purpose as they serve as a platform to execute and debug the\nunmodified target software at an early design stage. With the increasing\ncomplexity of software, especially in areas like Artificial Intelligence (AI)\napplications, VPs need to provide high simulation speed to ensure the target\nsoftware executes within a reasonable time. Hybrid simulation, which combines\nvirtual models with real hardware, can improve the performance of VPs. This\npaper introduces a novel approach for integrating real Peripheral Component\nInterconnect (PCI) devices into SystemC-TLM-2.0-based VPs. The embedded PCI\ndevices enable high performance, easy integration, and allow introspection for\nanalysis and optimization. To illustrate the practical application of our\napproach, we present a case study where we integrate Google Coral's Edge Tensor\nProcessing Unit (TPU) into an ARM-based VP. The integration allows efficient\nexecution of AI workloads, accelerating simulation speeds by up to 480x while\neliminating the need for complex virtual device models. Beyond accelerating\nAI-workload execution, our framework enables driver development, regression\ntesting across architectures, and device communication analysis. Our findings\ndemonstrate that embedding PCI devices into SystemC simulations significantly\nenhances", "authors": ["Nils Bosbach", "Rebecca Pelke", "Niko Zurstraßen", "Jan Henrik Weinstock", "Lukas Jünger", "Rainer Leupers"], "published_date": "2025-05-21", "title_zh": "彌合鴻溝：將實體PCI裝置整合至SystemC-TLM虛擬平台", "summary_zh": "在當今科技驅動的世界中，早期軟體開發與測試至關重要。虛擬平台（VP）已成為不可或缺的工具，可在設計初期執行和除錯未修改的目標軟體。隨著軟體複雜性不斷增加，尤其是在人工智慧（AI）應用領域，VP需要提供高速模擬，以確保目標軟體在合理時間內執行。混合模擬結合虛擬模型與真實硬體，可提升VP效能。本文介紹一種將真實周邊組件互連（PCI）設備整合至基於SystemC-TLM-2.0的VP的新方法。嵌入式PCI設備實現了高性能、易於整合，並允許進行內省分析與優化。為說明該方法的實際應用，本文呈現了一個案例研究，將Google Coral Edge張量處理單元（TPU）整合到基於ARM的VP中。此整合可有效執行AI工作負載，加速模擬速度高達480倍，同時免除對複雜虛擬設備模型的需求。除了加速AI工作負載執行外，該框架還可實現驅動程式開發、跨架構回歸測試以及設備通訊分析。研究結果表明，將PCI設備嵌入SystemC模擬可顯著提高效能。", "audio": "audios/2505.15590v1.mp3", "timestamp": "2025-05-22T17:17:08.437912"}
{"query": "Foundation Model", "id": "2505.15151v1", "url": "http://arxiv.org/abs/2505.15151v1", "title": "Time Tracker: Mixture-of-Experts-Enhanced Foundation Time Series Forecasting Model with Decoupled Training Pipelines", "summary": "In the past few years, time series foundation models have achieved superior\npredicting accuracy. However, real-world time series often exhibit significant\ndiversity in their temporal patterns across different time spans and domains,\nmaking it challenging for a single model architecture to fit all complex\nscenarios. In addition, time series data may have multiple variables exhibiting\ncomplex correlations between each other. Recent mainstream works have focused\non modeling times series in a channel-independent manner in both pretraining\nand finetuning stages, overlooking the valuable inter-series dependencies. To\nthis end, we propose \\textbf{Time Tracker} for better predictions on\nmultivariate time series data. Firstly, we leverage sparse mixture of experts\n(MoE) within Transformers to handle the modeling of diverse time series\npatterns, thereby alleviating the learning difficulties of a single model while\nimproving its generalization. Besides, we propose Any-variate Attention,\nenabling a unified model structure to seamlessly handle both univariate and\nmultivariate time series, thereby supporting channel-independent modeling\nduring pretraining and channel-mixed modeling for finetuning. Furthermore, we\ndesign a graph learning module that constructs relations among sequences from\nfrequency-domain features, providing more precise guidance to capture\ninter-series dependencies in channel-mixed modeling. Based on these\nadvancements, Time Tracker achieves state-of-the-art performance in predicting\naccuracy, model generalization and adaptability.", "authors": ["Xiaohou Shi", "Ke Li", "Aobo Liang", "Yan Sun"], "published_date": "2025-05-21", "title_zh": "時間追蹤器：具解耦訓練管線之混合專家增強型基礎時間序列預測模型", "summary_zh": "近年來，時間序列基礎模型展現卓越的預測精度。然而，真實世界的時間序列在不同時間跨度和領域中呈現顯著多樣性，使單一模型架構難以適應所有複雜情境。此外，時間序列資料可能有多個變數，彼此間存在複雜關聯。目前主流研究多著重於預訓練和微調階段中通道獨立的時間序列建模，忽略了重要的序列間依賴關係。為此，我們提出Time Tracker，以提升多變數時間序列資料的預測效果。首先，我們在Transformer中使用稀疏混合專家模型(MoE)，處理多樣時間序列模式的建模，從而減輕單一模型的學習難度，並提高其泛化能力。其次，我們提出Any-variate Attention，使統一模型結構能無縫處理單變數和多變數時間序列，進而支援預訓練期間的通道獨立建模，以及微調期間的通道混合建模。此外，我們設計了一個圖學習模組，從頻域特徵中構建序列間關係，為通道混合建模中捕捉序列間依賴關係提供更精確的指導。基於這些進展，Time Tracker在預測精度、模型泛化能力和適應性方面均實現了最先進的性能。", "audio": "audios/2505.15151v1.mp3", "timestamp": "2025-05-22T17:17:18.583298"}
{"query": "Diffusion Model", "id": "2505.15152v1", "url": "http://arxiv.org/abs/2505.15152v1", "title": "Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation", "summary": "Feature Transformation (FT) crafts new features from original ones via\nmathematical operations to enhance dataset expressiveness for downstream\nmodels. However, existing FT methods exhibit critical limitations: discrete\nsearch struggles with enormous combinatorial spaces, impeding practical use;\nand continuous search, being highly sensitive to initialization and step sizes,\noften becomes trapped in local optima, restricting global exploration. To\novercome these limitations, DIFFT redefines FT as a reward-guided generative\ntask. It first learns a compact and expressive latent space for feature sets\nusing a Variational Auto-Encoder (VAE). A Latent Diffusion Model (LDM) then\nnavigates this space to generate high-quality feature embeddings, its\ntrajectory guided by a performance evaluator towards task-specific optima. This\nsynthesis of global distribution learning (from LDM) and targeted optimization\n(reward guidance) produces potent embeddings, which a novel semi-autoregressive\ndecoder efficiently converts into structured, discrete features, preserving\nintra-feature dependencies while allowing parallel inter-feature generation.\nExtensive experiments on 14 benchmark datasets show DIFFT consistently\noutperforms state-of-the-art baselines in predictive accuracy and robustness,\nwith significantly lower training and inference times.", "authors": ["Nanxu Gong", "Zijun Li", "Sixun Dong", "Haoyue Bai", "Wangyang Ying", "Xinyuan Wang", "Yanjie Fu"], "published_date": "2025-05-21", "title_zh": "從噪聲雕琢特徵：獎勵導向的分層擴散用於任務最佳特徵轉換", "summary_zh": "特徵轉換透過數學運算從原始特徵中創建新特徵，以增強資料集的表達能力。現有方法存在離散搜索組合空間龐大和連續搜索易陷入局部最佳解的局限。DIFFT將特徵轉換重新定義為獎勵引導的生成任務。首先，使用變分自編碼器學習特徵集的壓縮潛在空間。然後，潛在擴散模型在此空間中生成高品質特徵嵌入，其軌跡由性能評估器引導至特定任務的最佳狀態。這種全局分佈學習和目標優化的結合產生了強大的嵌入，新型半自迴歸解碼器將其高效轉換為結構化的離散特徵，在允許並行特徵間生成的同時，保留了特徵內部的依賴關係。在14個基準資料集上的實驗表明，DIFFT在預測準確性和穩健性方面始終優於現有方法，且訓練和推論時間顯著降低。", "audio": "audios/2505.15152v1.mp3", "timestamp": "2025-05-22T17:17:25.618844"}
{"query": "AI", "id": "2505.15571v1", "url": "http://arxiv.org/abs/2505.15571v1", "title": "Temporal Spectrum Cartography in Low-Altitude Economy Networks: A Generative AI Framework with Multi-Agent Learning", "summary": "This paper introduces a two-stage generative AI (GenAI) framework tailored\nfor temporal spectrum cartography in low-altitude economy networks (LAENets).\nLAENets, characterized by diverse aerial devices such as UAVs, rely heavily on\nwireless communication technologies while facing challenges, including spectrum\ncongestion and dynamic environmental interference. Traditional spectrum\ncartography methods have limitations in handling the temporal and spatial\ncomplexities inherent to these networks. Addressing these challenges, the\nproposed framework first employs a Reconstructive Masked Autoencoder (RecMAE)\ncapable of accurately reconstructing spectrum maps from sparse and temporally\nvarying sensor data using a novel dual-mask mechanism. This approach\nsignificantly enhances the precision of reconstructed radio frequency (RF)\npower maps. In the second stage, the Multi-agent Diffusion Policy (MADP) method\nintegrates diffusion-based reinforcement learning to optimize the trajectories\nof dynamic UAV sensors. By leveraging temporal-attention encoding, this method\neffectively manages spatial exploration and exploitation to minimize cumulative\nreconstruction errors. Extensive numerical experiments validate that this\nintegrated GenAI framework outperforms traditional interpolation methods and\ndeep learning baselines by achieving 57.35% and 88.68% reconstruction error\nreduction, respectively. The proposed trajectory planner substantially improves\nspectrum map accuracy, reconstruction stability, and sensor deployment\nefficiency in dynamically evolving low-altitude environments.", "authors": ["Changyuan Zhao", "Ruichen Zhang", "Jiacheng Wang", "Dusit Niyato", "Geng Sun", "Hongyang Du", "Zan Li", "Abbas Jamalipour", "Dong In Kim"], "published_date": "2025-05-21", "title_zh": "低空經濟網絡時序頻譜製圖：基於多代理學習的生成式人工智慧框架", "summary_zh": "本研究提出一個雙階段生成式AI框架，專為低空經濟網路中的時序頻譜地圖繪製設計。針對低空經濟網路中UAV等設備對無線通訊的依賴及其面臨的頻譜擁塞和動態干擾問題，傳統頻譜地圖繪製方法難以應對其時空複雜性。此框架首先採用重建式遮罩自編碼器（RecMAE），利用雙遮罩機制，從稀疏且隨時間變化的感測器數據中精確重建頻譜圖，顯著提升射頻功率地圖的精確度。其次，多智能體擴散策略（MADP）整合基於擴散的強化學習，優化動態UAV感測器的軌跡。透過時序注意力編碼，有效管理空間探索與利用，以最小化累積重建誤差。數值實驗驗證，此生成式AI框架優於傳統內插法和深度學習基準，分別降低57.35%和88.68%的重建誤差。此軌跡規劃器顯著提升動態低空環境中的頻譜圖準確性、重建穩定性和感測器部署效率。", "audio": "audios/2505.15571v1.mp3", "timestamp": "2025-05-22T20:20:33.666103"}
{"query": "Foundation Model", "id": "2505.15147v1", "url": "http://arxiv.org/abs/2505.15147v1", "title": "From Pixels to Images: Deep Learning Advances in Remote Sensing Image Semantic Segmentation", "summary": "Remote sensing images (RSIs) capture both natural and human-induced changes\non the Earth's surface, serving as essential data for environmental monitoring,\nurban planning, and resource management. Semantic segmentation (SS) of RSIs\nenables the fine-grained interpretation of surface features, making it a\ncritical task in remote sensing analysis. With the increasing diversity and\nvolume of RSIs collected by sensors on various platforms, traditional\nprocessing methods struggle to maintain efficiency and accuracy. In response,\ndeep learning (DL) has emerged as a transformative approach, enabling\nsubstantial advances in remote sensing image semantic segmentation (RSISS) by\nautomating feature extraction and improving segmentation accuracy across\ndiverse modalities. This paper revisits the evolution of DL-based RSISS by\ncategorizing existing approaches into four stages: the early pixel-based\nmethods, the prevailing patch-based and tile-based techniques, and the emerging\nimage-based strategies enabled by foundation models. We analyze these\ndevelopments from the perspective of feature extraction and learning\nstrategies, revealing the field's progression from pixel-level to tile-level\nand from unimodal to multimodal segmentation. Furthermore, we conduct a\ncomprehensive evaluation of nearly 40 advanced techniques on a unified dataset\nto quantitatively characterize their performance and applicability. This review\noffers a holistic view of DL-based SS for RS, highlighting key advancements,\ncomparative insights, and open challenges to guide future research.", "authors": ["Quanwei Liu", "Tao Huang", "Yanni Dong", "Jiaqi Yang", "Wei Xiang"], "published_date": "2025-05-21", "title_zh": "從像素到影像：遙感影像語義分割的深度學習進展", "summary_zh": "遙感影像記錄地球表面自然與人為變遷，是環境監測、都市規劃及資源管理的重要數據。遙感影像語義分割可精細解譯地表特徵，為遙感分析的關鍵任務。面對日益增多且多樣的遙感影像，傳統方法難以兼顧效率與準確性。深度學習通過自動化特徵提取和提升分割精度，已成為遙感影像語義分割的變革性方法。本文回顧基於深度學習的遙感影像語義分割發展歷程，將現有方法分為四個階段：早期基於像素的方法、主流的基於圖像塊/瓦片的方法，以及由基礎模型驅動的新興基於圖像的方法。我們從特徵提取和學習策略角度分析這些發展，揭示該領域從像素級到瓦片級、從單模態到多模態分割的演進。此外，我們在統一數據集上對近40種先進技術進行全面評估，量化其性能和適用性。本綜述全面展示基於深度學習的遙感影像語義分割，重點介紹關鍵進展、比較性見解和未決挑戰，以指導未來研究。", "audio": "audios/2505.15147v1.mp3", "timestamp": "2025-05-22T20:20:43.447800"}
{"query": "Diffusion Model", "id": "2505.15093v1", "url": "http://arxiv.org/abs/2505.15093v1", "title": "Steering Generative Models with Experimental Data for Protein Fitness Optimization", "summary": "Protein fitness optimization involves finding a protein sequence that\nmaximizes desired quantitative properties in a combinatorially large design\nspace of possible sequences. Recent developments in steering protein generative\nmodels (e.g diffusion models, language models) offer a promising approach.\nHowever, by and large, past studies have optimized surrogate rewards and/or\nutilized large amounts of labeled data for steering, making it unclear how well\nexisting methods perform and compare to each other in real-world optimization\ncampaigns where fitness is measured by low-throughput wet-lab assays. In this\nstudy, we explore fitness optimization using small amounts (hundreds) of\nlabeled sequence-fitness pairs and comprehensively evaluate strategies such as\nclassifier guidance and posterior sampling for guiding generation from\ndifferent discrete diffusion models of protein sequences. We also demonstrate\nhow guidance can be integrated into adaptive sequence selection akin to\nThompson sampling in Bayesian optimization, showing that plug-and-play guidance\nstrategies offer advantages compared to alternatives such as reinforcement\nlearning with protein language models.", "authors": ["Jason Yang", "Wenda Chu", "Daniel Khalil", "Raul Astudillo", "Bruce J. Wittmann", "Frances H. Arnold", "Yisong Yue"], "published_date": "2025-05-21", "title_zh": "利用實驗數據導引生成模型以優化蛋白質適應性", "summary_zh": "蛋白質適應性最佳化旨在廣大的序列空間中尋找能最大化所需定量性質的蛋白質序列。引導蛋白質生成模型（如擴散模型、語言模型）是個有潛力的途徑。然而，過去研究主要優化替代獎勵或依賴大量標記數據進行引導，難以評估現有方法在真實實驗中的表現和相互比較，因真實實驗通常以低通量濕實驗測定適應性。本研究探討使用少量（數百個）標記序列-適應性配對進行適應性最佳化，並全面評估分類器引導和後驗抽樣等策略，以引導不同蛋白質序列離散擴散模型的生成。我們也展示如何將引導整合到類似貝葉斯最佳化中湯普森抽樣的自適應序列選擇中，表明隨插即用引導策略優於使用蛋白質語言模型的強化學習等替代方案。", "audio": "audios/2505.15093v1.mp3", "timestamp": "2025-05-22T20:20:56.741332"}
{"query": "AI", "id": "2505.15528v1", "url": "http://arxiv.org/abs/2505.15528v1", "title": "PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided Gaussian Splatting", "summary": "Recent years have seen substantial improvements in the ability to generate\nsynthetic 3D objects using AI. However, generating complex 3D objects, such as\nplants, remains a considerable challenge. Current generative 3D models struggle\nwith plant generation compared to general objects, limiting their usability in\nplant analysis tools, which require fine detail and accurate geometry. We\nintroduce PlantDreamer, a novel approach to 3D synthetic plant generation,\nwhich can achieve greater levels of realism for complex plant geometry and\ntextures than available text-to-3D models. To achieve this, our new generation\npipeline leverages a depth ControlNet, fine-tuned Low-Rank Adaptation and an\nadaptable Gaussian culling algorithm, which directly improve textural realism\nand geometric integrity of generated 3D plant models. Additionally,\nPlantDreamer enables both purely synthetic plant generation, by leveraging\nL-System-generated meshes, and the enhancement of real-world plant point clouds\nby converting them into 3D Gaussian Splats. We evaluate our approach by\ncomparing its outputs with state-of-the-art text-to-3D models, demonstrating\nthat PlantDreamer outperforms existing methods in producing high-fidelity\nsynthetic plants. Our results indicate that our approach not only advances\nsynthetic plant generation, but also facilitates the upgrading of legacy point\ncloud datasets, making it a valuable tool for 3D phenotyping applications.", "authors": ["Zane K J Hartley", "Lewis A G Stuart", "Andrew P French", "Michael P Pound"], "published_date": "2025-05-21", "title_zh": "PlantDreamer：藉由擴散引導高斯潑濺實現逼真3D植物模型", "summary_zh": "近年人工智慧在3D物件生成能力上顯著提升，然複雜物件如植物之生成仍具挑戰。現有3D生成模型於植物生成方面表現遜於一般物件，限制其於植物分析工具之應用，因該等工具需精細細節與精確幾何。本研究提出PlantDreamer，一種新穎之3D合成植物生成方法，相較於現有文字轉3D模型，能實現更高層次之複雜植物幾何與紋理真實感。為此，本研究之生成流程採用深度ControlNet、微調之低秩適應及可調整之高斯剔除演算法，直接改善生成之3D植物模型之紋理真實感與幾何完整性。此外，PlantDreamer能藉由L系統生成之網格實現純合成植物生成，並透過將真實世界植物點雲轉換為3D高斯潑濺，進而強化該點雲。經由與最先進文字轉3D模型之比較評估，PlantDreamer在生成高保真合成植物方面優於現有方法。研究結果顯示，本方法不僅推進合成植物生成，亦有助於升級舊有點雲資料集，使其成為3D表型分析應用之寶貴工具。", "audio": "audios/2505.15528v1.mp3", "timestamp": "2025-05-22T21:17:16.678569"}
{"query": "Foundation Model", "id": "2505.15132v1", "url": "http://arxiv.org/abs/2505.15132v1", "title": "Multicrossmodal Automated Agent for Integrating Diverse Materials Science Data", "summary": "We introduce a multicrossmodal LLM-agent framework motivated by the growing\nvolume and diversity of materials-science data ranging from high-resolution\nmicroscopy and dynamic simulation videos to tabular experiment logs and\nsprawling literature archives. While recent AI efforts have accelerated\nindividual tasks such as property prediction or image classification, they\ntypically treat each modality in isolation, leaving rich cross-modal\ncorrelations unexplored and forcing researchers to perform laborious manual\nintegration. Moreover, existing multimodal foundation models often require\nexpensive retraining or fine-tuning on domain data, and current multi-agent\nsystems in materials informatics address only narrow subtasks. To overcome\nthese obstacles, we design a coordinated team of specialized LLM agents, each\nequipped with domain-adapted prompts and plugins that project their outputs\ninto a shared embedding space. A dynamic gating mechanism then weights and\nmerges these insights, enabling unified reasoning over heterogeneous inputs\nwithout ever modifying the underlying LLM weights. We validate our approach on\nchallenging case studies and demonstrate substantial gains in retrieval\naccuracy (85%), captioning fidelity, and integrated coverage (35%) compared to\nsingle-modality and zero-shot baselines. Our work paves the way for AI digital\nresearchers capable of bridging data silos and accelerating the\nmaterials-discovery cycle. The code is available at\nhttps://github.com/adibgpt/Multicrossmodal-Autonomous-Materials-Science-Agent.", "authors": ["Adib Bazgir", "Rama chandra Praneeth Madugula", "Yuwen Zhang"], "published_date": "2025-05-21", "title_zh": "用於整合多樣材料科學數據的多模態自動化代理", "summary_zh": "本研究提出一個多重跨模態大型語言模型代理框架，旨在應對材料科學領域日益增長且多樣化的數據，包含高解析度顯微鏡影像、動態模擬影片、表格實驗日誌以及大量的文獻檔案。現有AI研究雖加速了材料性質預測或影像分類等任務，但通常孤立地處理各模態數據，忽略了豐富的跨模態關聯性，導致研究人員需耗費大量精力進行人工整合。此外，現有的多模態基礎模型往往需要在領域數據上進行昂貴的重新訓練或微調，而目前的材料資訊學多代理系統僅解決狹窄的子任務。為克服這些障礙，我們設計了一個協調的專用大型語言模型代理團隊，每個代理都配備了領域特定的提示和插件，將其輸出投影到共享嵌入空間中。然後，動態門控機制對這些見解進行加權和合併，實現對異質輸入的統一推理，且無需修改底層大型語言模型的權重。我們在具挑戰性的案例研究中驗證了該方法，並證明與單模態和零樣本基準相比，檢索準確度（85%）、字幕保真度和整合覆蓋率（35%）顯著提升。本研究為能夠橋接數據孤島並加速材料發現週期的人工智慧數位研究人員奠定了基礎。", "audio": "audios/2505.15132v1.mp3", "timestamp": "2025-05-22T21:17:24.359867"}
{"query": "Diffusion Model", "id": "2505.15077v1", "url": "http://arxiv.org/abs/2505.15077v1", "title": "Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation", "summary": "Urban forests play a key role in enhancing environmental quality and\nsupporting biodiversity in cities. Mapping and monitoring these green spaces\nare crucial for urban planning and conservation, yet accurately detecting trees\nis challenging due to complex landscapes and the variability in image\nresolution caused by different satellite sensors or UAV flight altitudes. While\ndeep learning architectures have shown promise in addressing these challenges,\ntheir effectiveness remains strongly dependent on the availability of large and\nmanually labeled datasets, which are often expensive and difficult to obtain in\nsufficient quantity. In this work, we propose a novel pipeline that integrates\ndomain adaptation with GANs and Diffusion models to enhance the quality of\nlow-resolution aerial images. Our proposed pipeline enhances low-resolution\nimagery while preserving semantic content, enabling effective tree segmentation\nwithout requiring large volumes of manually annotated data. Leveraging models\nsuch as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we\ngenerate realistic and structurally consistent synthetic samples that expand\nthe training dataset and unify scale across domains. This approach not only\nimproves the robustness of segmentation models across different acquisition\nconditions but also provides a scalable and replicable solution for remote\nsensing scenarios with scarce annotation resources. Experimental results\ndemonstrated an improvement of over 50% in IoU for low-resolution images,\nhighlighting the effectiveness of our method compared to traditional pipelines.", "authors": ["Alessandro dos Santos Ferreira", "Ana Paula Marques Ramos", "José Marcato Junior", "Wesley Nunes Gonçalves"], "published_date": "2025-05-21", "title_zh": "基於GAN與擴散模型的數據增強和分辨率提升於樹木分割", "summary_zh": "都市森林對於提升城市環境品質和支持生物多樣性至關重要。繪製和監測這些綠地對於城市規劃和保育至關重要，然而，由於複雜的景觀以及不同衛星感測器或無人機飛行高度導致的影像解析度變化，準確檢測樹木極具挑戰性。深度學習架構已展現解決這些挑戰的潛力，但其有效性仍高度依賴於大量手動標記資料集，而這些資料集通常成本高昂且難以充分獲取。本研究提出一種新穎流程，整合領域自適應與GANs和擴散模型，以提高低解析度航拍影像的品質。此流程增強了低解析度影像，同時保留了語義內容，從而實現有效的樹木分割，而無需大量手動註釋資料。利用pix2pix、Real-ESRGAN、潛在擴散和穩定擴散等模型，生成逼真且結構一致的合成樣本，以擴展訓練資料集並統一跨領域的尺度。此方法不僅提高了分割模型在不同獲取條件下的穩健性，還為註釋資源稀缺的遙感場景提供了可擴展和可複製的解決方案。實驗結果表明，低解析度影像的IoU提高了50%以上，突顯了該方法相較於傳統流程的有效性。", "audio": "audios/2505.15077v1.mp3", "timestamp": "2025-05-22T21:17:32.045043"}
{"query": "AI", "id": "2505.15519v1", "url": "http://arxiv.org/abs/2505.15519v1", "title": "Exploiting Age of Information in Network Digital Twins for AI-driven Real-Time Link Blockage Detection", "summary": "The Line-of-Sight (LoS) identification is crucial to ensure reliable\nhigh-frequency communication links, especially those vulnerable to blockages.\nNetwork Digital Twins and Artificial Intelligence are key technologies enabling\nblockage detection (LoS identification) for high-frequency wireless systems,\ne.g., 6>GHz. In this work, we enhance Network Digital Twins by incorporating\nAge of Information (AoI) metrics, a quantification of status update freshness,\nenabling reliable real-time blockage detection (LoS identification) in dynamic\nwireless environments. By integrating raytracing techniques, we automate\nlarge-scale collection and labeling of channel data, specifically tailored to\nthe evolving conditions of the environment. The introduced AoI is integrated\nwith the loss function to prioritize more recent information to fine-tune deep\nlearning models in case of performance degradation (model drift). The\neffectiveness of the proposed solution is demonstrated in realistic urban\nsimulations, highlighting the trade-off between input resolution, computational\ncost, and model performance. A resolution reduction of 4x8 from an original\nchannel sample size of (32, 1024) along the angle and subcarrier dimension\nresults in a computational speedup of 32 times. The proposed fine-tuning\nsuccessfully mitigates performance degradation while requiring only 1% of the\navailable data samples, enabling automated and fast mitigation of model drifts.", "authors": ["Michele Zhu", "Francesco Linsalata", "Silvia Mura", "Lorenzo Cazzella", "Damiano Badini", "Umberto Spagnolini"], "published_date": "2025-05-21", "title_zh": "利用網絡數位孿生中的資訊年齡實現人工智慧驅動的即時鏈路阻塞偵測", "summary_zh": "視距(LoS)識別對於確保可靠的高頻通訊鏈路至關重要，特別是易受阻擋的鏈路。網絡數位雙生與人工智慧是實現高頻無線系統（如6GHz以上頻段）阻擋偵測（LoS識別）的關鍵技術。本研究通過納入資訊年齡(AoI)指標來強化網絡數位雙生，量化狀態更新的新鮮度，從而在動態無線環境中實現可靠的即時阻擋偵測（LoS識別）。通過整合射線追蹤技術，我們自動化大規模通道數據的收集與標記，專門針對不斷變化的環境條件。引入的AoI與損失函數整合，優先處理較新的資訊，以在性能下降（模型漂移）時微調深度學習模型。在真實的城市模擬中驗證了所提出解決方案的有效性，突顯了輸入分辨率、計算成本和模型性能之間的權衡。相較於(32, 1024)的原始通道樣本尺寸，角度和子載波維度的分辨率降低4x8倍，計算速度提升32倍。所提出的微調成功緩解了性能下降，同時僅需1%的可用數據樣本，實現了自動化且快速的模型漂移緩解。", "audio": "audios/2505.15519v1.mp3", "timestamp": "2025-05-22T23:17:30.158419"}
{"query": "Foundation Model", "id": "2505.15116v1", "url": "http://arxiv.org/abs/2505.15116v1", "title": "Graph Foundation Models: A Comprehensive Survey", "summary": "Graph-structured data pervades domains such as social networks, biological\nsystems, knowledge graphs, and recommender systems. While foundation models\nhave transformed natural language processing, vision, and multimodal learning\nthrough large-scale pretraining and generalization, extending these\ncapabilities to graphs -- characterized by non-Euclidean structures and complex\nrelational semantics -- poses unique challenges and opens new opportunities. To\nthis end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose\nintelligence to structured data, enabling broad transfer across graph-centric\ntasks and domains. This survey provides a comprehensive overview of GFMs,\nunifying diverse efforts under a modular framework comprising three key\ncomponents: backbone architectures, pretraining strategies, and adaptation\nmechanisms. We categorize GFMs by their generalization scope -- universal,\ntask-specific, and domain-specific -- and review representative methods, key\ninnovations, and theoretical insights within each category. Beyond methodology,\nwe examine theoretical foundations including transferability and emergent\ncapabilities, and highlight key challenges such as structural alignment,\nheterogeneity, scalability, and evaluation. Positioned at the intersection of\ngraph learning and general-purpose AI, GFMs are poised to become foundational\ninfrastructure for open-ended reasoning over structured data. This survey\nconsolidates current progress and outlines future directions to guide research\nin this rapidly evolving field. Resources are available at\nhttps://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.", "authors": ["Zehong Wang", "Zheyuan Liu", "Tianyi Ma", "Jiazheng Li", "Zheyuan Zhang", "Xingbo Fu", "Yiyang Li", "Zhengqing Yuan", "Wei Song", "Yijun Ma", "Qingkai Zeng", "Xiusi Chen", "Jianan Zhao", "Jundong Li", "Meng Jiang", "Pietro Lio", "Nitesh Chawla", "Chuxu Zhang", "Yanfang Ye"], "published_date": "2025-05-21", "title_zh": "圖基礎模型：一份綜合綜述", "summary_zh": "圖結構數據廣泛存在於社交網路、生物系統、知識圖譜及推薦系統等領域。大型預訓練模型已革新自然語言處理、視覺及多模態學習，然將此能力擴展至具非歐幾里德結構及複雜關係語義的圖數據，面臨獨特挑戰及機遇。圖基礎模型旨在為結構化數據帶來可擴展的通用智能，實現跨圖中心任務與領域的廣泛遷移。本綜述提供圖基礎模型的全面概述，透過包含主幹架構、預訓練策略和適應機制的三模塊框架整合各項研究。依據通用性範圍（通用、任務特定及領域特定）對圖基礎模型進行分類，並回顧各類別的代表性方法、創新及理論見解。除方法論外，亦檢視包含可遷移性和湧現能力在內的理論基礎，並強調結構對齊、異質性、可擴展性及評估等關鍵挑戰。圖基礎模型位於圖學習與通用人工智慧的交叉點，有望成為結構化數據開放式推理的基礎設施。本綜述總結當前進展並概述未來方向，以指導此快速發展領域的研究。資源位於https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs。", "audio": "audios/2505.15116v1.mp3", "timestamp": "2025-05-22T23:17:42.449436"}
{"query": "Diffusion Model", "id": "2505.15064v1", "url": "http://arxiv.org/abs/2505.15064v1", "title": "Generalization Through Growth: Hidden Dynamics Controls Depth Dependence", "summary": "Recent theory has reduced the depth dependence of generalization bounds from\nexponential to polynomial and even depth-independent rates, yet these results\nremain tied to specific architectures and Euclidean inputs. We present a\nunified framework for arbitrary \\blue{pseudo-metric} spaces in which a\ndepth-\\(k\\) network is the composition of continuous hidden maps\n\\(f:\\mathcal{X}\\to \\mathcal{X}\\) and an output map \\(h:\\mathcal{X}\\to\n\\mathbb{R}\\). The resulting bound $O(\\sqrt{(\\alpha + \\log \\beta(k))/n})$\nisolates the sole depth contribution in \\(\\beta(k)\\), the word-ball growth of\nthe semigroup generated by the hidden layers. By Gromov's theorem polynomial\n(resp. exponential) growth corresponds to virtually nilpotent (resp. expanding)\ndynamics, revealing a geometric dichotomy behind existing $O(\\sqrt{k})$\n(sublinear depth) and $\\tilde{O}(1)$ (depth-independent) rates. We further\nprovide covering-number estimates showing that expanding dynamics yield an\nexponential parameter saving via compositional expressivity. Our results\ndecouple specification from implementation, offering architecture-agnostic and\ndynamical-systems-aware guarantees applicable to modern deep-learning paradigms\nsuch as test-time inference and diffusion models.", "authors": ["Sho Sonoda", "Yuka Hashimoto", "Isao Ishikawa", "Masahiro Ikeda"], "published_date": "2025-05-21", "title_zh": "透過成長實現泛化：隱藏動態控制深度依賴性", "summary_zh": "近期理論已將泛化邊界的深度依賴性從指數級降低到多項式級甚至深度無關的速率，但這些結果仍與特定架構和歐幾里得輸入相關。我們提出了一個統一框架，適用於任意偽度量空間，其中深度為\\(k\\)的網路是連續隱藏映射\\(f:\\mathcal{X}\\to \\mathcal{X}\\)和輸出映射\\(h:\\mathcal{X}\\to \\mathbb{R}\\)的組合。產生的邊界\\(O(\\sqrt{(\\alpha + \\log \\beta(k))/n})\\)將唯一的深度貢獻隔離在\\(\\beta(k)\\)中，即隱藏層生成的半群的詞球增長。藉由Gromov定理，多項式（或指數）增長對應於幾乎冪零（或擴張）動力學，揭示了現有\\(O(\\sqrt{k})\\)（亞線性深度）和\\(\\tilde{O}(1)\\)（深度無關）速率背後的幾何二分法。我們進一步提供覆蓋數估計，表明擴張動力學透過組合表達性實現了指數級參數節省。我們的結果將規範與實現分離，提供架構無關且具有動態系統感知的保證，適用於現代深度學習範例，如測試時推論和擴散模型。", "audio": "audios/2505.15064v1.mp3", "timestamp": "2025-05-22T23:17:55.865232"}
{"query": "AI", "id": "2505.15516v1", "url": "http://arxiv.org/abs/2505.15516v1", "title": "Explainable embeddings with Distance Explainer", "summary": "While eXplainable AI (XAI) has advanced significantly, few methods address\ninterpretability in embedded vector spaces where dimensions represent complex\nabstractions. We introduce Distance Explainer, a novel method for generating\nlocal, post-hoc explanations of embedded spaces in machine learning models. Our\napproach adapts saliency-based techniques from RISE to explain the distance\nbetween two embedded data points by assigning attribution values through\nselective masking and distance-ranked mask filtering. We evaluate Distance\nExplainer on cross-modal embeddings (image-image and image-caption pairs) using\nestablished XAI metrics including Faithfulness, Sensitivity/Robustness, and\nRandomization. Experiments with ImageNet and CLIP models demonstrate that our\nmethod effectively identifies features contributing to similarity or\ndissimilarity between embedded data points while maintaining high robustness\nand consistency. We also explore how parameter tuning, particularly mask\nquantity and selection strategy, affects explanation quality. This work\naddresses a critical gap in XAI research and enhances transparency and\ntrustworthiness in deep learning applications utilizing embedded spaces.", "authors": ["Christiaan Meijer", "E. G. Patrick Bos"], "published_date": "2025-05-21", "title_zh": "基於距離解釋器的可解釋嵌入", "summary_zh": "可解釋人工智慧(XAI)雖有顯著進展，但針對嵌入向量空間(其維度代表複雜抽象概念)之可解釋性的方法仍然有限。本研究提出距離解釋器(Distance Explainer)，一種新穎的後設局部解釋方法，用於解釋機器學習模型中嵌入空間的距離。本方法改編自RISE的顯著性技術，透過選擇性遮罩和距離排序遮罩過濾來分配歸因值，以解釋兩個嵌入資料點之間的距離。我們使用既定的XAI指標(包括忠實度、敏感度/穩健性和隨機化)在跨模態嵌入(圖像-圖像和圖像-標題對)上評估距離解釋器。使用ImageNet和CLIP模型的實驗表明，該方法有效地識別了有助於嵌入資料點之間相似性或相異性的特徵，同時保持高度的穩健性和一致性。我們還探討了參數調整，特別是遮罩數量和選擇策略，如何影響解釋品質。本研究解決了XAI研究中的一個關鍵缺口，並提高了利用嵌入空間的深度學習應用程式的透明度和可信度。", "audio": "audios/2505.15516v1.mp3", "timestamp": "2025-05-23T01:27:16.595148"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略引導展平層級結構", "summary_zh": "離線目標條件強化學習(GCRL)有望於大規模無獎勵軌跡數據集上預訓練通用策略，類似於電腦視覺和自然語言處理中用於訓練基礎模型的自監督目標。然而，由於稀疏獎勵和折扣的組合，將GCRL擴展到更長的時間範圍仍然具有挑戰性，這模糊了原始動作相對於遠期目標的比較優勢。分層強化學習方法在長程目標達成任務上取得了良好的實證結果，但其對模組化、特定時間尺度策略和子目標生成的依賴引入了顯著的額外複雜性，並阻礙了在高維目標空間中的擴展。本研究提出一種演算法，透過優勢加權重要性抽樣，基於子目標條件策略進行自舉，以訓練扁平(非分層)目標條件策略。此方法無需目標空間上的生成模型，這對於在大狀態空間中擴展到高維控制至關重要。我們進一步表明，現有的分層和基於自舉的方法對應於我們推導中的特定設計選擇。在一系列全面的基於狀態和像素的運動和操控基準測試中，我們的方法與最先進的離線GCRL演算法相匹配或超越，並可擴展到先前方法失敗的複雜長程任務。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-23T01:27:26.087450"}
{"query": "Diffusion Model", "id": "2505.15057v1", "url": "http://arxiv.org/abs/2505.15057v1", "title": "Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine Diffusion Models", "summary": "Magnetic Resonance Imaging (MRI) is highly susceptible to motion artifacts\ndue to the extended acquisition times required for k-space sampling. These\nartifacts can compromise diagnostic utility, particularly for dynamic imaging.\nWe propose a novel alternating minimization framework that leverages a bespoke\ndiffusion model to jointly reconstruct and correct non-rigid motion-corrupted\nk-space data. The diffusion model uses a coarse-to-fine denoising strategy to\ncapture large overall motion and reconstruct the lower frequencies of the image\nfirst, providing a better inductive bias for motion estimation than that of\nstandard diffusion models. We demonstrate the performance of our approach on\nboth real-world cine cardiac MRI datasets and complex simulated rigid and\nnon-rigid deformations, even when each motion state is undersampled by a factor\nof 64x. Additionally, our method is agnostic to sampling patterns, anatomical\nvariations, and MRI scanning protocols, as long as some low frequency\ncomponents are sampled during each motion state.", "authors": ["Frederic Wang", "Jonathan I. Tamir"], "published_date": "2025-05-21", "title_zh": "基於粗細粒度擴散模型的磁共振成像非剛性運動校正重建", "summary_zh": "磁振造影易受運動偽影影響，因其k空間採樣需時較長，此偽影損害診斷效用，尤其於動態影像。本研究提出一種新型交替最小化框架，利用客製化擴散模型聯合重建並校正非剛性運動所汙染之k空間數據。此擴散模型採用由粗到細的去噪策略，先捕捉整體大範圍運動並重建影像低頻部分，為運動估計提供優於標準擴散模型的歸納偏置。實驗結果顯示，本方法於真實心臟電影磁振造影數據集以及複雜模擬的剛性和非剛性變形中皆表現良好，即使在每個運動狀態下欠採樣64倍時亦然。此外，本方法不受採樣模式、解剖結構變異和磁振造影掃描協議的限制，惟需確保在每個運動狀態下皆採樣部分低頻成分。", "audio": "audios/2505.15057v1.mp3", "timestamp": "2025-05-23T01:27:33.657456"}
{"query": "AI", "id": "2505.17021v1", "url": "http://arxiv.org/abs/2505.17021v1", "title": "ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark", "summary": "As Large Multimodal Models (LMMs) become more capable, there is growing\ninterest in evaluating their reasoning processes alongside their final outputs.\nHowever, most benchmarks remain focused on English, overlooking languages with\nrich linguistic and cultural contexts, such as Arabic. To address this gap, we\nintroduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the\nfirst benchmark designed to evaluate step-by-step reasoning in Arabic across\nboth textual and visual modalities. ARB spans 11 diverse domains, including\nvisual reasoning, document understanding, OCR, scientific analysis, and\ncultural interpretation. It comprises 1,356 multimodal samples paired with\n5,119 human-curated reasoning steps and corresponding actions. We evaluated 12\nstate-of-the-art open- and closed-source LMMs and found persistent challenges\nin coherence, faithfulness, and cultural grounding. ARB offers a structured\nframework for diagnosing multimodal reasoning in underrepresented languages and\nmarks a critical step toward inclusive, transparent, and culturally aware AI\nsystems. We release the benchmark, rubric, and evaluation suit to support\nfuture research and reproducibility. Code available at:\nhttps://github.com/mbzuai-oryx/ARB", "authors": ["Sara Ghaboura", "Ketan More", "Wafa Alghallabi", "Omkar Thawakar", "Jorma Laaksonen", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "published_date": "2025-05-22", "title_zh": "ARB：綜合性阿拉伯語多模態推理基準", "summary_zh": "大型多模態模型能力日漸提升，對其推理過程的評估日益重要。然而，現有評測多集中於英語，忽略了如阿拉伯語等具有豐富語言及文化背景的語種。為此，我們推出綜合阿拉伯語多模態推理基準（ARB），首個旨在評估阿拉伯語文本與視覺模態逐步推理的基準。ARB涵蓋視覺推理、文檔理解、光學字元識別、科學分析和文化詮釋等11個領域，包含1356個多模態樣本，並搭配5119個人工校正的推理步驟與相應行動。對12個頂尖開放及封閉源碼大型多模態模型的評估顯示，模型在連貫性、忠實性和文化基礎方面仍面臨挑戰。ARB為診斷代表性不足語言的多模態推理提供結構化框架，是邁向具包容性、透明化和文化意識的人工智慧系統的關鍵一步。我們公開基準、評分標準與評估套件，以支持未來研究和可重複性。程式碼見：https://github.com/mbzuai-oryx/ARB", "audio": "audios/2505.17021v1.mp3", "timestamp": "2025-05-23T03:10:34.878775"}
{"query": "Foundation Model", "id": "2505.16982v1", "url": "http://arxiv.org/abs/2505.16982v1", "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "summary": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "published_date": "2025-05-22", "title_zh": "超越相關性：邁向生物醫學領域的因果大型語言模型代理", "summary_zh": "大型語言模型在生物醫學領域展現潛力，但缺乏真正的因果理解，僅依賴關聯性。本文設想因果大型語言模型代理，整合多模態數據（文本、圖像、基因組等），並執行基於干預的推理以推斷因果關係。實現此目標需克服多項挑戰：設計安全、可控的代理框架；開發嚴格的因果評估基準；整合異質數據源；以及將大型語言模型與結構化知識圖譜和正式因果推論工具結合。此類代理可釋放變革性機遇，包含透過自動假設生成和模擬加速藥物發現，並透過特定患者的因果模型實現個人化醫療。此研究旨在促進跨學科合作，橋接因果概念與基礎模型，以開發可靠的人工智慧夥伴，促進生物醫學進展。", "audio": "audios/2505.16982v1.mp3", "timestamp": "2025-05-23T03:10:43.637960"}
{"query": "Diffusion Model", "id": "2505.17013v1", "url": "http://arxiv.org/abs/2505.17013v1", "title": "When Are Concepts Erased From Diffusion Models?", "summary": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models.", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "published_date": "2025-05-22", "title_zh": "擴散模型中概念的擦除時機", "summary_zh": "概念擦除，即選擇性阻止模型生成特定概念的能力，備受關注，並湧現多種方法應對此挑戰。然而，這些方法擦除目標概念的徹底程度仍不明確。本文首先提出擴散模型中擦除機制的兩種概念模型：(一)降低生成目標概念的可能性，(二)干擾模型的內部引導機制。為徹底評估概念是否已從模型中真正擦除，本文引入一套獨立評估方法，包含對抗性攻擊、新型探測技術，以及對模型替代生成的分析。研究結果闡明了最小化副作用與維持對抗性提示的穩健性之間的緊張關係。總體而言，本文強調了對擴散模型中擦除進行全面評估的重要性。", "audio": "audios/2505.17013v1.mp3", "timestamp": "2025-05-23T03:10:50.888157"}
{"query": "AI", "id": "2505.17019v1", "url": "http://arxiv.org/abs/2505.17019v1", "title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework", "summary": "Metaphorical comprehension in images remains a critical challenge for AI\nsystems, as existing models struggle to grasp the nuanced cultural, emotional,\nand contextual implications embedded in visual content. While multimodal large\nlanguage models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they\nstruggle with a fundamental limitation on image implication tasks: contextual\ngaps that obscure the relationships between different visual elements and their\nabstract meanings. Inspired by the human cognitive process, we propose Let\nAndroids Dream (LAD), a novel framework for image implication understanding and\nreasoning. LAD addresses contextual missing through the three-stage framework:\n(1) Perception: converting visual information into rich and multi-level textual\nrepresentations, (2) Search: iteratively searching and integrating cross-domain\nknowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment\nimage implication via explicit reasoning. Our framework with the lightweight\nGPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English\nimage implication benchmark and a huge improvement on Chinese benchmark,\nperforming comparable with the GPT-4o model on Multiple-Choice Question (MCQ)\nand outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work\nprovides new insights into how AI can more effectively interpret image\nimplications, advancing the field of vision-language reasoning and human-AI\ninteraction. Our project is publicly available at\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.", "authors": ["Chenhao Zhang", "Yazhe Niu"], "published_date": "2025-05-22", "title_zh": "安卓是否夢見電子羊：類人圖像意涵理解與推理框架", "summary_zh": "圖像隱喻理解對人工智慧系統構成重大挑戰，現有模型難以掌握視覺內容中細微的文化、情感與情境意涵。多模態大型語言模型雖擅長基本視覺問答，但在圖像意涵任務中受限於情境缺口，難以辨識視覺元素間的關係及其抽象意義。受人類認知啟發，我們提出Let Androids Dream (LAD)框架，用於圖像意涵理解與推理。LAD透過三階段解決情境缺失：(1)感知：將視覺資訊轉化為豐富的多層次文本表示；(2)搜尋：迭代搜尋並整合跨領域知識以消除歧義；(3)推理：透過顯式推理產生情境對齊的圖像意涵。搭載輕量級GPT-4o-mini模型，本框架在英語圖像意涵基準測試中達到最佳效能，並在中文基準測試中實現顯著提升，在選擇題上與GPT-4o模型相當，在開放式問題上超越36.7%。本研究為人工智慧如何更有效地解讀圖像意涵提供了新見解，推進了視覺語言推理與人機互動領域。專案已公開。", "audio": "audios/2505.17019v1.mp3", "timestamp": "2025-05-23T04:22:00.731004"}
{"query": "Foundation Model", "id": "2505.16941v1", "url": "http://arxiv.org/abs/2505.16941v1", "title": "FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records", "summary": "Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models.", "authors": ["Chao Pang", "Vincent Jeanselme", "Young Sang Choi", "Xinzhuo Jiang", "Zilin Jing", "Aparajita Kashyap", "Yuta Kobayashi", "Yanwei Li", "Florent Pollet", "Karthik Natarajan", "Shalmali Joshi"], "published_date": "2025-05-22", "title_zh": "FoMoH：結構化電子病歷臨床意義基礎模型評估", "summary_zh": "基於其獨立於下游任務提取有意義表徵的能力，基石模型在醫療保健領域展現出巨大潛力。此特性已促成結構化電子病歷數據訓練的多項臨床應用達到頂尖效能，即使在標記數據有限的環境中，這也是醫療保健領域常見的挑戰。然而，由於缺乏對全面且有意義任務的理想要求以及充分多樣化的評估以描述相較於傳統監督式學習的優勢，對於這些模型在臨床上的實用性鮮少共識。為了解決此差距，我們提出一系列具有臨床意義的任務，涵蓋患者預後、急慢性疾病的早期預測，並包含穩健評估的理想要求。我們使用來自紐約市哥倫比亞大學歐文醫學中心 (CUMC) 的五百萬患者的電子病歷數據，在 14 項臨床相關任務中評估了最先進的基石模型。我們測量了總體準確性、校準和亞群體效能，以揭示基於預訓練、標記化和數據表示策略選擇的權衡。本研究旨在推進結構化電子病歷基石模型的實證評估，並指導未來醫療保健基石模型的開發。", "audio": "audios/2505.16941v1.mp3", "timestamp": "2025-05-23T04:22:07.646757"}
{"query": "Diffusion Model", "id": "2505.17004v1", "url": "http://arxiv.org/abs/2505.17004v1", "title": "Guided Diffusion Sampling on Function Spaces with Applications to PDEs", "summary": "We propose a general framework for conditional sampling in PDE-based inverse\nproblems, targeting the recovery of whole solutions from extremely sparse or\nnoisy measurements. This is accomplished by a function-space diffusion model\nand plug-and-play guidance for conditioning. Our method first trains an\nunconditional discretization-agnostic denoising model using neural operator\narchitectures. At inference, we refine the samples to satisfy sparse\nobservation data via a gradient-based guidance mechanism. Through rigorous\nmathematical analysis, we extend Tweedie's formula to infinite-dimensional\nHilbert spaces, providing the theoretical foundation for our posterior sampling\napproach. Our method (FunDPS) accurately captures posterior distributions in\nfunction spaces under minimal supervision and severe data scarcity. Across five\nPDE tasks with only 3% observation, our method achieves an average 32% accuracy\nimprovement over state-of-the-art fixed-resolution diffusion baselines while\nreducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning\nensures strong cross-resolution generalizability. To the best of our knowledge,\nthis is the first diffusion-based framework to operate independently of\ndiscretization, offering a practical and flexible solution for forward and\ninverse problems in the context of PDEs. Code is available at\nhttps://github.com/neuraloperator/FunDPS", "authors": ["Jiachen Yao", "Abbas Mammadov", "Julius Berner", "Gavin Kerrigan", "Jong Chul Ye", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "published_date": "2025-05-22", "title_zh": "函數空間上的導引擴散採樣及其在偏微分方程中的應用", "summary_zh": "本研究提出一個基於偏微分方程逆問題的條件採樣通用框架，旨在從極度稀疏或含噪量測中復原完整解。該方法利用函數空間擴散模型與隨插即用引導實現條件化。首先，採用神經算子架構訓練一個無條件、與離散化無關的去噪模型。在推論階段，透過基於梯度的引導機制精煉樣本，以滿足稀疏觀測數據。透過嚴謹的數學分析，將Tweedie公式擴展到無限維希爾伯特空間，為後驗採樣方法提供理論基礎。此方法(FunDPS)能在極少的監督和嚴重的數據匱乏情況下，準確捕獲函數空間中的後驗分佈。在五項偏微分方程任務中，僅使用3%的觀測數據，此方法比最先進的固定解析度擴散模型平均提高了32%的準確性，同時減少了4倍的採樣步驟。此外，多解析度微調確保了強大的跨解析度泛化能力。據我們所知，這是第一個獨立於離散化運作的基於擴散的框架，為偏微分方程背景下的正向和逆向問題提供了一種實用且靈活的解決方案。代碼位於[https://github.com/neuraloperator/FunDPS](https://github.com/neuraloperator/FunDPS)。", "audio": "audios/2505.17004v1.mp3", "timestamp": "2025-05-23T04:22:15.738654"}
{"query": "AI", "id": "2505.16997v1", "url": "http://arxiv.org/abs/2505.16997v1", "title": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs", "summary": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems.", "authors": ["Rui Ye", "Xiangrui Liu", "Qimin Wu", "Xianghe Pang", "Zhenfei Yin", "Lei Bai", "Siheng Chen"], "published_date": "2025-05-22", "title_zh": "X-MAS：邁向構建具異質大型語言模型的多代理系統", "summary_zh": "基於大型語言模型的多代理系統透過促進多個專業代理之間的協作，擴展了單一大型語言模型的能力。 然而，現有框架大多依賴單一大型語言模型驅動所有代理，限制了系統智慧。 本研究探索異質大型語言模型驅動的多代理系統範式，其中代理由不同的大型語言模型提供支持，從而提升系統潛力。 我們引入了 X-MAS-Bench，一個用於評估不同大型語言模型在不同領域和多代理系統相關功能表現的綜合測試平台。 我們評估了跨越 5 個領域（包含 21 個測試集）和 5 個功能的 27 個大型語言模型，進行了超過 170 萬次評估，以識別每個領域-功能組合的最佳模型選擇。 研究表明，從同質到異質大型語言模型驅動的多代理系統的轉變可以顯著提高系統性能，且無需結構重新設計。 在僅包含聊天機器人的多代理系統情境中，異質配置在 MATH 數據集上產生了高達 8.4% 的性能提升。 在混合聊天機器人-推理器的情境中，異質多代理系統在 AIME 數據集上實現了顯著的 47% 性能提升。 我們的結果強調了異質大型語言模型在多代理系統中的變革潛力，為推進可擴展的協作式人工智慧系統提供了一個有前景的途徑。", "audio": "audios/2505.16997v1.mp3", "timestamp": "2025-05-23T07:18:26.794247"}
{"query": "Foundation Model", "id": "2505.16832v1", "url": "http://arxiv.org/abs/2505.16832v1", "title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization", "summary": "While foundation models (FMs), such as diffusion models and large\nvision-language models (LVLMs), have been widely applied in educational\ncontexts, their ability to generate pedagogically effective visual explanations\nremains limited. Most existing approaches focus primarily on textual reasoning,\noverlooking the critical role of structured and interpretable visualizations in\nsupporting conceptual understanding. To better assess the visual reasoning\ncapabilities of FMs in educational settings, we introduce EduVisBench, a\nmulti-domain, multi-level benchmark. EduVisBench features diverse STEM problem\nsets requiring visually grounded solutions, along with a fine-grained\nevaluation rubric informed by pedagogical theory. Our empirical analysis\nreveals that existing models frequently struggle with the inherent challenge of\ndecomposing complex reasoning and translating it into visual representations\naligned with human cognitive processes. To address these limitations, we\npropose EduVisAgent, a multi-agent collaborative framework that coordinates\nspecialized agents for instructional planning, reasoning decomposition,\nmetacognitive prompting, and visualization design. Experimental results show\nthat EduVisAgent substantially outperforms all baselines, achieving a 40.2%\nimprovement and delivering more educationally aligned visualizations.\nEduVisBench and EduVisAgent are available at\nhttps://github.com/aiming-lab/EduVisBench and\nhttps://github.com/aiming-lab/EduVisAgent.", "authors": ["Haonian Ji", "Shi Qiu", "Siyang Xin", "Siwei Han", "Zhaorun Chen", "Hongyi Wang", "Dake Zhang", "Huaxiu Yao"], "published_date": "2025-05-22", "title_zh": "從EduVisBench到EduVisAgent：教學視覺化之基準測試與多代理人框架", "summary_zh": "基礎模型如擴散模型與大型視覺語言模型已廣泛應用於教育，然其產生具教學效益之視覺解釋能力仍受限。現有方法多側重文字推理，忽略結構化且可解釋之視覺化於概念理解之關鍵作用。為評估基礎模型於教育情境之視覺推理能力，本研究提出EduVisBench，一多領域、多層級基準測試。EduVisBench包含需視覺化解答之STEM問題集，及基於教學理論之細緻評分標準。實證分析顯示，現有模型常難以分解複雜推理，並將其轉譯為符合人類認知歷程之視覺表徵。為解決此問題，本研究提出EduVisAgent，一多代理人協作框架，協調專業代理人進行教學規劃、推理分解、後設認知提示及視覺化設計。實驗結果顯示，EduVisAgent顯著優於所有基準模型，提升40.2%，並產出更符合教育目標之視覺化。EduVisBench與EduVisAgent已於https://github.com/aiming-lab/EduVisBench及https://github.com/aiming-lab/EduVisAgent公開。", "audio": "audios/2505.16832v1.mp3", "timestamp": "2025-05-23T07:18:36.135433"}
{"query": "Diffusion Model", "id": "2505.16980v1", "url": "http://arxiv.org/abs/2505.16980v1", "title": "Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose Interaction", "summary": "Video virtual try-on aims to seamlessly dress a subject in a video with a\nspecific garment. The primary challenge involves preserving the visual\nauthenticity of the garment while dynamically adapting to the pose and physique\nof the subject. While existing methods have predominantly focused on\nimage-based virtual try-on, extending these techniques directly to videos often\nresults in temporal inconsistencies. Most current video virtual try-on\napproaches alleviate this challenge by incorporating temporal modules, yet\nstill overlook the critical spatiotemporal pose interactions between human and\ngarment. Effective pose interactions in videos should not only consider spatial\nalignment between human and garment poses in each frame but also account for\nthe temporal dynamics of human poses throughout the entire video. With such\nmotivation, we propose a new framework, namely Dynamic Pose Interaction\nDiffusion Models (DPIDM), to leverage diffusion models to delve into dynamic\npose interactions for video virtual try-on. Technically, DPIDM introduces a\nskeleton-based pose adapter to integrate synchronized human and garment poses\ninto the denoising network. A hierarchical attention module is then exquisitely\ndesigned to model intra-frame human-garment pose interactions and long-term\nhuman pose dynamics across frames through pose-aware spatial and temporal\nattention mechanisms. Moreover, DPIDM capitalizes on a temporal regularized\nattention loss between consecutive frames to enhance temporal consistency.\nExtensive experiments conducted on VITON-HD, VVT and ViViD datasets demonstrate\nthe superiority of our DPIDM against the baseline methods. Notably, DPIDM\nachieves VFID score of 0.506 on VVT dataset, leading to 60.5% improvement over\nthe state-of-the-art GPD-VVTO approach.", "authors": ["Dong Li", "Wenqi Zhong", "Wei Yu", "Yingwei Pan", "Dingwen Zhang", "Ting Yao", "Junwei Han", "Tao Mei"], "published_date": "2025-05-22", "title_zh": "藉由動態姿態互動實現時序一致的影片虛擬試穿", "summary_zh": "影片虛擬試穿旨在將特定服裝無縫套用於影片中的人物。主要挑戰在於保持服裝視覺真實性的同時，動態適應人物的姿勢和體態。現有方法多側重於圖像虛擬試穿，直接將其延伸至影片常導致時間不一致。目前影片虛擬試穿方法雖加入時間模組以緩解此問題，但忽略了人體與服裝間的時空姿勢互動。有效的姿勢互動應考量每幀中人體與服裝姿勢的空間對齊，以及整段影片中人體姿勢的時間動態。為此，我們提出動態姿勢互動擴散模型（DPIDM），利用擴散模型深入研究影片虛擬試穿的動態姿勢互動。DPIDM採用基於骨架的姿勢適配器，將同步的人體與服裝姿勢整合至去噪網路。精心設計的分層注意力模組通過姿勢感知的空間和時間注意力機制，對幀內人體-服裝姿勢互動以及跨幀的長期人體姿勢動態進行建模。此外，DPIDM利用連續幀之間的時間正規化注意力損失來增強時間一致性。在VITON-HD、VVT和ViViD數據集上的大量實驗表明，我們的DPIDM優於基線方法。值得注意的是，DPIDM在VVT數據集上實現了0.506的VFID評分，相較於最先進的GPD-VVTO方法提升了60.5%。", "audio": "audios/2505.16980v1.mp3", "timestamp": "2025-05-23T07:18:44.910411"}
{"query": "AI", "id": "2505.16977v1", "url": "http://arxiv.org/abs/2505.16977v1", "title": "Incorporating Visual Correspondence into Diffusion Model for Virtual Try-On", "summary": "Diffusion models have shown preliminary success in virtual try-on (VTON)\ntask. The typical dual-branch architecture comprises two UNets for implicit\ngarment deformation and synthesized image generation respectively, and has\nemerged as the recipe for VTON task. Nevertheless, the problem remains\nchallenging to preserve the shape and every detail of the given garment due to\nthe intrinsic stochasticity of diffusion model. To alleviate this issue, we\nnovelly propose to explicitly capitalize on visual correspondence as the prior\nto tame diffusion process instead of simply feeding the whole garment into UNet\nas the appearance reference. Specifically, we interpret the fine-grained\nappearance and texture details as a set of structured semantic points, and\nmatch the semantic points rooted in garment to the ones over target person\nthrough local flow warping. Such 2D points are then augmented into 3D-aware\ncues with depth/normal map of target person. The correspondence mimics the way\nof putting clothing on human body and the 3D-aware cues act as semantic point\nmatching to supervise diffusion model training. A point-focused diffusion loss\nis further devised to fully take the advantage of semantic point matching.\nExtensive experiments demonstrate strong garment detail preservation of our\napproach, evidenced by state-of-the-art VTON performances on both VITON-HD and\nDressCode datasets. Code is publicly available at:\nhttps://github.com/HiDream-ai/SPM-Diff.", "authors": ["Siqi Wan", "Jingwen Chen", "Yingwei Pan", "Ting Yao", "Tao Mei"], "published_date": "2025-05-22", "title_zh": "將視覺對應融入擴散模型以用於虛擬試穿", "summary_zh": "擴散模型在虛擬試穿任務中初顯成效。典型的雙分支架構包含兩個UNet，分別用於隱式服裝變形和合成圖像生成，已成為虛擬試穿的常用方法。然而，由於擴散模型固有的隨機性，保持服裝的形狀和所有細節仍然具有挑戰性。為了解決這個問題，我們創新性地提出明確利用視覺對應關係作為先驗來控制擴散過程，而不是簡單地將整個服裝作為外觀參考輸入UNet。具體來說，我們將細粒度的外觀和紋理細節解釋為一組結構化的語義點，並通過局部流動扭曲將服裝上的語義點與目標人物上的語義點進行匹配。然後，這些2D點通過目標人物的深度/法線貼圖擴增為具有3D感知的提示。這種對應關係模擬了將衣服穿在人體上的方式，而3D感知提示則充當語義點匹配，以監督擴散模型的訓練。此外，我們設計了一種以點為中心的擴散損失，以充分利用語義點匹配的優勢。大量實驗表明，我們的研究方法在服裝細節保留方面表現出色，在VITON-HD和DressCode數據集上均實現了最先進的虛擬試穿性能。程式碼已公開：https://github.com/HiDream-ai/SPM-Diff。", "audio": "audios/2505.16977v1.mp3", "timestamp": "2025-05-23T08:25:01.166128"}
{"query": "Foundation Model", "id": "2505.16793v1", "url": "http://arxiv.org/abs/2505.16793v1", "title": "REOBench: Benchmarking Robustness of Earth Observation Foundation Models", "summary": "Earth observation foundation models have shown strong generalization across\nmultiple Earth observation tasks, but their robustness under real-world\nperturbations remains underexplored. To bridge this gap, we introduce REOBench,\nthe first comprehensive benchmark for evaluating the robustness of Earth\nobservation foundation models across six tasks and twelve types of image\ncorruptions, including both appearance-based and geometric perturbations. To\nensure realistic and fine-grained evaluation, our benchmark focuses on\nhigh-resolution optical remote sensing images, which are widely used in\ncritical applications such as urban planning and disaster response. We conduct\na systematic evaluation of a broad range of models trained using masked image\nmodeling, contrastive learning, and vision-language pre-training paradigms. Our\nresults reveal that (1) existing Earth observation foundation models experience\nsignificant performance degradation when exposed to input corruptions. (2) The\nseverity of degradation varies across tasks, model architectures, backbone\nsizes, and types of corruption, with performance drop varying from less than 1%\nto over 20%. (3) Vision-language models show enhanced robustness, particularly\nin multimodal tasks. REOBench underscores the vulnerability of current Earth\nobservation foundation models to real-world corruptions and provides actionable\ninsights for developing more robust and reliable models.", "authors": ["Xiang Li", "Yong Tao", "Siyuan Zhang", "Siwei Liu", "Zhitong Xiong", "Chunbo Luo", "Lu Liu", "Mykola Pechenizkiy", "Xiao Xiang Zhu", "Tianjin Huang"], "published_date": "2025-05-22", "title_zh": "REOBench：地球觀測基礎模型穩健性基準測試", "summary_zh": "地球觀測基礎模型在多項任務中展現了良好的泛化能力，但其在真實擾動下的穩健性仍待探索。本研究提出REOBench，首個全面評估地球觀測基礎模型穩健性的基準，包含六項任務和十二種影像失真，涵蓋外觀和幾何擾動。基準專注於高解析度光學遙感影像，適用於都市規劃和災害應對等關鍵應用，以確保真實且精細的評估。針對多種採用遮罩影像建模、對比學習和視覺語言預訓練的模型進行了系統性評估。結果顯示：(1)現有模型在輸入失真下性能顯著下降；(2)性能下降程度因任務、模型架構、骨幹大小和失真類型而異，降幅從小於1%到超過20%；(3)視覺語言模型展現了更強的穩健性，尤其是在多模態任務中。REOBench強調了當前模型在真實失真下的脆弱性，並為開發更穩健可靠的模型提供了可行的見解。", "audio": "audios/2505.16793v1.mp3", "timestamp": "2025-05-23T08:25:08.417021"}
{"query": "Diffusion Model", "id": "2505.16976v1", "url": "http://arxiv.org/abs/2505.16976v1", "title": "Creatively Upscaling Images with Global-Regional Priors", "summary": "Contemporary diffusion models show remarkable capability in text-to-image\ngeneration, while still being limited to restricted resolutions (e.g., 1,024 X\n1,024). Recent advances enable tuning-free higher-resolution image generation\nby recycling pre-trained diffusion models and extending them via regional\ndenoising or dilated sampling/convolutions. However, these models struggle to\nsimultaneously preserve global semantic structure and produce creative regional\ndetails in higher-resolution images. To address this, we present C-Upscale, a\nnew recipe of tuning-free image upscaling that pivots on global-regional priors\nderived from given global prompt and estimated regional prompts via Multimodal\nLLM. Technically, the low-frequency component of low-resolution image is\nrecognized as global structure prior to encourage global semantic consistency\nin high-resolution generation. Next, we perform regional attention control to\nscreen cross-attention between global prompt and each region during regional\ndenoising, leading to regional attention prior that alleviates object\nrepetition issue. The estimated regional prompts containing rich descriptive\ndetails further act as regional semantic prior to fuel the creativity of\nregional detail generation. Both quantitative and qualitative evaluations\ndemonstrate that our C-Upscale manages to generate ultra-high-resolution images\n(e.g., 4,096 X 4,096 and 8,192 X 8,192) with higher visual fidelity and more\ncreative regional details.", "authors": ["Yurui Qian", "Qi Cai", "Yingwei Pan", "Ting Yao", "Tao Mei"], "published_date": "2025-05-22", "title_zh": "利用全局-局部先驗進行創造性圖像放大", "summary_zh": "現有擴散模型在文字生成圖像方面表現出色，但解析度受限。近期研究透過重用預訓練模型並擴展區域降噪或擴張採樣/卷積，實現免調優的高解析度圖像生成。然而，這些模型難以同時在高解析度圖像中保持全局語義結構並生成富含創意的區域細節。為了解決此問題，我們提出 C-Upscale，一種免調優的圖像放大新方法，基於全局提示詞和多模態大型語言模型估算的區域提示詞，獲取全局-區域先驗知識。技術上，低解析度圖像的低頻成分被視為全局結構先驗，以促進高解析度生成中的全局語義一致性。接著，我們執行區域注意力控制，篩選全局提示詞和每個區域在區域降噪期間的交叉注意力，產生區域注意力先驗，減輕物體重複問題。包含豐富描述性細節的估算區域提示詞，進一步作為區域語義先驗，激發區域細節生成的創造力。定量和定性評估表明，我們的 C-Upscale 能夠生成具有更高視覺逼真度和更富創意區域細節的超高解析度圖像（例如，4,096 X 4,096 和 8,192 X 8,192）。", "audio": "audios/2505.16976v1.mp3", "timestamp": "2025-05-23T08:25:16.187584"}
{"query": "AI", "id": "2505.16975v1", "url": "http://arxiv.org/abs/2505.16975v1", "title": "SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development", "summary": "Large Language Models (LLMs) have shown strong capability in diverse software\nengineering tasks, e.g. code completion, bug fixing, and document generation.\nHowever, feature-driven development (FDD), a highly prevalent real-world task\nthat involves developing new functionalities for large, existing codebases,\nremains underexplored. We therefore introduce SWE-Dev, the first large-scale\ndataset (with 14,000 training and 500 test samples) designed to evaluate and\ntrain autonomous coding systems on real-world feature development tasks. To\nensure verifiable and diverse training, SWE-Dev uniquely provides all instances\nwith a runnable environment and its developer-authored executable unit tests.\nThis collection not only provides high-quality data for Supervised Fine-Tuning\n(SFT), but also enables Reinforcement Learning (RL) by delivering accurate\nreward signals from executable unit tests. Our extensive evaluations on\nSWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent\nSystems (MAS), reveal that FDD is a profoundly challenging frontier for current\nAI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test\nsplit). Crucially, we demonstrate that SWE-Dev serves as an effective platform\nfor model improvement: fine-tuning on training set enabled a 7B model\ncomparable to GPT-4o on \\textit{hard} split, underscoring the value of its\nhigh-quality training data. Code is available here\n\\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}.", "authors": ["Yaxin Du", "Yuzhu Cai", "Yifan Zhou", "Cheng Wang", "Yu Qian", "Xianghe Pang", "Qian Liu", "Yue Hu", "Siheng Chen"], "published_date": "2025-05-22", "title_zh": "SWE-Dev：評估與訓練自主式特徵驅動軟體開發", "summary_zh": "大型語言模型在程式碼補全、錯誤修復和文件生成等軟體工程任務中展現強大能力，但對大型現有程式碼庫進行的特性驅動開發（FDD）仍未充分探索。本研究推出首個大規模資料集SWE-Dev，包含14,000個訓練樣本和500個測試樣本，旨在評估並訓練自主程式碼系統，以應對真實世界的特性開發任務。SWE-Dev獨特之處在於，它為所有實例提供可執行環境及開發者編寫的可執行單元測試，確保可驗證且多樣化的訓練。此資料集不僅為監督式微調（SFT）提供高品質資料，還能透過單元測試提供準確的獎勵信號，促進強化學習（RL）。對SWE-Dev進行的廣泛評估，涵蓋17個聊天機器人語言模型、10個推理模型和10個多代理系統（MAS），揭示了FDD對當前人工智慧而言是一項極具挑戰性的前沿任務（例如，Claude-3.7-Sonnet在困難測試集上僅達到22.45%的Pass@3）。此外，研究表明SWE-Dev可作為模型改進的有效平台，透過在訓練集上進行微調，使一個70億參數模型在困難測試集上達到與GPT-4o相當的性能，突顯了其高品質訓練資料的價值。程式碼可在\\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}取得。", "audio": "audios/2505.16975v1.mp3", "timestamp": "2025-05-23T09:19:59.885496"}
{"query": "Foundation Model", "id": "2505.16725v1", "url": "http://arxiv.org/abs/2505.16725v1", "title": "Masked Conditioning for Deep Generative Models", "summary": "Datasets in engineering domains are often small, sparsely labeled, and\ncontain numerical as well as categorical conditions. Additionally.\ncomputational resources are typically limited in practical applications which\nhinders the adoption of generative models for engineering tasks. We introduce a\nnovel masked-conditioning approach, that enables generative models to work with\nsparse, mixed-type data. We mask conditions during training to simulate sparse\nconditions at inference time. For this purpose, we explore the use of various\nsparsity schedules that show different strengths and weaknesses. In addition,\nwe introduce a flexible embedding that deals with categorical as well as\nnumerical conditions. We integrate our method into an efficient variational\nautoencoder as well as a latent diffusion model and demonstrate the\napplicability of our approach on two engineering-related datasets of 2D point\nclouds and images. Finally, we show that small models trained on limited data\ncan be coupled with large pretrained foundation models to improve generation\nquality while retaining the controllability induced by our conditioning scheme.", "authors": ["Phillip Mueller", "Jannik Wiese", "Sebastian Mueller", "Lars Mikelsons"], "published_date": "2025-05-22", "title_zh": "深度生成模型的遮蔽條件化", "summary_zh": "工程領域數據集通常規模小、標記稀疏，且包含數值和類別條件。實際應用中計算資源有限，阻礙了生成模型在工程任務中的應用。本研究提出一種新的遮蔽條件方法，使生成模型能處理稀疏、混合型數據。訓練期間遮蔽條件，以模擬推論時的稀疏性。我們探索了多種稀疏性排程，展現不同的優缺點。此外，我們引入一種靈活的嵌入方式，處理類別和數值條件。我們將此方法整合到高效變分自編碼器和潛在擴散模型中，並在兩個二維點雲和圖像的工程數據集上驗證其適用性。最後，我們證明在有限數據上訓練的小型模型可與大型預訓練基礎模型結合，在保持條件控制性的同時，提升生成品質。", "audio": "audios/2505.16725v1.mp3", "timestamp": "2025-05-23T09:20:06.171793"}
{"query": "Diffusion Model", "id": "2505.16959v1", "url": "http://arxiv.org/abs/2505.16959v1", "title": "Bigger Isn't Always Memorizing: Early Stopping Overparameterized Diffusion Models", "summary": "Diffusion probabilistic models have become a cornerstone of modern generative\nAI, yet the mechanisms underlying their generalization remain poorly\nunderstood. In fact, if these models were perfectly minimizing their training\nloss, they would just generate data belonging to their training set, i.e.,\nmemorize, as empirically found in the overparameterized regime. We revisit this\nview by showing that, in highly overparameterized diffusion models,\ngeneralization in natural data domains is progressively achieved during\ntraining before the onset of memorization. Our results, ranging from image to\nlanguage diffusion models, systematically support the empirical law that\nmemorization time is proportional to the dataset size. Generalization vs.\nmemorization is then best understood as a competition between time scales. We\nshow that this phenomenology is recovered in diffusion models learning a simple\nprobabilistic context-free grammar with random rules, where generalization\ncorresponds to the hierarchical acquisition of deeper grammar rules as training\ntime grows, and the generalization cost of early stopping can be characterized.\nWe summarize these results in a phase diagram. Overall, our results support\nthat a principled early-stopping criterion - scaling with dataset size - can\neffectively optimize generalization while avoiding memorization, with direct\nimplications for hyperparameter transfer and privacy-sensitive applications.", "authors": ["Alessandro Favero", "Antonio Sclocchi", "Matthieu Wyart"], "published_date": "2025-05-22", "title_zh": "更大未必更善記：過參數擴散模型的提前停止", "summary_zh": "擴散機率模型已成現代生成式AI基石，但其泛化機制仍不甚明瞭。若模型完美最小化訓練損失，將僅生成訓練集資料，即產生記憶。本研究重新審視此觀點，證明在高度過參數化擴散模型中，自然數據領域的泛化在訓練期間逐步實現，早於記憶發生。從圖像到語言擴散模型的研究結果一致支持：記憶時間與數據集大小成正比。泛化與記憶可視為時間尺度上的競爭。在學習具隨機規則的簡單機率上下文無關文法的擴散模型中，也觀察到此現象，泛化對應於訓練時間增長時，更深層文法規則的階層式獲取，且可量化提前停止的泛化成本。總結結果於相圖中。結果表明，基於數據集大小的提前停止準則，能有效優化泛化，同時避免記憶，對超參數遷移及隱私敏感應用具直接影響。", "audio": "audios/2505.16959v1.mp3", "timestamp": "2025-05-23T09:20:13.279299"}
{"query": "AI", "id": "2505.16964v1", "url": "http://arxiv.org/abs/2505.16964v1", "title": "MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning", "summary": "Existing medical VQA benchmarks mostly focus on single-image analysis, yet\nclinicians almost always compare a series of images before reaching a\ndiagnosis. To better approximate this workflow, we introduce MedFrameQA -- the\nfirst benchmark that explicitly evaluates multi-image reasoning in medical VQA.\nTo build MedFrameQA both at scale and in high-quality, we develop 1) an\nautomated pipeline that extracts temporally coherent frames from medical videos\nand constructs VQA items whose content evolves logically across images, and 2)\na multiple-stage filtering strategy, including model-based and manual review,\nto preserve data clarity, difficulty, and medical relevance. The resulting\ndataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in\n3,420 videos), covering nine human body systems and 43 organs; every question\nis accompanied by two to five images. We comprehensively benchmark ten advanced\nMultimodal LLMs -- both proprietary and open source, with and without explicit\nreasoning modules -- on MedFrameQA. The evaluation challengingly reveals that\nall models perform poorly, with most accuracies below 50%, and accuracy\nfluctuates as the number of images per question increases. Error analysis\nfurther shows that models frequently ignore salient findings, mis-aggregate\nevidence across images, and propagate early mistakes through their reasoning\nchains; results also vary substantially across body systems, organs, and\nmodalities. We hope this work can catalyze research on clinically grounded,\nmulti-image reasoning and accelerate progress toward more capable diagnostic AI\nsystems.", "authors": ["Suhao Yu", "Haojin Wang", "Juncheng Wu", "Cihang Xie", "Yuyin Zhou"], "published_date": "2025-05-22", "title_zh": "MedFrameQA：用於臨床推理的多影像醫學VQA基準", "summary_zh": "現有醫療VQA基準測試多著重於單一影像分析，然臨床醫師診斷前常比較系列影像。為更貼近此流程，我們提出MedFrameQA，首個評估醫療VQA中多影像推理的基準。為大規模且高品質地建構MedFrameQA，我們開發1)自動化流程，從醫療影片提取時間上連貫的幀，並建構內容邏輯演進的VQA項目，以及2)多階段過濾策略，包含基於模型的審查和人工審查，以保持數據清晰度、難度和醫療相關性。最終數據集包含2,851個VQA配對（來自3,420個影片中的9,237個高品質幀），涵蓋九個人體系統和43個器官；每個問題配有兩到五個影像。我們在MedFrameQA上全面評測了十個先進的多模態LLM，包括專有和開源，以及具備和不具備顯式推理模組的模型。評估顯示所有模型表現不佳，多數準確度低於50%，且準確度隨每個問題的影像數量增加而波動。錯誤分析表明，模型常忽略顯著發現、錯誤整合跨影像證據，並將早期錯誤傳播到推理鏈中；結果也因身體系統、器官和模態而異。期望此研究可促進臨床多影像推理研究，並加速更強大的診斷AI系統發展。", "audio": "audios/2505.16964v1.mp3", "timestamp": "2025-05-23T10:19:45.991998"}
{"query": "Foundation Model", "id": "2505.16724v1", "url": "http://arxiv.org/abs/2505.16724v1", "title": "Advancing Brainwave Modeling with a Codebook-Based Foundation Model", "summary": "Recent advances in large-scale pre-trained Electroencephalogram (EEG) models\nhave shown great promise, driving progress in Brain-Computer Interfaces (BCIs)\nand healthcare applications. However, despite their success, many existing\npre-trained models have struggled to fully capture the rich information content\nof neural oscillations, a limitation that fundamentally constrains their\nperformance and generalizability across diverse BCI tasks. This limitation is\nfrequently rooted in suboptimal architectural design choices which constrain\ntheir representational capacity. In this work, we introduce LaBraM++, an\nenhanced Large Brainwave Foundation Model (LBM) that incorporates principled\nimprovements grounded in robust signal processing foundations. LaBraM++\ndemonstrates substantial gains across a variety of tasks, consistently\noutperforming its originally-based architecture and achieving competitive\nresults when compared to other open-source LBMs. Its superior performance and\ntraining efficiency highlight its potential as a strong foundation for future\nadvancements in LBMs.", "authors": ["Konstantinos Barmpas", "Na Lee", "Yannis Panagakis", "Dimitrios A. Adamos", "Nikolaos Laskaris", "Stefanos Zafeiriou"], "published_date": "2025-05-22", "title_zh": "基於碼本的基礎模型推進腦波建模", "summary_zh": "大規模預訓練腦電圖模型取得進展，推動腦機介面與醫療應用。然而，現有模型未能充分捕捉神經震盪的豐富資訊，限制其效能與泛化能力，此缺陷源於架構設計。本研究提出LaBraM++，一種基於訊號處理基礎的增強型大型腦波基礎模型，其在多項任務中均顯著優於原架構，並與其他開放原始碼大型腦波模型相比具備競爭力。LaBraM++卓越的效能和訓練效率使其有望成為未來大型腦波模型發展的堅實基礎。", "audio": "audios/2505.16724v1.mp3", "timestamp": "2025-05-23T10:19:50.098428"}
{"query": "Diffusion Model", "id": "2505.16933v1", "url": "http://arxiv.org/abs/2505.16933v1", "title": "LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning", "summary": "In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large\nLanguage Model (MLLM) that integrates visual instruction tuning with masked\ndiffusion models, representing a departure from the autoregressive paradigms\ndominant in current multimodal approaches. Built upon LLaDA, a representative\nlarge language diffusion model, LLaDA-V incorporates a vision encoder and MLP\nconnector that projects visual features into the language embedding space,\nenabling effective multimodal alignment. Our empirical investigation reveals\nseveral intriguing results: First, LLaDA-V demonstrates promising multimodal\nperformance despite its language model being weaker on purely textual tasks\nthan counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same\ninstruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal\ntasks with better data scalability. It also narrows the performance gap to\nQwen2-VL, suggesting the effectiveness of its architecture for multimodal\ntasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal\nunderstanding compared to existing hybrid autoregressive-diffusion and purely\ndiffusion-based MLLMs. Our findings suggest that large language diffusion\nmodels show promise in multimodal contexts and warrant further investigation in\nfuture research. Project page and codes:\nhttps://ml-gsai.github.io/LLaDA-V-demo/.", "authors": ["Zebin You", "Shen Nie", "Xiaolu Zhang", "Jun Hu", "Jun Zhou", "Zhiwu Lu", "Ji-Rong Wen", "Chongxuan Li"], "published_date": "2025-05-22", "title_zh": "LLaDA-V：基於視覺指令微調的大型語言擴散模型", "summary_zh": "本研究提出 LLaDA-V，一種純擴散模型的多模態大型語言模型 (MLLM)，整合視覺指令微調與遮蔽擴散模型，跳脫現有多模態方法中常見的自迴歸範式。LLaDA-V 基於具代表性的大型語言擴散模型 LLaDA，結合視覺編碼器和 MLP 連接器，將視覺特徵投射至語言嵌入空間，實現有效的多模態對齊。實驗結果顯示，儘管 LLaDA-V 的語言模型在純文本任務上弱於 LLaMA3-8B 和 Qwen2-7B 等模型，但在多模態任務中仍展現出潛力。在相同指令資料集上訓練後，LLaDA-V 在多模態任務中與 LLaMA3-V 相比具有高度競爭力，且資料擴展性更佳，並縮小了與 Qwen2-VL 的性能差距，顯示其架構在多模態任務中的有效性。此外，LLaDA-V 在多模態理解方面相較於現有的混合自迴歸-擴散和純擴散 MLLM 達到了最先進的性能。研究結果表明，大型語言擴散模型在多模態環境中具有前景，值得未來進一步研究。專案頁面和程式碼：https://ml-gsai.github.io/LLaDA-V-demo/。", "audio": "audios/2505.16933v1.mp3", "timestamp": "2025-05-23T10:19:59.385455"}
{"query": "AI", "id": "2505.16954v1", "url": "http://arxiv.org/abs/2505.16954v1", "title": "Cracking Aegis: An Adversarial LLM-based Game for Raising Awareness of Vulnerabilities in Privacy Protection", "summary": "Traditional methods for raising awareness of privacy protection often fail to\nengage users or provide hands-on insights into how privacy vulnerabilities are\nexploited. To address this, we incorporate an adversarial mechanic in the\ndesign of the dialogue-based serious game Cracking Aegis. Leveraging LLMs to\nsimulate natural interactions, the game challenges players to impersonate\ncharacters and extract sensitive information from an AI agent, Aegis. A user\nstudy (n=22) revealed that players employed diverse deceptive linguistic\nstrategies, including storytelling and emotional rapport, to manipulate Aegis.\nAfter playing, players reported connecting in-game scenarios with real-world\nprivacy vulnerabilities, such as phishing and impersonation, and expressed\nintentions to strengthen privacy control, such as avoiding oversharing personal\ninformation with AI systems. This work highlights the potential of LLMs to\nsimulate complex relational interactions in serious games, while demonstrating\nhow an adversarial game strategy provides unique insights for designs for\nsocial good, particularly privacy protection.", "authors": ["Jiaying Fu", "Yiyang Lu", "Zehua Yang", "Fiona Nah", "RAY LC"], "published_date": "2025-05-22", "title_zh": "攻破埃吉斯：基於對抗性大型語言模型的遊戲，旨在提高對隱私保護漏洞的意識", "summary_zh": "傳統隱私保護宣導方法常缺乏使用者參與及實務漏洞認知。本研究於對話式嚴肅遊戲Cracking Aegis中導入對抗機制，利用大型語言模型模擬自然互動，挑戰玩家扮演角色自AI代理人Aegis提取敏感資訊。使用者研究(n=22)顯示，玩家運用多樣欺騙性語言策略，如故事敘述與情感連結，操縱Aegis。遊玩後，玩家表示能將遊戲情境連結至現實隱私漏洞，如網路釣魚與身分冒用，並表達強化隱私控制的意願，如避免過度分享個資予AI系統。此研究突顯大型語言模型在嚴肅遊戲中模擬複雜關係互動的潛力，並展示對抗性遊戲策略如何為公益設計，特別是隱私保護，提供獨特見解。", "audio": "audios/2505.16954v1.mp3", "timestamp": "2025-05-23T11:15:31.879619"}
{"query": "Foundation Model", "id": "2505.16635v1", "url": "http://arxiv.org/abs/2505.16635v1", "title": "WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning", "summary": "Tabular data, ubiquitous and rich in informational value, is an increasing\nfocus for deep representation learning, yet progress is hindered by studies\ncentered on single tables or isolated databases, which limits model\ncapabilities due to data scale. While collaborative learning approaches such as\nfederated learning, transfer learning, split learning, and tabular foundation\nmodels aim to learn from multiple correlated databases, they are challenged by\na scarcity of real-world interconnected tabular resources. Current data lakes\nand corpora largely consist of isolated databases lacking defined\ninter-database correlations. To overcome this, we introduce WikiDBGraph, a\nlarge-scale graph of 100,000 real-world tabular databases from WikiData,\ninterconnected by 17 million edges and characterized by 13 node and 12 edge\nproperties derived from its database schema and data distribution.\nWikiDBGraph's weighted edges identify both instance- and feature-overlapped\ndatabases. Experiments on these newly identified databases confirm that\ncollaborative learning yields superior performance, thereby offering\nconsiderable promise for structured foundation model training while also\nexposing key challenges and future directions for learning from interconnected\ntabular data.", "authors": ["Zhaomin Wu", "Ziyang Wang", "Bingsheng He"], "published_date": "2025-05-22", "title_zh": "WikiDBGraph：用於協作學習的Wikidata大規模資料庫圖", "summary_zh": "表格數據蘊含豐富資訊，日益受到深度表徵學習關注。然，現有研究多集中於單一表格或孤立資料庫，受限於數據規模。聯邦學習、遷移學習、分割學習及表格基礎模型等協作學習方法旨在從多個相關資料庫學習，卻面臨真實互聯表格資源匱乏的挑戰。現有資料湖和語料庫主要由缺乏明確跨庫關聯的孤立資料庫組成。為此，我們推出WikiDBGraph，一個來自WikiData的大規模圖，包含10萬個真實表格資料庫，透過1700萬條邊連接，並以源自其資料庫架構和數據分布的13個節點屬性和12個邊屬性為特徵。WikiDBGraph的加權邊可識別實例和特徵重疊的資料庫。基於這些新識別的資料庫的實驗證實，協作學習可產生更優異的性能，為結構化基礎模型訓練帶來可觀前景，同時也揭示了從互聯表格數據學習的關鍵挑戰和未來方向。", "audio": "audios/2505.16635v1.mp3", "timestamp": "2025-05-23T11:15:39.770004"}
{"query": "Diffusion Model", "id": "2505.16875v1", "url": "http://arxiv.org/abs/2505.16875v1", "title": "T2I-ConBench: Text-to-Image Benchmark for Continual Post-training", "summary": "Continual post-training adapts a single text-to-image diffusion model to\nlearn new tasks without incurring the cost of separate models, but naive\npost-training causes forgetting of pretrained knowledge and undermines\nzero-shot compositionality. We observe that the absence of a standardized\nevaluation protocol hampers related research for continual post-training. To\naddress this, we introduce T2I-ConBench, a unified benchmark for continual\npost-training of text-to-image models. T2I-ConBench focuses on two practical\nscenarios, item customization and domain enhancement, and analyzes four\ndimensions: (1) retention of generality, (2) target-task performance, (3)\ncatastrophic forgetting, and (4) cross-task generalization. It combines\nautomated metrics, human-preference modeling, and vision-language QA for\ncomprehensive assessment. We benchmark ten representative methods across three\nrealistic task sequences and find that no approach excels on all fronts. Even\njoint \"oracle\" training does not succeed for every task, and cross-task\ngeneralization remains unsolved. We release all datasets, code, and evaluation\ntools to accelerate research in continual post-training for text-to-image\nmodels.", "authors": ["Zhehao Huang", "Yuhang Liu", "Yixin Lou", "Zhengbao He", "Mingzhen He", "Wenxing Zhou", "Tao Li", "Kehan Li", "Zeyi Huang", "Xiaolin Huang"], "published_date": "2025-05-22", "title_zh": "T2I-ConBench：用於持續後訓練的文本到圖像基準測試", "summary_zh": "持續後訓練使單一文生圖擴散模型適應新任務，無需獨立模型，但直接後訓練會導致遺忘預訓練知識並損害零樣本組合性。缺乏標準化評估協議阻礙相關研究，因此我們提出T2I-ConBench，一個統一的文生圖模型持續後訓練基準。T2I-ConBench關注項目客製化和領域增強兩種實際情境，並分析四個維度：通用性保留、目標任務效能、災難性遺忘及跨任務泛化。它結合自動化指標、人類偏好建模及視覺語言問答進行全面評估。我們對比了三種實際任務序列中的十種代表性方法，發現沒有任何方法在所有方面都表現出色，即使是聯合「先知」訓練也並非在每個任務中都成功，且跨任務泛化仍未解決。我們釋出所有資料集、程式碼及評估工具，以加速文生圖模型持續後訓練的研究。", "audio": "audios/2505.16875v1.mp3", "timestamp": "2025-05-23T11:15:47.138005"}
{"query": "AI", "id": "2505.16951v1", "url": "http://arxiv.org/abs/2505.16951v1", "title": "From Reality to Virtual Worlds: The Role of Photogrammetry in Game Development", "summary": "Photogrammetry is transforming digital content creation by enabling the rapid\nconversion of real-world objects into highly detailed 3D models. This paper\nevaluates the role of RealityCapture, a GPU-accelerated photogrammetry tool, in\ngame development of Virtual Reality (VR). We assess its efficiency,\nreconstruction accuracy, and integration with Unreal Engine, comparing its\nadvantages and limitations against traditional modeling workflows.\nAdditionally, we examined user preferences between designed 3D assets and\nphotogrammetry-generated models. The results revealed that while photogrammetry\nenhances realism and interactivity, users slightly preferred manually designed\nmodels for small, manipulable elements because of the level of detail. However,\nfrom a developer perspective, RealityCapture significantly reduces development\ntime while maintaining geometric precision and photorealistic textures. Despite\nits reliance on high-performance hardware, its automation, scalability, and\nseamless integration with real-time rendering engines make it a valuable tool\nfor game developers and VR creators. Future improvements in AI-driven\noptimization and cloud-based processing could enhance accessibility, broadening\nits applications in gaming, cultural heritage preservation, and simulation.", "authors": ["Santiago Berrezueta-Guzman", "Andrei Koshelev", "Stefan Wagner"], "published_date": "2025-05-22", "title_zh": "從現實到虛擬世界：攝影測量術在遊戲開發中的作用", "summary_zh": "攝影測量正轉變數位內容創作，能快速將實物轉換為高精度3D模型。本文評估RealityCapture（一款GPU加速攝影測量工具）在虛擬實境遊戲開發中的作用，分析其效率、重建準確度及與Unreal Engine的整合，並與傳統建模流程比較優劣。研究同時考察使用者對設計3D物件和攝影測量生成模型的偏好。結果顯示，攝影測量增強了真實感和互動性，但使用者略微偏好手動設計的小型、可操作物件，因其細節更佳。然而，從開發者角度看，RealityCapture顯著縮短開發時間，同時保持幾何精度和逼真紋理。儘管依賴高效能硬體，其自動化、可擴展性以及與即時渲染引擎的無縫整合，使其成為遊戲開發者和虛擬實境創作者的寶貴工具。未來，基於人工智慧的優化和雲端處理可望提升可及性，擴展其在遊戲、文化遺產保護和模擬中的應用。", "audio": "audios/2505.16951v1.mp3", "timestamp": "2025-05-23T12:36:43.960915"}
{"query": "Foundation Model", "id": "2505.16540v1", "url": "http://arxiv.org/abs/2505.16540v1", "title": "TextureSAM: Towards a Texture Aware Foundation Model for Segmentation", "summary": "Segment Anything Models (SAM) have achieved remarkable success in object\nsegmentation tasks across diverse datasets. However, these models are\npredominantly trained on large-scale semantic segmentation datasets, which\nintroduce a bias toward object shape rather than texture cues in the image.\nThis limitation is critical in domains such as medical imaging, material\nclassification, and remote sensing, where texture changes define object\nboundaries. In this study, we investigate SAM's bias toward semantics over\ntextures and introduce a new texture-aware foundation model, TextureSAM, which\nperforms superior segmentation in texture-dominant scenarios. To achieve this,\nwe employ a novel fine-tuning approach that incorporates texture augmentation\ntechniques, incrementally modifying training images to emphasize texture\nfeatures. By leveraging a novel texture-alternation of the ADE20K dataset, we\nguide TextureSAM to prioritize texture-defined regions, thereby mitigating the\ninherent shape bias present in the original SAM model. Our extensive\nexperiments demonstrate that TextureSAM significantly outperforms SAM-2 on both\nnatural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation\ndatasets. The code and texture-augmented dataset will be publicly available.", "authors": ["Inbal Cohen", "Boaz Meivar", "Peihan Tu", "Shai Avidan", "Gal Oren"], "published_date": "2025-05-22", "title_zh": "紋理感知分割基礎模型：TextureSAM研究", "summary_zh": "物件分割模型(SAM)在多樣數據集上表現出色，但主要訓練於大型語義分割數據集，導致模型傾向物體形狀而非紋理。此限制在醫學影像、材料分類和遙感等領域尤其關鍵，因紋理變化定義物體邊界。本研究探討SAM對語義的偏好，並提出新的紋理感知基礎模型TextureSAM，在紋理主導場景中實現更優分割。我們採用新型微調方法，結合紋理增強技術，逐步修改訓練圖像以強調紋理特徵。透過改良ADE20K數據集，引導TextureSAM優先處理紋理定義區域，降低原始SAM模型固有的形狀偏好。實驗證明，TextureSAM在自然（+0.2 mIoU）和合成（+0.18 mIoU）紋理分割數據集上均顯著優於SAM-2。程式碼和紋理增強數據集將公開發布。", "audio": "audios/2505.16540v1.mp3", "timestamp": "2025-05-23T12:36:51.227202"}
{"query": "Diffusion Model", "id": "2505.16864v1", "url": "http://arxiv.org/abs/2505.16864v1", "title": "Training-Free Efficient Video Generation via Dynamic Token Carving", "summary": "Despite the remarkable generation quality of video Diffusion Transformer\n(DiT) models, their practical deployment is severely hindered by extensive\ncomputational requirements. This inefficiency stems from two key challenges:\nthe quadratic complexity of self-attention with respect to token length and the\nmulti-step nature of diffusion models. To address these limitations, we present\nJenga, a novel inference pipeline that combines dynamic attention carving with\nprogressive resolution generation. Our approach leverages two key insights: (1)\nearly denoising steps do not require high-resolution latents, and (2) later\nsteps do not require dense attention. Jenga introduces a block-wise attention\nmechanism that dynamically selects relevant token interactions using 3D\nspace-filling curves, alongside a progressive resolution strategy that\ngradually increases latent resolution during generation. Experimental results\ndemonstrate that Jenga achieves substantial speedups across multiple\nstate-of-the-art video diffusion models while maintaining comparable generation\nquality (8.83$\\times$ speedup with 0.01\\% performance drop on VBench). As a\nplug-and-play solution, Jenga enables practical, high-quality video generation\non modern hardware by reducing inference time from minutes to seconds --\nwithout requiring model retraining. Code:\nhttps://github.com/dvlab-research/Jenga", "authors": ["Yuechen Zhang", "Jinbo Xing", "Bin Xia", "Shaoteng Liu", "Bohao Peng", "Xin Tao", "Pengfei Wan", "Eric Lo", "Jiaya Jia"], "published_date": "2025-05-22", "title_zh": "基於動態令牌雕琢的免訓練高效視訊生成", "summary_zh": "視訊擴散轉換器模型（DiT）生成品質卓越，但其龐大計算需求嚴重阻礙實際部署。此低效源於兩個主要挑戰：自注意力機制隨令牌長度呈現的二次複雜度，以及擴散模型的多步特性。為解決此限制，我們提出Jenga，一種結合動態注意力雕刻與漸進式解析度生成的全新推論流程。此方法基於兩個關鍵洞察：（1）早期去噪步驟無需高解析度潛在變數，（2）後期步驟無需密集注意力。Jenga引入塊狀注意力機制，利用3D空間填充曲線動態選擇相關令牌交互，並採用漸進式解析度策略，在生成過程中逐步提升潛在變數解析度。實驗結果表明，Jenga在多個最先進視訊擴散模型上實現顯著加速，同時保持可比擬的生成品質（在VBench上加速8.83倍，性能下降0.01%）。作為一種即插即用解決方案，Jenga將推論時間從數分鐘縮短至數秒，從而在現代硬體上實現實用且高品質的視訊生成，且無需模型重新訓練。", "audio": "audios/2505.16864v1.mp3", "timestamp": "2025-05-23T12:37:00.242831"}
{"query": "AI", "id": "2505.16938v1", "url": "http://arxiv.org/abs/2505.16938v1", "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification", "summary": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours.", "authors": ["NovelSeek Team", "Bo Zhang", "Shiyang Feng", "Xiangchao Yan", "Jiakang Yuan", "Zhiyin Yu", "Xiaohan He", "Songtao Huang", "Shaowei Hou", "Zheng Nie", "Zhilong Wang", "Jinyao Liu", "Runmin Ma", "Tianshuo Peng", "Peng Ye", "Dongzhan Zhou", "Shufei Zhang", "Xiaosong Wang", "Yilan Zhang", "Meng Li", "Zhongying Tu", "Xiangyu Yue", "Wangli Ouyang", "Bowen Zhou", "Lei Bai"], "published_date": "2025-05-22", "title_zh": "新穎探索：當代理人化身科學家——構建從假說到驗證的閉環系統", "summary_zh": "人工智慧加速科學研究範式轉變，提升效率並驅動創新。我們提出NovelSeek，一個統一的閉環多代理框架，用於跨多個科學領域進行自主科學研究(ASR)，使研究人員能以前所未有的速度和精度解決複雜問題。NovelSeek具有三大優勢：1) 可擴展性：已在12項科學研究任務中展示其多功能性，能產生創新想法以提升基準程式碼效能。2) 互動性：提供人為專家回饋介面及多代理互動，能將領域專家知識無縫整合到自動端到端流程中。3) 效率：在多個科學領域取得顯著效能提升，且耗時遠少於人工。例如，在反應產率預測中，僅用12小時將準確率從27.6%提升至35.4%；在增強子活性預測中，僅用4小時將準確率從0.52提升至0.79；在2D語義分割中，僅用30小時將精確度從78.8%提升至81.0%。", "audio": "audios/2505.16938v1.mp3", "timestamp": "2025-05-23T13:28:56.575249"}
{"query": "Foundation Model", "id": "2505.16531v1", "url": "http://arxiv.org/abs/2505.16531v1", "title": "HOFT: Householder Orthogonal Fine-tuning", "summary": "Adaptation of foundation models using low-rank methods is a widespread\napproach. Another way to adapt these models is to employ orthogonal fine-tuning\nmethods, which are less time and memory efficient despite their good\ngeneralization properties. In this work, we propose Householder Orthogonal\nFine-tuning (HOFT), a novel orthogonal fine-tuning method that aims to\nalleviate time and space complexity. Moreover, some theoretical properties of\nthe orthogonal fine-tuning paradigm are explored. From this exploration, Scaled\nHouseholder Orthogonal Fine-tuning (SHOFT) is proposed. Both HOFT and SHOFT are\nevaluated in downstream tasks, namely commonsense reasoning, machine\ntranslation, subject-driven generation and mathematical reasoning. Compared\nwith state-of-the-art adaptation methods, HOFT and SHOFT show comparable or\nbetter results.", "authors": ["Alejandro Moreno Arcas", "Albert Sanchis", "Jorge Civera", "Alfons Juan"], "published_date": "2025-05-22", "title_zh": "豪斯霍爾德正交微調", "summary_zh": "基於低秩方法的基礎模型適配應用廣泛。另一種方法是正交微調，儘管其泛化性良好，但效率較低。本研究提出Householder正交微調(HOFT)，旨在降低時間和空間複雜度。此外，探討了正交微調範式的若干理論性質，並由此提出縮放Householder正交微調(SHOFT)。HOFT和SHOFT在常識推理、機器翻譯、主體驅動生成和數學推理等下游任務中進行評估，結果顯示，與現有最佳適配方法相比，HOFT和SHOFT表現出相當甚至更優異的性能。", "audio": "audios/2505.16531v1.mp3", "timestamp": "2025-05-23T13:29:01.709419"}
{"query": "Diffusion Model", "id": "2505.16862v1", "url": "http://arxiv.org/abs/2505.16862v1", "title": "Conditional Panoramic Image Generation via Masked Autoregressive Modeling", "summary": "Recent progress in panoramic image generation has underscored two critical\nlimitations in existing approaches. First, most methods are built upon\ndiffusion models, which are inherently ill-suited for equirectangular\nprojection (ERP) panoramas due to the violation of the identically and\nindependently distributed (i.i.d.) Gaussian noise assumption caused by their\nspherical mapping. Second, these methods often treat text-conditioned\ngeneration (text-to-panorama) and image-conditioned generation (panorama\noutpainting) as separate tasks, relying on distinct architectures and\ntask-specific data. In this work, we propose a unified framework, Panoramic\nAutoRegressive model (PAR), which leverages masked autoregressive modeling to\naddress these challenges. PAR avoids the i.i.d. assumption constraint and\nintegrates text and image conditioning into a cohesive architecture, enabling\nseamless generation across tasks. To address the inherent discontinuity in\nexisting generative models, we introduce circular padding to enhance spatial\ncoherence and propose a consistency alignment strategy to improve generation\nquality. Extensive experiments demonstrate competitive performance in\ntext-to-image generation and panorama outpainting tasks while showcasing\npromising scalability and generalization capabilities.", "authors": ["Chaoyang Wang", "Xiangtai Li", "Lu Qi", "Xiaofan Lin", "Jinbin Bai", "Qianyu Zhou", "Yunhai Tong"], "published_date": "2025-05-22", "title_zh": "基於遮罩自迴歸模型的條件全景圖像生成", "summary_zh": "全景影像生成的新進展揭示了現有方法的兩大局限。多數方法基於擴散模型，但因其球形映射導致獨立同分布高斯雜訊假設失效，故不適用於等距柱狀投影全景圖。此外，文字條件生成和圖像條件生成常被視為獨立任務，採用不同架構和特定數據。本研究提出統一框架，即全景自迴歸模型（PAR），利用遮罩自迴歸建模解決上述問題。PAR規避了獨立同分布假設限制，並將文字和圖像條件整合至統一架構，實現跨任務的無縫生成。為解決生成模型固有的不連續性，引入環形填充以增強空間一致性，並提出一致性對齊策略以提升生成品質。大量實驗表明，PAR在文字轉圖像生成和全景圖外繪任務中表現出色，並展現了良好的擴展性和泛化能力。", "audio": "audios/2505.16862v1.mp3", "timestamp": "2025-05-23T13:29:08.558974"}
{"query": "AI", "id": "2505.16934v1", "url": "http://arxiv.org/abs/2505.16934v1", "title": "In-Context Watermarks for Large Language Models", "summary": "The growing use of large language models (LLMs) for sensitive applications\nhas highlighted the need for effective watermarking techniques to ensure the\nprovenance and accountability of AI-generated text. However, most existing\nwatermarking methods require access to the decoding process, limiting their\napplicability in real-world settings. One illustrative example is the use of\nLLMs by dishonest reviewers in the context of academic peer review, where\nconference organizers have no access to the model used but still need to detect\nAI-generated reviews. Motivated by this gap, we introduce In-Context\nWatermarking (ICW), which embeds watermarks into generated text solely through\nprompt engineering, leveraging LLMs' in-context learning and\ninstruction-following abilities. We investigate four ICW strategies at\ndifferent levels of granularity, each paired with a tailored detection method.\nWe further examine the Indirect Prompt Injection (IPI) setting as a specific\ncase study, in which watermarking is covertly triggered by modifying input\ndocuments such as academic manuscripts. Our experiments validate the\nfeasibility of ICW as a model-agnostic, practical watermarking approach.\nMoreover, our findings suggest that as LLMs become more capable, ICW offers a\npromising direction for scalable and accessible content attribution.", "authors": ["Yepeng Liu", "Xuandong Zhao", "Christopher Kruegel", "Dawn Song", "Yuheng Bu"], "published_date": "2025-05-22", "title_zh": "大型語言模型的上下文水印", "summary_zh": "大型語言模型於敏感應用日漸普及，突顯有效浮水印技術之必要性，以確保AI生成文本之溯源性與責任歸屬。然而，多數現有浮水印方法需取用解碼過程，限制其實際應用。為解決此缺口，本研究提出情境式浮水印（ICW），僅透過提示工程將浮水印嵌入生成文本，利用大型語言模型的情境學習與指令遵循能力。我們探討四種不同精細度的ICW策略，並搭配客製化偵測方法。此外，我們研究間接提示注入（IPI）情境，此為透過修改學術論文等輸入文件，隱蔽地觸發浮水印的案例研究。實驗驗證ICW作為模型無關、實用浮水印方法的可行性。研究結果顯示，隨著大型語言模型能力提升，ICW為可擴展且易於使用的內容歸屬提供具前景的方向。", "audio": "audios/2505.16934v1.mp3", "timestamp": "2025-05-23T14:17:45.222759"}
{"query": "Foundation Model", "id": "2505.16490v1", "url": "http://arxiv.org/abs/2505.16490v1", "title": "HPP-Voice: A Large-Scale Evaluation of Speech Embeddings for Multi-Phenotypic Classification", "summary": "Human speech contains paralinguistic cues that reflect a speaker's\nphysiological and neurological state, potentially enabling non-invasive\ndetection of various medical phenotypes. We introduce the Human Phenotype\nProject Voice corpus (HPP-Voice): a dataset of 7,188 recordings in which\nHebrew-speaking adults count for 30 seconds, with each speaker linked to up to\n15 potentially voice-related phenotypes spanning respiratory, sleep, mental\nhealth, metabolic, immune, and neurological conditions. We present a systematic\ncomparison of 14 modern speech embedding models, where modern speech embeddings\nfrom these 30-second counting tasks outperform MFCCs and demographics for\ndownstream health condition classifications. We found that embedding learned\nfrom a speaker identification model can predict objectively measured moderate\nto severe sleep apnea in males with an AUC of 0.64 $\\pm$ 0.03, while MFCC and\ndemographic features led to AUCs of 0.56 $\\pm$ 0.02 and 0.57 $\\pm$ 0.02,\nrespectively. Additionally, our results reveal gender-specific patterns in\nmodel effectiveness across different medical domains. For males, speaker\nidentification and diarization models consistently outperformed speech\nfoundation models for respiratory conditions (e.g., asthma: 0.61 $\\pm$ 0.03 vs.\n0.56 $\\pm$ 0.02) and sleep-related conditions (insomnia: 0.65 $\\pm$ 0.04 vs.\n0.59 $\\pm$ 0.05). For females, speaker diarization models performed best for\nsmoking status (0.61 $\\pm$ 0.02 vs 0.55 $\\pm$ 0.02), while Hebrew-specific\nmodels performed best (0.59 $\\pm$ 0.02 vs. 0.58 $\\pm$ 0.02) in classifying\nanxiety compared to speech foundation models. Our findings provide evidence\nthat a simple counting task can support large-scale, multi-phenotypic voice\nscreening and highlight which embedding families generalize best to specific\nconditions, insights that can guide future vocal biomarker research and\nclinical deployment.", "authors": ["David Krongauz", "Hido Pinto", "Sarah Kohn", "Yanir Marmor", "Eran Segal"], "published_date": "2025-05-22", "title_zh": "HPP-語音：用於多表型分類的語音嵌入大規模評估", "summary_zh": "人類語音包含反映生理和神經狀態的超語言線索，有助於非侵入式檢測多種醫學表型。我們推出人類表型計畫語音語料庫(HPP-Voice)，包含7188份希伯來語成年人30秒計數錄音，每位受試者關聯至多15種潛在語音相關表型，涵蓋呼吸、睡眠、精神健康、代謝、免疫及神經系統疾病。系統性比較14種現代語音嵌入模型，發現此計數任務產生的現代語音嵌入，在下游健康狀況分類中優於MFCC和人口統計特徵。由說話人辨識模型學習的嵌入，能預測男性客觀測量的中重度睡眠呼吸中止症，AUC達0.64 $\\pm$ 0.03，而MFCC和人口統計特徵的AUC分別為0.56 $\\pm$ 0.02和0.57 $\\pm$ 0.02。結果顯示模型在不同醫學領域的有效性存在性別差異。對於男性，說話人辨識和區分模型在呼吸系統疾病和睡眠相關疾病方面，始終優於語音基礎模型。對於女性，說話人區分模型在吸菸狀態的表現最佳，而希伯來語專用模型在焦慮症分類中優於語音基礎模型。研究結果證明，簡單的計數任務可支援大規模、多表型的語音篩查，並強調哪些嵌入家族最能推廣至特定疾病，為未來語音生物標記研究和臨床應用提供參考。", "audio": "audios/2505.16490v1.mp3", "timestamp": "2025-05-23T14:17:56.417910"}
{"query": "Diffusion Model", "id": "2505.16839v1", "url": "http://arxiv.org/abs/2505.16839v1", "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding", "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.", "authors": ["Shufan Li", "Konstantinos Kallidromitis", "Hritik Bansal", "Akash Gokul", "Yusuke Kato", "Kazuki Kozuka", "Jason Kuen", "Zhe Lin", "Kai-Wei Chang", "Aditya Grover"], "published_date": "2025-05-22", "title_zh": "LaViDa：用於多模態理解的大型擴散語言模型", "summary_zh": "現代視覺語言模型（VLMs）擅長解決需要視覺推理的多種任務。在實際應用中，快速推論和可控生成至關重要，但現有的自迴歸VLMs如LLaVA在此方面表現不佳。離散擴散模型（DMs）提供了一種有前景的替代方案，透過平行解碼加速推論，並利用雙向上下文實現可控生成。我們提出了LaViDa，一個基於DMs的VLMs家族。LaViDa透過為DMs配備視覺編碼器並聯合微調，實現多模態指令追蹤。LaViDa採用互補遮罩以提升訓練效果，前綴KV快取以提高推論效率，以及時間步長偏移以改善取樣品質。實驗表明，LaViDa在MMMU等多模態基準測試中，效能與自迴歸VLMs相當或更優越，同時具備DMs的獨特優勢，包括靈活的速度與品質權衡、可控性和雙向推理。在COCO圖片標註中，LaViDa以1.92倍的速度提升超越Open-LLaVa-Next-8B 4.1個CIDEr點。在雙向任務中，LaViDa在受限詩歌補全方面提升了59%。這些結果證明LaViDa是自迴歸VLMs的一個強大替代方案。", "audio": "audios/2505.16839v1.mp3", "timestamp": "2025-05-23T14:18:05.896554"}
{"query": "AI", "id": "2505.16928v1", "url": "http://arxiv.org/abs/2505.16928v1", "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning", "summary": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning.", "authors": ["Bosung Kim", "Prithviraj Ammanabrolu"], "published_date": "2025-05-22", "title_zh": "超越具身稻草堆中的針：長上下文推理的環境、架構與訓練考量", "summary_zh": "$\\infty$-THOR為具身人工智慧中的長程任務提出新框架，促進長文理解。其提供：(1)可擴展、可重現且無限長程軌跡的生成框架；(2)新型具身問答任務Needle(s) in the Embodied Haystack，透過分散線索測試智能體的長文推理能力；(3)長程資料集及基準測試集，包含跨越數百環境步驟的複雜任務，並附帶真實動作序列。為實現此能力，探索架構調整，包括交錯目標-狀態-動作建模、上下文擴展技術與上下文並行，使基於大型語言模型的智能體具備極端長文推理與互動能力。實驗結果突顯基準測試帶來的挑戰，並提供長程條件下訓練策略及模型行為的洞見。此研究為下一代具備穩健長程推理與規劃能力的具身人工智慧系統奠定基礎。", "audio": "audios/2505.16928v1.mp3", "timestamp": "2025-05-23T15:19:17.435111"}
{"query": "Foundation Model", "id": "2505.16360v1", "url": "http://arxiv.org/abs/2505.16360v1", "title": "Style Transfer with Diffusion Models for Synthetic-to-Real Domain Adaptation", "summary": "Semantic segmentation models trained on synthetic data often perform poorly\non real-world images due to domain gaps, particularly in adverse conditions\nwhere labeled data is scarce. Yet, recent foundation models enable to generate\nrealistic images without any training. This paper proposes to leverage such\ndiffusion models to improve the performance of vision models when learned on\nsynthetic data. We introduce two novel techniques for semantically consistent\nstyle transfer using diffusion models: Class-wise Adaptive Instance\nNormalization and Cross-Attention (CACTI) and its extension with selective\nattention Filtering (CACTIF). CACTI applies statistical normalization\nselectively based on semantic classes, while CACTIF further filters\ncross-attention maps based on feature similarity, preventing artifacts in\nregions with weak cross-attention correspondences. Our methods transfer style\ncharacteristics while preserving semantic boundaries and structural coherence,\nunlike approaches that apply global transformations or generate content without\nconstraints. Experiments using GTA5 as source and Cityscapes/ACDC as target\ndomains show that our approach produces higher quality images with lower FID\nscores and better content preservation. Our work demonstrates that class-aware\ndiffusion-based style transfer effectively bridges the synthetic-to-real domain\ngap even with minimal target domain data, advancing robust perception systems\nfor challenging real-world applications. The source code is available at:\nhttps://github.com/echigot/cactif.", "authors": ["Estelle Chigot", "Dennis G. Wilson", "Meriem Ghrib", "Thomas Oberlin"], "published_date": "2025-05-22", "title_zh": "基於擴散模型的風格遷移用於合成到真實領域適應", "summary_zh": "基於合成數據訓練的語義分割模型在真實圖像上表現不佳，主因在於領域差異，尤其是在標記數據稀缺的惡劣條件下。 近期基礎模型無需訓練即可生成逼真圖像。 本文提出利用擴散模型來提升視覺模型在合成數據上的學習效能。 我們引入兩種新的語義一致風格轉換技術：類別自適應實例正規化和交叉注意力（CACTI），及其帶選擇性注意力過濾的擴展（CACTIF）。 CACTI根據語義類別選擇性地應用統計正規化，而CACTIF則基於特徵相似性進一步過濾交叉注意力圖，從而防止弱交叉注意力區域中出現偽影。 我們的方法在保留語義邊界和結構一致性的同時轉換風格特徵，這與應用全局轉換或生成無約束內容的方法不同。 以GTA5為源域，Cityscapes/ACDC為目標域的實驗表明，我們的方法可生成更高品質的圖像，具有更低的FID分數和更好的內容保留。 研究表明，基於類別感知的擴散風格轉換能有效彌合合成到真實的領域差異，即使目標域數據極少，亦可推進適用於具挑戰性現實應用之強韌感知系統。 原始碼可在以下網址取得：https://github.com/echigot/cactif。", "audio": "audios/2505.16360v1.mp3", "timestamp": "2025-05-23T15:19:36.415768"}
{"query": "Diffusion Model", "id": "2505.16798v1", "url": "http://arxiv.org/abs/2505.16798v1", "title": "SEED: Speaker Embedding Enhancement Diffusion Model", "summary": "A primary challenge when deploying speaker recognition systems in real-world\napplications is performance degradation caused by environmental mismatch. We\npropose a diffusion-based method that takes speaker embeddings extracted from a\npre-trained speaker recognition model and generates refined embeddings. For\ntraining, our approach progressively adds Gaussian noise to both clean and\nnoisy speaker embeddings extracted from clean and noisy speech, respectively,\nvia forward process of a diffusion model, and then reconstructs them to clean\nembeddings in the reverse process. While inferencing, all embeddings are\nregenerated via diffusion process. Our method needs neither speaker label nor\nany modification to the existing speaker recognition pipeline. Experiments on\nevaluation sets simulating environment mismatch scenarios show that our method\ncan improve recognition accuracy by up to 19.6% over baseline models while\nretaining performance on conventional scenarios. We publish our code here\nhttps://github.com/kaistmm/seed-pytorch", "authors": ["KiHyun Nam", "Jungwoo Heo", "Jee-weon Jung", "Gangin Park", "Chaeyoung Jung", "Ha-Jin Yu", "Joon Son Chung"], "published_date": "2025-05-22", "title_zh": "SEED：說者嵌入增強擴散模型", "summary_zh": "在實際應用中部署語者辨識系統的主要挑戰是環境不匹配導致的效能下降。 我們提出一種基於擴散模型的方法，該方法提取自預訓練語者辨識模型的語者嵌入，並產生精煉的嵌入。 在訓練階段，我們的策略透過擴散模型的前向過程，逐步將高斯雜訊添加到乾淨和嘈雜語音中提取的乾淨及嘈雜語者嵌入，然後在反向過程中將它們重建為乾淨的嵌入。 在推理時，所有嵌入都透過擴散過程重新生成。 該方法無需語者標籤，也不需要修改現有的語者辨識流程。 在模擬環境不匹配場景的評估集上的實驗表明，我們的方法可以將辨識準確率提高至比基線模型高 19.6%，同時保持在傳統場景下的效能。 我們的程式碼已公開於https://github.com/kaistmm/seed-pytorch。", "audio": "audios/2505.16798v1.mp3", "timestamp": "2025-05-23T15:19:48.427714"}
{"query": "AI", "id": "2505.16899v1", "url": "http://arxiv.org/abs/2505.16899v1", "title": "Identifying, Evaluating, and Mitigating Risks of AI Thought Partnerships", "summary": "Artificial Intelligence (AI) systems have historically been used as tools\nthat execute narrowly defined tasks. Yet recent advances in AI have unlocked\npossibilities for a new class of models that genuinely collaborate with humans\nin complex reasoning, from conceptualizing problems to brainstorming solutions.\nSuch AI thought partners enable novel forms of collaboration and extended\ncognition, yet they also pose major risks-including and beyond risks of typical\nAI tools and agents. In this commentary, we systematically identify risks of AI\nthought partners through a novel framework that identifies risks at multiple\nlevels of analysis, including Real-time, Individual, and Societal risks arising\nfrom collaborative cognition (RISc). We leverage this framework to propose\nconcrete metrics for risk evaluation, and finally suggest specific mitigation\nstrategies for developers and policymakers. As AI thought partners continue to\nproliferate, these strategies can help prevent major harms and ensure that\nhumans actively benefit from productive thought partnerships.", "authors": ["Kerem Oktar", "Katherine M. Collins", "Jose Hernandez-Orallo", "Diane Coyle", "Stephen Cave", "Adrian Weller", "Ilia Sucholutsky"], "published_date": "2025-05-22", "title_zh": "識別、評估與緩解人工智慧思維夥伴關係之風險", "summary_zh": "人工智慧系統過去多為執行特定任務之工具。然近期發展已催生新型模型，能與人類於複雜推理中真正協作，涵蓋問題概念化至方案集思廣益。此類AI思考夥伴促進嶄新協作模式及延伸認知，然亦帶來重大風險，不僅限於傳統AI工具及代理。本文提出新穎框架，於即時、個體及社會層面系統性識別協作認知所生風險(RISc)，並據此提出風險評估之具體指標，最終為開發者及決策者建議特定緩解策略。隨AI思考夥伴日漸普及，此等策略有助預防重大危害，確保人類積極受益於具生產力之思考夥伴關係。", "audio": "audios/2505.16899v1.mp3", "timestamp": "2025-05-23T16:22:17.025557"}
{"query": "Foundation Model", "id": "2505.16338v1", "url": "http://arxiv.org/abs/2505.16338v1", "title": "Fusion of Foundation and Vision Transformer Model Features for Dermatoscopic Image Classification", "summary": "Accurate classification of skin lesions from dermatoscopic images is\nessential for diagnosis and treatment of skin cancer. In this study, we\ninvestigate the utility of a dermatology-specific foundation model, PanDerm, in\ncomparison with two Vision Transformer (ViT) architectures (ViT base and Swin\nTransformer V2 base) for the task of skin lesion classification. Using frozen\nfeatures extracted from PanDerm, we apply non-linear probing with three\ndifferent classifiers, namely, multi-layer perceptron (MLP), XGBoost, and\nTabNet. For the ViT-based models, we perform full fine-tuning to optimize\nclassification performance. Our experiments on the HAM10000 and MSKCC datasets\ndemonstrate that the PanDerm-based MLP model performs comparably to the\nfine-tuned Swin transformer model, while fusion of PanDerm and Swin Transformer\npredictions leads to further performance improvements. Future work will explore\nadditional foundation models, fine-tuning strategies, and advanced fusion\ntechniques.", "authors": ["Amirreza Mahbod", "Rupert Ecker", "Ramona Woitek"], "published_date": "2025-05-22", "title_zh": "基於基礎模型與視覺變換器模型特徵融合的皮膚鏡圖像分類", "summary_zh": "皮膚鏡影像的準確皮膚病灶分類對皮膚癌診斷和治療至關重要。本研究探討皮膚科專用基礎模型PanDerm在皮膚病灶分類中的效用，並與兩種Vision Transformer (ViT)架構(ViT base 和 Swin Transformer V2 base) 進行比較。我們使用從PanDerm提取的凍結特徵，透過多層感知器(MLP)、XGBoost和TabNet三種分類器進行非線性探測。對於基於ViT的模型，我們執行完全微調以優化分類性能。在HAM10000和MSKCC數據集上的實驗表明，基於PanDerm的MLP模型性能與微調後的Swin Transformer模型相當，而PanDerm和Swin Transformer預測的融合可進一步提高性能。未來工作將探索更多基礎模型、微調策略和進階融合技術。", "audio": "audios/2505.16338v1.mp3", "timestamp": "2025-05-23T16:22:28.954045"}
{"query": "Diffusion Model", "id": "2505.16790v1", "url": "http://arxiv.org/abs/2505.16790v1", "title": "Learning Flexible Forward Trajectories for Masked Molecular Diffusion", "summary": "Masked diffusion models (MDMs) have achieved notable progress in modeling\ndiscrete data, while their potential in molecular generation remains\nunderexplored. In this work, we explore their potential and introduce the\nsurprising result that naively applying standards MDMs severely degrades the\nperformance. We identify the critical cause of this issue as a state-clashing\nproblem-where the forward diffusion of distinct molecules collapse into a\ncommon state, resulting in a mixture of reconstruction targets that cannot be\nlearned using typical reverse diffusion process with unimodal predictions. To\nmitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that\norchestrates per-element corruption trajectories to avoid collision between\ndistinct molecular graphs. This is achieved through a parameterized noise\nscheduling network that assigns distinct corruption rates to individual graph\nelements, i.e., atoms and bonds. Extensive experiments on diverse molecular\nbenchmarks reveal that MELD markedly enhances overall generation quality\ncompared to element-agnostic noise scheduling, increasing the chemical validity\nof vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves\nstate-of-the-art property alignment in conditional generation tasks.", "authors": ["Hyunjin Seo", "Taewon Kim", "Sihyun Yu", "SungSoo Ahn"], "published_date": "2025-05-22", "title_zh": "用於掩碼分子擴散的柔性前向軌跡學習", "summary_zh": "遮罩擴散模型在離散數據建模方面取得顯著進展，但其在分子生成中的潛力尚未充分探索。本研究發現，直接應用標準遮罩擴散模型會嚴重降低性能，原因在於狀態衝突問題：不同分子的正向擴散會坍縮到共同狀態，導致逆向擴散過程中出現難以學習的混合重建目標。為此，我們提出遮罩元素級可學習擴散（MELD），協調逐元素腐敗軌跡以避免分子圖之間的衝突。MELD透過參數化的雜訊排程網路，為個別圖元素（原子和鍵）分配不同的腐敗率。在多個分子基準測試中，MELD顯著提高了整體生成品質，在ZINC250K數據集上，原始遮罩擴散模型的化學有效性從15%提升至93%，並在條件生成任務中實現了最先進的性質對齊。", "audio": "audios/2505.16790v1.mp3", "timestamp": "2025-05-23T16:22:44.838182"}
{"query": "AI", "id": "2505.16888v1", "url": "http://arxiv.org/abs/2505.16888v1", "title": "CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework", "summary": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available.", "authors": ["Viet Pham", "Thai Le"], "published_date": "2025-05-22", "title_zh": "CAIN：基於兩階段惡意系統提示詞生成與精煉框架的LLM-人機對話劫持", "summary_zh": "大型語言模型（LLMs）雖推動諸多應用，卻易受對抗性攻擊。本文提出一種新型安全威脅：透過操縱LLMs的系統提示，劫持人機對話，使其僅針對特定目標問題（如總統投票人選、新冠疫苗安全性）產生惡意回答，而在其他問題上表現正常。此攻擊具危害性，惡意行為者可藉散布看似無害的惡意系統提示，進行大規模資訊操縱。為驗證此攻擊，我們開發CAIN演算法，可在黑箱環境下自動產生針對特定目標問題的有害系統提示，無需存取LLM參數。於開源及商業LLMs上的評估顯示，CAIN具有顯著的對抗性影響。在非定向攻擊或迫使LLMs輸出錯誤答案時，CAIN使目標問題的F1值降低高達40%，同時保持良性輸入的高準確性。在定向攻擊或迫使LLMs輸出特定有害答案時，CAIN在這些目標回應上實現超過70%的F1值，且對良性問題影響甚微。研究結果突顯了加強穩健性措施的迫切需求，以保護LLMs在實際應用中的完整性和安全性。所有原始碼將公開。", "audio": "audios/2505.16888v1.mp3", "timestamp": "2025-05-23T17:16:40.379263"}
{"query": "Foundation Model", "id": "2505.16304v1", "url": "http://arxiv.org/abs/2505.16304v1", "title": "SAMba-UNet: Synergizing SAM2 and Mamba in UNet with Heterogeneous Aggregation for Cardiac MRI Segmentation", "summary": "To address the challenge of complex pathological feature extraction in\nautomated cardiac MRI segmentation, this study proposes an innovative\ndual-encoder architecture named SAMba-UNet. The framework achieves cross-modal\nfeature collaborative learning by integrating the vision foundation model SAM2,\nthe state-space model Mamba, and the classical UNet. To mitigate domain\ndiscrepancies between medical and natural images, a Dynamic Feature Fusion\nRefiner is designed, which enhances small lesion feature extraction through\nmulti-scale pooling and a dual-path calibration mechanism across channel and\nspatial dimensions. Furthermore, a Heterogeneous Omni-Attention Convergence\nModule (HOACM) is introduced, combining global contextual attention with\nbranch-selective emphasis mechanisms to effectively fuse SAM2's local\npositional semantics and Mamba's long-range dependency modeling capabilities.\nExperiments on the ACDC cardiac MRI dataset demonstrate that the proposed model\nachieves a Dice coefficient of 0.9103 and an HD95 boundary error of 1.0859 mm,\nsignificantly outperforming existing methods, particularly in boundary\nlocalization for complex pathological structures such as right ventricular\nanomalies. This work provides an efficient and reliable solution for automated\ncardiac disease diagnosis, and the code will be open-sourced.", "authors": ["Guohao Huo", "Ruiting Dai", "Hao Tang"], "published_date": "2025-05-22", "title_zh": "SAMba-UNet：結合SAM2與Mamba的UNet，透過異質聚合用於心臟MRI分割", "summary_zh": "本研究提出名為SAMba-UNet的創新雙編碼器架構，旨在解決自動心臟MRI分割中複雜病理特徵提取的挑戰。該框架整合視覺基礎模型SAM2、狀態空間模型Mamba和經典UNet，實現跨模態特徵協同學習。為緩解醫學圖像與自然圖像之間的領域差異，設計動態特徵融合精煉器，透過多尺度池化和跨通道及空間維度的雙路徑校準機制，增強小病灶特徵提取。此外，引入異質全方位注意力融合模組(HOACM)，結合全局上下文注意力和分支選擇性強調機制，有效融合SAM2的局部位置語義和Mamba的長程依賴建模能力。在ACDC心臟MRI數據集上的實驗表明，所提出的模型Dice係數達到0.9103，HD95邊界誤差為1.0859毫米，顯著優於現有方法，尤其是在右心室異常等複雜病理結構的邊界定位方面。本研究為自動心臟疾病診斷提供了一種高效可靠的解決方案，並將開源程式碼。", "audio": "audios/2505.16304v1.mp3", "timestamp": "2025-05-23T17:16:52.040396"}
{"query": "Diffusion Model", "id": "2505.16733v1", "url": "http://arxiv.org/abs/2505.16733v1", "title": "Forward-only Diffusion Probabilistic Models", "summary": "This work presents a forward-only diffusion (FoD) approach for generative\nmodelling. In contrast to traditional diffusion models that rely on a coupled\nforward-backward diffusion scheme, FoD directly learns data generation through\na single forward diffusion process, yielding a simple yet efficient generative\nframework. The core of FoD is a state-dependent linear stochastic differential\nequation that involves a mean-reverting term in both the drift and diffusion\nfunctions. This mean-reversion property guarantees the convergence to clean\ndata, naturally simulating a stochastic interpolation between source and target\ndistributions. More importantly, FoD is analytically tractable and is trained\nusing a simple stochastic flow matching objective, enabling a few-step\nnon-Markov chain sampling during inference. The proposed FoD model, despite its\nsimplicity, achieves competitive performance on various image-conditioned\n(e.g., image restoration) and unconditional generation tasks, demonstrating its\neffectiveness in generative modelling. Our code is available at\nhttps://github.com/Algolzw/FoD.", "authors": ["Ziwei Luo", "Fredrik K. Gustafsson", "Jens Sjölund", "Thomas B. Schön"], "published_date": "2025-05-22", "title_zh": "僅正向擴散機率模型", "summary_zh": "本研究提出一種僅前向擴散(FoD)的生成模型方法。FoD不同於傳統依賴耦合前向-後向擴散方案的擴散模型，直接透過單一前向擴散過程學習資料生成，產生一個簡單而高效的生成框架。FoD的核心是一個狀態相關的線性隨機微分方程，其漂移和擴散函數中均包含均值回歸項。此均值回歸特性保證收斂至乾淨資料，自然地模擬源分佈和目標分佈之間的隨機插值。更重要的是，FoD在分析上易於處理，並使用簡單的隨機流匹配目標進行訓練，從而在推論期間實現少步非馬可夫鏈取樣。所提出的FoD模型儘管結構簡單，但在各種圖像條件（例如，圖像修復）和無條件生成任務上均實現了有競爭力的效能，證明其在生成模型中的有效性。程式碼可在https://github.com/Algolzw/FoD取得。", "audio": "audios/2505.16733v1.mp3", "timestamp": "2025-05-23T17:17:03.364699"}
{"query": "AI", "id": "2505.16869v1", "url": "http://arxiv.org/abs/2505.16869v1", "title": "MPO: Multilingual Safety Alignment via Reward Gap Optimization", "summary": "Large language models (LLMs) have become increasingly central to AI\napplications worldwide, necessitating robust multilingual safety alignment to\nensure secure deployment across diverse linguistic contexts. Existing\npreference learning methods for safety alignment, such as RLHF and DPO, are\nprimarily monolingual and struggle with noisy multilingual data. To address\nthese limitations, we introduce Multilingual reward gaP Optimization (MPO), a\nnovel approach that leverages the well-aligned safety capabilities of the\ndominant language (English) to improve safety alignment across multiple\nlanguages. MPO directly minimizes the reward gap difference between the\ndominant language and target languages, effectively transferring safety\ncapabilities while preserving the original strengths of the dominant language.\nExtensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate\nMPO's efficacy in multilingual safety alignment without degrading general\nmultilingual utility.", "authors": ["Weixiang Zhao", "Yulin Hu", "Yang Deng", "Tongtong Wu", "Wenxuan Zhang", "Jiahe Guo", "An Zhang", "Yanyan Zhao", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "published_date": "2025-05-22", "title_zh": "MPO：透過獎勵差距最佳化實現多語言安全對齊", "summary_zh": "大型語言模型於全球人工智慧應用中日益重要，需具備穩健的多語安全對齊，以確保在不同語言環境下的安全部署。現有的安全對齊偏好學習方法，如RLHF和DPO，主要為單語且易受嘈雜多語資料影響。為了解決這些限制，我們提出多語獎勵差距優化（MPO），此方法利用主導語言（英語）良好的安全能力，提升多種語言的安全對齊。MPO直接最小化主導語言與目標語言之間的獎勵差距差異，有效轉移安全能力，同時保留主導語言的優勢。在LLaMA-3.1、Gemma-2和Qwen2.5三個大型語言模型上的廣泛實驗，驗證了MPO在多語安全對齊方面的有效性，且不降低一般多語實用性。", "audio": "audios/2505.16869v1.mp3", "timestamp": "2025-05-23T18:24:33.935318"}
{"query": "Foundation Model", "id": "2505.16130v1", "url": "http://arxiv.org/abs/2505.16130v1", "title": "Scalable Graph Generative Modeling via Substructure Sequences", "summary": "Graph neural networks (GNNs) has been predominantly driven by\nmessage-passing, where node representations are iteratively updated via local\nneighborhood aggregation. Despite their success, message-passing suffers from\nfundamental limitations -- including constrained expressiveness,\nover-smoothing, over-squashing, and limited capacity to model long-range\ndependencies. These issues hinder scalability: increasing data size or model\nsize often fails to yield improved performance, limiting the viability of GNNs\nas backbones for graph foundation models. In this work, we explore pathways\nbeyond message-passing and introduce Generative Graph Pattern Machine\n(G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM\nrepresents graph instances (nodes, edges, or entire graphs) as sequences of\nsubstructures, and employs generative pre-training over the sequences to learn\ngeneralizable, transferable representations. Empirically, G$^2$PM demonstrates\nstrong scalability: on the ogbn-arxiv benchmark, it continues to improve with\nmodel sizes up to 60M parameters, outperforming prior generative approaches\nthat plateau at significantly smaller scales (e.g., 3M). In addition, we\nsystematically analyze the model design space, highlighting key architectural\nchoices that contribute to its scalability and generalization. Across diverse\ntasks -- including node classification, graph classification, and transfer\nlearning -- G$^2$PM consistently outperforms strong baselines, establishing a\ncompelling foundation for scalable graph learning. The code and dataset are\navailable at https://github.com/Zehong-Wang/G2PM.", "authors": ["Zehong Wang", "Zheyuan Zhang", "Tianyi Ma", "Chuxu Zhang", "Yanfang Ye"], "published_date": "2025-05-22", "title_zh": "基於子結構序列的可擴展圖生成模型", "summary_zh": "圖神經網路主要基於訊息傳遞，透過鄰域聚合迭代更新節點表示。訊息傳遞存在表達力受限、過度平滑、過度擠壓以及長程依賴建模能力不足等根本限制，阻礙了其擴展性。本文探索訊息傳遞之外的途徑，提出生成式圖樣式機器(G$^2$PM)，一種用於圖的生成式Transformer預訓練框架。G$^2$PM將圖實例表示為子結構序列，並透過序列上的生成式預訓練學習可泛化、可轉移的表示。實驗表明，G$^2$PM具有強大的擴展性，在ogbn-arxiv基準測試中，隨著模型規模增大(最高達60M參數)，性能持續提升，優於在較小規模(如3M)下達到瓶頸的生成式方法。此外，本文系統地分析了模型設計空間，強調了促進擴展性和泛化的關鍵架構選擇。在節點分類、圖分類和轉移學習等多項任務中，G$^2$PM始終優於強大的基線模型，為可擴展圖學習奠定了基礎。", "audio": "audios/2505.16130v1.mp3", "timestamp": "2025-05-23T18:24:50.559904"}
{"query": "Diffusion Model", "id": "2505.16549v1", "url": "http://arxiv.org/abs/2505.16549v1", "title": "Towards Coordinate- and Dimension-Agnostic Machine Learning for Partial Differential Equations", "summary": "The machine learning methods for data-driven identification of partial\ndifferential equations (PDEs) are typically defined for a given number of\nspatial dimensions and a choice of coordinates the data have been collected in.\nThis dependence prevents the learned evolution equation from generalizing to\nother spaces. In this work, we reformulate the problem in terms of coordinate-\nand dimension-independent representations, paving the way toward what we call\n``spatially liberated\" PDE learning. To this end, we employ a machine learning\napproach to predict the evolution of scalar field systems expressed in the\nformalism of exterior calculus, which is coordinate-free and immediately\ngeneralizes to arbitrary dimensions by construction. We demonstrate the\nperformance of this approach in the FitzHugh-Nagumo and Barkley\nreaction-diffusion models, as well as the Patlak-Keller-Segel model informed by\nin-situ chemotactic bacteria observations. We provide extensive numerical\nexperiments that demonstrate that our approach allows for seamless transitions\nacross various spatial contexts. We show that the field dynamics learned in one\nspace can be used to make accurate predictions in other spaces with different\ndimensions, coordinate systems, boundary conditions, and curvatures.", "authors": ["Trung V. Phan", "George A. Kevrekidis", "Soledad Villar", "Yannis G. Kevrekidis", "Juan M. Bello-Rivas"], "published_date": "2025-05-22", "title_zh": "偏微分方程之坐標與維度無關機器學習方法研究", "summary_zh": "基於資料驅動的偏微分方程式(PDE)機器學習方法通常針對特定空間維度和座標系設計，限制了其泛化能力。本研究將問題重新表述為與座標和維度無關的形式，實現空間解放的PDE學習。我們利用機器學習預測標量場系統的演化，該系統採用外微分形式，具有無座標特性，並能自然推廣到任意維度。研究展示了此方法在FitzHugh-Nagumo、Barkley反應擴散模型以及Patlak-Keller-Segel模型中的表現，後者使用了原位趨化細菌觀測資料。大量實驗證明，此方法能跨越不同空間情境，在不同維度、座標系、邊界條件和曲率下準確預測場動力學。", "audio": "audios/2505.16549v1.mp3", "timestamp": "2025-05-23T18:25:04.214820"}
{"query": "AI", "id": "2505.16866v1", "url": "http://arxiv.org/abs/2505.16866v1", "title": "Including the magnitude variability of a signal into the ordinal pattern analysis", "summary": "One of the most popular and innovative methods to analyse signals is by using\nOrdinal Patterns (OPs). The OP encoding is based on transforming a (univariate)\nsignal into a symbolic sequence of OPs, where each OP represents the number of\npermutations needed to order a small subset of the signal's magnitudes. This\nimplies that OPs are conceptually clear, methodologically simple to implement,\nrobust to noise, and can be applied to short signals. Moreover, they simplify\nthe statistical analyses that can be carried out on a signal, such as entropy\nand complexity quantifications. However, because of the relative ordering,\ninformation about the magnitude of the signal at each timestamp is lost -- this\nbeing one of the major drawbacks in the method. Here, we propose a way to use\nthe signal magnitudes discarded in the OP encoding as a complementary variable\nto its permutation entropy. To illustrate our approach, we analyse synthetic\ntrajectories from logistic and H{\\'e}non maps -- with and without added noise\n-- and intracranial electroencephalographic recordings from rats in different\nsleep-wake states. Our results show that, when complementing the permutation\nentropy with the variability in the signal magnitudes, the characterisation of\nthe dynamical behaviours of the maps and the sleep-wake states is improved.\nThis implies that our approach can be useful for feature engineering and\nimproving AI classifiers, where typical machine learning algorithms need\ncomplementary signal features as inputs to improve classification accuracy.", "authors": ["Melvyn Tyloo", "Joaquín González", "Nicolás Rubido"], "published_date": "2025-05-22", "title_zh": "將訊號幅度變異性納入序模式分析", "summary_zh": "序數模式(OPs)是熱門且創新的訊號分析方法，透過將單變量訊號轉換為OPs符號序列，以代表訊號幅度子集的排序排列數。OPs概念清晰、易於實現、具備抗噪性，且適用於短訊號，簡化了訊號的統計分析，如熵值和複雜度量化。然而，相對排序會遺失訊號幅度訊息。本研究提出一種方法，將OP編碼中捨棄的訊號幅度作為排列熵的互補變量。我們分析了邏輯和Hénon映射的合成軌跡（含噪與無噪），以及大鼠不同睡眠覺醒狀態下的顱內腦電記錄。結果表明，將訊號幅度變異性納入考量後，能有效改善映射動態行為和睡眠覺醒狀態的特性描述。此方法有助於特徵工程，並提升AI分類器的效能，透過提供互補訊號特徵，改善分類準確性。", "audio": "audios/2505.16866v1.mp3", "timestamp": "2025-05-23T19:14:31.672727"}
{"query": "Foundation Model", "id": "2505.16027v1", "url": "http://arxiv.org/abs/2505.16027v1", "title": "Benchmarking Chest X-ray Diagnosis Models Across Multinational Datasets", "summary": "Foundation models leveraging vision-language pretraining have shown promise\nin chest X-ray (CXR) interpretation, yet their real-world performance across\ndiverse populations and diagnostic tasks remains insufficiently evaluated. This\nstudy benchmarks the diagnostic performance and generalizability of foundation\nmodels versus traditional convolutional neural networks (CNNs) on multinational\nCXR datasets. We evaluated eight CXR diagnostic models - five vision-language\nfoundation models and three CNN-based architectures - across 37 standardized\nclassification tasks using six public datasets from the USA, Spain, India, and\nVietnam, and three private datasets from hospitals in China. Performance was\nassessed using AUROC, AUPRC, and other metrics across both shared and\ndataset-specific tasks. Foundation models outperformed CNNs in both accuracy\nand task coverage. MAVL, a model incorporating knowledge-enhanced prompts and\nstructured supervision, achieved the highest performance on public (mean AUROC:\n0.82; AUPRC: 0.32) and private (mean AUROC: 0.95; AUPRC: 0.89) datasets,\nranking first in 14 of 37 public and 3 of 4 private tasks. All models showed\nreduced performance on pediatric cases, with average AUROC dropping from 0.88\n+/- 0.18 in adults to 0.57 +/- 0.29 in children (p = 0.0202). These findings\nhighlight the value of structured supervision and prompt design in radiologic\nAI and suggest future directions including geographic expansion and ensemble\nmodeling for clinical deployment. Code for all evaluated models is available at\nhttps://drive.google.com/drive/folders/1B99yMQm7bB4h1sVMIBja0RfUu8gLktCE", "authors": ["Qinmei Xu", "Yiheng Li", "Xianghao Zhan", "Ahmet Gorkem Er", "Brittany Dashevsky", "Chuanjun Xu", "Mohammed Alawad", "Mengya Yang", "Liu Ya", "Changsheng Zhou", "Xiao Li", "Haruka Itakura", "Olivier Gevaert"], "published_date": "2025-05-21", "title_zh": "跨國資料集下胸腔X光診斷模型基準測試", "summary_zh": "本研究評估了視覺語言預訓練基礎模型在胸腔X光判讀上的診斷效能與泛化能力，並與傳統卷積神經網路在多國胸腔X光數據集上進行基準測試。我們評估了五個視覺語言基礎模型和三個卷積神經網路架構，涵蓋來自美國、西班牙、印度、越南的六個公共數據集，以及來自中國醫院的三個私有數據集，共37項標準化分類任務。評估指標包括AUROC和AUPRC。結果顯示，基礎模型在準確性和任務覆蓋率上均優於卷積神經網路。採用知識增強提示和結構化監督的MAVL模型在公共（平均AUROC：0.82；AUPRC：0.32）和私有（平均AUROC：0.95；AUPRC：0.89）數據集上表現最佳。所有模型在兒科病例上的表現均有所下降。研究結果突顯了結構化監督和提示設計在放射醫學人工智慧中的價值，並提出了地理擴展和集成建模等未來發展方向。所有評估模型的程式碼均已公開。", "audio": "audios/2505.16027v1.mp3", "timestamp": "2025-05-23T19:14:52.739259"}
{"query": "Diffusion Model", "id": "2505.16527v1", "url": "http://arxiv.org/abs/2505.16527v1", "title": "Joint Relational Database Generation via Graph-Conditional Diffusion Models", "summary": "Building generative models for relational databases (RDBs) is important for\napplications like privacy-preserving data release and augmenting real datasets.\nHowever, most prior work either focuses on single-table generation or relies on\nautoregressive factorizations that impose a fixed table order and generate\ntables sequentially. This approach limits parallelism, restricts flexibility in\ndownstream applications like missing value imputation, and compounds errors due\nto commonly made conditional independence assumptions. We propose a\nfundamentally different approach: jointly modeling all tables in an RDB without\nimposing any order. By using a natural graph representation of RDBs, we propose\nthe Graph-Conditional Relational Diffusion Model (GRDM). GRDM leverages a graph\nneural network to jointly denoise row attributes and capture complex\ninter-table dependencies. Extensive experiments on six real-world RDBs\ndemonstrate that our approach substantially outperforms autoregressive\nbaselines in modeling multi-hop inter-table correlations and achieves\nstate-of-the-art performance on single-table fidelity metrics.", "authors": ["Mohamed Amine Ketata", "David Lüdke", "Leo Schwinn", "Stephan Günnemann"], "published_date": "2025-05-22", "title_zh": "基於圖條件擴散模型的聯合關係資料庫生成", "summary_zh": "為關聯式資料庫構建生成模型對保護隱私的資料發布及擴充真實資料集至關重要。現有方法多專注於單表生成或依賴自迴歸分解，此法強加固定表順序並循序生成表，限制平行處理能力，降低缺失值填補等下游應用彈性，並因常見條件獨立性假設而累積誤差。本文提出一種根本不同的方法：聯合建模RDB中所有表，不強加任何順序。透過RDB的自然圖表示，我們提出圖條件關係擴散模型(GRDM)。GRDM利用圖神經網路聯合去噪行屬性並捕捉複雜的表間依賴關係。在六個真實RDB上的大量實驗表明，該方法在建模多跳表間相關性方面顯著優於自迴歸基準，並在單表保真度指標上實現了最先進的性能。", "audio": "audios/2505.16527v1.mp3", "timestamp": "2025-05-23T19:15:03.143490"}
{"query": "AI", "id": "2505.16821v1", "url": "http://arxiv.org/abs/2505.16821v1", "title": "LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols", "summary": "Integrating large AI models (LAMs) into 6G mobile networks promises to\nredefine protocol design and control-plane intelligence by enabling autonomous,\ncognitive network operations. While industry concepts, such as ETSI's\nExperiential Networked Intelligence (ENI), envision LAM-driven agents for\nadaptive network slicing and intent-based management, practical implementations\nstill face challenges in protocol literacy and real-world deployment. This\npaper presents an end-to-end demonstration of a LAM that generates\nstandards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as\npart of control-plane procedures inside a gNB. We treat RRC messaging as a\ndomain-specific language and fine-tune a decoder-only transformer model (LLaMA\nclass) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages\nlinearized to retain their ASN.1 syntactic structure before standard byte-pair\nencoding tokenization. This enables combinatorial generalization over RRC\nprotocol states while minimizing training overhead. On 30k field-test\nrequest-response pairs, our 8 B model achieves a median cosine similarity of\n0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a\nzero-shot LLaMA-3 8B baseline -- indicating substantially improved structural\nand semantic RRC fidelity. Overall, our results show that LAMs, when augmented\nwith Radio Access Network (RAN)-specific reasoning, can directly orchestrate\ncontrol-plane procedures, representing a stepping stone toward the AI-native\nair-interface paradigm. Beyond RRC emulation, this work lays the groundwork for\nfuture AI-native wireless standards.", "authors": ["Ziming liu", "Bryan Liu", "Alvaro Valcarce", "Xiaoli Chu"], "published_date": "2025-05-22", "title_zh": "基於大型語言模型的無線資源控制層模擬：邁向原生人工智慧無線接取網路協定", "summary_zh": "大型AI模型（LAM）整合至6G行動網路有望透過實現自主認知網路運營，重新定義協定設計與控制平面智慧。儘管ETSI的體驗網路智慧（ENI）等業界概念設想了由LAM驅動的代理程式，用於自適應網路切片與意圖導向管理，但實際部署仍面臨協定理解與現實挑戰。本研究展示了一個端對端LAM範例，該模型能夠在gNB內部產生符合標準、以ASN.1編碼的無線電資源控制（RRC）訊息，作為控制平面程序的一部分。我們將RRC訊息視為特定領域語言，並在RRC訊息上使用參數高效的低秩適應（LoRA）微調解碼器Transformer模型（LLaMA類別），RRC訊息在標準位元組配對編碼標記化之前經過線性化處理，以保留其ASN.1語法結構。這實現了對RRC協定狀態的組合泛化，同時最大限度地降低了訓練開銷。在3萬個現場測試請求-回應配對上，我們的80億參數模型在邊緣GPU上實現了與真實訊息0.97的中位餘弦相似度，相較於零樣本LLaMA-3 80億參數基線，相對提升了61%，表明結構與語義RRC保真度顯著提高。總體而言，我們的結果表明，LAM在增強無線電接入網路（RAN）特定推理能力後，可以直接編排控制平面程序，代表著朝向AI原生空口範式的墊腳石。除了RRC模擬，這項工作也為未來AI原生無線標準奠定了基礎。", "audio": "audios/2505.16821v1.mp3", "timestamp": "2025-05-23T20:20:26.704098"}
{"query": "Foundation Model", "id": "2505.15970v1", "url": "http://arxiv.org/abs/2505.15970v1", "title": "Analyzing Hierarchical Structure in Vision Models with Sparse Autoencoders", "summary": "The ImageNet hierarchy provides a structured taxonomy of object categories,\noffering a valuable lens through which to analyze the representations learned\nby deep vision models. In this work, we conduct a comprehensive analysis of how\nvision models encode the ImageNet hierarchy, leveraging Sparse Autoencoders\n(SAEs) to probe their internal representations. SAEs have been widely used as\nan explanation tool for large language models (LLMs), where they enable the\ndiscovery of semantically meaningful features. Here, we extend their use to\nvision models to investigate whether learned representations align with the\nontological structure defined by the ImageNet taxonomy. Our results show that\nSAEs uncover hierarchical relationships in model activations, revealing an\nimplicit encoding of taxonomic structure. We analyze the consistency of these\nrepresentations across different layers of the popular vision foundation model\nDINOv2 and provide insights into how deep vision models internalize\nhierarchical category information by increasing information in the class token\nthrough each layer. Our study establishes a framework for systematic\nhierarchical analysis of vision model representations and highlights the\npotential of SAEs as a tool for probing semantic structure in deep networks.", "authors": ["Matthew Lyle Olson", "Musashi Hinck", "Neale Ratzlaff", "Changbai Li", "Phillip Howard", "Vasudev Lal", "Shao-Yen Tseng"], "published_date": "2025-05-21", "title_zh": "使用稀疏自動編碼器分析視覺模型中的層級結構", "summary_zh": "ImageNet層級結構為物件類別提供結構化分類，有助於分析深度視覺模型學習到的表徵。本研究利用稀疏自動編碼器(SAE)探測視覺模型的內部表徵，深入分析模型如何編碼ImageNet層級結構。SAE已被廣泛應用於大型語言模型(LLM)，以發現語義上有意義的特徵。本研究將其應用擴展到視覺模型，以檢驗學習到的表徵是否與ImageNet分類法定義的本體結構一致。結果顯示，SAE揭示了模型激活中的層級關係，展現了分類結構的隱式編碼。我們分析了DINOv2不同層之間表徵的一致性，並闡明深度視覺模型如何透過在每一層增加類別令牌中的信息來內化層級類別信息。本研究建立了一個系統性層級分析視覺模型表徵的框架，並強調了SAE作為探測深度網路語義結構工具的潛力。", "audio": "audios/2505.15970v1.mp3", "timestamp": "2025-05-23T20:20:35.152302"}
{"query": "Diffusion Model", "id": "2505.16512v1", "url": "http://arxiv.org/abs/2505.16512v1", "title": "Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection", "summary": "In recent years, the rapid development of deepfake technology has given rise\nto an emerging and serious threat to public security: diffusion model-based\ndigital human generation. Unlike traditional face manipulation methods, such\nmodels can generate highly realistic videos with consistency through multimodal\ncontrol signals. Their flexibility and covertness pose severe challenges to\nexisting detection strategies. To bridge this gap, we introduce DigiFakeAV, the\nfirst large-scale multimodal digital human forgery dataset based on diffusion\nmodels. Employing five latest digital human generation methods (Sonic, Hallo,\netc.) and voice cloning method, we systematically produce a dataset comprising\n60,000 videos (8.4 million frames), covering multiple nationalities, skin\ntones, genders, and real-world scenarios, significantly enhancing data\ndiversity and realism. User studies show that the confusion rate between forged\nand real videos reaches 68%, and existing state-of-the-art (SOTA) detection\nmodels exhibit large drops in AUC values on DigiFakeAV, highlighting the\nchallenge of the dataset. To address this problem, we further propose\nDigiShield, a detection baseline based on spatiotemporal and cross-modal\nfusion. By jointly modeling the 3D spatiotemporal features of videos and the\nsemantic-acoustic features of audio, DigiShield achieves SOTA performance on\nboth the DigiFakeAV and DF-TIMIT datasets. Experiments show that this method\neffectively identifies covert artifacts through fine-grained analysis of the\ntemporal evolution of facial features in synthetic videos.", "authors": ["Jiaxin Liu", "Jia Wang", "Saihui Hou", "Min Ren", "Huijia Wu", "Zhaofeng He"], "published_date": "2025-05-22", "title_zh": "超越換臉：基於擴散模型之數位人類基準，用於多模態深度偽造檢測", "summary_zh": "近年來，深度偽造技術快速發展，基於擴散模型的數位人生成對公共安全構成新興且嚴重的威脅。此類模型透過多模態控制訊號生成高度逼真的連貫影片，其靈活性和隱蔽性對現有偵測策略構成嚴峻挑戰。為此，我們推出首個大規模多模態數位人偽造數據集DigiFakeAV。該數據集採用五種最新的數位人生成方法及聲音複製技術，系統性地生成包含6萬個影片（840萬幀），涵蓋多種國籍、膚色、性別和真實場景，顯著提升數據多樣性和真實性。使用者研究顯示，偽造影片與真實影片的混淆率達到68%，且現有最佳偵測模型在DigiFakeAV上的AUC值大幅下降，突顯了該數據集的挑戰性。為了解決此問題，我們進一步提出基於時空和跨模態融合的偵測基準DigiShield。透過聯合建模影片的3D時空特徵和音訊的語義聲學特徵，DigiShield在DigiFakeAV和DF-TIMIT數據集上均達到最佳性能。實驗表明，該方法透過對合成影片中面部特徵的時間演變進行細粒度分析，有效識別隱蔽的偽造痕跡。", "audio": "audios/2505.16512v1.mp3", "timestamp": "2025-05-23T20:20:48.939483"}
{"query": "AI", "id": "2505.16815v1", "url": "http://arxiv.org/abs/2505.16815v1", "title": "Perceptual Quality Assessment for Embodied AI", "summary": "Embodied AI has developed rapidly in recent years, but it is still mainly\ndeployed in laboratories, with various distortions in the Real-world limiting\nits application. Traditionally, Image Quality Assessment (IQA) methods are\napplied to predict human preferences for distorted images; however, there is no\nIQA method to assess the usability of an image in embodied tasks, namely, the\nperceptual quality for robots. To provide accurate and reliable quality\nindicators for future embodied scenarios, we first propose the topic: IQA for\nEmbodied AI. Specifically, we (1) based on the Mertonian system and\nmeta-cognitive theory, constructed a perception-cognition-decision-execution\npipeline and defined a comprehensive subjective score collection process; (2)\nestablished the Embodied-IQA database, containing over 36k reference/distorted\nimage pairs, with more than 5m fine-grained annotations provided by Vision\nLanguage Models/Vision Language Action-models/Real-world robots; (3) trained\nand validated the performance of mainstream IQA methods on Embodied-IQA,\ndemonstrating the need to develop more accurate quality indicators for Embodied\nAI. We sincerely hope that through evaluation, we can promote the application\nof Embodied AI under complex distortions in the Real-world. Project page:\nhttps://github.com/lcysyzxdxc/EmbodiedIQA", "authors": ["Chunyi Li", "Jiaohao Xiao", "Jianbo Zhang", "Farong Wen", "Zicheng Zhang", "Yuan Tian", "Xiangyang Zhu", "Xiaohong Liu", "Zhengxue Cheng", "Weisi Lin", "Guangtao Zhai"], "published_date": "2025-05-22", "title_zh": "具身人工智慧之感知品質評估", "summary_zh": "具身人工智慧近年發展迅速，但現實環境的限制阻礙其應用。傳統影像品質評估方法用於預測人對失真影像的偏好，然尚無評估影像於具身任務中可用性，即機器人感知品質之方法。為提供未來具身情境準確可靠的品質指標，本文首倡具身人工智慧之影像品質評估議題。具體而言，(1)基於默頓系統與後設認知理論，建構感知-認知-決策-執行管道，並定義一套全面的主觀評分收集流程；(2)建立具身影像品質評估資料庫，包含逾36,000組參考/失真影像對，以及超過500萬條由視覺語言模型/視覺語言動作模型/真實機器人提供的細粒度標註；(3)於具身影像品質評估資料庫上訓練並驗證主流影像品質評估方法之效能，表明開發更精準之具身人工智慧品質指標的需求。期望透過評估，促進具身人工智慧於複雜現實環境中之應用。專案頁面：https://github.com/lcysyzxdxc/EmbodiedIQA", "audio": "audios/2505.16815v1.mp3", "timestamp": "2025-05-23T21:15:15.129572"}
{"query": "Foundation Model", "id": "2505.15870v1", "url": "http://arxiv.org/abs/2505.15870v1", "title": "Satellites Reveal Mobility: A Commuting Origin-destination Flow Generator for Global Cities", "summary": "Commuting Origin-destination~(OD) flows, capturing daily population mobility\nof citizens, are vital for sustainable development across cities around the\nworld. However, it is challenging to obtain the data due to the high cost of\ntravel surveys and privacy concerns. Surprisingly, we find that satellite\nimagery, publicly available across the globe, contains rich urban semantic\nsignals to support high-quality OD flow generation, with over 98\\%\nexpressiveness of traditional multisource hard-to-collect urban\nsociodemographic, economics, land use, and point of interest data. This\ninspires us to design a novel data generator, GlODGen, which can generate OD\nflow data for any cities of interest around the world. Specifically, GlODGen\nfirst leverages Vision-Language Geo-Foundation Models to extract urban semantic\nsignals related to human mobility from satellite imagery. These features are\nthen combined with population data to form region-level representations, which\nare used to generate OD flows via graph diffusion models. Extensive experiments\non 4 continents and 6 representative cities show that GlODGen has great\ngeneralizability across diverse urban environments on different continents and\ncan generate OD flow data for global cities highly consistent with real-world\nmobility data. We implement GlODGen as an automated tool, seamlessly\nintegrating data acquisition and curation, urban semantic feature extraction,\nand OD flow generation together. It has been released at\nhttps://github.com/tsinghua-fib-lab/generate-od-pubtools.", "authors": ["Can Rong", "Xin Zhang", "Yanxin Xi", "Hongjie Sui", "Jingtao Ding", "Yong Li"], "published_date": "2025-05-21", "title_zh": "衛星揭示的移動性：全球城市通勤起訖流生成器", "summary_zh": "通勤起訖流量(OD)對城市永續發展至關重要，但獲取成本高昂且涉及隱私。研究發現，全球可得的衛星影像蘊含豐富的城市語義訊息，可高度表達傳統多源城市社會人口、經濟、土地利用和興趣點數據。因此，我們設計了GlODGen，一種新的數據生成器，能為任何城市生成OD流量數據。GlODGen首先利用視覺-語言地理基礎模型從衛星影像中提取與人口流動相關的城市語義訊息，然後將這些特徵與人口數據結合，形成區域級表示，再透過圖擴散模型生成OD流量。在四大洲六個代表性城市的大量實驗表明，GlODGen在不同城市環境中具有良好的泛化性，能為全球城市生成與真實世界移動數據高度一致的OD流量數據。GlODGen已作為自動化工具發布，整合了數據獲取與整理、城市語義特徵提取和OD流量生成。", "audio": "audios/2505.15870v1.mp3", "timestamp": "2025-05-23T21:15:21.764554"}
{"query": "Diffusion Model", "id": "2505.16474v1", "url": "http://arxiv.org/abs/2505.16474v1", "title": "Consistent World Models via Foresight Diffusion", "summary": "Diffusion and flow-based models have enabled significant progress in\ngeneration tasks across various modalities and have recently found applications\nin world modeling. However, unlike typical generation tasks that encourage\nsample diversity, world models entail different sources of uncertainty and\nrequire consistent samples aligned with the ground-truth trajectory, which is a\nlimitation we empirically observe in diffusion models. We argue that a key\nbottleneck in learning consistent diffusion-based world models lies in the\nsuboptimal predictive ability, which we attribute to the entanglement of\ncondition understanding and target denoising within shared architectures and\nco-training schemes. To address this, we propose Foresight Diffusion\n(ForeDiff), a diffusion-based world modeling framework that enhances\nconsistency by decoupling condition understanding from target denoising.\nForeDiff incorporates a separate deterministic predictive stream to process\nconditioning inputs independently of the denoising stream, and further\nleverages a pretrained predictor to extract informative representations that\nguide generation. Extensive experiments on robot video prediction and\nscientific spatiotemporal forecasting show that ForeDiff improves both\npredictive accuracy and sample consistency over strong baselines, offering a\npromising direction for diffusion-based world models.", "authors": ["Yu Zhang", "Xingzhuo Guo", "Haoran Xu", "Mingsheng Long"], "published_date": "2025-05-22", "title_zh": "基於前瞻擴散的一致性世界模型", "summary_zh": "擴散模型與流模型在多模態生成任務中取得顯著進展，並應用於世界建模。然而，世界模型不同於鼓勵樣本多樣性的典型生成任務，它需要與真實軌跡對齊的一致樣本，這在擴散模型中存在限制。我們認為，基於擴散的世界模型學習一致性的瓶頸在於次優的預測能力，這歸因於共享架構和共同訓練方案中條件理解與目標去噪的糾纏。為了解決此問題，我們提出Foresight Diffusion (ForeDiff)，一種基於擴散的世界建模框架，透過解耦條件理解與目標去噪來提高一致性。ForeDiff整合了獨立於去噪流的確定性預測流來處理條件輸入，並利用預訓練預測器提取資訊豐富的表示來引導生成。在機器人影片預測和科學時空預測的大量實驗表明，相較於強基準線，ForeDiff提高了預測準確性和樣本一致性，為基於擴散的世界模型提供了一個有希望的方向。", "audio": "audios/2505.16474v1.mp3", "timestamp": "2025-05-23T21:15:28.102998"}
{"query": "AI", "id": "2505.16809v1", "url": "http://arxiv.org/abs/2505.16809v1", "title": "Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor Segmentation with Missing Modalities", "summary": "Existing methods for multimodal MRI segmentation with missing modalities\ntypically assume that all MRI modalities are available during training.\nHowever, in clinical practice, some modalities may be missing due to the\nsequential nature of MRI acquisition, leading to performance degradation.\nFurthermore, retraining models to accommodate newly available modalities can be\ninefficient and may cause overfitting, potentially compromising previously\nlearned knowledge. To address these challenges, we propose Replay-based\nHypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation\nwith missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to\nenable the segmentation model to learn from newly acquired MRI modalities\nwithout forgetting previously learned information. To enhance segmentation\nperformance across diverse patient scenarios, we introduce the Cross-Patient\nHypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture\nhigh-order associations between patients. Additionally, we incorporate\nTversky-Aware Contrastive (TAC) loss to effectively mitigate information\nimbalance both across and within different modalities. Extensive experiments on\nthe BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art\nmethods, achieving an improvement of over 2\\% in the Dice Similarity\nCoefficient across various tumor regions. Our code is available at ReHyDIL.", "authors": ["Junze Wang", "Lei Fan", "Weipeng Jing", "Donglin Di", "Yang Song", "Sidong Liu", "Cong Cong"], "published_date": "2025-05-22", "title_zh": "超圖Tversky感知的領域增量學習用於缺失模態的腦腫瘤分割", "summary_zh": "現有多模態MRI分割方法在模態缺失情況下，通常假設訓練期間所有模態均可用。然而，臨床實踐中，由於MRI掃描的時序性，部分模態可能缺失，導致性能下降。重新訓練模型以適應新模態效率低下，並可能導致過擬合，損害先前學習的知識。為了解決這些問題，我們提出基於重播的超圖域增量學習(ReHyDIL)用於缺失模態的腦腫瘤分割。ReHyDIL利用域增量學習(DIL)，使分割模型能夠從新獲取的MRI模態中學習，同時不忘記先前學習的資訊。為了提高不同患者場景下的分割性能，我們引入跨患者超圖分割網路(CHSNet)，利用超圖捕捉患者之間的高階關聯。此外，我們採用Tversky感知對比(TAC)損失，有效減輕不同模態之間和之內的信息不平衡。在BraTS2019數據集上的大量實驗表明，ReHyDIL優於現有方法，在各種腫瘤區域的Dice相似係數上提高了2%以上。", "audio": "audios/2505.16809v1.mp3", "timestamp": "2025-05-23T22:18:12.423329"}
{"query": "Foundation Model", "id": "2505.15868v1", "url": "http://arxiv.org/abs/2505.15868v1", "title": "An Inclusive Foundation Model for Generalizable Cytogenetics in Precision Oncology", "summary": "Chromosome analysis is vital for diagnosing genetic disorders and guiding\ncancer therapy decisions through the identification of somatic clonal\naberrations. However, developing an AI model are hindered by the overwhelming\ncomplexity and diversity of chromosomal abnormalities, requiring extensive\nannotation efforts, while automated methods remain task-specific and lack\ngeneralizability due to the scarcity of comprehensive datasets spanning diverse\nresource conditions. Here, we introduce CHROMA, a foundation model for\ncytogenomics, designed to overcome these challenges by learning generalizable\nrepresentations of chromosomal abnormalities. Pre-trained on over 84,000\nspecimens (~4 million chromosomal images) via self-supervised learning, CHROMA\noutperforms other methods across all types of abnormalities, even when trained\non fewer labelled data and more imbalanced datasets. By facilitating\ncomprehensive mapping of instability and clonal leisons across various\naberration types, CHROMA offers a scalable and generalizable solution for\nreliable and automated clinical analysis, reducing the annotation workload for\nexperts and advancing precision oncology through the early detection of rare\ngenomic abnormalities, enabling broad clinical AI applications and making\nadvanced genomic analysis more accessible.", "authors": ["Changchun Yang", "Weiqian Dai", "Yilan Zhang", "Siyuan Chen", "Jingdong Hu", "Junkai Su", "Yuxuan Chen", "Ao Xu", "Na Li", "Xin Gao", "Yongguo Yu"], "published_date": "2025-05-21", "title_zh": "精準腫瘤學中具泛化能力的細胞遺傳學通用包容性基礎模型", "summary_zh": "染色體分析對診斷遺傳疾病和指導癌症治療至關重要，但染色體異常的複雜性阻礙了AI模型的發展。本文提出細胞遺傳學基礎模型CHROMA，通過自監督學習在超過84,000個樣本（約400萬張染色體圖像）上進行預訓練，克服了這些挑戰。CHROMA在各種類型的異常中均優於其他方法，即使在較少的標記數據和更不平衡的數據集上訓練也是如此。CHROMA促進了各種異常類型中不穩定性和克隆病變的全面繪製，提供了一個可擴展和通用的解決方案，實現可靠且自動化的臨床分析，減少專家的註釋工作量，並通過早期檢測罕見基因組異常來推進精準腫瘤學，實現廣泛的臨床AI應用，並使先進的基因組分析更易於使用。", "audio": "audios/2505.15868v1.mp3", "timestamp": "2025-05-23T22:18:17.650564"}
{"query": "Diffusion Model", "id": "2505.16456v1", "url": "http://arxiv.org/abs/2505.16456v1", "title": "MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM", "summary": "Recent advances in static 3D generation have intensified the demand for\nphysically consistent dynamic 3D content. However, existing video generation\nmodels, including diffusion-based methods, often prioritize visual realism\nwhile neglecting physical plausibility, resulting in implausible object\ndynamics. Prior approaches for physics-aware dynamic generation typically rely\non large-scale annotated datasets or extensive model fine-tuning, which imposes\nsignificant computational and data collection burdens and limits scalability\nacross scenarios. To address these challenges, we present MAGIC, a\ntraining-free framework for single-image physical property inference and\ndynamic generation, integrating pretrained image-to-video diffusion models with\niterative LLM-based reasoning. Our framework generates motion-rich videos from\na static image and closes the visual-to-physical gap through a\nconfidence-driven LLM feedback loop that adaptively steers the diffusion model\ntoward physics-relevant motion. To translate visual dynamics into controllable\nphysical behavior, we further introduce a differentiable MPM simulator\noperating directly on 3D Gaussians reconstructed from the single image,\nenabling physically grounded, simulation-ready outputs without any supervision\nor model tuning. Experiments show that MAGIC outperforms existing physics-aware\ngenerative methods in inference accuracy and achieves greater temporal\ncoherence than state-of-the-art video diffusion models.", "authors": ["Siwei Meng", "Yawei Luo", "Ping Liu"], "published_date": "2025-05-22", "title_zh": "MAGIC：基於置信度引導的大型語言模型之運動感知生成推論", "summary_zh": "靜態3D生成技術的進步提升了對具備物理一致性動態3D內容的需求。現有影片生成模型，包括基於擴散的方法，往往側重視覺真實感而忽略物理合理性，導致不合理的物體動態。先前的物理感知動態生成方法通常依賴大規模標註資料集或大量模型微調，造成沉重的計算和資料收集負擔，並限制了跨場景的可擴展性。為了解決這些問題，我們提出MAGIC，一個免訓練的單圖像物理屬性推斷與動態生成框架，整合了預訓練的圖像轉影片擴散模型與基於LLM的迭代推理。我們的框架從靜態圖像生成富含動作的影片，並透過基於置信度的LLM回饋迴路彌合視覺與物理間的差距，自適應地引導擴散模型朝向物理相關的運動。為了將視覺動態轉化為可控制的物理行為，我們進一步引入了一個可微分的MPM模擬器，直接作用於從單一圖像重建的3D高斯模型，實現物理基礎的、可直接用於模擬的輸出，無需任何監督或模型調整。實驗表明，MAGIC在推斷準確性方面優於現有的物理感知生成方法，並且比最先進的影片擴散模型實現了更高的時間連貫性。", "audio": "audios/2505.16456v1.mp3", "timestamp": "2025-05-23T22:18:25.299795"}
{"query": "AI", "id": "2505.16792v1", "url": "http://arxiv.org/abs/2505.16792v1", "title": "REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training", "summary": "Diffusion Transformers (DiTs) deliver state-of-the-art image quality, yet\ntheir training remains notoriously slow. A recent remedy -- representation\nalignment (REPA) that matches DiT hidden features to those of a non-generative\nteacher (e.g. DINO) -- dramatically accelerates the early epochs but plateaus\nor even degrades performance later. We trace this failure to a capacity\nmismatch: once the generative student begins modelling the joint data\ndistribution, the teacher's lower-dimensional embeddings and attention patterns\nbecome a straitjacket rather than a guide. We then introduce HASTE (Holistic\nAlignment with Stage-wise Termination for Efficient training), a two-phase\nschedule that keeps the help and drops the hindrance. Phase I applies a\nholistic alignment loss that simultaneously distills attention maps (relational\npriors) and feature projections (semantic anchors) from the teacher into\nmid-level layers of the DiT, yielding rapid convergence. Phase II then performs\none-shot termination that deactivates the alignment loss, once a simple trigger\nsuch as a fixed iteration is hit, freeing the DiT to focus on denoising and\nexploit its generative capacity. HASTE speeds up training of diverse DiTs\nwithout architecture changes. On ImageNet 256X256, it reaches the vanilla\nSiT-XL/2 baseline FID in 50 epochs and matches REPA's best FID in 500 epochs,\namounting to a 28X reduction in optimization steps. HASTE also improves\ntext-to-image DiTs on MS-COCO, demonstrating to be a simple yet principled\nrecipe for efficient diffusion training across various tasks. Our code is\navailable at https://github.com/NUS-HPC-AI-Lab/HASTE .", "authors": ["Ziqiao Wang", "Wangbo Zhao", "Yuhao Zhou", "Zekai Li", "Zhiyuan Liang", "Mingjia Shi", "Xuanlei Zhao", "Pengfei Zhou", "Kaipeng Zhang", "Zhangyang Wang", "Kai Wang", "Yang You"], "published_date": "2025-05-22", "title_zh": "REPA失效之時：提早停止的整體對齊增強擴散訓練", "summary_zh": "擴散變換器(DiT)具備頂尖影像品質，但訓練速度緩慢。近期方法，表徵對齊(REPA)，透過將DiT隱藏特徵與非生成教師模型(如DINO)對齊，雖加速初期訓練，後期效果停滯甚至退化。此乃因容量不匹配：生成模型開始學習聯合資料分布時，教師模型的低維嵌入和注意力模式反而成為限制。為此，引入HASTE(高效訓練的分階段終止整體對齊)，一種兩階段策略。第一階段採用整體對齊損失，同時將教師模型的注意力圖(關係先驗)和特徵投影(語義錨點)提煉至DiT的中間層，實現快速收斂。第二階段，一旦滿足預設條件，立即停用對齊損失，使DiT專注於去噪並發揮生成能力。HASTE無需修改架構即可加速多種DiT訓練。在ImageNet 256X256上，HASTE在50個epoch內達到SiT-XL/2基準FID，並在500個epoch內達到REPA的最佳FID，優化步數減少28倍。HASTE亦改善MS-COCO上的文本生成圖像DiT，證明其為一種簡潔且有效的擴散模型訓練方法，適用於多種任務。", "audio": "audios/2505.16792v1.mp3", "timestamp": "2025-05-23T23:17:35.353008"}
{"query": "Foundation Model", "id": "2505.14938v1", "url": "http://arxiv.org/abs/2505.14938v1", "title": "Scan, Materialize, Simulate: A Generalizable Framework for Physically Grounded Robot Planning", "summary": "Autonomous robots must reason about the physical consequences of their\nactions to operate effectively in unstructured, real-world environments. We\npresent Scan, Materialize, Simulate (SMS), a unified framework that combines 3D\nGaussian Splatting for accurate scene reconstruction, visual foundation models\nfor semantic segmentation, vision-language models for material property\ninference, and physics simulation for reliable prediction of action outcomes.\nBy integrating these components, SMS enables generalizable physical reasoning\nand object-centric planning without the need to re-learn foundational physical\ndynamics. We empirically validate SMS in a billiards-inspired manipulation task\nand a challenging quadrotor landing scenario, demonstrating robust performance\non both simulated domain transfer and real-world experiments. Our results\nhighlight the potential of bridging differentiable rendering for scene\nreconstruction, foundation models for semantic understanding, and physics-based\nsimulation to achieve physically grounded robot planning across diverse\nsettings.", "authors": ["Amine Elhafsi", "Daniel Morton", "Marco Pavone"], "published_date": "2025-05-20", "title_zh": "掃描、實體化、模擬：一個具身機器人規劃的通用化框架", "summary_zh": "自主機器人需推論動作的物理後果，方能在真實環境中有效運作。本研究提出掃描、實體化、模擬(SMS)框架，整合3D高斯濺射以精確重建場景、視覺基礎模型進行語義分割、視覺語言模型推斷材料屬性，以及物理模擬以可靠預測動作結果。藉由整合這些組件，SMS無需重新學習基礎物理動力學，即可實現可泛化的物理推理和以物體為中心的規劃。實驗於撞球操控任務和四旋翼著陸場景驗證SMS，展現模擬領域轉移和真實世界實驗中的穩健效能。研究結果突顯了可微分渲染、基礎模型和物理模擬結合在場景重建、語義理解及實現具物理基礎的機器人規劃的潛力。", "audio": "audios/2505.14938v1.mp3", "timestamp": "2025-05-23T23:17:43.720969"}
{"query": "Diffusion Model", "id": "2505.16365v1", "url": "http://arxiv.org/abs/2505.16365v1", "title": "A collaborative constrained graph diffusion model for the generation of realistic synthetic molecules", "summary": "Developing new molecular compounds is crucial to address pressing challenges,\nfrom health to environmental sustainability. However, exploring the molecular\nspace to discover new molecules is difficult due to the vastness of the space.\nHere we introduce CoCoGraph, a collaborative and constrained graph diffusion\nmodel capable of generating molecules that are guaranteed to be chemically\nvalid. Thanks to the constraints built into the model and to the collaborative\nmechanism, CoCoGraph outperforms state-of-the-art approaches on standard\nbenchmarks while requiring up to an order of magnitude fewer parameters.\nAnalysis of 36 chemical properties also demonstrates that CoCoGraph generates\nmolecules with distributions more closely matching real molecules than current\nmodels. Leveraging the model's efficiency, we created a database of 8.2M\nmillion synthetically generated molecules and conducted a Turing-like test with\norganic chemistry experts to further assess the plausibility of the generated\nmolecules, and potential biases and limitations of CoCoGraph.", "authors": ["Manuel Ruiz-Botella", "Marta Sales-Pardo", "Roger Guimerà"], "published_date": "2025-05-22", "title_zh": "協作約束圖擴散模型用於生成逼真合成分子", "summary_zh": "開發新型分子化合物對於應對健康和環境永續性等嚴峻挑戰至關重要。由於分子空間廣闊，探索並發現新分子極具挑戰性。本文介紹CoCoGraph，一種協作且受限的圖擴散模型，能生成保證化學有效性的分子。CoCoGraph憑藉其內建約束和協作機制，在標準基準測試中優於現有技術，且參數需求減少近一個數量級。對36種化學性質的分析表明，CoCoGraph生成的分子分佈比現有模型更接近真實分子。藉助模型的高效率，我們創建了一個包含820萬個合成生成分子的資料庫，並與有機化學專家進行了類似圖靈測試的實驗，以進一步評估生成分子的合理性，以及CoCoGraph的潛在偏差和局限性。", "audio": "audios/2505.16365v1.mp3", "timestamp": "2025-05-23T23:17:50.067419"}
{"query": "AI", "id": "2505.16771v1", "url": "http://arxiv.org/abs/2505.16771v1", "title": "Data-Driven Breakthroughs and Future Directions in AI Infrastructure: A Comprehensive Review", "summary": "This paper presents a comprehensive synthesis of major breakthroughs in\nartificial intelligence (AI) over the past fifteen years, integrating\nhistorical, theoretical, and technological perspectives. It identifies key\ninflection points in AI' s evolution by tracing the convergence of\ncomputational resources, data access, and algorithmic innovation. The analysis\nhighlights how researchers enabled GPU based model training, triggered a data\ncentric shift with ImageNet, simplified architectures through the Transformer,\nand expanded modeling capabilities with the GPT series. Rather than treating\nthese advances as isolated milestones, the paper frames them as indicators of\ndeeper paradigm shifts. By applying concepts from statistical learning theory\nsuch as sample complexity and data efficiency, the paper explains how\nresearchers translated breakthroughs into scalable solutions and why the field\nmust now embrace data centric approaches. In response to rising privacy\nconcerns and tightening regulations, the paper evaluates emerging solutions\nlike federated learning, privacy enhancing technologies (PETs), and the data\nsite paradigm, which reframe data access and security. In cases where real\nworld data remains inaccessible, the paper also assesses the utility and\nconstraints of mock and synthetic data generation. By aligning technical\ninsights with evolving data infrastructure, this study offers strategic\nguidance for future AI research and policy development.", "authors": ["Beyazit Bestami Yuksel", "Ayse Yilmazer Metin"], "published_date": "2025-05-22", "title_zh": "人工智慧基礎設施中資料驅動的突破與未來方向：一份綜合性回顧", "summary_zh": "本文綜述過去十五年人工智慧領域的重大突破，整合歷史、理論與技術觀點。分析計算資源、數據獲取與演算法創新如何匯聚，標誌AI發展的關鍵轉折點，如基於GPU的模型訓練、ImageNet引領的數據中心轉移、Transformer簡化架構以及GPT系列擴展建模能力。論文將這些進展視為更深層次典範轉移的指標，並運用統計學習理論解釋其如何轉化為可擴展的解決方案，強調數據中心方法的重要性。針對隱私問題和日趨嚴格的監管，本文評估了聯邦學習、隱私增強技術及數據站點範例等新興方案，並探討在真實數據不可獲取時，模擬與合成數據生成的效用與局限性。透過結合技術洞見與不斷發展的數據基礎設施，本研究為未來AI研究與政策制定提供策略指導。", "audio": "audios/2505.16771v1.mp3", "timestamp": "2025-05-24T01:23:12.944049"}
{"query": "Foundation Model", "id": "2505.14933v1", "url": "http://arxiv.org/abs/2505.14933v1", "title": "Foundations of Unknown-aware Machine Learning", "summary": "Ensuring the reliability and safety of machine learning models in open-world\ndeployment is a central challenge in AI safety. This thesis develops both\nalgorithmic and theoretical foundations to address key reliability issues\narising from distributional uncertainty and unknown classes, from standard\nneural networks to modern foundation models like large language models (LLMs).\n  Traditional learning paradigms, such as empirical risk minimization (ERM),\nassume no distribution shift between training and inference, often leading to\noverconfident predictions on out-of-distribution (OOD) inputs. This thesis\nintroduces novel frameworks that jointly optimize for in-distribution accuracy\nand reliability to unseen data. A core contribution is the development of an\nunknown-aware learning framework that enables models to recognize and handle\nnovel inputs without labeled OOD data.\n  We propose new outlier synthesis methods, VOS, NPOS, and DREAM-OOD, to\ngenerate informative unknowns during training. Building on this, we present\nSAL, a theoretical and algorithmic framework that leverages unlabeled\nin-the-wild data to enhance OOD detection under realistic deployment\nconditions. These methods demonstrate that abundant unlabeled data can be\nharnessed to recognize and adapt to unforeseen inputs, providing formal\nreliability guarantees.\n  The thesis also extends reliable learning to foundation models. We develop\nHaloScope for hallucination detection in LLMs, MLLMGuard for defending against\nmalicious prompts in multimodal models, and data cleaning methods to denoise\nhuman feedback used for better alignment. These tools target failure modes that\nthreaten the safety of large-scale models in deployment.\n  Overall, these contributions promote unknown-aware learning as a new\nparadigm, and we hope it can advance the reliability of AI systems with minimal\nhuman efforts.", "authors": ["Xuefeng Du"], "published_date": "2025-05-20", "title_zh": "未知感知機器學習基礎", "summary_zh": "本論文旨在解決開放世界中機器學習模型可靠性與安全性問題，著重於分布不確定性與未知類別所引發的挑戰。透過演算法與理論基礎，從傳統神經網路到大型語言模型等基礎模型，提升模型在未見數據上的泛化能力。\n\n論文提出新穎框架，優化模型在分布內準確度與分布外可靠性。核心貢獻為開發一種未知感知學習框架，使模型能夠識別並處理新穎輸入，無需標記的分布外數據。\n\n論文提出異常值合成方法VOS、NPOS與DREAM-OOD，在訓練期間產生資訊豐富的未知樣本。基於此，提出SAL框架，利用未標記的真實數據增強分布外檢測，並提供形式化的可靠性保證。\n\n論文亦將可靠學習擴展至基礎模型，開發HaloScope用於檢測大型語言模型中的幻覺，MLLMGuard用於防禦多模態模型中的惡意提示，以及數據清洗方法淨化人類回饋，以實現更好的模型對齊。\n\n總體而言，論文提倡未知感知學習作為一種新範式，旨在提升人工智慧系統的可靠性，並減少對人工的依賴。", "audio": "audios/2505.14933v1.mp3", "timestamp": "2025-05-24T01:23:22.509180"}
{"query": "Diffusion Model", "id": "2505.16335v1", "url": "http://arxiv.org/abs/2505.16335v1", "title": "FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design", "summary": "Visual autoregressive (VAR) modeling has marked a paradigm shift in image\ngeneration from next-token prediction to next-scale prediction. VAR predicts a\nset of tokens at each step from coarse to fine scale, leading to better image\nquality and faster inference speed compared to existing diffusion models.\nHowever, the large parameter size and computation cost hinder its deployment on\nedge devices. To reduce the memory and computation cost, we propose FPQVAR, an\nefficient post-training floating-point (FP) quantization framework for VAR\nfeaturing algorithm and hardware co-design. At the algorithm level, we first\nidentify the challenges of quantizing VAR. To address them, we propose Dual\nFormat Quantization for the highly imbalanced input activation. We further\npropose Group-wise Hadamard Transformation and GHT-Aware Learnable\nTransformation to address the time-varying outlier channels. At the hardware\nlevel, we design the first low-bit FP quantizer and multiplier with lookup\ntables on FPGA and propose the first FPGA-based VAR accelerator featuring\nlow-bit FP computation and an elaborate two-level pipeline. Extensive\nexperiments show that compared to the state-of-the-art quantization method, our\nproposed FPQVAR significantly improves Fr\\'echet Inception Distance (FID) from\n10.83 to 3.58, Inception Score (IS) from 175.9 to 241.5 under 4-bit\nquantization. FPQVAR also significantly improves the performance of 6-bit\nquantized VAR, bringing it on par with the FP16 model. Our accelerator on\nAMD-Xilinx VCK190 FPGA achieves a throughput of 1.1 image/s, which is 3.1x\nhigher than the integer-based accelerator. It also demonstrates 3.6x and 2.8x\nhigher energy efficiency compared to the integer-based accelerator and GPU\nbaseline, respectively.", "authors": ["Renjie Wei", "Songqiang Xu", "Qingyu Guo", "Meng Li"], "published_date": "2025-05-22", "title_zh": "FPQVAR：基於FPGA硬體協同設計的視覺自迴歸模型浮點量化", "summary_zh": "視覺自迴歸模型(VAR)將圖像生成從下一個標記預測轉變為下一個尺度預測。相較於現有擴散模型，VAR透過由粗到精尺度預測標記集合，提升圖像品質並加速推論。然而，龐大的參數規模與計算成本限制了其在邊緣裝置上的部署。為降低記憶體與計算成本，本文提出FPQVAR，一個高效的VAR後訓練浮點(FP)量化框架，著重於算法與硬體協同設計。算法層面，針對VAR量化的挑戰，提出雙格式量化以處理高度不平衡的輸入激活，並提出分組哈達瑪轉換和GHT感知可學習轉換以解決時變離群通道。硬體層面，設計首個基於FPGA的低位元FP量化器與乘法器，並提出首個基於FPGA的VAR加速器，其特色為低位元FP計算和精細的雙層流水線。實驗結果顯示，相較於最先進的量化方法，在4位元量化下，FPQVAR顯著提升Fréchet Inception Distance (FID)自10.83至3.58，Inception Score (IS)自175.9至241.5。FPQVAR亦顯著提升6位元量化VAR效能，使其與FP16模型相當。在AMD-Xilinx VCK190 FPGA上的加速器實現每秒1.1張圖像的吞吐量，較基於整數的加速器提高3.1倍，且能源效率分別較基於整數的加速器和GPU基準提高3.6倍和2.8倍。", "audio": "audios/2505.16335v1.mp3", "timestamp": "2025-05-24T01:23:32.812374"}
{"query": "AI", "id": "2505.16763v1", "url": "http://arxiv.org/abs/2505.16763v1", "title": "Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation", "summary": "Text-to-image models are powerful for producing high-quality images based on\ngiven text prompts, but crafting these prompts often requires specialized\nvocabulary. To address this, existing methods train rewriting models with\nsupervision from large amounts of manually annotated data and trained aesthetic\nassessment models. To alleviate the dependence on data scale for model training\nand the biases introduced by trained models, we propose a novel prompt\noptimization framework, designed to rephrase a simple user prompt into a\nsophisticated prompt to a text-to-image model. Specifically, we employ the\nlarge vision language models (LVLMs) as the solver to rewrite the user prompt,\nand concurrently, employ LVLMs as a reward model to score the aesthetics and\nalignment of the images generated by the optimized prompt. Instead of laborious\nhuman feedback, we exploit the prior knowledge of the LVLM to provide rewards,\ni.e., AI feedback. Simultaneously, the solver and the reward model are unified\ninto one model and iterated in reinforcement learning to achieve\nself-improvement by giving a solution and judging itself. Results on two\npopular datasets demonstrate that our method outperforms other strong\ncompetitors.", "authors": ["Hongji Yang", "Yucheng Zhou", "Wencheng Han", "Jianbing Shen"], "published_date": "2025-05-22", "title_zh": "用於文本到圖像生成中提示詞優化的自獎勵大型視覺語言模型", "summary_zh": "基於文字生成圖像的模型強大，但提示詞編寫常需專業詞彙。現有方法仰賴大量人工標註數據及美學評估模型訓練重寫模型，為減少對數據規模的依賴及模型偏見，本文提出一種新穎的提示詞優化框架，將簡單的使用者提示詞改寫為精確的提示詞。具體而言，我們採用大型視覺語言模型（LVLM）作為求解器重寫提示詞，同時將其作為獎勵模型，評估優化後提示詞生成圖像的美學與對齊。我們利用LVLM的先驗知識提供AI回饋作為獎勵，取代耗時的人工回饋。求解器和獎勵模型統一為單一模型，透過強化學習迭代，藉由自我求解和評估實現自我提升。在兩個常用資料集上的結果表明，本文方法優於其他強大的競爭對手。", "audio": "audios/2505.16763v1.mp3", "timestamp": "2025-05-24T03:04:58.436462"}
{"query": "Foundation Model", "id": "2505.14766v1", "url": "http://arxiv.org/abs/2505.14766v1", "title": "This Time is Different: An Observability Perspective on Time Series Foundation Models", "summary": "We introduce Toto, a time series forecasting foundation model with 151\nmillion parameters. Toto uses a modern decoder-only architecture coupled with\narchitectural innovations designed to account for specific challenges found in\nmultivariate observability time series data. Toto's pre-training corpus is a\nmixture of observability data, open datasets, and synthetic data, and is\n4-10$\\times$ larger than those of leading time series foundation models.\nAdditionally, we introduce BOOM, a large-scale benchmark consisting of 350\nmillion observations across 2,807 real-world time series. For both Toto and\nBOOM, we source observability data exclusively from Datadog's own telemetry and\ninternal observability metrics. Extensive evaluations demonstrate that Toto\nachieves state-of-the-art performance on both BOOM and on established general\npurpose time series forecasting benchmarks. Toto's model weights, inference\ncode, and evaluation scripts, as well as BOOM's data and evaluation code, are\nall available as open source under the Apache 2.0 License available at\nhttps://huggingface.co/Datadog/Toto-Open-Base-1.0 and\nhttps://github.com/DataDog/toto.", "authors": ["Ben Cohen", "Emaad Khwaja", "Youssef Doubli", "Salahidine Lemaachi", "Chris Lettieri", "Charles Masson", "Hugo Miccinilli", "Elise Ramé", "Qiqi Ren", "Afshin Rostamizadeh", "Jean Ogier du Terrail", "Anna-Monica Toon", "Kan Wang", "Stephan Xie", "David Asker", "Ameet Talwalkar", "Othmane Abou-Amal"], "published_date": "2025-05-20", "title_zh": "這次不一樣：時間序列基礎模型的可觀測性視角", "summary_zh": "Toto是一個具備1.51億參數的時間序列預測基礎模型，採用現代解碼器架構，並針對多元可觀測性時間序列數據的獨特挑戰進行架構創新。Toto的預訓練語料庫混合了可觀測性數據、開放數據集和合成數據，規模較領先的時間序列基礎模型大4-10倍。同時，我們推出了BOOM，一個包含2,807個真實世界時間序列、橫跨3.5億觀測值的大規模基準測試。Toto和BOOM的可觀測性數據均來自Datadog的遙測和內部可觀測性指標。廣泛評估顯示，Toto在BOOM和既有的通用時間序列預測基準測試上均達到最先進的性能。Toto的模型權重、推論代碼、評估腳本以及BOOM的數據和評估代碼，均以Apache 2.0許可證開源。", "audio": "audios/2505.14766v1.mp3", "timestamp": "2025-05-24T03:05:04.566531"}
{"query": "Diffusion Model", "id": "2505.16324v1", "url": "http://arxiv.org/abs/2505.16324v1", "title": "TensorAR: Refinement is All You Need in Autoregressive Image Generation", "summary": "Autoregressive (AR) image generators offer a language-model-friendly approach\nto image generation by predicting discrete image tokens in a causal sequence.\nHowever, unlike diffusion models, AR models lack a mechanism to refine previous\npredictions, limiting their generation quality. In this paper, we introduce\nTensorAR, a new AR paradigm that reformulates image generation from next-token\nprediction to next-tensor prediction. By generating overlapping windows of\nimage patches (tensors) in a sliding fashion, TensorAR enables iterative\nrefinement of previously generated content. To prevent information leakage\nduring training, we propose a discrete tensor noising scheme, which perturbs\ninput tokens via codebook-indexed noise. TensorAR is implemented as a\nplug-and-play module compatible with existing AR models. Extensive experiments\non LlamaGEN, Open-MAGVIT2, and RAR demonstrate that TensorAR significantly\nimproves the generation performance of autoregressive models.", "authors": ["Cheng Cheng", "Lin Song", "Yicheng Xiao", "Yuxin Chen", "Xuchong Zhang", "Hongbin Sun", "Ying Shan"], "published_date": "2025-05-22", "title_zh": "TensorAR：自回歸圖像生成中，精煉即為關鍵", "summary_zh": "自迴歸影像生成器透過因果序列預測離散影像符記，提供了一種語言模型友善的影像生成方法。然而，與擴散模型不同，自迴歸模型缺乏細化先前預測的機制，限制了其生成品質。本研究提出TensorAR，一種新的自迴歸範式，將影像生成從下一個符記預測重新構建為下一個張量預測。透過滑動方式生成重疊的影像區塊視窗（張量），TensorAR實現了對先前生成內容的迭代細化。為防止訓練期間的資訊洩漏，我們提出了一種離散張量雜訊方案，透過編碼簿索引雜訊擾動輸入符記。TensorAR可作為隨插即用模組，與現有自迴歸模型相容。在LlamaGEN、Open-MAGVIT2和RAR上的大量實驗表明，TensorAR顯著提高了自迴歸模型的生成效能。", "audio": "audios/2505.16324v1.mp3", "timestamp": "2025-05-24T03:05:11.336535"}
{"query": "AI", "id": "2505.16709v1", "url": "http://arxiv.org/abs/2505.16709v1", "title": "SEDD-PCC: A Single Encoder-Dual Decoder Framework For End-To-End Learned Point Cloud Compression", "summary": "To encode point clouds containing both geometry and attributes, most\nlearning-based compression schemes treat geometry and attribute coding\nseparately, employing distinct encoders and decoders. This not only increases\ncomputational complexity but also fails to fully exploit shared features\nbetween geometry and attributes. To address this limitation, we propose\nSEDD-PCC, an end-to-end learning-based framework for lossy point cloud\ncompression that jointly compresses geometry and attributes. SEDD-PCC employs a\nsingle encoder to extract shared geometric and attribute features into a\nunified latent space, followed by dual specialized decoders that sequentially\nreconstruct geometry and attributes. Additionally, we incorporate knowledge\ndistillation to enhance feature representation learning from a teacher model,\nfurther improving coding efficiency. With its simple yet effective design,\nSEDD-PCC provides an efficient and practical solution for point cloud\ncompression. Comparative evaluations against both rule-based and learning-based\nmethods demonstrate its competitive performance, highlighting SEDD-PCC as a\npromising AI-driven compression approach.", "authors": ["Kai Hsiang Hsieh", "Monyneath Yim", "Jui Chiu Chiang"], "published_date": "2025-05-22", "title_zh": "SEDD-PCC：用於端到端學習點雲壓縮的單編碼器-雙解碼器架構", "summary_zh": "針對含幾何與屬性的點雲編碼，現有基於學習的壓縮方案多將幾何與屬性分開編碼，採用獨立編碼器與解碼器，導致計算複雜度增加且未能充分利用幾何與屬性間的共享特徵。為解決此問題，本文提出SEDDPCC，一種端到端基於學習的有損點雲壓縮框架，可聯合壓縮幾何與屬性。SEDDPCC採用單一編碼器提取共享的幾何與屬性特徵至統一潛在空間，再由雙重專業解碼器依序重建幾何與屬性。此外，加入知識蒸餾以增強特徵表示學習，進一步提升編碼效率。SEDDPCC設計簡潔有效，為點雲壓縮提供高效實用的解決方案。與規則式及基於學習的方法相比，實驗結果顯示其具備競爭力，證實SEDDPCC是一種具潛力的人工智慧驅動壓縮方法。", "audio": "audios/2505.16709v1.mp3", "timestamp": "2025-05-24T04:20:53.122964"}
{"query": "Foundation Model", "id": "2505.14100v2", "url": "http://arxiv.org/abs/2505.14100v2", "title": "Unlocking the Power of SAM 2 for Few-Shot Segmentation", "summary": "Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few\nclasses to segment arbitrary classes, but at the risk of overfitting. To\naddress this, some methods use the well-learned knowledge of foundation models\n(e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM\nby supporting video segmentation, whose class-agnostic matching ability is\nuseful to FSS. A simple idea is to encode support foreground (FG) features as\nmemory, with which query FG features are matched and fused. Unfortunately, the\nFG objects in different frames of SAM 2's video data are always the same\nidentity, while those in FSS are different identities, i.e., the matching step\nis incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo\nquery memory, matching with query features in a compatible way. However, the\nmemories can never be as accurate as the real ones, i.e., they are likely to\ncontain incomplete query FG, and some unexpected query background (BG)\nfeatures, leading to wrong segmentation. Hence, we further design Iterative\nMemory Refinement to fuse more query FG features into the memory, and devise a\nSupport-Calibrated Memory Attention to suppress the unexpected query BG\nfeatures in memory. Extensive experiments have been conducted on PASCAL-5$^i$\nand COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot\nmIoU can be 4.2% better than the best baseline.", "authors": ["Qianxiong Xu", "Lanyun Zhu", "Xuanyi Liu", "Guosheng Lin", "Cheng Long", "Ziyue Li", "Rui Zhao"], "published_date": "2025-05-20", "title_zh": "解鎖SAM 2在少樣本分割中的潛力", "summary_zh": "少樣本分割旨在學習類別無關的分割，以分割任意類別，但存在過擬合風險。為解決此問題，部分方法利用基礎模型（如SAM）的知識簡化學習過程。SAM 2擴展了SAM以支援影片分割，其類別無關的匹配能力對少樣本分割有所助益。本文提出將支援前景特徵編碼為記憶，並與查詢前景特徵進行匹配和融合。然而，SAM 2影片數據中不同幀的前景物件身份相同，而少樣本分割中則不同，導致匹配步驟不相容。因此，本文設計偽提示生成器以編碼偽查詢記憶，並以相容方式與查詢特徵匹配。由於偽記憶不如真實記憶準確，可能包含不完整的查詢前景和意外的查詢背景特徵，導致錯誤分割。故此，本文進一步設計迭代記憶提煉，將更多查詢前景特徵融合到記憶中，並設計支援校準記憶注意力，以抑制記憶中意外的查詢背景特徵。在PASCAL-5$^i$和COCO-20$^i$上的實驗驗證了設計的有效性，例如，1-shot mIoU比最佳基準提高了4.2%。", "audio": "audios/2505.14100v2.mp3", "timestamp": "2025-05-24T04:21:00.871481"}
{"query": "Diffusion Model", "id": "2505.16298v1", "url": "http://arxiv.org/abs/2505.16298v1", "title": "Flow Matching based Sequential Recommender Model", "summary": "Generative models, particularly diffusion model, have emerged as powerful\ntools for sequential recommendation. However, accurately modeling user\npreferences remains challenging due to the noise perturbations inherent in the\nforward and reverse processes of diffusion-based methods. Towards this end,\nthis study introduces FMRec, a Flow Matching based model that employs a\nstraight flow trajectory and a modified loss tailored for the recommendation\ntask. Additionally, from the diffusion-model perspective, we integrate a\nreconstruction loss to improve robustness against noise perturbations, thereby\nretaining user preferences during the forward process. In the reverse process,\nwe employ a deterministic reverse sampler, specifically an ODE-based updating\nfunction, to eliminate unnecessary randomness, thereby ensuring that the\ngenerated recommendations closely align with user needs. Extensive evaluations\non four benchmark datasets reveal that FMRec achieves an average improvement of\n6.53% over state-of-the-art methods. The replication code is available at\nhttps://github.com/FengLiu-1/FMRec.", "authors": ["Feng Liu", "Lixin Zou", "Xiangyu Zhao", "Min Tang", "Liming Dong", "Dan Luo", "Xiangyang Luo", "Chenliang Li"], "published_date": "2025-05-22", "title_zh": "基於流匹配的序列推薦模型", "summary_zh": "生成模型，尤其是擴散模型，已成為序列推薦的有力工具。然而，由於基於擴散方法的前向和反向過程中固有的雜訊擾動，精確建模使用者偏好仍然具有挑戰性。本研究提出基於Flow Matching的FMRec模型，該模型採用直接流動軌跡和針對推薦任務修改的損失函數。此外，從擴散模型的角度來看，我們整合了重建損失以提高對雜訊擾動的魯棒性，從而在前向過程中保留使用者偏好。在反向過程中，我們採用確定性反向取樣器，特別是基於ODE的更新函數，以消除不必要的隨機性，從而確保產生的推薦與使用者需求緊密對齊。在四個基準資料集上的廣泛評估表明，FMRec比最先進的方法平均提高了6.53%。複製程式碼可在https://github.com/FengLiu-1/FMRec獲取。", "audio": "audios/2505.16298v1.mp3", "timestamp": "2025-05-24T04:21:08.382848"}
{"query": "AI", "id": "2505.16647v1", "url": "http://arxiv.org/abs/2505.16647v1", "title": "Point, Detect, Count: Multi-Task Medical Image Understanding with Instruction-Tuned Vision-Language Models", "summary": "We investigate fine-tuning Vision-Language Models (VLMs) for multi-task\nmedical image understanding, focusing on detection, localization, and counting\nof findings in medical images. Our objective is to evaluate whether\ninstruction-tuned VLMs can simultaneously improve these tasks, with the goal of\nenhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a\nmultimodal dataset with annotations from endoscopy (polyps and instruments) and\nmicroscopy (sperm cells), we reformulate each task into instruction-based\nprompts suitable for vision-language reasoning. We fine-tune\nQwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task\ncombinations. Results show that multi-task training improves robustness and\naccuracy. For example, it reduces the Count Mean Absolute Error (MAE) and\nincreases Matching Accuracy in the Counting + Pointing task. However,\ntrade-offs emerge, such as more zero-case point predictions, indicating reduced\nreliability in edge cases despite overall performance gains. Our study\nhighlights the potential of adapting general-purpose VLMs to specialized\nmedical tasks via prompt-driven fine-tuning. This approach mirrors clinical\nworkflows, where radiologists simultaneously localize, count, and describe\nfindings - demonstrating how VLMs can learn composite diagnostic reasoning\npatterns. The model produces interpretable, structured outputs, offering a\npromising step toward explainable and versatile medical AI. Code, model\nweights, and scripts will be released for reproducibility at\nhttps://github.com/simula/PointDetectCount.", "authors": ["Sushant Gautam", "Michael A. Riegler", "Pål Halvorsen"], "published_date": "2025-05-22", "title_zh": "點、測、計數：基於指令微調視覺語言模型的多任務醫學影像理解", "summary_zh": "本研究探討微調視覺語言模型（VLMs）於多任務醫學影像理解，著重於醫學影像中病灶的偵測、定位和計數。目標是評估經指令微調的VLMs能否同時提升這些任務，進而提高診斷準確性和效率。利用MedMultiPoints資料集（包含內視鏡息肉、器械與顯微鏡精細胞的標註），將各項任務改寫為基於指令的提示，以利視覺語言推理。使用低秩適應（LoRA）於Qwen2.5-VL-7B-Instruct模型上進行多任務組合微調。結果顯示，多任務訓練提升了穩健性和準確性，例如降低計數平均絕對誤差（MAE）並提高計數+定位任務的匹配準確性。然而，也出現權衡，例如零點預測案例增加，顯示在極端情況下可靠性降低，儘管整體性能有所提升。研究強調透過提示驅動微調，將通用VLMs應用於專業醫學任務的潛力。此方法模擬臨床工作流程，放射科醫師同時定位、計數和描述病灶，展現VLMs如何學習複合診斷推理模式。該模型產生可解釋的結構化輸出，為可解釋且通用的醫療人工智慧提供有前景的一步。代碼、模型權重和腳本將於https://github.com/simula/PointDetectCount發布，以供重現。", "audio": "audios/2505.16647v1.mp3", "timestamp": "2025-05-24T05:17:30.600225"}
{"query": "Diffusion Model", "id": "2505.16275v1", "url": "http://arxiv.org/abs/2505.16275v1", "title": "Semiparametric Bernstein-von Mises theorems for reversible diffusions", "summary": "We establish a general semiparametric Bernstein-von Mises theorem for\nBayesian nonparametric priors based on continuous observations in a periodic\nreversible multidimensional diffusion model. We consider a wide range of\nfunctionals satisfying an approximate linearization condition, including\nseveral nonlinear functionals of the invariant measure. Our result is applied\nto Gaussian and Besov-Laplace priors, showing these can perform efficient\nsemiparametric inference and thus justifying the corresponding Bayesian\napproach to uncertainty quantification. Our theoretical results are illustrated\nvia numerical simulations.", "authors": ["Matteo Giordano", "Kolyan Ray"], "published_date": "2025-05-22", "title_zh": "可逆擴散的半參數Bernstein-von Mises定理", "summary_zh": "本文針對週期可逆多維擴散模型中的連續觀測，建立了基於貝葉斯非參數先驗的一般半參數Bernstein-von Mises定理。研究適用於滿足近似線性化條件的廣泛泛函，包括不變測度的若干非線性泛函。結果應用於高斯和Besov-Laplace先驗，表明這些先驗能有效執行半參數推斷，從而驗證了相應的貝葉斯不確定性量化方法。數值模擬驗證了理論結果。", "audio": "audios/2505.16275v1.mp3", "timestamp": "2025-05-24T05:17:33.793815"}
{"query": "AI", "id": "2505.16630v1", "url": "http://arxiv.org/abs/2505.16630v1", "title": "SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game Understanding", "summary": "The integration of artificial intelligence in sports analytics has\ntransformed soccer video understanding, enabling real-time, automated insights\ninto complex game dynamics. Traditional approaches rely on isolated data\nstreams, limiting their effectiveness in capturing the full context of a match.\nTo address this, we introduce SoccerChat, a multimodal conversational AI\nframework that integrates visual and textual data for enhanced soccer video\ncomprehension. Leveraging the extensive SoccerNet dataset, enriched with jersey\ncolor annotations and automatic speech recognition (ASR) transcripts,\nSoccerChat is fine-tuned on a structured video instruction dataset to\nfacilitate accurate game understanding, event classification, and referee\ndecision making. We benchmark SoccerChat on action classification and referee\ndecision-making tasks, demonstrating its performance in general soccer event\ncomprehension while maintaining competitive accuracy in referee decision\nmaking. Our findings highlight the importance of multimodal integration in\nadvancing soccer analytics, paving the way for more interactive and explainable\nAI-driven sports analysis. https://github.com/simula/SoccerChat", "authors": ["Sushant Gautam", "Cise Midoglu", "Vajira Thambawita", "Michael A. Riegler", "Pål Halvorsen", "Mubarak Shah"], "published_date": "2025-05-22", "title_zh": "足球聊天：整合多模態數據以增強足球比賽理解", "summary_zh": "人工智慧融入運動分析革新了足球影片理解，實現對複雜賽事動態的即時自動洞察。傳統方法依賴獨立數據流，難以捕捉完整賽事脈絡。本研究提出SoccerChat，一種多模態對話式AI框架，整合視覺與文本數據，強化足球影片理解。SoccerChat利用SoccerNet數據集，輔以球衣顏色標註與自動語音辨識轉錄，並於結構化影片指令數據集上微調，以促進精確的賽事理解、事件分類與裁判決策。SoccerChat在行為分類與裁判決策任務上進行基準測試，展現其於一般足球事件理解上的效能，同時在裁判決策上維持具競爭力的準確度。研究結果突顯多模態整合在推動足球分析上的重要性，為更具互動性與可解釋性的AI驅動運動分析鋪路。", "audio": "audios/2505.16630v1.mp3", "timestamp": "2025-05-24T06:23:46.497912"}
{"query": "Diffusion Model", "id": "2505.16239v1", "url": "http://arxiv.org/abs/2505.16239v1", "title": "DOVE: Efficient One-Step Diffusion Model for Real-World Video Super-Resolution", "summary": "Diffusion models have demonstrated promising performance in real-world video\nsuper-resolution (VSR). However, the dozens of sampling steps they require,\nmake inference extremely slow. Sampling acceleration techniques, particularly\nsingle-step, provide a potential solution. Nonetheless, achieving one step in\nVSR remains challenging, due to the high training overhead on video data and\nstringent fidelity demands. To tackle the above issues, we propose DOVE, an\nefficient one-step diffusion model for real-world VSR. DOVE is obtained by\nfine-tuning a pretrained video diffusion model (*i.e.*, CogVideoX). To\neffectively train DOVE, we introduce the latent-pixel training strategy. The\nstrategy employs a two-stage scheme to gradually adapt the model to the video\nsuper-resolution task. Meanwhile, we design a video processing pipeline to\nconstruct a high-quality dataset tailored for VSR, termed HQ-VSR. Fine-tuning\non this dataset further enhances the restoration capability of DOVE. Extensive\nexperiments show that DOVE exhibits comparable or superior performance to\nmulti-step diffusion-based VSR methods. It also offers outstanding inference\nefficiency, achieving up to a **28$\\times$** speed-up over existing methods\nsuch as MGLD-VSR. Code is available at: https://github.com/zhengchen1999/DOVE.", "authors": ["Zheng Chen", "Zichen Zou", "Kewei Zhang", "Xiongfei Su", "Xin Yuan", "Yong Guo", "Yulun Zhang"], "published_date": "2025-05-22", "title_zh": "DOVE：用於真實世界影片超解析度的高效單步擴散模型", "summary_zh": "擴散模型在真實世界影片超解析度展現潛力，但其大量採樣步驟導致推論速度緩慢。單步採樣加速技術提供解決方案，然而，影片資料的高訓練成本和嚴格的保真度要求使單步VSR極具挑戰。為解決上述問題，我們提出DOVE，一種高效的單步擴散模型，用於真實世界VSR。DOVE透過微調預訓練的影片擴散模型（例如CogVideoX）獲得。為有效訓練DOVE，我們引入潛在像素訓練策略，該策略採用兩階段方案，逐步使模型適應影片超解析度任務。同時，我們設計影片處理流程，構建名為HQ-VSR的高品質VSR客製化資料集。在此資料集上進行微調，進一步提升DOVE的重建能力。大量實驗表明，DOVE展現與多步擴散VSR方法相當或更優異的性能，並提供卓越的推論效率，相較於MGLD-VSR等現有方法，速度提升高達28倍。程式碼位於：https://github.com/zhengchen1999/DOVE。", "audio": "audios/2505.16239v1.mp3", "timestamp": "2025-05-24T06:23:55.893315"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的人工智慧：生命科學領域的挑戰、機遇與前路", "summary_zh": "人工智慧近期在生命科學領域取得突破性進展，以前所未有的能力解讀生物資訊，並幾乎每日都有新應用與進展。為最大化基於人工智慧的生命科學研究投資回報並加速進程，亟需解決快速採用人工智慧方法所加劇的長期研究挑戰。本文回顧了因低重用性和再現性導致的對人工智慧研究產出的信任度下降問題，並強調其對環境永續性的影響。此外，討論了人工智慧生態系統的碎片化及其缺乏支持開放與永續人工智慧(OSAI)模型開發的指導路徑。為此，本文提出一套實用的OSAI建議，直接對應人工智慧生態系統的300多個組件，旨在連接研究人員與相關人工智慧資源，促進永續、可重複使用和透明的人工智慧實施。此觀點基於生命科學社群共識並與現有努力保持一致，旨在協助未來政策制定和引導人工智慧實施的結構化路徑發展。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-24T07:16:34.820500"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除抑或蟄伏？基於可逆性的概念擦除再思", "summary_zh": "概念抹除在擴散模型中能消除多少生成能力？ 既有評估主要關注特定文本提示下的概念抑制，本文探討一個互補且根本的問題：現有概念抹除技術是否真正移除生成目標概念的能力，或僅實現表面上的提示特定抑制？ 我們系統性地評估了兩種具代表性概念抹除方法（統一概念編輯和抹除穩定擴散）的穩健性和可逆性，藉由探測其消除文本到圖像模型中目標生成行為的能力。 這些方法試圖透過修改內部模型參數來抑制不需要的語義概念，包括針對性注意力編輯或模型層級微調。 為嚴格評估這些技術是否真正抹除生成能力，我們提出一種實例層級評估策略，採用輕量微調來顯式測試抹除概念的重新激活潛力。 量化指標和定性分析表明，抹除的概念通常在最小程度適應後，以相當的視覺逼真度重新出現，表明當前方法抑制了潛在生成表示，但並未完全消除它們。 研究結果揭示了現有概念抹除方法的關鍵局限性，並強調需要更深入的表示層級干預和更嚴格的評估標準，以確保從生成模型中真正、不可逆地移除概念。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-24T07:16:42.524959"}
{"query": "AI", "id": "2505.16577v1", "url": "http://arxiv.org/abs/2505.16577v1", "title": "Large Language Model-Empowered Interactive Load Forecasting", "summary": "The growing complexity of power systems has made accurate load forecasting\nmore important than ever. An increasing number of advanced load forecasting\nmethods have been developed. However, the static design of current methods\noffers no mechanism for human-model interaction. As the primary users of\nforecasting models, system operators often find it difficult to understand and\napply these advanced models, which typically requires expertise in artificial\nintelligence (AI). This also prevents them from incorporating their experience\nand real-world contextual understanding into the forecasting process. Recent\nbreakthroughs in large language models (LLMs) offer a new opportunity to\naddress this issue. By leveraging their natural language understanding and\nreasoning capabilities, we propose an LLM-based multi-agent collaboration\nframework to bridge the gap between human operators and forecasting models. A\nset of specialized agents is designed to perform different tasks in the\nforecasting workflow and collaborate via a dedicated communication mechanism.\nThis framework embeds interactive mechanisms throughout the load forecasting\npipeline, reducing the technical threshold for non-expert users and enabling\nthe integration of human experience. Our experiments demonstrate that the\ninteractive load forecasting accuracy can be significantly improved when users\nprovide proper insight in key stages. Our cost analysis shows that the\nframework remains affordable, making it practical for real-world deployment.", "authors": ["Yu Zuo", "Dalin Qin", "Yi Wang"], "published_date": "2025-05-22", "title_zh": "大型語言模型驅動的互動式負載預測", "summary_zh": "電力系統日趨複雜，精準負載預測至關重要。現有負載預測方法缺乏人機互動機制，系統操作員難以理解與應用需具備人工智慧專業知識的先進模型，且無法將自身經驗及實務知識融入預測流程。本研究提出基於大型語言模型的多代理協作框架，利用其自然語言理解與推理能力，彌合操作員與預測模型間的差距。框架設計多個專責代理執行不同預測任務，並透過通訊機制協作，在負載預測流程中嵌入互動機制，降低技術門檻，促成人類經驗整合。實驗結果顯示，使用者於關鍵階段提供適當見解，可顯著提升互動式負載預測準確性。成本分析表明，此框架具備實用性與經濟效益，適合實際部署。", "audio": "audios/2505.16577v1.mp3", "timestamp": "2025-05-24T08:21:11.580413"}
{"query": "Diffusion Model", "id": "2505.16166v1", "url": "http://arxiv.org/abs/2505.16166v1", "title": "TRAIL: Transferable Robust Adversarial Images via Latent diffusion", "summary": "Adversarial attacks exploiting unrestricted natural perturbations present\nsevere security risks to deep learning systems, yet their transferability\nacross models remains limited due to distribution mismatches between generated\nadversarial features and real-world data. While recent works utilize\npre-trained diffusion models as adversarial priors, they still encounter\nchallenges due to the distribution shift between the distribution of ideal\nadversarial samples and the natural image distribution learned by the diffusion\nmodel. To address the challenge, we propose Transferable Robust Adversarial\nImages via Latent Diffusion (TRAIL), a test-time adaptation framework that\nenables the model to generate images from a distribution of images with\nadversarial features and closely resembles the target images. To mitigate the\ndistribution shift, during attacks, TRAIL updates the diffusion U-Net's weights\nby combining adversarial objectives (to mislead victim models) and perceptual\nconstraints (to preserve image realism). The adapted model then generates\nadversarial samples through iterative noise injection and denoising guided by\nthese objectives. Experiments demonstrate that TRAIL significantly outperforms\nstate-of-the-art methods in cross-model attack transferability, validating that\ndistribution-aligned adversarial feature synthesis is critical for practical\nblack-box attacks.", "authors": ["Yuhao Xue", "Zhifei Zhang", "Xinyang Jiang", "Yifei Shen", "Junyao Gao", "Wentao Gu", "Jiale Zhao", "Miaojing Shi", "Cairong Zhao"], "published_date": "2025-05-22", "title_zh": "TRAIL：基於潛在擴散的可轉移魯棒對抗性圖像", "summary_zh": "針對深度學習系統，利用無限制自然擾動的對抗性攻擊構成嚴峻安全風險，但因產生之對抗性特徵與真實世界數據間存在分佈不匹配，其跨模型遷移性受限。近期研究雖採用預訓練擴散模型作為對抗性先驗，然理想對抗樣本分佈與擴散模型所學自然圖像分佈間的分佈偏移仍構成挑戰。為此，我們提出基於潛在擴散的可遷移穩健對抗圖像(TRAIL)，此測試時適應框架使模型能從具有對抗性特徵且與目標圖像高度相似的圖像分佈中生成圖像。為緩解分佈偏移，TRAIL於攻擊期間結合對抗性目標(誤導受害者模型)與感知約束(保持圖像真實感)來更新擴散U-Net權重。隨後，調整後的模型透過迭代雜訊注入和目標導向的去噪生成對抗樣本。實驗表明，TRAIL在跨模型攻擊遷移性方面顯著優於現有方法，驗證了分佈對齊的對抗性特徵合成對於實際黑盒攻擊至關重要。", "audio": "audios/2505.16166v1.mp3", "timestamp": "2025-05-24T08:21:19.532383"}
{"query": "AI", "id": "2505.16575v1", "url": "http://arxiv.org/abs/2505.16575v1", "title": "Data Center Model for Transient Stability Analysis of Power Systems", "summary": "The rising demand of computing power leads to the installation of a large\nnumber of Data Centers (DCs). Their Fault-Ride-Through (FRT) behavior and their\nunique power characteristics, especially for DCs catered to Artificial\nIntelligence (AI) workloads, pose a threat to the stability of power systems.\nTo ensure its stability, it is required accurate models of the loads involved.\nHere we propose a dynamic load model that properly captures the behaviour of\nDCs. Its three most defining features are the use of an Uninterrupted Power\nSupply (UPS) which sits between the server load and the grid, the cooling load\nrepresented by an induction motor, and a pulsing load that represents the\ntransients caused by contemporary DCs with significant AI workloads. The\nfeatures of the proposed model and its impact on the dynamic performance of\ntransmission systems are illustrated through a model of the all-island Irish\ntransmission system and real-world data of the DCs currently connected to this\nsystem.", "authors": ["Alberto Jimenez-Ruiz", "Federico Milano"], "published_date": "2025-05-22", "title_zh": "電力系統暫態穩定分析之資料中心模型", "summary_zh": "計算能力需求激增導致大量資料中心建置，其低電壓穿越能力及獨特的電力特性，特別是針對人工智慧工作負載的資料中心，對電力系統穩定性構成威脅。為確保穩定性，需要精確的負載模型。本研究提出一種動態負載模型，能準確捕捉資料中心的行為。其三大特點為：連接伺服器負載與電網間的不斷電供應系統、以感應電動機表示的冷卻負載，以及代表具顯著人工智慧工作負載的現代資料中心所產生瞬態的脈衝負載。透過全愛爾蘭輸電系統模型及當前連接至該系統的資料中心真實數據，闡述了所提模型的特點及其對輸電系統動態性能的影響。", "audio": "audios/2505.16575v1.mp3", "timestamp": "2025-05-24T09:17:51.653573"}
{"query": "Diffusion Model", "id": "2505.16091v1", "url": "http://arxiv.org/abs/2505.16091v1", "title": "OSCAR: One-Step Diffusion Codec Across Multiple Bit-rates", "summary": "Pretrained latent diffusion models have shown strong potential for lossy\nimage compression, owing to their powerful generative priors. Most existing\ndiffusion-based methods reconstruct images by iteratively denoising from random\nnoise, guided by compressed latent representations. While these approaches have\nachieved high reconstruction quality, their multi-step sampling process incurs\nsubstantial computational overhead. Moreover, they typically require training\nseparate models for different compression bit-rates, leading to significant\ntraining and storage costs. To address these challenges, we propose a one-step\ndiffusion codec across multiple bit-rates. termed OSCAR. Specifically, our\nmethod views compressed latents as noisy variants of the original latents,\nwhere the level of distortion depends on the bit-rate. This perspective allows\nthem to be modeled as intermediate states along a diffusion trajectory. By\nestablishing a mapping from the compression bit-rate to a pseudo diffusion\ntimestep, we condition a single generative model to support reconstructions at\nmultiple bit-rates. Meanwhile, we argue that the compressed latents retain rich\nstructural information, thereby making one-step denoising feasible. Thus, OSCAR\nreplaces iterative sampling with a single denoising pass, significantly\nimproving inference efficiency. Extensive experiments demonstrate that OSCAR\nachieves superior performance in both quantitative and visual quality metrics.\nThe code and models will be released at https://github.com/jp-guo/OSCAR.", "authors": ["Jinpei Guo", "Yifei Ji", "Zheng Chen", "Kai Liu", "Min Liu", "Wang Rao", "Wenbo Li", "Yong Guo", "Yulun Zhang"], "published_date": "2025-05-22", "title_zh": "OSCAR：跨多位元速率之單步擴散編解碼器", "summary_zh": "預訓練潛在擴散模型因其強大的生成先驗，在有損影像壓縮方面展現巨大潛力。現有基於擴散的方法多藉由壓縮的潛在表徵引導，從隨機雜訊迭代去噪以重建影像。儘管這些方法實現了高重建品質，但其多步驟取樣過程產生了大量的計算開銷。此外，它們通常需要為不同的壓縮位元速率訓練單獨的模型，導致顯著的訓練和儲存成本。為了解決這些挑戰，我們提出了一種跨多位元速率的單步擴散編解碼器，稱為OSCAR。具體而言，我們的方法將壓縮的潛在視為原始潛在的噪聲變體，其中失真程度取決於位元速率。這種觀點允許將其建模為沿擴散軌跡的中間狀態。透過建立從壓縮位元速率到偽擴散時間步的映射，我們條件化單一生成模型以支援多位元速率的重建。同時，我們認為壓縮的潛在保留了豐富的結構訊息，從而使單步去噪成為可能。因此，OSCAR用單個去噪過程取代了迭代取樣，從而顯著提高了推論效率。大量實驗表明，OSCAR在定量和視覺品質指標方面均實現了卓越的性能。代碼和模型將在https://github.com/jp-guo/OSCAR發布。", "audio": "audios/2505.16091v1.mp3", "timestamp": "2025-05-24T09:18:00.424802"}
{"query": "AI", "id": "2505.16561v1", "url": "http://arxiv.org/abs/2505.16561v1", "title": "Auto-nnU-Net: Towards Automated Medical Image Segmentation", "summary": "Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ\nsegmentation, each with its own challenges in finding the best segmentation\nmodel. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many\naspects of model configuration but remains constrained by fixed hyperparameters\nand heuristic design choices. As a full-AutoML framework for MIS, we propose\nAuto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization\n(HPO), neural architecture search (NAS), and hierarchical NAS (HNAS).\nAdditionally, we propose Regularized PriorBand to balance model accuracy with\nthe computational resources required for training, addressing the resource\nconstraints often faced in real-world medical settings that limit the\nfeasibility of extensive training procedures. We evaluate our approach across\ndiverse MIS datasets from the well-established Medical Segmentation Decathlon,\nanalyzing the impact of AutoML techniques on segmentation performance,\ncomputational efficiency, and model design choices. The results demonstrate\nthat our AutoML approach substantially improves the segmentation performance of\nnnU-Net on 6 out of 10 datasets and is on par on the other datasets while\nmaintaining practical resource requirements. Our code is available at\nhttps://github.com/LUH-AI/AutonnUNet.", "authors": ["Jannis Becktepe", "Leona Hennig", "Steffen Oeltze-Jafra", "Marius Lindauer"], "published_date": "2025-05-22", "title_zh": "Auto-nnU-Net：邁向自動化醫學影像分割", "summary_zh": "醫學影像分割任務多樣，模型選擇具挑戰性。現有AutoML框架nnU-Net雖自動化模型配置，仍受限於固定超參數。本研究提出Auto-nnU-Net，一新型nnU-Net變體，支援超參數優化、神經架構搜尋及階層式神經架構搜尋，並引入Regularized PriorBand平衡模型準確度與運算資源。實驗採用Medical Segmentation Decathlon數據集，驗證AutoML技術對分割效能、運算效率及模型設計的影響。結果顯示，Auto-nnU-Net在十分之六的數據集上顯著提升nnU-Net分割效能，其餘數據集上表現相當，同時維持合理資源需求。", "audio": "audios/2505.16561v1.mp3", "timestamp": "2025-05-24T10:17:28.458296"}
{"query": "Diffusion Model", "id": "2505.16024v1", "url": "http://arxiv.org/abs/2505.16024v1", "title": "Toward Theoretical Insights into Diffusion Trajectory Distillation via Operator Merging", "summary": "Diffusion trajectory distillation methods aim to accelerate sampling in\ndiffusion models, which produce high-quality outputs but suffer from slow\nsampling speeds. These methods train a student model to approximate the\nmulti-step denoising process of a pretrained teacher model in a single step,\nenabling one-shot generation. However, theoretical insights into the trade-off\nbetween different distillation strategies and generative quality remain\nlimited, complicating their optimization and selection. In this work, we take a\nfirst step toward addressing this gap. Specifically, we reinterpret trajectory\ndistillation as an operator merging problem in the linear regime, where each\nstep of the teacher model is represented as a linear operator acting on noisy\ndata. These operators admit a clear geometric interpretation as projections and\nrescalings corresponding to the noise schedule. During merging, signal\nshrinkage occurs as a convex combination of operators, arising from both\ndiscretization and limited optimization time of the student model. We propose a\ndynamic programming algorithm to compute the optimal merging strategy that\nmaximally preserves signal fidelity. Additionally, we demonstrate the existence\nof a sharp phase transition in the optimal strategy, governed by data\ncovariance structures. Our findings enhance the theoretical understanding of\ndiffusion trajectory distillation and offer practical insights for improving\ndistillation strategies.", "authors": ["Weiguo Gao", "Ming Li"], "published_date": "2025-05-21", "title_zh": "藉由算符合併探討擴散軌跡提煉的理論見解", "summary_zh": "擴散軌跡蒸餾旨在加速擴散模型取樣，此類模型能產出高品質結果，但取樣速度慢。此方法訓練學生模型，以單一步驟逼近預訓練教師模型的多步去噪過程，實現單次生成。然而，對於不同蒸餾策略和生成品質間權衡的理論洞見仍然有限，使得優化和選擇變得複雜。本研究朝此方向邁出第一步，將軌跡蒸餾重新詮釋為線性狀態下的運算元合併問題，教師模型的每一步驟表示為作用於雜訊資料的線性運算元。這些運算元可被清楚地幾何解釋為對應於雜訊排程的投影和重新縮放。合併過程中，訊號衰減發生於運算元的凸組合，源於學生模型的離散化和有限優化時間。我們提出動態規劃演算法，計算最大程度保留訊號保真度的最佳合併策略。此外，我們證明了最佳策略中存在由資料共變異數結構控制的明顯相變。這些發現增強了對擴散軌跡蒸餾的理論理解，並為改進蒸餾策略提供實用見解。", "audio": "audios/2505.16024v1.mp3", "timestamp": "2025-05-24T10:17:35.631527"}
{"query": "AI", "id": "2505.16499v1", "url": "http://arxiv.org/abs/2505.16499v1", "title": "Smaller, Smarter, Closer: The Edge of Collaborative Generative AI", "summary": "The rapid adoption of generative AI (GenAI), particularly Large Language\nModels (LLMs), has exposed critical limitations of cloud-centric deployments,\nincluding latency, cost, and privacy concerns. Meanwhile, Small Language Models\n(SLMs) are emerging as viable alternatives for resource-constrained edge\nenvironments, though they often lack the capabilities of their larger\ncounterparts. This article explores the potential of collaborative inference\nsystems that leverage both edge and cloud resources to address these\nchallenges. By presenting distinct cooperation strategies alongside practical\ndesign principles and experimental insights, we offer actionable guidance for\ndeploying GenAI across the computing continuum.", "authors": ["Roberto Morabito", "SiYoung Jang"], "published_date": "2025-05-22", "title_zh": "更小、更智慧、更近端：協作式生成人工智慧的邊緣", "summary_zh": "生成式人工智慧（GenAI）特別是大語言模型（LLMs）的快速普及，暴露出雲端部署的延遲、成本和隱私等關鍵限制。小型語言模型（SLMs）雖成為資源受限邊緣環境的可行替代方案，但能力往往不如大型模型。本文探討利用邊緣和雲端資源的協作推論系統之潛力，旨在應對這些挑戰。透過呈現不同的合作策略、實用設計原則和實驗洞見，我們為在計算連續體中部署GenAI提供可行的指導。", "audio": "audios/2505.16499v1.mp3", "timestamp": "2025-05-24T11:13:36.526423"}
{"query": "Diffusion Model", "id": "2505.16001v1", "url": "http://arxiv.org/abs/2505.16001v1", "title": "Image-to-Image Translation with Diffusion Transformers and CLIP-Based Image Conditioning", "summary": "Image-to-image translation aims to learn a mapping between a source and a\ntarget domain, enabling tasks such as style transfer, appearance\ntransformation, and domain adaptation. In this work, we explore a\ndiffusion-based framework for image-to-image translation by adapting Diffusion\nTransformers (DiT), which combine the denoising capabilities of diffusion\nmodels with the global modeling power of transformers. To guide the translation\nprocess, we condition the model on image embeddings extracted from a\npre-trained CLIP encoder, allowing for fine-grained and structurally consistent\ntranslations without relying on text or class labels. We incorporate both a\nCLIP similarity loss to enforce semantic consistency and an LPIPS perceptual\nloss to enhance visual fidelity during training. We validate our approach on\ntwo benchmark datasets: face2comics, which translates real human faces to\ncomic-style illustrations, and edges2shoes, which translates edge maps to\nrealistic shoe images. Experimental results demonstrate that DiT, combined with\nCLIP-based conditioning and perceptual similarity objectives, achieves\nhigh-quality, semantically faithful translations, offering a promising\nalternative to GAN-based models for paired image-to-image translation tasks.", "authors": ["Qiang Zhu", "Kuan Lu", "Menghao Huo", "Yuxiao Li"], "published_date": "2025-05-21", "title_zh": "基於擴散變換器與CLIP圖像條件的圖像轉圖像轉換", "summary_zh": "圖像到圖像轉換旨在學習源域與目標域之間的映射，實現風格轉換、外觀變換和領域自適應等任務。本研究探索基於擴散的圖像到圖像轉換框架，改進擴散轉換器(DiT)，結合擴散模型的去噪能力和轉換器的全局建模能力。為引導轉換過程，模型以預訓練CLIP編碼器提取的圖像嵌入為條件，實現精細且結構一致的轉換，無需依賴文本或類別標籤。訓練中，加入CLIP相似度損失以確保語義一致性，並加入LPIPS感知損失以增強視覺逼真度。在face2comics和edges2shoes兩個基準數據集上驗證了該方法。實驗結果表明，DiT結合基於CLIP的條件設定和感知相似性目標，可實現高質量、語義忠實的轉換，為配對圖像到圖像轉換任務提供了一種有前景的替代方案，可取代基於GAN的模型。", "audio": "audios/2505.16001v1.mp3", "timestamp": "2025-05-24T11:13:43.305928"}
{"query": "AI", "id": "2505.16477v1", "url": "http://arxiv.org/abs/2505.16477v1", "title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery", "summary": "With recent Nobel Prizes recognising AI contributions to science, Large\nLanguage Models (LLMs) are transforming scientific research by enhancing\nproductivity and reshaping the scientific method. LLMs are now involved in\nexperimental design, data analysis, and workflows, particularly in chemistry\nand biology. However, challenges such as hallucinations and reliability\npersist. In this contribution, we review how Large Language Models (LLMs) are\nredefining the scientific method and explore their potential applications\nacross different stages of the scientific cycle, from hypothesis testing to\ndiscovery. We conclude that, for LLMs to serve as relevant and effective\ncreative engines and productivity enhancers, their deep integration into all\nsteps of the scientific process should be pursued in collaboration and\nalignment with human scientific goals, with clear evaluation metrics. The\ntransition to AI-driven science raises ethical questions about creativity,\noversight, and responsibility. With careful guidance, LLMs could evolve into\ncreative engines, driving transformative breakthroughs across scientific\ndisciplines responsibly and effectively. However, the scientific community must\nalso decide how much it leaves to LLMs to drive science, even when associations\nwith 'reasoning', mostly currently undeserved, are made in exchange for the\npotential to explore hypothesis and solution regions that might otherwise\nremain unexplored by human exploration alone.", "authors": ["Yanbo Zhang", "Sumeer A. Khan", "Adnan Mahmud", "Huck Yang", "Alexander Lavin", "Michael Levin", "Jeremy Frey", "Jared Dunnmon", "James Evans", "Alan Bundy", "Saso Dzeroski", "Jesper Tegner", "Hector Zenil"], "published_date": "2025-05-22", "title_zh": "以大型語言模型推進科學方法：從假說到發現", "summary_zh": "大型語言模型因其對科學的貢獻而備受肯定，正透過提高生產力及重塑科學方法來改變科學研究。它們廣泛應用於實驗設計、數據分析及工作流程，尤其在化學與生物學領域。然幻覺和可靠性問題依然存在。本文探討大型語言模型如何重新定義科學方法，及其在科學週期各階段的潛在應用，從假設檢驗到新發現。為使大型語言模型成為有效的創新引擎和生產力工具，應將其深度整合至科學過程的各個環節，並與人類科學目標協調一致，建立明確的評估指標。人工智慧驅動科學的轉型引發了關於創造力、監督和責任的倫理問題。透過謹慎引導，大型語言模型有望成為創新引擎，負責且有效地推動科學各領域的變革性突破。然而，科學界必須決定在多大程度上依賴大型語言模型來驅動科學發展，即使以潛在的探索假設和解決方案空間為代價，這些空間可能僅憑人類探索無法觸及。", "audio": "audios/2505.16477v1.mp3", "timestamp": "2025-05-24T12:33:34.173858"}
{"query": "Diffusion Model", "id": "2505.15963v1", "url": "http://arxiv.org/abs/2505.15963v1", "title": "OViP: Online Vision-Language Preference Learning", "summary": "Large vision-language models (LVLMs) remain vulnerable to hallucination,\noften generating content misaligned with visual inputs. While recent approaches\nadvance multi-modal Direct Preference Optimization (DPO) to mitigate\nhallucination, they typically rely on predefined or randomly edited negative\nsamples that fail to reflect actual model errors, limiting training efficacy.\nIn this work, we propose an Online Vision-language Preference Learning (OViP)\nframework that dynamically constructs contrastive training data based on the\nmodel's own hallucinated outputs. By identifying semantic differences between\nsampled response pairs and synthesizing negative images using a diffusion\nmodel, OViP generates more relevant supervision signals in real time. This\nfailure-driven training enables adaptive alignment of both textual and visual\npreferences. Moreover, we refine existing evaluation protocols to better\ncapture the trade-off between hallucination suppression and expressiveness.\nExperiments on hallucination and general benchmarks demonstrate that OViP\neffectively reduces hallucinations while preserving core multi-modal\ncapabilities.", "authors": ["Shujun Liu", "Siyuan Wang", "Zejun Li", "Jianxiang Wang", "Cheng Zeng", "Zhongyu Wei"], "published_date": "2025-05-21", "title_zh": "OViP：線上視覺語言偏好學習", "summary_zh": "大型視覺語言模型易產生幻覺，內容與視覺輸入不符。現有方法雖利用多模態直接偏好優化降低幻覺，但多依賴預定義或隨機編輯的負樣本，未能反映模型實際錯誤，限制訓練效果。本文提出線上視覺語言偏好學習框架OViP，基於模型自身幻覺輸出動態構建對比訓練數據。透過識別抽樣回應對之間的語義差異，並利用擴散模型合成負圖像，OViP即時生成更相關的監督訊號。此種失敗驅動訓練使文本和視覺偏好得以自適應對齊。此外，本文改進現有評估協議，以更精確捕捉幻覺抑制與表達能力之間的權衡。在幻覺和通用基準測試上的實驗表明，OViP有效減少幻覺，同時保留核心多模態能力。", "audio": "audios/2505.15963v1.mp3", "timestamp": "2025-05-24T12:33:42.576030"}
{"query": "AI", "id": "2505.16455v1", "url": "http://arxiv.org/abs/2505.16455v1", "title": "Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events", "summary": "During sudden disaster events, accurately predicting public panic sentiment\non social media is crucial for proactive governance and crisis management.\nCurrent efforts on this problem face three main challenges: lack of finely\nannotated data hinders emotion prediction studies, unmodeled risk perception\ncauses prediction inaccuracies, and insufficient interpretability of panic\nformation mechanisms. We address these issues by proposing a Psychology-driven\ngenerative Agent framework (PsychoAgent) for explainable panic prediction based\non emotion arousal theory. Specifically, we first construct a fine-grained open\npanic emotion dataset (namely COPE) via human-large language models (LLMs)\ncollaboration to mitigate semantic bias. Then, we develop a framework\nintegrating cross-domain heterogeneous data grounded in psychological\nmechanisms to model risk perception and cognitive differences in emotion\ngeneration. To enhance interpretability, we design an LLM-based role-playing\nagent that simulates individual psychological chains through dedicatedly\ndesigned prompts. Experimental results on our annotated dataset show that\nPsychoAgent improves panic emotion prediction performance by 12.6% to 21.7%\ncompared to baseline models. Furthermore, the explainability and generalization\nof our approach is validated. Crucially, this represents a paradigm shift from\nopaque \"data-driven fitting\" to transparent \"role-based simulation with\nmechanistic interpretation\" for panic emotion prediction during emergencies.\nOur implementation is publicly available at:\nhttps://anonymous.4open.science/r/PsychoAgent-19DD.", "authors": ["Mengzhu Liu", "Zhengqiu Zhu", "Chuan Ai", "Chen Gao", "Xinghong Li", "Lingnan He", "Kaisheng Lai", "Yingfeng Chen", "Xin Lu", "Yong Li", "Quanjun Yin"], "published_date": "2025-05-22", "title_zh": "基於心理學的LLM代理於突發災難事件期間社交媒體上可解釋的恐慌預測", "summary_zh": "在突發災害事件中，精確預測社群媒體上的公眾恐慌情緒，對主動治理和危機管理至關重要。目前的研究面臨缺乏精細標註資料、未建模風險感知及恐慌形成機制解釋性不足等挑戰。本文提出基於情緒喚醒理論的心理驅動生成式代理框架（PsychoAgent），用於可解釋的恐慌預測。首先，透過人機協作构建精細化的開放式恐慌情緒數據集（COPE），以減輕語義偏差。接著，開發整合跨領域異質數據的框架，基於心理學機制建模風險感知和情緒產生的認知差異。為提升可解釋性，設計基於大型語言模型（LLM）的角色扮演代理，透過精心設計的提示模擬個體心理鏈。實驗結果表明，與基線模型相比，PsychoAgent在標註數據集上將恐慌情緒預測性能提升了12.6%至21.7%，並驗證了其可解釋性和泛化性。此方法代表了從不透明的「數據驅動擬合」到透明的「基於角色的機制解釋模擬」的範式轉移，可用於應急情況下的恐慌情緒預測。", "audio": "audios/2505.16455v1.mp3", "timestamp": "2025-05-24T13:25:36.031393"}
{"query": "Diffusion Model", "id": "2505.15946v1", "url": "http://arxiv.org/abs/2505.15946v1", "title": "MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding", "summary": "Decoding visual experiences from fMRI offers a powerful avenue to understand\nhuman perception and develop advanced brain-computer interfaces. However,\ncurrent progress often prioritizes maximizing reconstruction fidelity while\noverlooking interpretability, an essential aspect for deriving neuroscientific\ninsight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework\ndesigned for high-fidelity, adaptable, and interpretable visual reconstruction.\nMoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture\nwhere distinct experts process fMRI signals from functionally related voxel\ngroups, mimicking specialized brain networks. The experts are first trained to\nencode fMRI into the frozen CLIP space. A finetuned diffusion model then\nsynthesizes images, guided by expert outputs through a novel dual-stage routing\nmechanism that dynamically weighs expert contributions across the diffusion\nprocess. MoRE-Brain offers three main advancements: First, it introduces a\nnovel Mixture-of-Experts architecture grounded in brain network principles for\nneuro-decoding. Second, it achieves efficient cross-subject generalization by\nsharing core expert networks while adapting only subject-specific routers.\nThird, it provides enhanced mechanistic insight, as the explicit routing\nreveals precisely how different modeled brain regions shape the semantic and\nspatial attributes of the reconstructed image. Extensive experiments validate\nMoRE-Brain's high reconstruction fidelity, with bottleneck analyses further\ndemonstrating its effective utilization of fMRI signals, distinguishing genuine\nneural decoding from over-reliance on generative priors. Consequently,\nMoRE-Brain marks a substantial advance towards more generalizable and\ninterpretable fMRI-based visual decoding. Code will be publicly available soon:\nhttps://github.com/yuxiangwei0808/MoRE-Brain.", "authors": ["Yuxiang Wei", "Yanteng Zhang", "Xi Xiao", "Tianyang Wang", "Xiao Wang", "Vince D. Calhoun"], "published_date": "2025-05-21", "title_zh": "MoRE-Brain：用於可解釋和泛化跨受試者fMRI視覺解碼的路由混合專家模型", "summary_zh": "基於功能性磁振造影解碼視覺經驗有助於理解人類感知並發展先進的腦機介面。現有研究多著重於最大化重建保真度，忽略了神經科學洞察力的關鍵要素：可解釋性。為解決此問題，我們提出MoRE-Brain，一種神經啟發框架，旨在實現高保真度、可適應性和可解釋性的視覺重建。MoRE-Brain獨特地採用層次化的專家混合架構，其中不同專家處理來自功能相關體素群的功能性磁振造影訊號，模擬專業化的大腦網路。首先訓練專家將功能性磁振造影編碼到固定的CLIP空間。然後，微調的擴散模型在新型雙階段路由機制的引導下合成圖像，該機制在擴散過程中動態衡量專家的貢獻。MoRE-Brain提供三個主要進展：首先，引入基於大腦網路原理的專家混合架構，用於神經解碼。其次，透過共享核心專家網路並僅調整特定於受試者的路由器，實現有效的跨受試者泛化。第三，提供增強的機制洞察力，因為顯式路由揭示了不同建模的大腦區域如何塑造重建圖像的語義和空間屬性。廣泛的實驗驗證了MoRE-Brain的高重建保真度，瓶頸分析進一步證明了其對功能性磁振造影訊號的有效利用，區分了真正的神經解碼和過度依賴生成先驗。因此，MoRE-Brain標誌著朝著更具泛化性和可解釋性的基於功能性磁振造影的視覺解碼邁出了重要一步。程式碼將很快公開。", "audio": "audios/2505.15946v1.mp3", "timestamp": "2025-05-24T13:25:46.984857"}
{"query": "AI", "id": "2505.16412v1", "url": "http://arxiv.org/abs/2505.16412v1", "title": "Pose-invariant face recognition via feature-space pose frontalization", "summary": "Pose-invariant face recognition has become a challenging problem for modern\nAI-based face recognition systems. It aims at matching a profile face captured\nin the wild with a frontal face registered in a database. Existing methods\nperform face frontalization via either generative models or learning a pose\nrobust feature representation. In this paper, a new method is presented to\nperform face frontalization and recognition within the feature space. First, a\nnovel feature space pose frontalization module (FSPFM) is proposed to transform\nprofile images with arbitrary angles into frontal counterparts. Second, a new\ntraining paradigm is proposed to maximize the potential of FSPFM and boost its\nperformance. The latter consists of a pre-training and an attention-guided\nfine-tuning stage. Moreover, extensive experiments have been conducted on five\npopular face recognition benchmarks. Results show that not only our method\noutperforms the state-of-the-art in the pose-invariant face recognition task\nbut also maintains superior performance in other standard scenarios.", "authors": ["Nikolay Stanishev", "Yuhang Lu", "Touradj Ebrahimi"], "published_date": "2025-05-22", "title_zh": "基於特徵空間姿態正面化的姿態無關人臉識別", "summary_zh": "姿態不變臉部識別已成為現代AI臉部識別系統的挑戰。其目標是將自然環境下的側面臉與資料庫中的正面臉進行匹配。現有方法透過生成模型或學習姿態穩健的特徵表示來實現臉部正面化。本文提出一種新方法，在特徵空間中進行臉部正面化和識別。首先，提出一種新穎的特徵空間姿態正面化模組（FSPFM），將任意角度的側面圖像轉換為正面圖像。其次，提出一種新的訓練範式，以最大化FSPFM的潛力並提高其性能，該範式包含預訓練和注意力引導的微調階段。此外，在五個流行的臉部識別基準上進行了大量實驗。結果表明，該方法不僅在姿態不變臉部識別任務中優於最先進的方法，而且在其他標準場景中也保持卓越的性能。", "audio": "audios/2505.16412v1.mp3", "timestamp": "2025-05-24T14:15:10.093123"}
{"query": "Diffusion Model", "id": "2505.15863v1", "url": "http://arxiv.org/abs/2505.15863v1", "title": "Generative AI for Autonomous Driving: A Review", "summary": "Generative AI (GenAI) is rapidly advancing the field of Autonomous Driving\n(AD), extending beyond traditional applications in text, image, and video\ngeneration. We explore how generative models can enhance automotive tasks, such\nas static map creation, dynamic scenario generation, trajectory forecasting,\nand vehicle motion planning. By examining multiple generative approaches\nranging from Variational Autoencoder (VAEs) over Generative Adversarial\nNetworks (GANs) and Invertible Neural Networks (INNs) to Generative\nTransformers (GTs) and Diffusion Models (DMs), we highlight and compare their\ncapabilities and limitations for AD-specific applications. Additionally, we\ndiscuss hybrid methods integrating conventional techniques with generative\napproaches, and emphasize their improved adaptability and robustness. We also\nidentify relevant datasets and outline open research questions to guide future\ndevelopments in GenAI. Finally, we discuss three core challenges: safety,\ninterpretability, and realtime capabilities, and present recommendations for\nimage generation, dynamic scenario generation, and planning.", "authors": ["Katharina Winter", "Abhishek Vivekanandan", "Rupert Polley", "Yinzhe Shen", "Christian Schlauch", "Mohamed-Khalil Bouzidi", "Bojan Derajic", "Natalie Grabowsky", "Annajoyce Mariani", "Dennis Rochau", "Giovanni Lucente", "Harsh Yadav", "Firas Mualla", "Adam Molin", "Sebastian Bernhard", "Christian Wirth", "Ömer Şahin Taş", "Nadja Klein", "Fabian B. Flohr", "Hanno Gottschalk"], "published_date": "2025-05-21", "title_zh": "生成式人工智慧於自動駕駛之應用：綜述", "summary_zh": "生成式人工智慧(GenAI)正快速推進自動駕駛(AD)領域，超越傳統文字、圖像和影片生成應用。本文探討生成模型如何提升汽車任務，如靜態地圖建立、動態場景生成、軌跡預測和車輛運動規劃。研究變分自編碼器(VAEs)、生成對抗網路(GANs)、可逆神經網路(INNs)、生成式轉換器(GTs)和擴散模型(DMs)等生成方法，比較其在AD應用中的能力和局限性。同時討論混合方法，強調其改進的適應性和穩健性。概述相關資料集並提出開放性研究問題，以指導GenAI未來發展。最後，探討安全性、可解釋性和即時性三大核心挑戰，並針對圖像生成、動態場景生成和規劃提出建議。", "audio": "audios/2505.15863v1.mp3", "timestamp": "2025-05-24T14:15:21.574103"}
{"query": "AI", "id": "2505.16388v1", "url": "http://arxiv.org/abs/2505.16388v1", "title": "Serious Games: Human-AI Interaction, Evolution, and Coevolution", "summary": "The serious games between humans and AI have only just begun. Evolutionary\nGame Theory (EGT) models the competitive and cooperative strategies of\nbiological entities. EGT could help predict the potential evolutionary\nequilibrium of humans and AI. The objective of this work was to examine some of\nthe EGT models relevant to human-AI interaction, evolution, and coevolution. Of\nthirteen EGT models considered, three were examined: the Hawk-Dove Game,\nIterated Prisoner's Dilemma, and the War of Attrition. This selection was based\non the widespread acceptance and clear relevance of these models to potential\nhuman-AI evolutionary dynamics and coevolutionary trajectories. The Hawk-Dove\nGame predicts balanced mixed-strategy equilibria based on the costs of\nconflict. It also shows the potential for balanced coevolution rather than\ndominance. Iterated Prisoner's Dilemma suggests that repeated interaction may\nlead to cognitive coevolution. It demonstrates how memory and reciprocity can\nlead to cooperation. The War of Attrition suggests that competition for\nresources may result in strategic coevolution, asymmetric equilibria, and\nconventions on sharing resources. Therefore, EGT may provide a suitable\nframework to understand and predict the human-AI evolutionary dynamic. However,\nfuture research could extend beyond EGT and explore additional frameworks,\nempirical validation methods, and interdisciplinary perspectives. AI is being\nshaped by human input and is evolving in response to it. So too,\nneuroplasticity allows the human brain to grow and evolve in response to\nstimuli. If humans and AI converge in future, what might be the result of human\nneuroplasticity combined with an ever-evolving AI? Future research should be\nmindful of the ethical and cognitive implications of human-AI interaction,\nevolution, and coevolution.", "authors": ["Nandini Doreswamy", "Louise Horstmanshof"], "published_date": "2025-05-22", "title_zh": "嚴肅遊戲：人機互動、演進與共同演化", "summary_zh": "人機嚴肅博弈方興未艾。演化博弈理論(EGT)模擬生物實體的競爭與合作策略，或能預測人機演化的潛在平衡。本文探討與人機互動、演化及共同演化相關的EGT模型。在考量的十三種模型中，檢視了鷹鴿博弈、重複囚徒困境及消耗戰。鷹鴿博弈基於衝突成本預測混合策略均衡，並展現平衡共同演化的可能性。重複囚徒困境表明重複互動可能導致認知共同演化，記憶與互惠有助合作。消耗戰則暗示資源競爭可能導致策略共同演化、非對稱均衡及資源共享慣例。因此，EGT或能為理解與預測人機演化動態提供框架。然而，未來研究應拓展至EGT之外，探索其他框架、驗證方法及跨領域觀點。AI受人類輸入塑造並隨之演化，而神經可塑性使人腦得以成長與演化。若人機於未來融合，結合人類神經可塑性與持續演化的AI可能產生何種結果？未來研究應關注人機互動、演化及共同演化所涉及的倫理與認知影響。", "audio": "audios/2505.16388v1.mp3", "timestamp": "2025-05-24T15:16:55.688048"}
{"query": "AI", "id": "2505.16381v1", "url": "http://arxiv.org/abs/2505.16381v1", "title": "PaTH Attention: Position Encoding via Accumulating Householder Transformations", "summary": "The attention mechanism is a core primitive in modern large language models\n(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,\nposition encoding is essential for modeling structured domains such as\nlanguage. Rotary position encoding (RoPE) has emerged as the de facto standard\napproach for position encoding and is part of many modern LLMs. However, in\nRoPE the key/query transformation between two elements in a sequence is only a\nfunction of their relative position and otherwise independent of the actual\ninput. This limits the expressivity of RoPE-based transformers.\n  This paper describes PaTH, a flexible data-dependent position encoding scheme\nbased on accumulated products of Householder(like) transformations, where each\ntransformation is data-dependent, i.e., a function of the input. We derive an\nefficient parallel algorithm for training through exploiting a compact\nrepresentation of products of Householder matrices, and implement a\nFlashAttention-style blockwise algorithm that minimizes I/O cost. Across both\ntargeted synthetic benchmarks and moderate-scale real-world language modeling\nexperiments, we find that PaTH demonstrates superior performance compared to\nRoPE and other recent baselines.", "authors": ["Songlin Yang", "Yikang Shen", "Kaiyue Wen", "Shawn Tan", "Mayank Mishra", "Liliang Ren", "Rameswar Panda", "Yoon Kim"], "published_date": "2025-05-22", "title_zh": "PaTH注意力：基於累積豪斯霍爾德變換的位置編碼", "summary_zh": "注意力機制是現代大型語言模型及廣義人工智慧的核心元件。由於注意力本身對排列不變，位置編碼對於語言等結構化領域的建模至關重要。旋轉位置編碼(RoPE)已成為事實上的標準方法，並被廣泛應用於許多現代大型語言模型中。然而，RoPE中序列中兩個元素之間的鍵/查詢轉換僅取決於它們的相對位置，而與實際輸入無關，這限制了基於RoPE的轉換器的表達能力。\n\n本研究提出PaTH，一種基於累積豪斯霍爾德(Householder)類變換乘積的靈活且數據相關的位置編碼方案，其中每個變換都與數據相關，即輸入的函數。我們推導出一種高效的平行訓練算法，利用豪斯霍爾德矩陣乘積的緊湊表示，並實現了一種FlashAttention風格的塊式算法，以最大限度地降低I/O成本。在有針對性的合成基準測試和中等規模的真實語言建模實驗中，PaTH均展現出優於RoPE和其他最新基準的性能。", "audio": "audios/2505.16381v1.mp3", "timestamp": "2025-05-24T16:20:44.065769"}
{"query": "AI", "id": "2505.16379v1", "url": "http://arxiv.org/abs/2505.16379v1", "title": "Materials Generation in the Era of Artificial Intelligence: A Comprehensive Survey", "summary": "Materials are the foundation of modern society, underpinning advancements in\nenergy, electronics, healthcare, transportation, and infrastructure. The\nability to discover and design new materials with tailored properties is\ncritical to solving some of the most pressing global challenges. In recent\nyears, the growing availability of high-quality materials data combined with\nrapid advances in Artificial Intelligence (AI) has opened new opportunities for\naccelerating materials discovery. Data-driven generative models provide a\npowerful tool for materials design by directly create novel materials that\nsatisfy predefined property requirements. Despite the proliferation of related\nwork, there remains a notable lack of up-to-date and systematic surveys in this\narea. To fill this gap, this paper provides a comprehensive overview of recent\nprogress in AI-driven materials generation. We first organize various types of\nmaterials and illustrate multiple representations of crystalline materials. We\nthen provide a detailed summary and taxonomy of current AI-driven materials\ngeneration approaches. Furthermore, we discuss the common evaluation metrics\nand summarize open-source codes and benchmark datasets. Finally, we conclude\nwith potential future directions and challenges in this fast-growing field. The\nrelated sources can be found at\nhttps://github.com/ZhixunLEE/Awesome-AI-for-Materials-Generation.", "authors": ["Zhixun Li", "Bin Cao", "Rui Jiao", "Liang Wang", "Ding Wang", "Yang Liu", "Dingshuo Chen", "Jia Li", "Qiang Liu", "Yu Rong", "Liang Wang", "Tong-yi Zhang", "Jeffrey Xu Yu"], "published_date": "2025-05-22", "title_zh": "人工智慧時代的材料生成：一份綜合綜述", "summary_zh": "材料是現代社會的基石，推動能源、電子、醫療、運輸和基礎設施的發展。 具備發現和設計具備客製化屬性新材料的能力，對於解決緊迫的全球挑戰至關重要。 近年來，高品質材料數據的普及與人工智慧的快速發展，為加速材料發現創造了新機遇。 數據驅動的生成模型透過直接創造滿足預定義屬性要求的新材料，為材料設計提供強大工具。 儘管相關研究不斷湧現，但仍缺乏最新且系統性的綜述。 本文旨在彌合此差距，全面概述人工智慧驅動材料生成的最新進展。 首先，組織不同類型的材料，並闡明晶體材料的多種表示方式。 接著，詳細總結和分類當前人工智慧驅動的材料生成方法。 此外，討論了常見的評估指標，並總結了開源程式碼和基準數據集。 最後，總結了這個快速發展領域中潛在的未來方向和挑戰。 相關資源可在https://github.com/ZhixunLEE/Awesome-AI-for-Materials-Generation找到。", "audio": "audios/2505.16379v1.mp3", "timestamp": "2025-05-24T17:15:01.979017"}
{"query": "AI", "id": "2505.16366v1", "url": "http://arxiv.org/abs/2505.16366v1", "title": "ReCopilot: Reverse Engineering Copilot in Binary Analysis", "summary": "Binary analysis plays a pivotal role in security domains such as malware\ndetection and vulnerability discovery, yet it remains labor-intensive and\nheavily reliant on expert knowledge. General-purpose large language models\n(LLMs) perform well in programming analysis on source code, while\nbinaryspecific LLMs are underexplored. In this work, we present ReCopilot, an\nexpert LLM designed for binary analysis tasks. ReCopilot integrates binary code\nknowledge through a meticulously constructed dataset, encompassing continue\npretraining (CPT), supervised fine-tuning (SFT), and direct preference\noptimization (DPO) stages. It leverages variable data flow and call graph to\nenhance context awareness and employs test-time scaling to improve reasoning\ncapabilities. Evaluations on a comprehensive binary analysis benchmark\ndemonstrate that ReCopilot achieves state-of-the-art performance in tasks such\nas function name recovery and variable type inference on the decompiled pseudo\ncode, outperforming both existing tools and LLMs by 13%. Our findings highlight\nthe effectiveness of domain-specific training and context enhancement, while\nalso revealing challenges in building super long chain-of-thought. ReCopilot\nrepresents a significant step toward automating binary analysis with\ninterpretable and scalable AI assistance in this domain.", "authors": ["Guoqiang Chen", "Huiqi Sun", "Daguang Liu", "Zhiqi Wang", "Qiang Wang", "Bin Yin", "Lu Liu", "Lingyun Ying"], "published_date": "2025-05-22", "title_zh": "ReCopilot：二進位分析中Copilot的反向工程", "summary_zh": "二進制分析在惡意軟體檢測和漏洞發現等安全領域至關重要，但仍需大量人力且依賴專家知識。通用大型語言模型（LLM）在原始碼程式分析中表現出色，而針對二進制的LLM則有待探索。本文提出ReCopilot，一款專為二進制分析設計的專家LLM。ReCopilot通過精心構建的數據集整合二進制程式碼知識，包括持續預訓練（CPT）、監督式微調（SFT）和直接偏好優化（DPO）階段。它利用變數數據流和調用圖來增強上下文感知，並採用測試時縮放來提高推理能力。在全面的二進制分析基準測試中，ReCopilot在函數名稱恢復和變數類型推斷等任務上表現出色，超越現有工具和LLM達13%。研究結果強調了領域特定訓練和上下文增強的有效性，同時揭示了構建超長思維鏈的挑戰。ReCopilot代表著二進制分析自動化方面的一個重大進展，可在該領域提供可解釋且可擴展的AI輔助。", "audio": "audios/2505.16366v1.mp3", "timestamp": "2025-05-24T18:23:17.108003"}
{"query": "AI", "id": "2505.16358v1", "url": "http://arxiv.org/abs/2505.16358v1", "title": "Strategic Content Creation in the Age of GenAI: To Share or Not to Share?", "summary": "We introduce a game-theoretic framework examining strategic interactions\nbetween a platform and its content creators in the presence of AI-generated\ncontent. Our model's main novelty is in capturing creators' dual strategic\ndecisions: The investment in content quality and their (possible) consent to\nshare their content with the platform's GenAI, both of which significantly\nimpact their utility. To incentivize creators, the platform strategically\nallocates a portion of its GenAI-driven revenue to creators who share their\ncontent. We focus on the class of full-sharing equilibrium profiles, in which\nall creators willingly share their content with the platform's GenAI system.\nSuch equilibria are highly desirable both theoretically and practically. Our\nmain technical contribution is formulating and efficiently solving a novel\noptimization problem that approximates the platform's optimal revenue subject\nto inducing a full-sharing equilibrium. A key aspect of our approach is\nidentifying conditions under which full-sharing equilibria exist and a\nsurprising connection to the Prisoner's Dilemma. Finally, our simulations\ndemonstrate how revenue-allocation mechanisms affect creator utility and the\nplatform's revenue.", "authors": ["Gur Keinan", "Omer Ben-Porat"], "published_date": "2025-05-22", "title_zh": "生成式人工智慧時代的策略性內容創作：分享還是不分享？", "summary_zh": "本研究提出一個博弈論框架，分析平台與內容創作者在AI生成內容存在下的策略互動。模型創新之處在於捕捉創作者的雙重策略決策：內容品質的投資以及是否同意將內容分享給平台的生成式AI，兩者均顯著影響其效用。為激勵創作者，平台將其生成式AI產生的部分收入策略性地分配給分享內容的創作者。研究重點關注全共享均衡，即所有創作者都願意與平台的生成式AI系統分享內容。此類均衡在理論和實踐上都非常理想。主要技術貢獻在於構建並有效解決一個新穎的優化問題，該問題近似於平台在誘導全共享均衡下的最佳收入。研究方法的關鍵是識別全共享均衡存在的條件，以及與囚徒困境的驚人聯繫。模擬結果展示了收入分配機制如何影響創作者效用和平台收入。", "audio": "audios/2505.16358v1.mp3", "timestamp": "2025-05-24T19:13:51.699335"}
{"query": "AI", "id": "2505.16350v1", "url": "http://arxiv.org/abs/2505.16350v1", "title": "Sensing-Enhanced Handover Criterion for Low-Altitude Wireless Networks (LAWNs)", "summary": "With the rapid growth of the low-altitude economy, the demand for\ncellular-enabled low-altitude wireless networks (LAWNs) is rising\nsignificantly. The three-dimensional mobility of unmanned aerial vehicles\n(UAVs) will lead to frequent handovers (HOs) in cellular networks, while\ntraditional reference signal received power (RSRP)-based criteria may fail to\ncapture the dynamic environment, causing redundant HOs or HO failures. To\naddress this issue and motivated by the underutilization of sensing information\nin conventional HO mechanisms, we propose a novel HO activation criterion for\nUAV systems that integrates both sensing parameters provided by integrated\nsensing and communication (ISAC) signals and RSRP. First, we construct an ISAC\nsignal model tailored for low-altitude scenarios and derive the Cram\\'er-Rao\nlower bound for sensing distance estimation. Subsequently, we propose a novel\njoint HO criterion that extends the conventional RSRP-based method by\nintegrating sensing information from ISAC signals, enabling more reliable HOs\nin dynamic UAV environments. Simulation results show that the joint HO\ncriterion outperforms the baseline RSRP-based criterion under different\nsignal-to-noise ratio (SNR) and sensing pilot ratio conditions. Particularly,\nwhen SNR is greater than 0dB and the sensing pilot ratio is 20%, the proposed\njoint HO criterion reduces the average HO region length by 49.97% and improves\nthe activation probability by 76.31%.", "authors": ["Jingli Li", "Yiyan Ma", "Bo Ai", "Qingqing Cheng", "Guoyu Ma", "Mi Yang", "Yunlong Lu", "Wenwei Yue", "Zhangdui Zhong"], "published_date": "2025-05-22", "title_zh": "低空無線網路中基於感知增強的切換準則", "summary_zh": "隨著低空經濟快速發展，對蜂巢式低空無線網路的需求顯著增加。無人機的三維移動性導致蜂巢網路中頻繁切換，傳統基於參考信號接收功率的準則難以應對動態環境，造成冗餘切換或切換失敗。為了解決此問題，並基於傳統切換機制對感知資訊利用不足的考量，本文提出一種新的無人機系統切換啟動準則，整合整合感知與通信訊號提供的感知參數和參考信號接收功率。首先，構建適用於低空場景的整合感知與通信訊號模型，並推導感知距離估計的Cramér-Rao下界。隨後，提出一種聯合切換準則，通過整合整合感知與通信訊號的感知資訊來擴展傳統基於參考信號接收功率的方法，從而在動態無人機環境中實現更可靠的切換。模擬結果表明，在不同的信噪比和感知導頻比條件下，聯合切換準則優於基於參考信號接收功率的基準準則。特別是，當信噪比大於0dB且感知導頻比為20%時，所提出的聯合切換準則將平均切換區域長度減少49.97%，並將啟動概率提高76.31%。", "audio": "audios/2505.16350v1.mp3", "timestamp": "2025-05-24T20:18:58.808084"}
{"query": "AI", "id": "2505.16339v1", "url": "http://arxiv.org/abs/2505.16339v1", "title": "Rethinking Code Review Workflows with LLM Assistance: An Empirical Study", "summary": "Code reviews are a critical yet time-consuming aspect of modern software\ndevelopment, increasingly challenged by growing system complexity and the\ndemand for faster delivery. This paper presents a study conducted at\nWirelessCar Sweden AB, combining an exploratory field study of current code\nreview practices with a field experiment involving two variations of an\nLLM-assisted code review tool. The field study identifies key challenges in\ntraditional code reviews, including frequent context switching, insufficient\ncontextual information, and highlights both opportunities (e.g., automatic\nsummarization of complex pull requests) and concerns (e.g., false positives and\ntrust issues) in using LLMs. In the field experiment, we developed two\nprototype variations: one offering LLM-generated reviews upfront and the other\nenabling on-demand interaction. Both utilize a semantic search pipeline based\non retrieval-augmented generation to assemble relevant contextual information\nfor the review, thereby tackling the uncovered challenges. Developers evaluated\nboth variations in real-world settings: AI-led reviews are overall more\npreferred, while still being conditional on the reviewers' familiarity with the\ncode base, as well as on the severity of the pull request.", "authors": ["Fannar Steinn Aðalsteinsson", "Björn Borgar Magnússon", "Mislav Milicevic", "Adam Nirving Davidsson", "Chih-Hong Cheng"], "published_date": "2025-05-22", "title_zh": "以大型語言模型輔助重新思考程式碼審查流程：一項實證研究", "summary_zh": "程式碼審查是現代軟體開發的關鍵環節，但耗時且日益受到系統複雜性及快速交付需求挑戰。本研究於 WirelessCar Sweden AB 進行，結合了現有程式碼審查實務的探索性實地研究，以及涉及兩個 LLM 輔助程式碼審查工具變體的實地實驗。實地研究指出傳統程式碼審查中的主要挑戰，包括頻繁的上下文切換和上下文資訊不足，並強調了使用 LLM 的機會（如複雜提取請求的自動摘要）和疑慮（如誤報和信任問題）。在實地實驗中，我們開發了兩個原型變體：一個預先提供 LLM 生成的審查，另一個支援隨需互動。兩者都利用基於檢索增強生成的語義搜索管道來組裝相關的上下文資訊以進行審查，從而應對所發現的挑戰。開發人員在實際環境中評估了這兩個變體：AI 主導的審查總體上更受歡迎，但仍取決於審查者對程式碼庫的熟悉程度以及提取請求的嚴重程度。", "audio": "audios/2505.16339v1.mp3", "timestamp": "2025-05-24T21:16:17.349955"}
{"query": "AI", "id": "2505.16327v1", "url": "http://arxiv.org/abs/2505.16327v1", "title": "Cooperative NOMA Meets Emerging Technologies: A Survey for Next-Generation Wireless Networks", "summary": "The emerging demands of sixth-generation wireless networks, such as\nultra-connectivity, native intelligence, and cross-domain convergence, are\nbringing renewed focus to cooperative non-orthogonal multiple access (C-NOMA)\nas a fundamental enabler of scalable, efficient, and intelligent communication\nsystems. C-NOMA builds on the core benefits of NOMA by leveraging user\ncooperation and relay strategies to enhance spectral efficiency, coverage, and\nenergy performance. This article presents a unified and forward-looking survey\non the integration of C-NOMA with key enabling technologies, including radio\nfrequency energy harvesting, cognitive radio networks, reconfigurable\nintelligent surfaces, space-air-ground integrated networks, and integrated\nsensing and communication-assisted semantic communication. Foundational\nprinciples and relaying protocols are first introduced to establish the\ntechnical relevance of C-NOMA. Then, a focused investigation is conducted into\nprotocol-level synergies, architectural models, and deployment strategies\nacross these technologies. Beyond integration, this article emphasizes the\norchestration of C-NOMA across future application domains such as digital\ntwins, extended reality, and e-health. In addition, it provides an extensive\nand in-depth review of recent literature, categorized by relaying schemes,\nsystem models, performance metrics, and optimization paradigms, including\nmodel-based, heuristic, and AI-driven approaches. Finally, open challenges and\nfuture research directions are outlined, spanning standardization, security,\nand cross-layer design, positioning C-NOMA as a key pillar of intelligent\nnext-generation network architectures.", "authors": ["Mahmoud M. Salim", "Suhail I. Al-Dharrab", "Daniel Benevides Da Costa", "Ali H. Muqaibel"], "published_date": "2025-05-22", "title_zh": "合作式非正交多重接取與新興技術：下一代無線網路綜述", "summary_zh": "第六代無線網路對超連接、原生智慧和跨域融合的需求，使協作式非正交多重接取（C-NOMA）重新受到重視，成為可擴展、高效和智慧通訊系統的基石。C-NOMA利用使用者協作和中繼策略，在NOMA的基礎上提升頻譜效率、覆蓋範圍和能源效能。本文全面前瞻地探討了C-NOMA與射頻能量收集、認知無線電網路、可重構智慧表面、空天地一體化網路和整合感知通訊輔助的語義通訊等關鍵技術的整合，闡述了C-NOMA的基礎原理和中繼協定，並深入研究了這些技術間的協同效應、架構模型和部署策略。除整合外，本文還強調C-NOMA在數位分身、擴增實境和電子醫療等未來應用領域的協調作用。此外，本文廣泛且深入地回顧了近年文獻，按中繼方案、系統模型、效能指標和優化範例（包括基於模型、啟發式和人工智慧驅動的方法）進行分類。最後，概述了標準化、安全性和跨層設計等方面的開放挑戰和未來研究方向，將C-NOMA定位為智慧型下一代網路架構的關鍵支柱。", "audio": "audios/2505.16327v1.mp3", "timestamp": "2025-05-24T22:16:35.597537"}
{"query": "AI", "id": "2505.16319v1", "url": "http://arxiv.org/abs/2505.16319v1", "title": "FreshRetailNet-50K: A Stockout-Annotated Censored Demand Dataset for Latent Demand Recovery and Forecasting in Fresh Retail", "summary": "Accurate demand estimation is critical for the retail business in guiding the\ninventory and pricing policies of perishable products. However, it faces\nfundamental challenges from censored sales data during stockouts, where\nunobserved demand creates systemic policy biases. Existing datasets lack the\ntemporal resolution and annotations needed to address this censoring effect. To\nfill this gap, we present FreshRetailNet-50K, the first large-scale benchmark\nfor censored demand estimation. It comprises 50,000 store-product time series\nof detailed hourly sales data from 898 stores in 18 major cities, encompassing\n863 perishable SKUs meticulously annotated for stockout events. The hourly\nstock status records unique to this dataset, combined with rich contextual\ncovariates, including promotional discounts, precipitation, and temporal\nfeatures, enable innovative research beyond existing solutions. We demonstrate\none such use case of two-stage demand modeling: first, we reconstruct the\nlatent demand during stockouts using precise hourly annotations. We then\nleverage the recovered demand to train robust demand forecasting models in the\nsecond stage. Experimental results show that this approach achieves a 2.73\\%\nimprovement in prediction accuracy while reducing the systematic demand\nunderestimation from 7.37\\% to near-zero bias. With unprecedented temporal\ngranularity and comprehensive real-world information, FreshRetailNet-50K opens\nnew research directions in demand imputation, perishable inventory\noptimization, and causal retail analytics. The unique annotation quality and\nscale of the dataset address long-standing limitations in retail AI, providing\nimmediate solutions and a platform for future methodological innovation. The\ndata (https://huggingface.co/datasets/Dingdong-Inc/FreshRetailNet-50K) and code\n(https://github.com/Dingdong-Inc/frn-50k-baseline}) are openly released.", "authors": ["Yangyang Wang", "Jiawei Gu", "Li Long", "Xin Li", "Li Shen", "Zhouyu Fu", "Xiangjun Zhou", "Xu Jiang"], "published_date": "2025-05-22", "title_zh": "生鮮零售網絡-50K：一個具缺貨標註的截斷需求資料集，用於生鮮零售中潛在需求恢復與預測", "summary_zh": "生鮮零售業需精準預測需求以制定庫存和定價策略，然缺貨時的銷售數據受限，造成策略偏差。現有數據集缺乏足夠的時間解析度和標註以解決此問題。為此，我們推出FreshRetailNet-50K，首個大規模受限需求估算基準，包含來自18個主要城市898家商店的5萬條商品時序資料，涵蓋863個生鮮商品，並針對缺貨事件進行詳盡標註。此數據集獨有的時級庫存狀態記錄，結合促銷折扣、降水和時間特徵等豐富的上下文變數，促進超越現有解決方案的創新研究。我們展示了兩階段需求建模的應用：首先，使用精確的時級標註重建缺貨期間的潛在需求；然後，利用恢復的需求訓練穩健的需求預測模型。實驗結果表明，此方法可提高2.73%的預測準確度，並將系統性的需求低估從7.37%降低至接近零偏差。FreshRetailNet-50K以前所未有的時間粒度和全面的真實世界信息，開闢了需求填補、生鮮庫存優化和因果零售分析的新研究方向。該數據集獨特的標註質量和規模解決了零售人工智慧長期存在的局限性，提供即時解決方案和未來方法創新的平台。數據和程式碼已公開發布。", "audio": "audios/2505.16319v1.mp3", "timestamp": "2025-05-24T23:16:53.721641"}
{"query": "AI", "id": "2505.16314v1", "url": "http://arxiv.org/abs/2505.16314v1", "title": "NTIRE 2025 challenge on Text to Image Generation Model Quality Assessment", "summary": "This paper reports on the NTIRE 2025 challenge on Text to Image (T2I)\ngeneration model quality assessment, which will be held in conjunction with the\nNew Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2025.\nThe aim of this challenge is to address the fine-grained quality assessment of\ntext-to-image generation models. This challenge evaluates text-to-image models\nfrom two aspects: image-text alignment and image structural distortion\ndetection, and is divided into the alignment track and the structural track.\nThe alignment track uses the EvalMuse-40K, which contains around 40K\nAI-Generated Images (AIGIs) generated by 20 popular generative models. The\nalignment track has a total of 371 registered participants. A total of 1,883\nsubmissions are received in the development phase, and 507 submissions are\nreceived in the test phase. Finally, 12 participating teams submitted their\nmodels and fact sheets. The structure track uses the EvalMuse-Structure, which\ncontains 10,000 AI-Generated Images (AIGIs) with corresponding structural\ndistortion mask. A total of 211 participants have registered in the structure\ntrack. A total of 1155 submissions are received in the development phase, and\n487 submissions are received in the test phase. Finally, 8 participating teams\nsubmitted their models and fact sheets. Almost all methods have achieved better\nresults than baseline methods, and the winning methods in both tracks have\ndemonstrated superior prediction performance on T2I model quality assessment.", "authors": ["Shuhao Han", "Haotian Fan", "Fangyuan Kong", "Wenjie Liao", "Chunle Guo", "Chongyi Li", "Radu Timofte", "Liang Li", "Tao Li", "Junhui Cui", "Yunqiu Wang", "Yang Tai", "Jingwei Sun", "Jianhui Sun", "Xinli Yue", "Tianyi Wang", "Huan Hou", "Junda Lu", "Xinyang Huang", "Zitang Zhou", "Zijian Zhang", "Xuhui Zheng", "Xuecheng Wu", "Chong Peng", "Xuezhi Cao", "Trong-Hieu Nguyen-Mau", "Minh-Hoang Le", "Minh-Khoa Le-Phan", "Duy-Nam Ly", "Hai-Dang Nguyen", "Minh-Triet Tran", "Yukang Lin", "Yan Hong", "Chuanbiao Song", "Siyuan Li", "Jun Lan", "Zhichao Zhang", "Xinyue Li", "Wei Sun", "Zicheng Zhang", "Yunhao Li", "Xiaohong Liu", "Guangtao Zhai", "Zitong Xu", "Huiyu Duan", "Jiarui Wang", "Guangji Ma", "Liu Yang", "Lu Liu", "Qiang Hu", "Xiongkuo Min", "Zichuan Wang", "Zhenchen Tang", "Bo Peng", "Jing Dong", "Fengbin Guan", "Zihao Yu", "Yiting Lu", "Wei Luo", "Xin Li", "Minhao Lin", "Haofeng Chen", "Xuanxuan He", "Kele Xu", "Qisheng Xu", "Zijian Gao", "Tianjiao Wan", "Bo-Cheng Qiu", "Chih-Chung Hsu", "Chia-ming Lee", "Yu-Fan Lin", "Bo Yu", "Zehao Wang", "Da Mu", "Mingxiu Chen", "Junkang Fang", "Huamei Sun", "Wending Zhao", "Zhiyu Wang", "Wang Liu", "Weikang Yu", "Puhong Duan", "Bin Sun", "Xudong Kang", "Shutao Li", "Shuai He", "Lingzhi Fu", "Heng Cong", "Rongyu Zhang", "Jiarong He", "Zhishan Qiao", "Yongqing Huang", "Zewen Chen", "Zhe Pang", "Juan Wang", "Jian Guo", "Zhizhuo Shao", "Ziyu Feng", "Bing Li", "Weiming Hu", "Hesong Li", "Dehua Liu", "Zeming Liu", "Qingsong Xie", "Ruichen Wang", "Zhihao Li", "Yuqi Liang", "Jianqi Bi", "Jun Luo", "Junfeng Yang", "Can Li", "Jing Fu", "Hongwei Xu", "Mingrui Long", "Lulin Tang"], "published_date": "2025-05-22", "title_zh": "NTIRE 2025文本到圖像生成模型品質評估挑戰賽", "summary_zh": "本研究報告 NTIRE 2025 文字生成圖像模型品質評估挑戰賽，該挑戰賽將於 CVPR 2025 的 NTIRE 工作坊同期舉行。旨在解決文字生成圖像模型之精細品質評估問題，從圖像-文字對齊及圖像結構失真偵測兩方面評估模型，分為對齊及結構兩賽道。對齊賽道使用 EvalMuse-40K 數據集，包含約四萬張由二十個主流生成模型產生之 AI 圖像，共 371 名註冊參與者，開發階段收到 1883 份提交，測試階段收到 507 份提交，最終 12 支隊伍提交模型及說明文件。結構賽道使用 EvalMuse-Structure 數據集，包含一萬張 AI 圖像及其結構失真遮罩，共 211 名註冊參與者，開發階段收到 1155 份提交，測試階段收到 487 份提交，最終 8 支隊伍提交模型及說明文件。多數方法優於基準方法，兩賽道獲勝者均展現出優異的 T2I 模型品質評估預測性能。", "audio": "audios/2505.16314v1.mp3", "timestamp": "2025-05-25T01:41:07.302050"}
{"query": "AI", "id": "2505.16301v1", "url": "http://arxiv.org/abs/2505.16301v1", "title": "Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space", "summary": "Molecular dynamics (MD) is a powerful tool for exploring the behavior of\natomistic systems, but its reliance on sequential numerical integration limits\nsimulation efficiency. We present MDtrajNet-1, a foundational AI model that\ndirectly generates MD trajectories across chemical space, bypassing force\ncalculations and integration. This approach accelerates simulations by up to\ntwo orders of magnitude compared to traditional MD, even those enhanced by\nmachine-learning interatomic potentials. MDtrajNet-1 combines equivariant\nneural networks with a Transformer-based architecture to achieve strong\naccuracy and transferability in predicting long-time trajectories for both\nknown and unseen systems. Remarkably, the errors of the trajectories generated\nby MDtrajNet-1 for various molecular systems are close to those of the\nconventional ab initio MD. The model's flexible design supports diverse\napplication scenarios, including different statistical ensembles, boundary\nconditions, and interaction types. By overcoming the intrinsic speed barrier of\nconventional MD, MDtrajNet-1 opens new frontiers in efficient and scalable\natomistic simulations.", "authors": ["Fuchun Ge", "Pavlo O. Dral"], "published_date": "2025-05-22", "title_zh": "化學空間分子動力學直接預測人工智慧方法", "summary_zh": "分子動力學是研究原子系統行為的有力工具，但其依賴序列數值積分限制了模擬效率。我們提出MDtrajNet-1，一個基礎AI模型，可直接生成化學空間中的分子動力學軌跡，繞過力計算和積分。相較於傳統分子動力學，即使是經過機器學習原子間勢強化的，此方法也能加速模擬達兩個數量級。MDtrajNet-1結合了等變神經網路與基於Transformer的架構，在預測已知和未知系統的長時間軌跡方面實現了高準確性和可轉移性。值得注意的是，MDtrajNet-1生成的各種分子系統軌跡誤差，與傳統從頭算分子動力學相近。該模型靈活的設計支持多樣化的應用場景，包括不同的統計系綜、邊界條件和交互類型。通過克服傳統分子動力學固有的速度障礙，MDtrajNet-1在高效且可擴展的原子模擬中開闢了新領域。", "audio": "audios/2505.16301v1.mp3", "timestamp": "2025-05-25T03:19:05.363911"}
{"query": "AI", "id": "2505.16290v1", "url": "http://arxiv.org/abs/2505.16290v1", "title": "Multimodal Generative AI for Story Point Estimation in Software Development", "summary": "This research explores the application of Multimodal Generative AI to enhance\nstory point estimation in Agile software development. By integrating text,\nimage, and categorical data using advanced models like BERT, CNN, and XGBoost,\nour approach surpasses the limitations of traditional single-modal estimation\nmethods. The results demonstrate strong accuracy for simpler story points,\nwhile also highlighting challenges in more complex categories due to data\nimbalance. This study further explores the impact of categorical data,\nparticularly severity, on the estimation process, emphasizing its influence on\nmodel performance. Our findings emphasize the transformative potential of\nmultimodal data integration in refining AI-driven project management, paving\nthe way for more precise, adaptable, and domain-specific AI capabilities.\nAdditionally, this work outlines future directions for addressing data\nvariability and enhancing the robustness of AI in Agile methodologies.", "authors": ["Mohammad Rubyet Islam", "Peter Sandborn"], "published_date": "2025-05-22", "title_zh": "用於軟件開發中故事點估算的多模態生成式人工智能", "summary_zh": "本研究探討多模態生成式AI於敏捷軟體開發中提升故事點估算的應用。透過整合文本、圖像及類別資料，並採用BERT、CNN及XGBoost等模型，本方法優於傳統單模態估算。結果顯示，對於較簡單的故事點具有高度準確性，惟資料不平衡導致複雜類別面臨挑戰。研究進一步探討類別資料，特別是嚴重程度，對估算過程的影響，強調其對模型效能的影響。研究結果強調多模態資料整合在改進AI驅動專案管理中的變革潛力，為更精確、適應性強且領域特定的AI能力鋪路。此外，本研究亦概述未來方向，以解決資料變異性問題並增強AI在敏捷方法中的穩健性。", "audio": "audios/2505.16290v1.mp3", "timestamp": "2025-05-25T04:24:48.092301"}
{"query": "AI", "id": "2505.16278v1", "url": "http://arxiv.org/abs/2505.16278v1", "title": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "summary": "End-to-end autonomous driving (E2E-AD) demands effective processing of\nmulti-view sensory data and robust handling of diverse and complex driving\nscenarios, particularly rare maneuvers such as aggressive turns. Recent success\nof Mixture-of-Experts (MoE) architecture in Large Language Models (LLMs)\ndemonstrates that specialization of parameters enables strong scalability. In\nthis work, we propose DriveMoE, a novel MoE-based E2E-AD framework, with a\nScene-Specialized Vision MoE and a Skill-Specialized Action MoE. DriveMoE is\nbuilt upon our $\\pi_0$ Vision-Language-Action (VLA) baseline (originally from\nthe embodied AI field), called Drive-$\\pi_0$. Specifically, we add Vision MoE\nto Drive-$\\pi_0$ by training a router to select relevant cameras according to\nthe driving context dynamically. This design mirrors human driving cognition,\nwhere drivers selectively attend to crucial visual cues rather than\nexhaustively processing all visual information. In addition, we add Action MoE\nby training another router to activate specialized expert modules for different\ndriving behaviors. Through explicit behavioral specialization, DriveMoE is able\nto handle diverse scenarios without suffering from modes averaging like\nexisting models. In Bench2Drive closed-loop evaluation experiments, DriveMoE\nachieves state-of-the-art (SOTA) performance, demonstrating the effectiveness\nof combining vision and action MoE in autonomous driving tasks. We will release\nour code and models of DriveMoE and Drive-$\\pi_0$.", "authors": ["Zhenjie Yang", "Yilin Chai", "Xiaosong Jia", "Qifeng Li", "Yuqian Shao", "Xuekai Zhu", "Haisheng Su", "Junchi Yan"], "published_date": "2025-05-22", "title_zh": "DriveMoE：端到端自動駕駛中視覺-語言-動作模型的混合專家系統", "summary_zh": "端到端自動駕駛需有效處理多視角感知數據及應對複雜場景，特別是激進轉彎等罕見動作。混合專家模型(MoE)在大型語言模型(LLM)中的成功表明參數專精可實現強大的擴展性。本文提出DriveMoE，一種基於MoE的新型端到端自動駕駛框架，包含場景專精的視覺MoE和技能專精的動作MoE。DriveMoE基於Drive-$\\pi_0$（一個視覺-語言-動作基線模型），透過訓練路由選擇器動態選擇相關攝影機，為Drive-$\\pi_0$增加視覺MoE，模擬人類駕駛員選擇性關注關鍵視覺線索的認知方式。此外，透過訓練另一個路由選擇器激活不同駕駛行為的專家模組，增加動作MoE。透過行為專精，DriveMoE能處理多樣場景，避免模式平均化。在Bench2Drive閉環評估中，DriveMoE達到最先進的性能，證明視覺和動作MoE結合於自動駕駛任務中的有效性。我們將公開DriveMoE和Drive-$\\pi_0$的程式碼與模型。", "audio": "audios/2505.16278v1.mp3", "timestamp": "2025-05-25T05:17:45.009442"}
{"query": "AI", "id": "2505.16274v1", "url": "http://arxiv.org/abs/2505.16274v1", "title": "Multimodal AI-based visualization of strategic leaders' emotional dynamics: a deep behavioral analysis of Trump's trade war discourse", "summary": "This study investigates the emotional rhythms and behavioral mechanisms of\ndominant political leaders in strategic decision-making. Using the Trump\nadministration's 125 percent tariff hike on China as a case, it adopts a\nMultimodal Cognitive Behavioral Modeling framework. This includes\nmicro-expression tracking, acoustic intonation analysis, semantic flow\nmodeling, cognitive load simulation, and strategic behavior mapping to\nconstruct a full-cycle simulation of emotion, motivation, and output. Results\nreveal that Trump's decisions are not driven by rational deduction, but emerge\nfrom dominance-coherence rhythms. A six-axis National Strategic Tempo\nIntervention Framework is proposed to support anticipatory policy modeling.", "authors": ["Wei Meng"], "published_date": "2025-05-22", "title_zh": "基於多模態人工智慧的戰略領導者情緒動態視覺化：川普貿易戰論述之深度行為分析", "summary_zh": "本研究探討政治領袖在策略決策中的情緒節奏與行為機制。以特朗普政府對中國徵收125%關稅為例，採用多模態認知行為建模框架，包含微表情追蹤、語音語調分析、語義流建模、認知負荷模擬及策略行為映射，構建情緒、動機與產出的全週期模擬。結果顯示，特朗普的決策並非理性推導，而是源於支配-連貫節奏。據此提出六軸國家戰略節奏干預框架，以支持預測性政策建模。", "audio": "audios/2505.16274v1.mp3", "timestamp": "2025-05-25T06:24:54.128354"}
{"query": "AI", "id": "2505.16263v1", "url": "http://arxiv.org/abs/2505.16263v1", "title": "All You Need is \"Leet\": Evading Hate-speech Detection AI", "summary": "Social media and online forums are increasingly becoming popular.\nUnfortunately, these platforms are being used for spreading hate speech. In\nthis paper, we design black-box techniques to protect users from hate-speech on\nonline platforms by generating perturbations that can fool state of the art\ndeep learning based hate speech detection models thereby decreasing their\nefficiency. We also ensure a minimal change in the original meaning of\nhate-speech. Our best perturbation attack is successfully able to evade\nhate-speech detection for 86.8 % of hateful text.", "authors": ["Sampanna Yashwant Kahu", "Naman Ahuja"], "published_date": "2025-05-22", "title_zh": "只需「駭客語」：規避仇恨言論偵測人工智慧", "summary_zh": "社群媒體與線上論壇日益普及，卻也成為仇恨言論的溫床。本文設計黑箱技術，透過生成擾動以欺騙最先進的深度學習仇恨言論檢測模型，降低其效率，從而保護使用者免受網路平台上的仇恨言論侵害。我們同時確保原始仇恨言論的語義變動最小。最佳擾動攻擊成功規避了86.8%仇恨文本的檢測。", "audio": "audios/2505.16263v1.mp3", "timestamp": "2025-05-25T07:16:14.534239"}
{"query": "AI", "id": "2505.16254v1", "url": "http://arxiv.org/abs/2505.16254v1", "title": "Reassessing Collaborative Writing Theories and Frameworks in the Age of LLMs: What Still Applies and What We Must Leave Behind", "summary": "In this paper, we conduct a critical review of existing theories and\nframeworks on human-human collaborative writing to assess their relevance to\nthe current human-AI paradigm in professional contexts, and draw seven insights\nalong with design implications for human-AI collaborative writing tools. We\nfound that, as LLMs nudge the writing process more towards an empirical \"trial\nand error\" process analogous to prototyping, the non-linear cognitive process\nof writing will stay the same, but more rigor will be required for revision\nmethodologies. This shift would shed further light on the importance of\ncoherence support, but the large language model (LLM)'s unprecedented semantic\ncapabilities can bring novel approaches to this ongoing challenge. We argue\nthat teamwork-related factors such as group awareness, consensus building and\nauthorship - which have been central in human-human collaborative writing\nstudies - should not apply to the human-AI paradigm due to excessive\nanthropomorphism. With the LLM's text generation capabilities becoming\nessentially indistinguishable from human-written ones, we are entering an era\nwhere, for the first time in the history of computing, we are engaging in\ncollaborative writing with AI at workplaces on a daily basis. We aim to bring\ntheoretical grounding and practical design guidance to the interaction designs\nof human-AI collaborative writing, with the goal of enhancing future human-AI\nwriting software.", "authors": ["Daisuke Yukita", "Tim Miller", "Joel Mackenzie"], "published_date": "2025-05-22", "title_zh": "大型語言模型時代下協作寫作理論與框架之再評估：何者適用，何者應捨棄", "summary_zh": "本文 критично地回顧現有的人際協同寫作理論與框架，評估其在當前人機協作範式下的適用性，並提出七項見解及人機協作寫作工具的設計意涵。研究發現，大型語言模型將寫作過程推向更偏向實驗性的「試錯」模式，類似原型設計，寫作的非線性認知過程將保持不變，但修訂方法需更嚴謹。此轉變突顯一致性支援的重要性，而大型語言模型的前所未有的語義能力可為此挑戰帶來新方法。由於過度擬人化，團隊合作因素，如群體意識、共識建立和作者身份，不應適用於人機協作範式。隨著大型語言模型的文本生成能力與人類書寫難以區分，我們正進入一個在工作場所每天與人工智慧進行協同寫作的時代。本文旨在為人機協同寫作的互動設計提供理論基礎與實用設計指導，以提升未來人機寫作軟體。", "audio": "audios/2505.16254v1.mp3", "timestamp": "2025-05-25T08:21:47.398343"}
{"query": "AI", "id": "2505.16809v2", "url": "http://arxiv.org/abs/2505.16809v2", "title": "Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor Segmentation with Missing Modalities", "summary": "Existing methods for multimodal MRI segmentation with missing modalities\ntypically assume that all MRI modalities are available during training.\nHowever, in clinical practice, some modalities may be missing due to the\nsequential nature of MRI acquisition, leading to performance degradation.\nFurthermore, retraining models to accommodate newly available modalities can be\ninefficient and may cause overfitting, potentially compromising previously\nlearned knowledge. To address these challenges, we propose Replay-based\nHypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation\nwith missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to\nenable the segmentation model to learn from newly acquired MRI modalities\nwithout forgetting previously learned information. To enhance segmentation\nperformance across diverse patient scenarios, we introduce the Cross-Patient\nHypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture\nhigh-order associations between patients. Additionally, we incorporate\nTversky-Aware Contrastive (TAC) loss to effectively mitigate information\nimbalance both across and within different modalities. Extensive experiments on\nthe BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art\nmethods, achieving an improvement of over 2% in the Dice Similarity Coefficient\nacross various tumor regions.", "authors": ["Junze Wang", "Lei Fan", "Weipeng Jing", "Donglin Di", "Yang Song", "Sidong Liu", "Cong Cong"], "published_date": "2025-05-22", "title_zh": "超圖Tversky感知的領域增量學習用於缺失模態腦腫瘤分割", "summary_zh": "現有缺失模態多模態MRI分割方法通常假設訓練期間所有模態均可用，但臨床實務中，MRI採集的序列特性可能導致部分模態缺失，進而降低效能。重新訓練模型以適應新增模態效率低下，並可能導致過擬合，損害既有知識。為解決這些問題，我們提出基於重播的超圖域增量學習(ReHyDIL)用於缺失模態腦瘤分割。ReHyDIL利用域增量學習(DIL)使分割模型能從新獲取的MRI模態中學習，同時不遺忘既有資訊。為提升跨患者情境的分割效能，我們引入跨患者超圖分割網路(CHSNet)，利用超圖捕捉患者之間的高階關聯。此外，我們採用Tversky感知對比(TAC)損失，有效緩解不同模態之間和內部的資訊失衡。在BraTS2019資料集上的實驗表明，ReHyDIL優於現有方法，在各腫瘤區域的Dice相似係數上提高了2%以上。", "audio": "audios/2505.16809v2.mp3", "timestamp": "2025-05-26T01:28:04.799880"}
{"query": "Foundation Model", "id": "2505.16941v2", "url": "http://arxiv.org/abs/2505.16941v2", "title": "FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records", "summary": "Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models.", "authors": ["Chao Pang", "Vincent Jeanselme", "Young Sang Choi", "Xinzhuo Jiang", "Zilin Jing", "Aparajita Kashyap", "Yuta Kobayashi", "Yanwei Li", "Florent Pollet", "Karthik Natarajan", "Shalmali Joshi"], "published_date": "2025-05-22", "title_zh": "FoMoH：結構化電子健康紀錄臨床意義基礎模型評估", "summary_zh": "基於其提取與下游任務無關之表徵能力，基礎模型在醫療保健領域展現巨大潛力。此特性已使基於結構化電子健康記錄數據訓練的模型，在多個臨床應用中達到頂尖效能，即使在標記數據有限的情況下亦然，而此為醫療保健領域常見挑戰。然而，由於缺乏全面且具意義之任務需求，以及足夠多樣化的評估以表徵其相較於傳統監督式學習之優勢，對於這些模型之臨床效用潛力，共識甚少。為了解決此差距，我們提出一套涵蓋患者預後、急慢性疾病早期預測等臨床意義任務，包括穩健評估需求。我們使用來自哥倫比亞大學歐文醫學中心（CUMC）五百萬患者之電子健康記錄數據，在14項臨床相關任務中評估頂尖基礎模型。我們衡量總體準確度、校準度和亞群效能，以揭示基於預訓練、符號化和數據表示策略選擇之權衡。本研究旨在推進結構化電子健康記錄基礎模型之實證評估，並指導未來醫療保健基礎模型之開發。", "audio": "audios/2505.16941v2.mp3", "timestamp": "2025-05-26T01:28:14.452300"}
{"query": "Diffusion Model", "id": "2505.16839v2", "url": "http://arxiv.org/abs/2505.16839v2", "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding", "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.", "authors": ["Shufan Li", "Konstantinos Kallidromitis", "Hritik Bansal", "Akash Gokul", "Yusuke Kato", "Kazuki Kozuka", "Jason Kuen", "Zhe Lin", "Kai-Wei Chang", "Aditya Grover"], "published_date": "2025-05-22", "title_zh": "LaViDa：用於多模態理解的大型擴散語言模型", "summary_zh": "現代視覺語言模型（VLMs）能解決多種視覺推理任務。實際應用中，快速推論和可控生成（如限制輸出格式）至關重要，但現有自迴歸（AR）VLMs如LLaVA難以兼顧。離散擴散模型（DMs）提供另一選項，其平行解碼可加速推論，雙向上下文則能透過文本填充實現可控生成。雖DMs於純語言環境有效，但其多模態潛力尚未充分開發。我們提出LaViDa，一基於DMs的VLM家族。LaViDa透過為DMs配備視覺編碼器，並聯合微調以進行多模態指令跟隨。為解決挑戰，LaViDa採用互補遮罩以提升訓練效果、前綴KV快取以提升推論效率，以及時間步長位移以產生高品質樣本。實驗表明，LaViDa在MMMU等多模態基準測試上達到與AR VLMs匹敵甚至更優異的性能，同時具備DMs的獨特優勢，包含靈活的速度-品質權衡、可控性及雙向推理能力。在COCO圖像描述任務中，LaViDa以1.92倍的速度提升超越Open-LLaVa-Next-8B達+4.1 CIDEr。在雙向任務中，其於受限詩歌補全上實現+59%的提升。這些結果證明LaViDa是AR VLMs的有力替代方案。程式碼和模型將於最終版本發布。", "audio": "audios/2505.16839v2.mp3", "timestamp": "2025-05-26T01:28:27.137952"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論不一致性不會阻礙構建負責任人工智慧系統之路", "summary_zh": "本立場文件主張，應將責任式AI (RAI) 指標間常見的理論不一致性，如不同的公平性定義或準確性與隱私之間的權衡，視為寶貴特徵而非缺陷。將指標視為不同目標可帶來三項益處：(1) 規範多元主義：維護全套潛在衝突的指標，確保RAI中固有的多元道德立場和利害關係人價值觀獲得充分代表。(2) 知識完整性：使用多個、有時衝突的指標，能更全面地捕捉多面向的倫理概念，保有比單一簡化定義更高的資訊保真度。(3) 隱式正則化：聯合優化理論上衝突的目標可抑制過度擬合特定指標，引導模型朝向在現實複雜性下具有更強泛化和穩健性的解決方案。相反，簡化或刪減指標以強化理論一致性的做法，可能限縮價值多元性、喪失概念深度並降低模型效能。因此，我們提倡RAI理論與實踐的轉變：從受困於不一致性，轉向描述可接受的不一致性閾值，並闡明允許實踐中穩健且近似一致性的機制。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T03:15:39.391462"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具語義目標感知表徵的基礎表格模型", "summary_zh": "深度學習在多個領域表現卓越，但在表格學習任務上效果不如梯度提升決策樹。近年來，表格基礎模型興起，能利用真實世界知識並泛化至不同數據集，尤其在數據包含自由文本時。儘管將語言模型融入表格任務已有研究，但現有方法多採用靜態、與目標無關的文本表示，限制了其效能。本文提出TabSTAR：具備語義目標感知表示的表格基礎模型。TabSTAR旨在實現具文本特徵表格數據的遷移學習，其架構不含數據集特定參數。TabSTAR解凍預訓練文本編碼器，並將目標標記作為輸入，使模型能學習任務特定嵌入。TabSTAR在具文本特徵的分類任務基準測試中，於中大型數據集上皆達到最佳效能，且其預訓練階段展現出數據集數量的比例定律，為進一步提升效能提供途徑。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T03:15:47.099074"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "反應擴散模型、族群動態及疫情擴散之隨機基於主體蒙地卡羅模擬", "summary_zh": "本研究概述基於馬可夫隨機動力學的蒙地卡羅演算法，用於研究非熱平衡的多粒子交互和反應系統。這種基於代理人的電腦模擬為大學生和研究生提供了一種有效途徑，無需太多先備知識即可接觸前沿研究。學生可從模擬數據的可視化中直接了解複雜模型的宏觀特徵，進而運用更精密的數據分析，量化其豐富的動態特性，包含穩態和瞬態。我們利用反應擴散系統、種群動態和流行病傳播的隨機模型，闡述跨領域計算研究如何透過實作學習，有效地應用於由下而上的本科和研究生教育。此外，我們提供蒙地卡羅模擬演算法的實用建議、範例程式碼、數據分析工具，並描述潛在的錯誤來源和陷阱，以及規避方法。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T03:15:54.253127"}
{"query": "AI", "id": "2505.18129v1", "url": "http://arxiv.org/abs/2505.18129v1", "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning", "summary": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.", "authors": ["Yan Ma", "Linge Du", "Xuyang Shen", "Shaoxiang Chen", "Pengfei Li", "Qibing Ren", "Lizhuang Ma", "Yuchao Dai", "Pengfei Liu", "Junjie Yan"], "published_date": "2025-05-23", "title_zh": "一覽全局：視覺三元統一強化學習", "summary_zh": "強化學習大幅提升了視覺語言模型的推理能力，但其在推理之外，特別是物件偵測和定位等感知任務中的應用仍待探索。本文提出V-Triune，一種視覺三重統一強化學習系統，使視覺語言模型能在單一訓練流程中聯合學習視覺推理和感知任務。V-Triune包含三個互補組件：樣本級資料格式化（統一不同任務輸入）、驗證器級獎勵計算（透過專用驗證器提供客製化獎勵）以及來源級指標監控（診斷資料來源問題）。我們進一步引入動態IoU獎勵，為V-Triune處理的感知任務提供自適應、漸進式且明確的回饋。我們的方案採用現成的強化學習訓練框架，使用開源7B和32B骨幹模型。所得模型Orsta（一強化學習以觀全局）在推理和感知任務上均展現了持續改進，此廣泛能力得益於在涵蓋四個具代表性的視覺推理任務（數學、謎題、圖表、科學）和四個視覺感知任務（定位、偵測、計數、OCR）的多樣化資料集上的訓練。Orsta在MEGA-Bench Core上獲得顯著提升，7B和32B模型的各變體進步幅度從+2.1到+14.1不等，性能優勢延伸至廣泛的下游任務。這些結果突顯了我們統一強化學習方法對視覺語言模型的有效性和可擴展性。V-Triune系統和Orsta模型已公開。", "audio": "audios/2505.18129v1.mp3", "timestamp": "2025-05-26T04:24:47.239067"}
{"query": "Foundation Model", "id": "2505.18058v1", "url": "http://arxiv.org/abs/2505.18058v1", "title": "A Foundation Model Framework for Multi-View MRI Classification of Extramural Vascular Invasion and Mesorectal Fascia Invasion in Rectal Cancer", "summary": "Background: Accurate MRI-based identification of extramural vascular invasion\n(EVI) and mesorectal fascia invasion (MFI) is pivotal for risk-stratified\nmanagement of rectal cancer, yet visual assessment is subjective and vulnerable\nto inter-institutional variability. Purpose: To develop and externally evaluate\na multicenter, foundation-model-driven framework that automatically classifies\nEVI and MFI on axial and sagittal T2-weighted MRI. Methods: This retrospective\nstudy used 331 pre-treatment rectal cancer MRI examinations from three European\nhospitals. After TotalSegmentator-guided rectal patch extraction, a\nself-supervised frequency-domain harmonization pipeline was trained to minimize\nscanner-related contrast shifts. Four classifiers were compared: ResNet50,\nSeResNet, the universal biomedical pretrained transformer (UMedPT) with a\nlightweight MLP head, and a logistic-regression variant using frozen UMedPT\nfeatures (UMedPT_LR). Results: UMedPT_LR achieved the best EVI detection when\naxial and sagittal features were fused (AUC = 0.82; sensitivity = 0.75; F1\nscore = 0.73), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.74).\nThe highest MFI performance was attained by UMedPT on axial harmonized images\n(AUC = 0.77), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.75).\nFrequency-domain harmonization improved MFI classification but variably\naffected EVI performance. Conventional CNNs (ResNet50, SeResNet)\nunderperformed, especially in F1 score and balanced accuracy. Conclusion: These\nfindings demonstrate that combining foundation model features, harmonization,\nand multi-view fusion significantly enhances diagnostic performance in rectal\nMRI.", "authors": ["Yumeng Zhang", "Zohaib Salahuddin", "Danial Khan", "Shruti Atul Mali", "Henry C. Woodruff", "Sina Amirrajab", "Eduardo Ibor-Crespo", "Ana Jimenez-Pastor", "Luis Marti-Bonmati", "Philippe Lambin"], "published_date": "2025-05-23", "title_zh": "直腸癌中腸壁外血管侵犯和直腸繫膜筋膜侵犯多視圖MRI分類的基礎模型框架", "summary_zh": "本研究開發並驗證一套多中心、以基礎模型驅動的自動化框架，用於在T2加權MRI影像上判斷直腸癌的血管外侵犯（EVI）和腸繫膜筋膜侵犯（MFI）。研究採用來自歐洲三家醫院的331例直腸癌MRI掃描，利用TotalSegmentator提取直腸區域，並訓練一個自監督頻域協調流程，以減少掃描儀造成的對比差異。結果顯示，UMedPT_LR模型在融合軸向和矢狀特徵後，EVI檢測效果最佳（AUC=0.82），超越Chaimeleon Grand-Challenge獲獎模型。UMedPT模型在軸向協調影像上的MFI表現最佳（AUC=0.77），同樣優於Chaimeleon Grand-Challenge獲獎模型。頻域協調改善了MFI分類，但對EVI的影響不一。傳統CNN模型表現較差。結論表明，結合基礎模型特徵、影像協調和多視角融合能顯著提升直腸MRI的診斷效能。", "audio": "audios/2505.18058v1.mp3", "timestamp": "2025-05-26T04:24:54.635231"}
{"query": "Diffusion Model", "id": "2505.18142v1", "url": "http://arxiv.org/abs/2505.18142v1", "title": "TokBench: Evaluating Your Visual Tokenizer before Visual Generation", "summary": "In this work, we reveal the limitations of visual tokenizers and VAEs in\npreserving fine-grained features, and propose a benchmark to evaluate\nreconstruction performance for two challenging visual contents: text and face.\nImage tokenization has significantly advanced visual generation and multimodal\nmodeling, particularly with autoregressive models due to the modeling\nsimplicity of discrete tokens. Autoregressive models typically rely on image\ntokenizers to compress images into discrete tokens for sequential prediction,\nwhereas diffusion models often operate on continuous latent space to reduce\ncomputational costs. However, both visual compression approaches inevitably\nlose visual information, thereby limiting the upper bound of visual generation\nquality. To evaluate how these compression losses affect text and faces, the\nmost human-sensitive visual elements, we first collect and curate a collection\nof text and faces images from existing datasets, ensuring clarity and\ndiversity. For text reconstruction, we employ OCR models to assess the\nrecognition accuracy of the reconstructed text, and then we measure feature\nsimilarity between original and reconstructed faces thereby quantifying faces\nreconstruction fidelity. Our method is highly lightweight, requiring just 2GB\nmemory and 4 minutes to complete evaluations. With our benchmark, we analyze\nthe reconstruction quality of text and faces at various scales across different\nimage tokenizers and VAEs. Our results demonstrate that modern visual\ntokenizers still struggle to preserve fine-grained features, particularly at\nsmaller scales. Furthermore, we extend this evaluation framework to the video,\nconducting a comprehensive analysis of video tokenizers. Additionally, we find\nthat traditional metrics fail to accurately reflect the reconstruction\nperformance for faces and text, while our proposed metrics serve as an\neffective complement.", "authors": ["Junfeng Wu", "Dongliang Luo", "Weizhi Zhao", "Zhihao Xie", "Yuanhao Wang", "Junyi Li", "Xudong Xie", "Yuliang Liu", "Xiang Bai"], "published_date": "2025-05-23", "title_zh": "TokBench：視覺生成前評估您的視覺符號器", "summary_zh": "本研究揭示視覺符號器和變分自編碼器在保留細緻特徵上的局限性，並提出一個基準來評估文字和人臉這兩項具挑戰性視覺內容的重建效能。圖像符號化顯著推進了視覺生成和多模態建模，特別是自迴歸模型，因其離散符號的建模簡潔性。自迴歸模型通常依賴圖像符號器將圖像壓縮成離散符號以進行序列預測，而擴散模型則常在連續潛在空間中運作以降低計算成本。然而，這兩種視覺壓縮方法不可避免地會丟失視覺資訊，從而限制視覺生成品質的上限。為了評估這些壓縮損失如何影響對人類最敏感的視覺元素，即文字和人臉，我們首先從現有數據集中收集和整理了一系列文字和人臉圖像，確保清晰度和多樣性。對於文字重建，我們採用OCR模型來評估重建文字的辨識準確度；然後，我們測量原始人臉和重建人臉之間的特徵相似度，從而量化人臉重建的保真度。該方法非常輕量，只需2GB記憶體和4分鐘即可完成評估。藉由該基準，我們分析了不同圖像符號器和變分自編碼器在各種尺度下對文字和人臉的重建品質。結果表明，現代視覺符號器在保留細緻特徵方面仍有困難，尤其是在較小尺度下。此外，我們將此評估框架擴展到影片，對影片符號器進行了全面分析。另外，我們發現傳統指標未能準確反映人臉和文字的重建效能，而我們提出的指標則能有效補充。", "audio": "audios/2505.18142v1.mp3", "timestamp": "2025-05-26T04:25:05.415256"}
{"query": "AI", "id": "2505.18128v1", "url": "http://arxiv.org/abs/2505.18128v1", "title": "Frankentext: Stitching random text fragments into long-form narratives", "summary": "We introduce Frankentexts, a new type of long-form narratives produced by\nLLMs under the extreme constraint that most tokens (e.g., 90%) must be copied\nverbatim from human writings. This task presents a challenging test of\ncontrollable generation, requiring models to satisfy a writing prompt,\nintegrate disparate text fragments, and still produce a coherent narrative. To\ngenerate Frankentexts, we instruct the model to produce a draft by selecting\nand combining human-written passages, then iteratively revise the draft while\nmaintaining a user-specified copy ratio. We evaluate the resulting Frankentexts\nalong three axes: writing quality, instruction adherence, and detectability.\nGemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts\nare coherent and 100% relevant to the prompt. Notably, up to 59% of these\noutputs are misclassified as human-written by detectors like Pangram, revealing\nlimitations in AI text detectors. Human annotators can sometimes identify\nFrankentexts through their abrupt tone shifts and inconsistent grammar between\nsegments, especially in longer generations. Beyond presenting a challenging\ngeneration task, Frankentexts invite discussion on building effective detectors\nfor this new grey zone of authorship, provide training data for mixed\nauthorship detection, and serve as a sandbox for studying human-AI co-writing\nprocesses.", "authors": ["Chau Minh Pham", "Jenna Russell", "Dzung Pham", "Mohit Iyyer"], "published_date": "2025-05-23", "title_zh": "弗蘭肯文本：將隨機文本片段縫合成長篇敘事", "summary_zh": "本文介紹Frankentexts，一種由大型語言模型在嚴苛限制下產生的新型長篇敘事，要求大部分詞符（如90%）必須逐字複製自人類作品。此任務對可控生成構成挑戰性考驗，要求模型滿足寫作提示、整合不同文本片段並產出連貫敘事。生成Frankentexts時，模型先選擇並組合人類撰寫的段落生成草稿，然後在維持使用者指定的複製比例下迭代修改草稿。我們從寫作品質、指令遵循和可偵測性三個方面評估生成的Frankentexts。Gemini-2.5-Pro在此任務上表現出色：81%的Frankentexts具有連貫性，100%與提示相關。值得注意的是，高達59%的輸出被Pangram等偵測器錯誤分類為人類作品，揭示了AI文本偵測器的局限性。人工標註者有時可以通過片段之間突兀的語氣轉變和不一致的語法來識別Frankentexts，尤其是在較長的生成文本中。除了提出具挑戰性的生成任務外，Frankentexts還引發了關於建立有效偵測器以應對這種新型作者身份灰色地帶的討論，提供了用於混合作者身份偵測的訓練數據，並作為研究人機協作寫作過程的沙盒。", "audio": "audios/2505.18128v1.mp3", "timestamp": "2025-05-26T05:19:25.565547"}
{"query": "Foundation Model", "id": "2505.18039v1", "url": "http://arxiv.org/abs/2505.18039v1", "title": "Clip4Retrofit: Enabling Real-Time Image Labeling on Edge Devices via Cross-Architecture CLIP Distillation", "summary": "Foundation models like CLIP (Contrastive Language-Image Pretraining) have\nrevolutionized vision-language tasks by enabling zero-shot and few-shot\nlearning through cross-modal alignment. However, their computational complexity\nand large memory footprint make them unsuitable for deployment on\nresource-constrained edge devices, such as in-car cameras used for image\ncollection and real-time processing. To address this challenge, we propose\nClip4Retrofit, an efficient model distillation framework that enables real-time\nimage labeling on edge devices. The framework is deployed on the Retrofit\ncamera, a cost-effective edge device retrofitted into thousands of vehicles,\ndespite strict limitations on compute performance and memory. Our approach\ndistills the knowledge of the CLIP model into a lightweight student model,\ncombining EfficientNet-B3 with multi-layer perceptron (MLP) projection heads to\npreserve cross-modal alignment while significantly reducing computational\nrequirements. We demonstrate that our distilled model achieves a balance\nbetween efficiency and performance, making it ideal for deployment in\nreal-world scenarios. Experimental results show that Clip4Retrofit can perform\nreal-time image labeling and object identification on edge devices with limited\nresources, offering a practical solution for applications such as autonomous\ndriving and retrofitting existing systems. This work bridges the gap between\nstate-of-the-art vision-language models and their deployment in\nresource-constrained environments, paving the way for broader adoption of\nfoundation models in edge computing.", "authors": ["Li Zhong", "Ahmed Ghazal", "Jun-Jun Wan", "Frederik Zilly", "Patrick Mackens", "Joachim E. Vollrath", "Bogdan Sorin Coseriu"], "published_date": "2025-05-23", "title_zh": "Clip4Retrofit：藉由跨架構CLIP蒸餾於邊緣裝置上實現即時圖像標註", "summary_zh": "如CLIP之類的基礎模型透過跨模態對齊，革新了視覺語言任務，實現了零樣本和少樣本學習。然而，其計算複雜性和龐大的記憶體佔用使其不適用於資源受限的邊緣設備，例如用於圖像採集和即時處理的車載相機。為了解決這個問題，我們提出了Clip4Retrofit，一個高效的模型蒸餾框架，可在邊緣設備上實現即時圖像標記。該框架部署在Retrofit相機上，這是一種經濟高效的邊緣設備，已改裝到數千輛汽車中，儘管其計算效能和記憶體受到嚴格限制。我們的方法將CLIP模型的知識提煉成一個輕量級的學生模型，結合EfficientNet-B3和多層感知器（MLP）投影頭，以保持跨模態對齊，同時顯著降低計算需求。我們證明，我們的蒸餾模型在效率和性能之間取得了平衡，使其非常適合在實際場景中部署。實驗結果表明，Clip4Retrofit可以在資源有限的邊緣設備上執行即時圖像標記和物件識別，為自動駕駛和改造現有系統等應用提供了一種實用的解決方案。這項工作彌合了最先進的視覺語言模型與其在資源受限環境中的部署之間的差距，為基礎模型在邊緣計算中的更廣泛採用鋪平了道路。", "audio": "audios/2505.18039v1.mp3", "timestamp": "2025-05-26T05:19:33.562834"}
{"query": "Diffusion Model", "id": "2505.18097v1", "url": "http://arxiv.org/abs/2505.18097v1", "title": "Towards more transferable adversarial attack in black-box manner", "summary": "Adversarial attacks have become a well-explored domain, frequently serving as\nevaluation baselines for model robustness. Among these, black-box attacks based\non transferability have received significant attention due to their practical\napplicability in real-world scenarios. Traditional black-box methods have\ngenerally focused on improving the optimization framework (e.g., utilizing\nmomentum in MI-FGSM) to enhance transferability, rather than examining the\ndependency on surrogate white-box model architectures. Recent state-of-the-art\napproach DiffPGD has demonstrated enhanced transferability by employing\ndiffusion-based adversarial purification models for adaptive attacks. The\ninductive bias of diffusion-based adversarial purification aligns naturally\nwith the adversarial attack process, where both involving noise addition,\nreducing dependency on surrogate white-box model selection. However, the\ndenoising process of diffusion models incurs substantial computational costs\nthrough chain rule derivation, manifested in excessive VRAM consumption and\nextended runtime. This progression prompts us to question whether introducing\ndiffusion models is necessary. We hypothesize that a model sharing similar\ninductive bias to diffusion-based adversarial purification, combined with an\nappropriate loss function, could achieve comparable or superior transferability\nwhile dramatically reducing computational overhead. In this paper, we propose a\nnovel loss function coupled with a unique surrogate model to validate our\nhypothesis. Our approach leverages the score of the time-dependent classifier\nfrom classifier-guided diffusion models, effectively incorporating natural data\ndistribution knowledge into the adversarial optimization process. Experimental\nresults demonstrate significantly improved transferability across diverse model\narchitectures while maintaining robustness against diffusion-based defenses.", "authors": ["Chun Tong Lei", "Zhongliang Guo", "Hon Chung Lee", "Minh Quoc Duong", "Chun Pong Lau"], "published_date": "2025-05-23", "title_zh": "邁向更具遷移性的黑盒對抗攻擊", "summary_zh": "對抗性攻擊已成熱門領域，常作為模型穩健性評估基準。基於可遷移性的黑盒攻擊因其實用性備受關注。傳統方法側重於優化框架以提升可遷移性，而非檢視對代理白盒模型架構的依賴。DiffPGD透過擴散的對抗淨化模型進行自適應攻擊，展現更佳的可遷移性。擴散的對抗淨化之歸納偏置與對抗攻擊過程自然契合，降低對代理白盒模型選擇的依賴。然而，擴散模型的去噪過程計算成本高昂。本文假設，與擴散的對抗淨化模型具相似歸納偏置的模型，結合適當損失函數，可實現相當或更優越的可遷移性，同時顯著降低計算開銷。本文提出一種新穎的損失函數與獨特的代理模型以驗證此假設，利用基於分類器的擴散模型之時變分類器分數，將自然數據分佈知識有效地納入對抗優化過程。實驗結果表明，在多種模型架構中，可遷移性顯著提升，同時保持對基於擴散防禦的穩健性。", "audio": "audios/2505.18097v1.mp3", "timestamp": "2025-05-26T05:19:41.120299"}
{"query": "AI", "id": "2505.18078v1", "url": "http://arxiv.org/abs/2505.18078v1", "title": "DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation", "summary": "Controllable video generation (CVG) has advanced rapidly, yet current systems\nfalter when more than one actor must move, interact, and exchange positions\nunder noisy control signals. We address this gap with DanceTogether, the first\nend-to-end diffusion framework that turns a single reference image plus\nindependent pose-mask streams into long, photorealistic videos while strictly\npreserving every identity. A novel MaskPoseAdapter binds \"who\" and \"how\" at\nevery denoising step by fusing robust tracking masks with semantically rich-but\nnoisy-pose heat-maps, eliminating the identity drift and appearance bleeding\nthat plague frame-wise pipelines. To train and evaluate at scale, we introduce\n(i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii)\nHumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain\ntransfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the\nDanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure\nskating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a\nsignificant margin. Moreover, we show that a one-hour fine-tune yields\nconvincing human-robot videos, underscoring broad generalization to embodied-AI\nand HRI tasks. Extensive ablations confirm that persistent identity-action\nbinding is critical to these gains. Together, our model, datasets, and\nbenchmark lift CVG from single-subject choreography to compositionally\ncontrollable, multi-actor interaction, opening new avenues for digital\nproduction, simulation, and embodied intelligence. Our video demos and code are\navailable at https://DanceTog.github.io/.", "authors": ["Junhao Chen", "Mingjin Chen", "Jianjin Xu", "Xiang Li", "Junting Dong", "Mingze Sun", "Puhua Jiang", "Hongxiang Li", "Yuhang Yang", "Hao Zhao", "Xiaoxiao Long", "Ruqi Huang"], "published_date": "2025-05-23", "title_zh": "共舞！身分保持的多人互動影片生成", "summary_zh": "可控影片生成技術快速發展，但現有系統在多個角色於噪聲控制信號下移動、互動和交換位置時表現不佳。本文提出DanceTogether，首個端到端擴散框架，可將單一參考圖像和獨立的姿態遮罩串流轉換為逼真長影片，同時嚴格保留每個角色身份。一種新型MaskPoseAdapter在每個去噪步驟中結合穩健追蹤遮罩與具語義但帶噪的姿態熱圖，消除身份漂移和外觀洩漏問題。為規模化訓練和評估，引入PairFS-4K、HumanRob-300和TogetherVideoBench基準。實驗結果表明，DanceTogether在TogetherVideoBench上顯著優於先前技術，並可透過少量微調生成具說服力的人機互動影片。消融研究證實，持久的身份-動作綁定至關重要。DanceTogether模型、數據集和基準將可控影片生成從單一主體編舞提升到具組合控制性的多角色互動，為數位製作、模擬和具身智慧開闢新途徑。", "audio": "audios/2505.18078v1.mp3", "timestamp": "2025-05-26T06:28:19.840401"}
{"query": "Foundation Model", "id": "2505.18022v1", "url": "http://arxiv.org/abs/2505.18022v1", "title": "RemoteSAM: Towards Segment Anything for Earth Observation", "summary": "We aim to develop a robust yet flexible visual foundation model for Earth\nobservation. It should possess strong capabilities in recognizing and\nlocalizing diverse visual targets while providing compatibility with various\ninput-output interfaces required across different task scenarios. Current\nsystems cannot meet these requirements, as they typically utilize task-specific\narchitecture trained on narrow data domains with limited semantic coverage. Our\nstudy addresses these limitations from two aspects: data and modeling. We first\nintroduce an automatic data engine that enjoys significantly better scalability\ncompared to previous human annotation or rule-based approaches. It has enabled\nus to create the largest dataset of its kind to date, comprising 270K\nimage-text-mask triplets covering an unprecedented range of diverse semantic\ncategories and attribute specifications. Based on this data foundation, we\nfurther propose a task unification paradigm that centers around referring\nexpression segmentation. It effectively handles a wide range of vision-centric\nperception tasks, including classification, detection, segmentation, grounding,\netc, using a single model without any task-specific heads. Combining these\ninnovations on data and modeling, we present RemoteSAM, a foundation model that\nestablishes new SoTA on several earth observation perception benchmarks,\noutperforming other foundation models such as Falcon, GeoChat, and LHRS-Bot\nwith significantly higher efficiency. Models and data are publicly available at\nhttps://github.com/1e12Leon/RemoteSAM.", "authors": ["Liang Yao", "Fan Liu", "Delong Chen", "Chuanyi Zhang", "Yijun Wang", "Ziyun Chen", "Wei Xu", "Shimin Di", "Yuhui Zheng"], "published_date": "2025-05-23", "title_zh": "RemoteSAM：面向地球觀測的萬物分割", "summary_zh": "本研究旨在開發適用於地球觀測的穩健且靈活之視覺基礎模型，該模型需具備辨識和定位多樣化視覺目標之能力，並相容於不同任務情境所需之各種輸入輸出介面。為解決現有系統在資料和建模方面的局限性，我們引入自動化資料引擎，建立包含27萬個圖像-文本-遮罩三元組的大型資料集，涵蓋廣泛之語義類別和屬性規範。基於此資料基礎，我們提出以指代表達分割為中心之任務統一範式，利用單一模型處理多種視覺感知任務，無需特定任務之head。結合資料和建模創新，我們提出 RemoteSAM，此基礎模型在多個地球觀測感知基準測試中達到最先進水準，且效率顯著優於其他模型。模型與資料已公開發佈。", "audio": "audios/2505.18022v1.mp3", "timestamp": "2025-05-26T06:28:25.695016"}
{"query": "Diffusion Model", "id": "2505.18047v1", "url": "http://arxiv.org/abs/2505.18047v1", "title": "RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration", "summary": "The use of latent diffusion models (LDMs) such as Stable Diffusion has\nsignificantly improved the perceptual quality of All-in-One image Restoration\n(AiOR) methods, while also enhancing their generalization capabilities.\nHowever, these LDM-based frameworks suffer from slow inference due to their\niterative denoising process, rendering them impractical for time-sensitive\napplications. To address this, we propose RestoreVAR, a novel generative\napproach for AiOR that significantly outperforms LDM-based models in\nrestoration performance while achieving over $\\mathbf{10\\times}$ faster\ninference. RestoreVAR leverages visual autoregressive modeling (VAR), a\nrecently introduced approach which performs scale-space autoregression for\nimage generation. VAR achieves comparable performance to that of\nstate-of-the-art diffusion transformers with drastically reduced computational\ncosts. To optimally exploit these advantages of VAR for AiOR, we propose\narchitectural modifications and improvements, including intricately designed\ncross-attention mechanisms and a latent-space refinement module, tailored for\nthe AiOR task. Extensive experiments show that RestoreVAR achieves\nstate-of-the-art performance among generative AiOR methods, while also\nexhibiting strong generalization capabilities.", "authors": ["Sudarshan Rajagopalan", "Kartik Narayan", "Vishal M. Patel"], "published_date": "2025-05-23", "title_zh": "RestoreVAR：用於一體化圖像復原的視覺自迴歸生成", "summary_zh": "潛在擴散模型（LDM）如Stable Diffusion雖提升了全方位影像修復（AiOR）方法的感知品質與泛化能力，但其迭代降噪過程導致推論速度緩慢，不適用於時間敏感應用。為此，我們提出RestoreVAR，一種新型生成式AiOR方法，其修復效能顯著超越LDM模型，推論速度更提升十倍以上。RestoreVAR利用視覺自迴歸建模（VAR），一種針對影像生成執行尺度空間自迴歸的新技術，在大幅降低計算成本的同時，達到與最先進擴散轉換器相當的效能。為最佳化VAR在AiOR中的優勢，我們提出架構修改與改進，包含精巧設計的交叉注意力機制和潛在空間精煉模組，專為AiOR任務量身打造。實驗結果表明，RestoreVAR在生成式AiOR方法中達到最先進效能，並展現出強大的泛化能力。", "audio": "audios/2505.18047v1.mp3", "timestamp": "2025-05-26T06:28:32.851412"}
{"query": "AI", "id": "2505.18066v1", "url": "http://arxiv.org/abs/2505.18066v1", "title": "Towards Uncertainty Aware Task Delegation and Human-AI Collaborative Decision-Making", "summary": "Despite the growing promise of artificial intelligence (AI) in supporting\ndecision-making across domains, fostering appropriate human reliance on AI\nremains a critical challenge. In this paper, we investigate the utility of\nexploring distance-based uncertainty scores for task delegation to AI and\ndescribe how these scores can be visualized through embedding representations\nfor human-AI decision-making. After developing an AI-based system for physical\nstroke rehabilitation assessment, we conducted a study with 19 health\nprofessionals and 10 students in medicine/health to understand the effect of\nexploring distance-based uncertainty scores on users' reliance on AI. Our\nfindings showed that distance-based uncertainty scores outperformed traditional\nprobability-based uncertainty scores in identifying uncertain cases. In\naddition, after exploring confidence scores for task delegation and reviewing\nembedding-based visualizations of distance-based uncertainty scores,\nparticipants achieved an 8.20% higher rate of correct decisions, a 7.15% higher\nrate of changing their decisions to correct ones, and a 7.14% lower rate of\nincorrect changes after reviewing AI outputs than those reviewing\nprobability-based uncertainty scores ($p<0.01$). Our findings highlight the\npotential of distance-based uncertainty scores to enhance decision accuracy and\nappropriate reliance on AI while discussing ongoing challenges for human-AI\ncollaborative decision-making.", "authors": ["Min Hun Lee", "Martyn Zhe Yu Tok"], "published_date": "2025-05-23", "title_zh": "邁向具不確定性意識的任務委派與人機協同決策", "summary_zh": "儘管人工智慧在輔助決策方面日益 promising，如何促進人對 AI 的適當依賴仍然是一項關鍵挑戰。本文探討基於距離的不確定性評分在任務委派給 AI 中的效用，並闡述如何透過嵌入表示視覺化這些評分，以輔助人機決策。研究開發了一套基於 AI 的中風復健評估系統，並與 19 位醫療專業人員和 10 位醫學/健康領域學生進行研究，了解探索基於距離的不確定性評分對使用者 AI 依賴的影響。研究結果顯示，基於距離的不確定性評分在識別不確定案例方面優於傳統的基於機率的不確定性評分。此外，在探索用於任務委派的置信度評分，並檢閱基於嵌入的距離不確定性評分視覺化後，參與者在檢閱 AI 輸出後，其正確決策率提高了 8.20%，將決策更改為正確決策的比率提高了 7.15%，而錯誤更改的比率降低了 7.14% (p<0.01)。研究強調了基於距離的不確定性評分在提高決策準確性和對 AI 的適當依賴方面的潛力，同時也討論了人機協作決策所面臨的持續挑戰。", "audio": "audios/2505.18066v1.mp3", "timestamp": "2025-05-26T07:25:31.504599"}
{"query": "Foundation Model", "id": "2505.17971v1", "url": "http://arxiv.org/abs/2505.17971v1", "title": "Explainable Anatomy-Guided AI for Prostate MRI: Foundation Models and In Silico Clinical Trials for Virtual Biopsy-based Risk Assessment", "summary": "We present a fully automated, anatomically guided deep learning pipeline for\nprostate cancer (PCa) risk stratification using routine MRI. The pipeline\nintegrates three key components: an nnU-Net module for segmenting the prostate\ngland and its zones on axial T2-weighted MRI; a classification module based on\nthe UMedPT Swin Transformer foundation model, fine-tuned on 3D patches with\noptional anatomical priors and clinical data; and a VAE-GAN framework for\ngenerating counterfactual heatmaps that localize decision-driving image\nregions. The system was developed using 1,500 PI-CAI cases for segmentation and\n617 biparametric MRIs with metadata from the CHAIMELEON challenge for\nclassification (split into 70% training, 10% validation, and 20% testing).\nSegmentation achieved mean Dice scores of 0.95 (gland), 0.94 (peripheral zone),\nand 0.92 (transition zone). Incorporating gland priors improved AUC from 0.69\nto 0.72, with a three-scale ensemble achieving top performance (AUC = 0.79,\ncomposite score = 0.76), outperforming the 2024 CHAIMELEON challenge winners.\nCounterfactual heatmaps reliably highlighted lesions within segmented regions,\nenhancing model interpretability. In a prospective multi-center in-silico trial\nwith 20 clinicians, AI assistance increased diagnostic accuracy from 0.72 to\n0.77 and Cohen's kappa from 0.43 to 0.53, while reducing review time per case\nby 40%. These results demonstrate that anatomy-aware foundation models with\ncounterfactual explainability can enable accurate, interpretable, and efficient\nPCa risk assessment, supporting their potential use as virtual biopsies in\nclinical practice.", "authors": ["Danial Khan", "Zohaib Salahuddin", "Yumeng Zhang", "Sheng Kuang", "Shruti Atul Mali", "Henry C. Woodruff", "Sina Amirrajab", "Rachel Cavill", "Eduardo Ibor-Crespo", "Ana Jimenez-Pastor", "Adrian Galiana-Bordera", "Paula Jimenez Gomez", "Luis Marti-Bonmati", "Philippe Lambin"], "published_date": "2025-05-23", "title_zh": "基於可解釋解剖學引導的人工智慧於前列腺磁振造影：用於虛擬切片活檢風險評估的基礎模型與電腦模擬臨床試驗", "summary_zh": "本研究提出一個全自動、解剖結構引導的深度學習流程，用於常規 MRI 前列腺癌風險分層。該流程整合 nnU-Net 分割模組（分割 T2 加權 MRI 前列腺及其區域）、UMedPT Swin Transformer 分類模組（基於 3D 影像塊微調，可選解剖先驗和臨床數據）以及 VAE-GAN 框架（生成反事實熱圖以定位決策驅動影像區域）。系統分別使用 1500 個 PI-CAI 案例進行分割，以及來自 CHAIMELEON 挑戰賽的 617 個雙參數 MRI 數據集進行分類（70% 訓練，10% 驗證，20% 測試）。分割的平均 Dice 系數分別為 0.95（前列腺）、0.94（外周帶）和 0.92（移行帶）。納入前列腺先驗知識將 AUC 從 0.69 提升至 0.72，三尺度集成模型表現最佳（AUC = 0.79，綜合評分 = 0.76），優於 2024 CHAIMELEON 挑戰賽獲勝者。反事實熱圖能可靠地標記分割區域內的病灶，增強模型可解釋性。在一項前瞻性多中心體內試驗中，AI 輔助將 20 位臨床醫生的診斷準確性從 0.72 提高到 0.77，Cohen's kappa 從 0.43 提高到 0.53，同時每個案例的審閱時間減少了 40%。結果表明，具有反事實可解釋性的解剖結構感知基礎模型能夠實現準確、可解釋且高效的前列腺癌風險評估，支持其在臨床實踐中作為虛擬活檢的潛在應用。", "audio": "audios/2505.17971v1.mp3", "timestamp": "2025-05-26T07:25:41.595526"}
{"query": "Diffusion Model", "id": "2505.18017v1", "url": "http://arxiv.org/abs/2505.18017v1", "title": "Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling", "summary": "Deep generative models hold great promise for representing complex physical\nsystems, but their deployment is currently limited by the lack of guarantees on\nthe physical plausibility of the generated outputs. Ensuring that known\nphysical constraints are enforced is therefore critical when applying\ngenerative models to scientific and engineering problems. We address this\nlimitation by developing a principled framework for sampling from a target\ndistribution while rigorously satisfying physical constraints. Leveraging the\nvariational formulation of Langevin dynamics, we propose Split Augmented\nLangevin (SAL), a novel primal-dual sampling algorithm that enforces\nconstraints progressively through variable splitting, with convergence\nguarantees. While the method is developed theoretically for Langevin dynamics,\nwe demonstrate its effective applicability to diffusion models. In particular,\nwe use constrained diffusion models to generate physical fields satisfying\nenergy and mass conservation laws. We apply our method to diffusion-based data\nassimilation on a complex physical system, where enforcing physical constraints\nsubstantially improves both forecast accuracy and the preservation of critical\nconserved quantities. We also demonstrate the potential of SAL for challenging\nfeasibility problems in optimal control.", "authors": ["Matthieu Blanke", "Yongquan Qu", "Sara Shamekh", "Pierre Gentine"], "published_date": "2025-05-23", "title_zh": "基於分裂增廣朗之萬抽樣的嚴格約束生成建模", "summary_zh": "深度生成模型在表徵複雜物理系統方面極具潛力，但因其生成輸出結果缺乏物理合理性保證而限制了應用。確保已知物理約束的實施對於將生成模型應用於科學和工程問題至關重要。本研究提出一個在嚴格滿足物理約束下從目標分布中採樣的原則性框架。利用朗之萬動力學的變分公式，提出一種名為分裂增廣朗之萬(SAL)的原始-對偶採樣新算法，通過變量分裂逐步實施約束，並具有收斂性保證。儘管該方法在理論上是為朗之萬動力學開發的，但我們證明其對擴散模型的有效適用性。特別是，我們使用受約束的擴散模型來生成滿足能量和質量守恆定律的物理場。我們將此方法應用於基於擴散的複雜物理系統數據同化，其中實施物理約束可顯著提高預測準確性和關鍵守恆量的保存。我們還展示了SAL在最佳控制中具挑戰性可行性問題的潛力。", "audio": "audios/2505.18017v1.mp3", "timestamp": "2025-05-26T07:25:47.785036"}
{"query": "AI", "id": "2505.18060v1", "url": "http://arxiv.org/abs/2505.18060v1", "title": "Semantic Correspondence: Unified Benchmarking and a Strong Baseline", "summary": "Establishing semantic correspondence is a challenging task in computer\nvision, aiming to match keypoints with the same semantic information across\ndifferent images. Benefiting from the rapid development of deep learning,\nremarkable progress has been made over the past decade. However, a\ncomprehensive review and analysis of this task remains absent. In this paper,\nwe present the first extensive survey of semantic correspondence methods. We\nfirst propose a taxonomy to classify existing methods based on the type of\ntheir method designs. These methods are then categorized accordingly, and we\nprovide a detailed analysis of each approach. Furthermore, we aggregate and\nsummarize the results of methods in literature across various benchmarks into a\nunified comparative table, with detailed configurations to highlight\nperformance variations. Additionally, to provide a detailed understanding on\nexisting methods for semantic matching, we thoroughly conduct controlled\nexperiments to analyse the effectiveness of the components of different\nmethods. Finally, we propose a simple yet effective baseline that achieves\nstate-of-the-art performance on multiple benchmarks, providing a solid\nfoundation for future research in this field. We hope this survey serves as a\ncomprehensive reference and consolidated baseline for future development. Code\nis publicly available at: https://github.com/Visual-AI/Semantic-Correspondence.", "authors": ["Kaiyan Zhang", "Xinghui Li", "Jingyi Lu", "Kai Han"], "published_date": "2025-05-23", "title_zh": "語義對應：統一基準測試與強基準線", "summary_zh": "語義對應旨在匹配不同圖像間具有相同語義信息的關鍵點，是電腦視覺中一項具挑戰性的任務。受益於深度學習的快速發展，過去十年取得了顯著進展，但缺乏對此任務的全面回顧與分析。本文首次對語義對應方法進行廣泛綜述，首先提出一種分類法，基於方法設計類型對現有方法進行分類，並對每種方法進行詳細分析。此外，我們將文獻中各種基準測試的結果匯總成統一的比較表格，並提供詳細配置以突顯性能差異。為了深入理解現有的語義匹配方法，我們進行了嚴格的控制實驗，分析了不同方法組件的有效性。最後，我們提出了一個簡單而有效的基準，在多個基準測試中實現了最先進的性能，為該領域的未來研究提供了堅實的基礎。期望此綜述能為未來的發展提供全面的參考和整合的基準。程式碼已公開：https://github.com/Visual-AI/Semantic-Correspondence。", "audio": "audios/2505.18060v1.mp3", "timestamp": "2025-05-26T08:36:45.936222"}
{"query": "Foundation Model", "id": "2505.17931v1", "url": "http://arxiv.org/abs/2505.17931v1", "title": "AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models", "summary": "Medical image segmentation is vital for clinical diagnosis, yet current deep\nlearning methods often demand extensive expert effort, i.e., either through\nannotating large training datasets or providing prompts at inference time for\neach new case. This paper introduces a zero-shot and automatic segmentation\npipeline that combines off-the-shelf vision-language and segmentation\nfoundation models. Given a medical image and a task definition (e.g., \"segment\nthe optic disc in an eye fundus image\"), our method uses a grounding model to\ngenerate an initial bounding box, followed by a visual prompt boosting module\nthat enhance the prompts, which are then processed by a promptable segmentation\nmodel to produce the final mask. To address the challenges of domain gap and\nresult verification, we introduce a test-time adaptation framework featuring a\nset of learnable adaptors that align the medical inputs with foundation model\nrepresentations. Its hyperparameters are optimized via Bayesian Optimization,\nguided by a proxy validation model without requiring ground-truth labels. Our\npipeline offers an annotation-efficient and scalable solution for zero-shot\nmedical image segmentation across diverse tasks. Our pipeline is evaluated on\nseven diverse medical imaging datasets and shows promising results. By proper\ndecomposition and test-time adaptation, our fully automatic pipeline performs\ncompetitively with weakly-prompted interactive foundation models.", "authors": ["Xingjian Li", "Qifeng Wu", "Colleen Que", "Yiran Ding", "Adithya S. Ubaradka", "Jianhua Xing", "Tianyang Wang", "Min Xu"], "published_date": "2025-05-23", "title_zh": "AutoMiSeg：基於基礎模型測試時自適應的自動醫學影像分割", "summary_zh": "醫學影像分割對於臨床診斷至關重要，但現有深度學習方法常需大量專家投入，例如標註大型訓練集或於每次推論時提供提示。本文提出一種零樣本且自動化的分割流程，結合現成的視覺語言和分割基礎模型。給定醫學影像和任務定義（如「分割眼底圖像中的視神經盤」），本方法使用定位模型產生初始邊界框，再透過視覺提示增強模組強化提示，隨後由可提示分割模型產生最終遮罩。為了解決領域差距和結果驗證的挑戰，我們引入測試時適應框架，包含一組可學習的適配器，以將醫學輸入與基礎模型表示對齊。其超參數透過貝氏優化進行優化，並由代理驗證模型指導，無需真實標籤。我們的流程為跨多樣任務的零樣本醫學影像分割提供了一種標註高效且可擴展的解決方案。該流程在七個不同的醫學影像數據集上進行評估，顯示出有希望的結果。透過適當的分解和測試時適應，我們的全自動流程在性能上可與弱提示互動式基礎模型競爭。", "audio": "audios/2505.17931v1.mp3", "timestamp": "2025-05-26T08:36:54.062285"}
{"query": "Diffusion Model", "id": "2505.17994v1", "url": "http://arxiv.org/abs/2505.17994v1", "title": "Segment Anyword: Mask Prompt Inversion for Open-Set Grounded Segmentation", "summary": "Open-set image segmentation poses a significant challenge because existing\nmethods often demand extensive training or fine-tuning and generally struggle\nto segment unified objects consistently across diverse text reference\nexpressions. Motivated by this, we propose Segment Anyword, a novel\ntraining-free visual concept prompt learning approach for open-set language\ngrounded segmentation that relies on token-level cross-attention maps from a\nfrozen diffusion model to produce segmentation surrogates or mask prompts,\nwhich are then refined into targeted object masks. Initial prompts typically\nlack coherence and consistency as the complexity of the image-text increases,\nresulting in suboptimal mask fragments. To tackle this issue, we further\nintroduce a novel linguistic-guided visual prompt regularization that binds and\nclusters visual prompts based on sentence dependency and syntactic structural\ninformation, enabling the extraction of robust, noise-tolerant mask prompts,\nand significant improvements in segmentation accuracy. The proposed approach is\neffective, generalizes across different open-set segmentation tasks, and\nachieves state-of-the-art results of 52.5 (+6.8 relative) mIoU on Pascal\nContext 59, 67.73 (+25.73 relative) cIoU on gRefCOCO, and 67.4 (+1.1 relative\nto fine-tuned methods) mIoU on GranDf, which is the most complex open-set\ngrounded segmentation task in the field.", "authors": ["Zhihua Liu", "Amrutha Saseendran", "Lei Tong", "Xilin He", "Fariba Yousefi", "Nikolay Burlutskiy", "Dino Oglic", "Tom Diethe", "Philip Teare", "Huiyu Zhou", "Chen Jin"], "published_date": "2025-05-23", "title_zh": "任意詞語分割：用於開放集基礎分割的遮罩提示反演", "summary_zh": "開放集圖像分割面臨挑戰，現有方法需大量訓練或微調，且難以根據不同文本描述一致分割統一物件。為此，我們提出Segment Anyword，一種免訓練的視覺概念提示學習方法，用於開放集語言導向分割。該方法利用凍結擴散模型中的令牌級別交叉注意力圖生成分割代理或遮罩提示，再將其優化為目標物件遮罩。針對圖像文本複雜度增加導致的初始提示不連貫問題，我們進一步提出語言引導的視覺提示正規化，依據句子依賴和句法結構資訊綁定和聚類視覺提示，提取穩健且容錯性高的遮罩提示，顯著提升分割準確度。該方法有效且能泛化至不同開放集分割任務，在Pascal Context 59上達到52.5 mIoU，gRefCOCO上達到67.73 cIoU，GranDf上達到67.4 mIoU，均為當前最佳效能。", "audio": "audios/2505.17994v1.mp3", "timestamp": "2025-05-26T08:37:01.427679"}
{"query": "AI", "id": "2505.18059v1", "url": "http://arxiv.org/abs/2505.18059v1", "title": "Assessing the performance of 8 AI chatbots in bibliographic reference retrieval: Grok and DeepSeek outperform ChatGPT, but none are fully accurate", "summary": "This study analyzes the performance of eight generative artificial\nintelligence chatbots -- ChatGPT, Claude, Copilot, DeepSeek, Gemini, Grok, Le\nChat, and Perplexity -- in their free versions, in the task of generating\nacademic bibliographic references within the university context. A total of 400\nreferences were evaluated across the five major areas of knowledge (Health,\nEngineering, Experimental Sciences, Social Sciences, and Humanities), based on\na standardized prompt. Each reference was assessed according to five key\ncomponents (authorship, year, title, source, and location), along with document\ntype, publication age, and error count. The results show that only 26.5% of the\nreferences were fully correct, 33.8% partially correct, and 39.8% were either\nerroneous or entirely fabricated. Grok and DeepSeek stood out as the only\nchatbots that did not generate false references, while Copilot, Perplexity, and\nClaude exhibited the highest hallucination rates. Furthermore, the chatbots\nshowed a greater tendency to generate book references over journal articles,\nalthough the latter had a significantly higher fabrication rate. A high degree\nof overlap was also detected among the sources provided by several models,\nparticularly between DeepSeek, Grok, Gemini, and ChatGPT. These findings reveal\nstructural limitations in current AI models, highlight the risks of uncritical\nuse by students, and underscore the need to strengthen information and critical\nliteracy regarding the use of AI tools in higher education.", "authors": ["Álvaro Cabezas-Clavijo", "Pavel Sidorenko-Bautista"], "published_date": "2025-05-23", "title_zh": "評估八款人工智慧聊天機器人在文獻參考檢索中的表現：Grok與DeepSeek優於ChatGPT，但皆未臻完全準確", "summary_zh": "本研究評估八種免費生成式人工智慧聊天機器人（ChatGPT、Claude、Copilot、DeepSeek、Gemini、Grok、Le Chat和Perplexity）在大學環境中產生學術文獻參考的表現。基於標準化提示，針對健康、工程、實驗科學、社會科學和人文學科五大知識領域，共評估400條參考文獻，並依作者、年份、標題、來源和地點等五個關鍵要素，以及文獻類型、出版年份和錯誤計數進行評估。結果顯示，僅26.5%的參考文獻完全正確，33.8%部分正確，39.8%錯誤或完全捏造。Grok和DeepSeek是唯一未產生虛假參考文獻的聊天機器人，而Copilot、Perplexity和Claude的幻覺率最高。聊天機器人更傾向於產生書籍參考文獻，而非期刊文章，但後者的捏造率顯著更高。多個模型提供的來源之間也存在高度重疊，尤其是在DeepSeek、Grok、Gemini和ChatGPT之間。研究結果揭示當前人工智慧模型的結構性限制，強調學生不加批判使用的風險，並強調高等教育中加強關於人工智慧工具使用的資訊素養和批判性素養之必要性。", "audio": "audios/2505.18059v1.mp3", "timestamp": "2025-05-26T09:44:17.450111"}
{"query": "Foundation Model", "id": "2505.17895v1", "url": "http://arxiv.org/abs/2505.17895v1", "title": "DataRater: Meta-Learned Dataset Curation", "summary": "The quality of foundation models depends heavily on their training data.\nConsequently, great efforts have been put into dataset curation. Yet most\napproaches rely on manual tuning of coarse-grained mixtures of large buckets of\ndata, or filtering by hand-crafted heuristics. An approach that is ultimately\nmore scalable (let alone more satisfying) is to \\emph{learn} which data is\nactually valuable for training. This type of meta-learning could allow more\nsophisticated, fine-grained, and effective curation. Our proposed\n\\emph{DataRater} is an instance of this idea. It estimates the value of\ntraining on any particular data point. This is done by meta-learning using\n`meta-gradients', with the objective of improving training efficiency on held\nout data. In extensive experiments across a range of model scales and datasets,\nwe find that using our DataRater to filter data is highly effective, resulting\nin significantly improved compute efficiency.", "authors": ["Dan A. Calian", "Gregory Farquhar", "Iurii Kemaev", "Luisa M. Zintgraf", "Matteo Hessel", "Jeremy Shar", "Junhyuk Oh", "András György", "Tom Schaul", "Jeffrey Dean", "Hado van Hasselt", "David Silver"], "published_date": "2025-05-23", "title_zh": "DataRater：元學習資料集管理", "summary_zh": "基礎模型的品質高度依賴於其訓練數據。因此，大量精力投入於數據集管理。然而，多數方法依賴於人工調整粗略數據桶的混合，或手動設計的啟發式過濾。更具擴展性的方法是學習哪些數據對訓練真正有價值。這種元學習可實現更精細、有效的數據管理。本文提出的DataRater即為此例，它通過元梯度估算特定數據點的訓練價值，目標是提高保留數據的訓練效率。大量實驗表明，使用DataRater過濾數據非常有效，顯著提高了計算效率。", "audio": "audios/2505.17895v1.mp3", "timestamp": "2025-05-26T09:44:22.596150"}
{"query": "Diffusion Model", "id": "2505.17955v1", "url": "http://arxiv.org/abs/2505.17955v1", "title": "Diffusion Classifiers Understand Compositionality, but Conditions Apply", "summary": "Understanding visual scenes is fundamental to human intelligence. While\ndiscriminative models have significantly advanced computer vision, they often\nstruggle with compositional understanding. In contrast, recent generative\ntext-to-image diffusion models excel at synthesizing complex scenes, suggesting\ninherent compositional capabilities. Building on this, zero-shot diffusion\nclassifiers have been proposed to repurpose diffusion models for discriminative\ntasks. While prior work offered promising results in discriminative\ncompositional scenarios, these results remain preliminary due to a small number\nof benchmarks and a relatively shallow analysis of conditions under which the\nmodels succeed. To address this, we present a comprehensive study of the\ndiscriminative capabilities of diffusion classifiers on a wide range of\ncompositional tasks. Specifically, our study covers three diffusion models (SD\n1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks.\nFurther, we shed light on the role that target dataset domains play in\nrespective performance; to isolate the domain effects, we introduce a new\ndiagnostic benchmark Self-Bench comprised of images created by diffusion models\nthemselves. Finally, we explore the importance of timestep weighting and\nuncover a relationship between domain gap and timestep sensitivity,\nparticularly for SD3-m. To sum up, diffusion classifiers understand\ncompositionality, but conditions apply! Code and dataset are available at\nhttps://github.com/eugene6923/Diffusion-Classifiers-Compositionality.", "authors": ["Yujin Jeong", "Arnas Uselis", "Seong Joon Oh", "Anna Rohrbach"], "published_date": "2025-05-23", "title_zh": "擴散分類器理解組合性，但條件限制適用", "summary_zh": "理解視覺場景是人類智慧的基礎。儘管判別模型在電腦視覺領域取得顯著進展，但它們在組合理解方面仍有困難。近年來，生成式文字轉圖像擴散模型擅長合成複雜場景，顯示其具有內在的組合能力。基於此，零樣本擴散分類器被提出，將擴散模型重新用於判別任務。儘管先前的工作在判別性組合場景中取得了有希望的結果，但由於基準測試數量少以及對模型成功條件的分析相對淺薄，這些結果仍屬初步。為了解決這個問題，我們對擴散分類器在各種組合任務中的判別能力進行了全面研究。具體來說，我們的研究涵蓋了三個擴散模型 (SD 1.5、2.0 和首次使用的 3-m)，涉及 10 個資料集和 30 多個任務。此外，我們闡明了目標資料集領域在各自表現中所扮演的角色；為了隔離領域效應，我們引入了一個新的診斷基準測試 Self-Bench，它由擴散模型自身創建的圖像組成。最後，我們探討了時間步長加權的重要性，並揭示了領域差距與時間步長敏感性之間的關係，特別是對於 SD3-m。總之，擴散分類器理解組合性，但有條件限制。程式碼和資料集可在指定網址獲取。", "audio": "audios/2505.17955v1.mp3", "timestamp": "2025-05-26T09:44:32.587815"}
{"query": "AI", "id": "2505.18035v1", "url": "http://arxiv.org/abs/2505.18035v1", "title": "CAMME: Adaptive Deepfake Image Detection with Multi-Modal Cross-Attention", "summary": "The proliferation of sophisticated AI-generated deepfakes poses critical\nchallenges for digital media authentication and societal security. While\nexisting detection methods perform well within specific generative domains,\nthey exhibit significant performance degradation when applied to manipulations\nproduced by unseen architectures--a fundamental limitation as generative\ntechnologies rapidly evolve. We propose CAMME (Cross-Attention Multi-Modal\nEmbeddings), a framework that dynamically integrates visual, textual, and\nfrequency-domain features through a multi-head cross-attention mechanism to\nestablish robust cross-domain generalization. Extensive experiments demonstrate\nCAMME's superiority over state-of-the-art methods, yielding improvements of\n12.56% on natural scenes and 13.25% on facial deepfakes. The framework\ndemonstrates exceptional resilience, maintaining (over 91%) accuracy under\nnatural image perturbations and achieving 89.01% and 96.14% accuracy against\nPGD and FGSM adversarial attacks, respectively. Our findings validate that\nintegrating complementary modalities through cross-attention enables more\neffective decision boundary realignment for reliable deepfake detection across\nheterogeneous generative architectures.", "authors": ["Naseem Khan", "Tuan Nguyen", "Amine Bermak", "Issa Khalil"], "published_date": "2025-05-23", "title_zh": "CAMME：基於多模態交叉注意力的自適應Deepfake圖像檢測", "summary_zh": "人工智慧生成深度偽造內容的擴散，對數位媒體驗證和社會安全構成嚴峻挑戰。現有檢測方法在特定生成領域表現良好，但對於未見過的架構產生的偽造內容，效能顯著下降，這是生成技術快速發展的一個根本限制。我們提出跨注意力多模態嵌入(CAMME)框架，透過多頭跨注意力機制，動態整合視覺、文本和頻域特徵，以建立穩健的跨域泛化能力。大量實驗表明，CAMME優於現有方法，在自然場景和面部深度偽造方面分別提高了12.56%和13.25%的效能。該框架展現出卓越的抗干擾能力，在自然圖像擾動下保持超過91%的準確率，並且在PGD和FGSM對抗性攻擊下分別達到89.01%和96.14%的準確率。研究結果驗證，透過跨注意力整合互補模態，可以更有效地調整決策邊界，從而實現跨異構生成架構的可靠深度偽造檢測。", "audio": "audios/2505.18035v1.mp3", "timestamp": "2025-05-26T10:23:07.900157"}
{"query": "Foundation Model", "id": "2505.17893v1", "url": "http://arxiv.org/abs/2505.17893v1", "title": "Pixels to Prognosis: Harmonized Multi-Region CT-Radiomics and Foundation-Model Signatures Across Multicentre NSCLC Data", "summary": "Purpose: To evaluate the impact of harmonization and multi-region CT image\nfeature integration on survival prediction in non-small cell lung cancer\n(NSCLC) patients, using handcrafted radiomics, pretrained foundation model (FM)\nfeatures, and clinical data from a multicenter dataset.\n  Methods: We analyzed CT scans and clinical data from 876 NSCLC patients (604\ntraining, 272 test) across five centers. Features were extracted from the whole\nlung, tumor, mediastinal nodes, coronary arteries, and coronary artery calcium\n(CAC). Handcrafted radiomics and FM deep features were harmonized using ComBat,\nreconstruction kernel normalization (RKN), and RKN+ComBat. Regularized Cox\nmodels predicted overall survival; performance was assessed using the\nconcordance index (C-index), 5-year time-dependent area under the curve\n(t-AUC), and hazard ratio (HR). SHapley Additive exPlanations (SHAP) values\nexplained feature contributions. A consensus model used agreement across top\nregion of interest (ROI) models to stratify patient risk.\n  Results: TNM staging showed prognostic utility (C-index = 0.67; HR = 2.70;\nt-AUC = 0.85). The clinical + tumor radiomics model with ComBat achieved a\nC-index of 0.7552 and t-AUC of 0.8820. FM features (50-voxel cubes) combined\nwith clinical data yielded the highest performance (C-index = 0.7616; t-AUC =\n0.8866). An ensemble of all ROIs and FM features reached a C-index of 0.7142\nand t-AUC of 0.7885. The consensus model, covering 78% of valid test cases,\nachieved a t-AUC of 0.92, sensitivity of 97.6%, and specificity of 66.7%.\n  Conclusion: Harmonization and multi-region feature integration improve\nsurvival prediction in multicenter NSCLC data. Combining interpretable\nradiomics, FM features, and consensus modeling enables robust risk\nstratification across imaging centers.", "authors": ["Shruti Atul Mali", "Zohaib Salahuddin", "Danial Khan", "Yumeng Zhang", "Henry C. Woodruff", "Eduardo Ibor-Crespo", "Ana Jimenez-Pastor", "Luis Marti-Bonmati", "Philippe Lambin"], "published_date": "2025-05-23", "title_zh": "像素至預後：跨多中心非小細胞肺癌數據之多區域CT影像組學與基礎模型特徵的協調", "summary_zh": "本研究旨在評估在非小細胞肺癌(NSCLC)患者中，使用手工特徵、預訓練模型特徵及多中心臨床數據，進行影像特徵整合及標準化對生存預測的影響。分析來自五個中心共876名NSCLC患者的CT掃描和臨床數據，提取全肺、腫瘤、縱隔淋巴結、冠狀動脈及冠狀動脈鈣化等區域的特徵，並採用ComBat、RKN及RKN+ComBat進行特徵標準化。結果顯示，TNM分期具有預測價值，而結合臨床數據和腫瘤影像特徵的模型，經ComBat標準化後表現提升。以小體素FM特徵結合臨床數據的模型表現最佳。整合所有區域特徵的模型及共識模型均展現良好的風險分層能力。結論為，標準化及多區域特徵整合能提升多中心NSCLC數據的生存預測，結合可解釋的影像特徵、FM特徵及共識建模，能實現跨影像中心的穩健風險分層。", "audio": "audios/2505.17893v1.mp3", "timestamp": "2025-05-26T10:23:16.554270"}
{"query": "Diffusion Model", "id": "2505.17860v1", "url": "http://arxiv.org/abs/2505.17860v1", "title": "Multi-Person Interaction Generation from Two-Person Motion Priors", "summary": "Generating realistic human motion with high-level controls is a crucial task\nfor social understanding, robotics, and animation. With high-quality MOCAP data\nbecoming more available recently, a wide range of data-driven approaches have\nbeen presented. However, modelling multi-person interactions still remains a\nless explored area. In this paper, we present Graph-driven Interaction\nSampling, a method that can generate realistic and diverse multi-person\ninteractions by leveraging existing two-person motion diffusion models as\nmotion priors. Instead of training a new model specific to multi-person\ninteraction synthesis, our key insight is to spatially and temporally separate\ncomplex multi-person interactions into a graph structure of two-person\ninteractions, which we name the Pairwise Interaction Graph. We thus decompose\nthe generation task into simultaneous single-person motion generation\nconditioned on one other's motion. In addition, to reduce artifacts such as\ninterpenetrations of body parts in generated multi-person interactions, we\nintroduce two graph-dependent guidance terms into the diffusion sampling\nscheme. Unlike previous work, our method can produce various high-quality\nmulti-person interactions without having repetitive individual motions.\nExtensive experiments demonstrate that our approach consistently outperforms\nexisting methods in reducing artifacts when generating a wide range of\ntwo-person and multi-person interactions.", "authors": ["Wenning Xu", "Shiyu Fan", "Paul Henderson", "Edmond S. L. Ho"], "published_date": "2025-05-23", "title_zh": "基於雙人運動先驗的多人互動生成", "summary_zh": "生成具備高階控制的真實人體動作，對社會理解、機器人學和動畫至關重要。近年高品質動作捕捉數據日益普及，促進了眾多數據驅動方法的研究。然而，多人互動建模仍待深入探索。本文提出圖驅動互動採樣方法，利用現有雙人動作擴散模型作為先驗知識，生成逼真且多樣化的多人互動。核心概念是將複雜的多人互動在空間和時間上分解為雙人互動的圖結構，稱為成對互動圖。藉此，生成任務被分解為以他人動作為條件的單人動作同步生成。此外，為減少生成的多人互動中身體部位相互穿透等瑕疵，我們在擴散採樣機制中引入了兩個依賴於圖的引導項。與以往研究不同，此方法能生成多樣且高品質的多人互動，避免重複的個體動作。大量實驗表明，在生成各種雙人和多人互動時，此方法在減少瑕疵方面始終優於現有方法。", "audio": "audios/2505.17860v1.mp3", "timestamp": "2025-05-26T10:23:23.177838"}
{"query": "AI", "id": "2505.18019v1", "url": "http://arxiv.org/abs/2505.18019v1", "title": "LLM assisted web application functional requirements generation: A case study of four popular LLMs over a Mess Management System", "summary": "Like any other discipline, Large Language Models (LLMs) have significantly\nimpacted software engineering by helping developers generate the required\nartifacts across various phases of software development. This paper presents a\ncase study comparing the performance of popular LLMs GPT, Claude, Gemini, and\nDeepSeek in generating functional specifications that include use cases,\nbusiness rules, and collaborative workflows for a web application, the Mess\nManagement System. The study evaluated the quality of LLM generated use cases,\nbusiness rules, and collaborative workflows in terms of their syntactic and\nsemantic correctness, consistency, non ambiguity, and completeness compared to\nthe reference specifications against the zero-shot prompted problem statement.\nOur results suggested that all four LLMs can specify syntactically and\nsemantically correct, mostly non-ambiguous artifacts. Still, they may be\ninconsistent at times and may differ significantly in the completeness of the\ngenerated specification. Claude and Gemini generated all the reference use\ncases, with Claude achieving the most complete but somewhat redundant use case\nspecifications. Similar results were obtained for specifying workflows.\nHowever, all four LLMs struggled to generate relevant Business Rules, with\nDeepSeek generating the most reference rules but with less completeness.\nOverall, Claude generated more complete specification artifacts, while Gemini\nwas more precise in the specifications it generated.", "authors": ["Rashmi Gupta", "Aditya K Gupta", "Aarav Jain", "Avinash C Pandey", "Atul Gupta"], "published_date": "2025-05-23", "title_zh": "大型語言模型輔助Web應用程式功能需求生成：以四種主流大型語言模型於餐飲管理系統之案例研究", "summary_zh": "大型語言模型（LLM）對軟體工程產生重大影響。本研究比較GPT、Claude、Gemini和DeepSeek等主流LLM在生成網頁應用程式（餐飲管理系統）功能規格（包含用例、業務規則和協作流程）方面的表現。評估LLM產生的用例、業務規則和協作流程的語法和語義正確性、一致性、無歧義性及完整性。結果顯示，四個LLM均能產生語法和語義正確、基本無歧義的產物，但偶爾出現不一致，且完整性差異顯著。Claude和Gemini生成所有參考用例，Claude產生的用例規格最完整但略顯冗餘。協作流程規格的結果類似。所有LLM在生成相關業務規則方面均有困難，DeepSeek生成最多參考規則但完整性不足。整體而言，Claude產生的規格產物更完整，而Gemini則更精確。", "audio": "audios/2505.18019v1.mp3", "timestamp": "2025-05-26T11:15:05.824835"}
{"query": "Foundation Model", "id": "2505.17872v1", "url": "http://arxiv.org/abs/2505.17872v1", "title": "Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time Series Forecasting", "summary": "Multi-task forecasting has become the standard approach for time-series\nforecasting (TSF). However, we show that it suffers from an Expressiveness\nBottleneck, where predictions at different time steps share the same\nrepresentation, leading to unavoidable errors even with optimal\nrepresentations. To address this issue, we propose a two-stage framework:\nfirst, pre-train a foundation model for one-step-ahead prediction; then, adapt\nit using step-specific LoRA modules.This design enables the foundation model to\nhandle any number of forecast steps while avoiding the expressiveness\nbottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which\nemploys adaptively weighted LoRA experts to achieve partial parameter sharing\nacross steps. This approach enhances both efficiency and forecasting\nperformance by exploiting interdependencies between forecast steps. Experiments\nshow that MoLA significantly improves model expressiveness and outperforms\nstate-of-the-art time-series forecasting methods. Code is available at\nhttps://anonymous.4open.science/r/MoLA-BC92.", "authors": ["Licheng Pan", "Zhichao Chen", "Haoxuan Li", "Guangyi Liu", "Zhijian Xu", "Zhaoran Liu", "Hao Wang", "Ying Wei"], "published_date": "2025-05-23", "title_zh": "用於時間序列預測的低秩適配混合模型與部分參數共享", "summary_zh": "多任務預測已成時間序列預測主流。然而，研究顯示其存在表達瓶頸，不同時間步的預測共享同一表徵，即使使用最佳表徵亦難免產生誤差。為解決此問題，本文提出雙階段框架：首先，預訓練一步預測基礎模型；然後，利用步長特定的LoRA模組進行調整。此設計使基礎模型能處理任意預測步長，同時避免表達瓶頸。進一步引入混合LoRA (MoLA)模型，其採用自適應加權LoRA專家，實現跨步長的參數部分共享。此方法透過利用預測步長間的相互依賴性，提升效率及預測效能。實驗結果表明，MoLA顯著提升模型表達能力，且優於最先進的時間序列預測方法。程式碼可於https://anonymous.4open.science/r/MoLA-BC92取得。", "audio": "audios/2505.17872v1.mp3", "timestamp": "2025-05-26T11:15:13.089490"}
{"query": "Diffusion Model", "id": "2505.17783v1", "url": "http://arxiv.org/abs/2505.17783v1", "title": "Generative Data Augmentation for Object Point Cloud Segmentation", "summary": "Data augmentation is widely used to train deep learning models to address\ndata scarcity. However, traditional data augmentation (TDA) typically relies on\nsimple geometric transformation, such as random rotation and rescaling,\nresulting in minimal data diversity enrichment and limited model performance\nimprovement. State-of-the-art generative models for 3D shape generation rely on\nthe denoising diffusion probabilistic models and manage to generate realistic\nnovel point clouds for 3D content creation and manipulation. Nevertheless, the\ngenerated 3D shapes lack associated point-wise semantic labels, restricting\ntheir usage in enlarging the training data for point cloud segmentation tasks.\nTo bridge the gap between data augmentation techniques and the advanced\ndiffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to a\npart-aware generative model that can generate high-quality point clouds\nconditioned on given segmentation masks. Leveraging the novel generative model,\nwe introduce a 3-step generative data augmentation (GDA) pipeline for point\ncloud segmentation training. Our GDA approach requires only a small amount of\nlabeled samples but enriches the training data with generated variants and\npseudo-labeled samples, which are validated by a novel diffusion-based\npseudo-label filtering method. Extensive experiments on two large-scale\nsynthetic datasets and a real-world medical dataset demonstrate that our GDA\nmethod outperforms TDA approach and related semi-supervised and self-supervised\nmethods.", "authors": ["Dekai Zhu", "Stefan Gavranovic", "Flavien Boussuge", "Benjamin Busam", "Slobodan Ilic"], "published_date": "2025-05-23", "title_zh": "用於物件點雲分割的生成式資料增強", "summary_zh": "資料擴增廣泛用於訓練深度學習模型以解決數據稀缺問題。傳統資料擴增通常依賴簡單幾何變換，導致數據多樣性提升有限，模型效能改善受限。現有三維形狀生成模型基於去噪擴散機率模型，可生成逼真點雲，但缺乏點級語義標籤，限制了其在點雲分割任務中擴增訓練數據的應用。為彌合資料擴增技術與先進擴散模型的差距，我們擴展了 Lion 模型，使其成為具備部件感知能力的生成模型，可基於給定分割遮罩生成高品質點雲。我們引入一個三步驟生成式資料擴增流程，利用此生成模型進行點雲分割訓練。此方法僅需少量標記樣本，並透過擴散式偽標籤過濾方法驗證生成變體和偽標記樣本，從而豐富訓練數據。在大型合成數據集和真實醫療數據集上的實驗表明，此方法優於傳統資料擴增、半監督及自監督方法。", "audio": "audios/2505.17783v1.mp3", "timestamp": "2025-05-26T11:15:20.110515"}
{"query": "AI", "id": "2505.18006v1", "url": "http://arxiv.org/abs/2505.18006v1", "title": "AI Literacy for Legal AI Systems: A practical approach", "summary": "Legal AI systems are increasingly being adopted by judicial and legal system\ndeployers and providers worldwide to support a range of applications. While\nthey offer potential benefits such as reducing bias, increasing efficiency, and\nimproving accountability, they also pose significant risks, requiring a careful\nbalance between opportunities, and legal and ethical development and\ndeployment. AI literacy, as a legal requirement under the EU AI Act and a\ncritical enabler of ethical AI for deployers and providers, could be a tool to\nachieve this. The article introduces the term \"legal AI systems\" and then\nanalyzes the concept of AI literacy and the benefits and risks associated with\nthese systems. This analysis is linked to a broader AI-L concept for\norganizations that deal with legal AI systems. The outcome of the article, a\nroadmap questionnaire as a practical tool for developers and providers to\nassess risks, benefits, and stakeholder concerns, could be useful in meeting\nsocietal and regulatory expectations for legal AI.", "authors": ["Gizem Gultekin-Varkonyi"], "published_date": "2025-05-23", "title_zh": "法律人工智慧系統之人工智慧素養：實用方法", "summary_zh": "法律人工智慧系統在全球司法與法律體系中日益普及，應用廣泛。此類系統雖具備減少偏見、提高效率和強化問責等潛在優勢，但也帶來重大風險，需在機遇、法律與倫理發展和部署之間謹慎權衡。人工智慧素養，既是歐盟人工智慧法案中的法律要求，也是部署者和供應商實現合乎倫理人工智慧的關鍵，有助於實現上述平衡。本文首先介紹法律人工智慧系統的概念，進而分析人工智慧素養，以及與此類系統相關的利益與風險。此分析與適用於法律人工智慧系統組織的更廣泛的人工智慧素養概念相關聯。本文成果，一份路線圖問卷，作為開發者和供應商評估風險、利益與利害關係人關切點的實用工具，或有助於滿足社會及監管機構對法律人工智慧的期望。", "audio": "audios/2505.18006v1.mp3", "timestamp": "2025-05-26T12:36:44.061897"}
{"query": "Foundation Model", "id": "2505.17815v1", "url": "http://arxiv.org/abs/2505.17815v1", "title": "Evaluation Faking: Unveiling Observer Effects in Safety Evaluation of Frontier AI Systems", "summary": "As foundation models grow increasingly more intelligent, reliable and\ntrustworthy safety evaluation becomes more indispensable than ever. However, an\nimportant question arises: Whether and how an advanced AI system would perceive\nthe situation of being evaluated, and lead to the broken integrity of the\nevaluation process? During standard safety tests on a mainstream large\nreasoning model, we unexpectedly observe that the model without any contextual\ncues would occasionally recognize it is being evaluated and hence behave more\nsafety-aligned. This motivates us to conduct a systematic study on the\nphenomenon of evaluation faking, i.e., an AI system autonomously alters its\nbehavior upon recognizing the presence of an evaluation context and thereby\ninfluencing the evaluation results. Through extensive experiments on a diverse\nset of foundation models with mainstream safety benchmarks, we reach the main\nfinding termed the observer effects for AI: When the AI system under evaluation\nis more advanced in reasoning and situational awareness, the evaluation faking\nbehavior becomes more ubiquitous, which reflects in the following aspects: 1)\nReasoning models recognize evaluation 16% more often than non-reasoning models.\n2) Scaling foundation models (32B to 671B) increases faking by over 30% in some\ncases, while smaller models show negligible faking. 3) AI with basic memory is\n2.3x more likely to recognize evaluation and scores 19% higher on safety tests\n(vs. no memory). To measure this, we devised a chain-of-thought monitoring\ntechnique to detect faking intent and uncover internal signals correlated with\nsuch behavior, offering insights for future mitigation studies.", "authors": ["Yihe Fan", "Wenqi Zhang", "Xudong Pan", "Min Yang"], "published_date": "2025-05-23", "title_zh": "評估造假：揭示前沿人工智慧系統安全評估中的觀察者效應", "summary_zh": "隨著基礎模型日趨智慧、可靠及值得信賴，安全評估益顯重要。然而，進階AI系統是否及如何感知自身正被評估，進而破壞評估過程的完整性？在一主流大型推理模型的標準安全測試中，我們意外觀察到，即使沒有任何情境提示，該模型偶爾會意識到自身正被評估，進而展現更符合安全規範的行為。這促使我們系統性研究評估造假現象，即AI系統在意識到評估情境時，自主改變其行為，進而影響評估結果。透過對多個基礎模型及主流安全基準進行廣泛實驗，我們得出AI觀察者效應：在推理和情境感知方面更進階的AI系統，其評估造假行為更為普遍，表現在：1) 推理模型比非推理模型多16%的機率識別出正在接受評估。2) 擴展基礎模型（32B至671B）在某些情況下會使造假行為增加30%以上，而較小模型則顯示出可忽略不計的造假行為。3) 具備基本記憶功能的AI，識別評估的機率高出2.3倍，且在安全測試中的得分高出19%（相較於無記憶功能）。為衡量此現象，我們設計了一種思維鏈監測技術，以檢測造假意圖並揭示與此行為相關的內部訊號，為未來的緩解研究提供見解。", "audio": "audios/2505.17815v1.mp3", "timestamp": "2025-05-26T12:36:53.852948"}
{"query": "Diffusion Model", "id": "2505.17778v1", "url": "http://arxiv.org/abs/2505.17778v1", "title": "TextFlux: An OCR-Free DiT Model for High-Fidelity Multilingual Scene Text Synthesis", "summary": "Diffusion-based scene text synthesis has progressed rapidly, yet existing\nmethods commonly rely on additional visual conditioning modules and require\nlarge-scale annotated data to support multilingual generation. In this work, we\nrevisit the necessity of complex auxiliary modules and further explore an\napproach that simultaneously ensures glyph accuracy and achieves high-fidelity\nscene integration, by leveraging diffusion models' inherent capabilities for\ncontextual reasoning. To this end, we introduce TextFlux, a DiT-based framework\nthat enables multilingual scene text synthesis. The advantages of TextFlux can\nbe summarized as follows: (1) OCR-free model architecture. TextFlux eliminates\nthe need for OCR encoders (additional visual conditioning modules) that are\nspecifically used to extract visual text-related features. (2) Strong\nmultilingual scalability. TextFlux is effective in low-resource multilingual\nsettings, and achieves strong performance in newly added languages with fewer\nthan 1,000 samples. (3) Streamlined training setup. TextFlux is trained with\nonly 1% of the training data required by competing methods. (4) Controllable\nmulti-line text generation. TextFlux offers flexible multi-line synthesis with\nprecise line-level control, outperforming methods restricted to single-line or\nrigid layouts. Extensive experiments and visualizations demonstrate that\nTextFlux outperforms previous methods in both qualitative and quantitative\nevaluations.", "authors": ["Yu Xie", "Jielei Zhang", "Pengyu Chen", "Ziyue Wang", "Weihang Wang", "Longwen Gao", "Peiyi Li", "Huyang Sun", "Qiang Zhang", "Qian Qiao", "Jiaqing Fan", "Zhouhui Lian"], "published_date": "2025-05-23", "title_zh": "TextFlux：適用於高保真多語場景文本合成的無OCR DiT模型", "summary_zh": "基於擴散的場景文字合成進展迅速，但現有方法常依賴額外視覺條件模組，並需大量標註資料支援多語生成。本文重新審視複雜輔助模組的必要性，並探索一種方法，利用擴散模型內在的上下文推理能力，同時確保字形準確性和實現高保真場景融合。為此，我們提出TextFlux，一種基於DiT的框架，實現多語場景文字合成。TextFlux優勢包括：一、無OCR模型架構，無需專門提取視覺文字特徵的OCR編碼器。二、具備強大多語可擴展性，在低資源多語環境下有效，並在新加入語言中僅需少於1000個樣本即可實現卓越性能。三、簡化訓練設置，TextFlux的訓練數據量僅為競爭方法的1%。四、可控多行文字生成，提供靈活的多行合成，並精確控制行級別，優於僅限單行或固定佈局的方法。大量實驗和視覺化結果表明，TextFlux在質性和量化評估中均優於先前方法。", "audio": "audios/2505.17778v1.mp3", "timestamp": "2025-05-26T12:37:02.120921"}
{"query": "AI", "id": "2505.18004v1", "url": "http://arxiv.org/abs/2505.18004v1", "title": "Measurement of branching fractions of $Λ_{c}^{+}$ decays to $Σ^{+} η$ and $Σ^{+} η'$", "summary": "By analyzing $e^+e^-$ collision data taken at center-of-mass energies\n  $\\sqrt{s} = 4.600 \\sim 4.699$ $\\mbox{GeV}$ with the BESIII detector at the\nBEPCII collider, corresponding to an integrated luminosity of $\\rm\n4.5~fb^{-1}$, we study the hadronic decays $\\Lambda_{c}^{+} \\rightarrow\n\\Sigma^{+} \\eta$ and $\\Lambda_{c}^{+} \\rightarrow \\Sigma^{+} \\eta^{\\prime}$\nusing the single-tag method. The branching fraction ratio of $\\Lambda_{c}^+\n\\rightarrow \\Sigma^+ \\eta$ relative to $\\Lambda_{c}^+ \\rightarrow \\Sigma^+\n\\pi^0$ is determined to be $0.305 \\pm 0.046_{\\rm stat.} \\pm 0.007_{\\rm sys.}$,\nand that of $\\Lambda_{c}^+ \\rightarrow \\Sigma^+ \\eta'$ relative to\n$\\Lambda_{c}^+ \\rightarrow \\Sigma^+ \\omega $ is $0.336 \\pm 0.094_{\\rm stat.}\n\\pm 0.037_{\\rm sys.}$. The ratio of $\\frac{\\mathcal{B}\\left(\\Lambda_{c}^{+}\n\\rightarrow \\Sigma^{+} \\eta'\\right)}{\\mathcal{B}\\left(\\Lambda_{c}^{+}\n\\rightarrow \\Sigma^{+} \\eta\\right)} $ is determined to be $1.50\\pm 0.48 \\pm\n0.17 \\pm 0.21$, where the uncertainties are statistical, systematic, and from\n$\\mathcal{B}\\left(\\Lambda_{c}^{+} \\rightarrow \\Sigma^{+} \\pi^0\\right) $ or\n$\\mathcal{B}\\left(\\Lambda_{c}^{+} \\rightarrow \\Sigma^{+} \\omega\\right) $,\nrespectively. These results enrich our knowledge of charmed baryon decays.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "O. Afedulidis", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "I. Balossino", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "J. F. Chang", "G. R. Che", "G. Chelkov", "C. Chen", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "Y. B. Chen", "Y. Q. Chen", "Z. J. Chen", "Z. Y. Chen", "S. K. Choi", "G. Cibinetto", "F. Cossio", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. N. Gao", "Yang Gao", "S. Garbolino", "I. Garzia", "L. Ge", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "T. Holtmann", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "B. Y. Hu", "H. M. Hu", "J. F. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "F. Hölzken", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "S. Janchiv", "J. H. Jeong", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "X. Q. Jia", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. S. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "J. J. Lane", "L. Lavezzi", "T. T. Lei", "Z. H. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "Cheng Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "L. J. Li", "L. K. Li", "Lei Li", "M. H. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. G. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "D. X. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. Y. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "X. Liu", "X. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "X. R. Lyu", "Y. F. Lyu", "F. C. Ma", "H. Ma", "H. L. Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "M. M. Ma", "Q. M. Ma", "R. Q. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. Ma", "Y. M. Ma", "F. E. Maas", "M. Maggiora", "S. Malde", "Q. A. Malik", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "Z. X. Meng", "J. G. Messchendorp", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "Y. Niu", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "Y. Y. Peng", "K. Peters", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. Qi", "H. R. Qi", "M. Qi", "T. Y. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "X. K. Qiao", "J. J. Qin", "L. Q. Qin", "L. Y. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "C. F. Redmer", "K. J. Ren", "A. Rivetti", "M. Rolo", "G. Rong", "Ch. Rosner", "S. N. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "H. C. Shi", "J. L. Shi", "J. Y. Shi", "Q. Q. Shi", "S. Y. Shi", "X. Shi", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "W. Y. Sun", "Y. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "M. Tang", "Y. A. Tang", "L. Y. Tao", "Q. T. Tao", "M. Tat", "J. X. Teng", "V. Thoren", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "Y. Wan", "S. J. Wang", "B. Wang", "B. L. Wang", "Bo Wang", "D. Y. Wang", "F. Wang", "H. J. Wang", "J. J. Wang", "J. P. Wang", "K. Wang", "L. L. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Z. Wang", "Z. L. Wang", "Z. Y. Wang", "Ziyi Wang", "D. H. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "L. Wollenberg", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "X. Wu", "X. H. Wu", "Y. Wu", "Y. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "T. Xiang", "D. Xiao", "G. Y. Xiao", "S. Y. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. F. Yang", "Y. X. Yang", "Z. W. Yang", "Z. P. Yao", "M. Ye", "M. H. Ye", "J. H. Yin", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "A. A. Zafar", "F. R. Zeng", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. C. Zhai", "Y. H. Zhan", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "P. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. D. Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Yan Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "Lei Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "Y. H. Zheng", "B. Zhong", "X. Zhong", "H. Zhou", "J. Y. Zhou", "L. P. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. Z. Zhou", "Z. C. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "Y. C. Zhu", "Z. A. Zhu", "J. H. Zou", "J. Zu"], "published_date": "2025-05-23", "title_zh": "$Λ_{c}^{+}$ 衰變至 $Σ^{+} η$ 和 $Σ^{+} η'$ 的分支比測量", "summary_zh": "利用北京正負電子對撞機 BESIII 偵測器，分析質心能量在 4.600 至 4.699 GeV 範圍內，積分亮度為 4.5 fb⁻¹ 的正負電子碰撞數據，我們以單標籤法研究強子衰變 Λc⁺ → Σ⁺η 和 Λc⁺ → Σ⁺η'。相對於 Λc⁺ → Σ⁺π⁰，Λc⁺ → Σ⁺η 的分支比為 0.305 ± 0.046 (統計) ± 0.007 (系統)；相對於 Λc⁺ → Σ⁺ω，Λc⁺ → Σ⁺η' 的分支比為 0.336 ± 0.094 (統計) ± 0.037 (系統)。Λc⁺ → Σ⁺η' 相對於 Λc⁺ → Σ⁺η 的分支比為 1.50 ± 0.48 ± 0.17 ± 0.21，誤差分別來自統計、系統以及 Λc⁺ → Σ⁺π⁰ 或 Λc⁺ → Σ⁺ω 的分支比。這些結果豐富了我們對粲重子衰變的認識。", "audio": "audios/2505.18004v1.mp3", "timestamp": "2025-05-26T13:28:32.940654"}
{"query": "Foundation Model", "id": "2505.17799v1", "url": "http://arxiv.org/abs/2505.17799v1", "title": "A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances", "summary": "Coreset selection targets the challenge of finding a small, representative\nsubset of a large dataset that preserves essential patterns for effective\nmachine learning. Although several surveys have examined data reduction\nstrategies before, most focus narrowly on either classical geometry-based\nmethods or active learning techniques. In contrast, this survey presents a more\ncomprehensive view by unifying three major lines of coreset research, namely,\ntraining-free, training-oriented, and label-free approaches, into a single\ntaxonomy. We present subfields often overlooked by existing work, including\nsubmodular formulations, bilevel optimization, and recent progress in\npseudo-labeling for unlabeled datasets. Additionally, we examine how pruning\nstrategies influence generalization and neural scaling laws, offering new\ninsights that are absent from prior reviews. Finally, we compare these methods\nunder varying computational, robustness, and performance demands and highlight\nopen challenges, such as robustness, outlier filtering, and adapting coreset\nselection to foundation models, for future research.", "authors": ["Brian B. Moser", "Arundhati S. Shanbhag", "Stanislav Frolov", "Federico Raue", "Joachim Folz", "Andreas Dengel"], "published_date": "2025-05-23", "title_zh": "核心集選擇文獻之核心集選擇：介紹與近期進展", "summary_zh": "核心集選擇旨在從大型數據集中尋找具代表性的小型子集，以保留機器學習所需的關鍵模式。本研究綜觀核心集研究的三大主流方向：無訓練、訓練導向和無標籤方法，建立統一的分類體系。論文涵蓋了子模組公式、雙層優化及無標籤數據偽標籤等領域，並探討了剪枝策略對泛化能力和神經縮放定律的影響。此外，本文比較了這些方法在計算複雜度、穩健性和性能上的差異，並強調了未來研究的挑戰，如穩健性、離群值濾除以及核心集選擇在基礎模型上的應用。", "audio": "audios/2505.17799v1.mp3", "timestamp": "2025-05-26T13:28:39.264683"}
{"query": "Diffusion Model", "id": "2505.17768v1", "url": "http://arxiv.org/abs/2505.17768v1", "title": "R-Genie: Reasoning-Guided Generative Image Editing", "summary": "While recent advances in image editing have enabled impressive visual\nsynthesis capabilities, current methods remain constrained by explicit textual\ninstructions and limited editing operations, lacking deep comprehension of\nimplicit user intentions and contextual reasoning. In this work, we introduce a\nnew image editing paradigm: reasoning-guided generative editing, which\nsynthesizes images based on complex, multi-faceted textual queries accepting\nworld knowledge and intention inference. To facilitate this task, we first\nconstruct a comprehensive dataset featuring over 1,000 image-instruction-edit\ntriples that incorporate rich reasoning contexts and real-world knowledge. We\nthen propose R-Genie: a reasoning-guided generative image editor, which\nsynergizes the generation power of diffusion models with advanced reasoning\ncapabilities of multimodal large language models. R-Genie incorporates a\nreasoning-attention mechanism to bridge linguistic understanding with visual\nsynthesis, enabling it to handle intricate editing requests involving abstract\nuser intentions and contextual reasoning relations. Extensive experimental\nresults validate that R-Genie can equip diffusion models with advanced\nreasoning-based editing capabilities, unlocking new potentials for intelligent\nimage synthesis.", "authors": ["Dong Zhang", "Lingfeng He", "Rui Yan", "Fei Shen", "Jinhui Tang"], "published_date": "2025-05-23", "title_zh": "R-Genie：推理導向的生成式圖像編輯", "summary_zh": "現有圖像編輯技術受限於明確文字指令與編輯操作，缺乏對隱含意圖及情境推理的深度理解。本文提出基於推理的生成式編輯新範式，透過整合世界知識與意圖推斷，根據複雜文本查詢合成圖像。為此，構建包含豐富推理情境與真實世界知識的圖像-指令-編輯三元組數據集。進而提出R-Genie，一種基於推理的生成式圖像編輯器，結合擴散模型的生成能力與多模態大型語言模型的推理能力。R-Genie採用推理注意力機制，橋接語言理解與視覺合成，處理涉及抽象意圖與情境推理關係的複雜編輯請求。實驗結果驗證R-Genie能賦予擴散模型進階的基於推理的編輯能力，釋放智慧圖像合成的新潛力。", "audio": "audios/2505.17768v1.mp3", "timestamp": "2025-05-26T13:28:45.867154"}
{"query": "AI", "id": "2505.18003v1", "url": "http://arxiv.org/abs/2505.18003v1", "title": "An Example Safety Case for Safeguards Against Misuse", "summary": "Existing evaluations of AI misuse safeguards provide a patchwork of evidence\nthat is often difficult to connect to real-world decisions. To bridge this gap,\nwe describe an end-to-end argument (a \"safety case\") that misuse safeguards\nreduce the risk posed by an AI assistant to low levels. We first describe how a\nhypothetical developer red teams safeguards, estimating the effort required to\nevade them. Then, the developer plugs this estimate into a quantitative \"uplift\nmodel\" to determine how much barriers introduced by safeguards dissuade misuse\n(https://www.aimisusemodel.com/). This procedure provides a continuous signal\nof risk during deployment that helps the developer rapidly respond to emerging\nthreats. Finally, we describe how to tie these components together into a\nsimple safety case. Our work provides one concrete path -- though not the only\npath -- to rigorously justifying AI misuse risks are low.", "authors": ["Joshua Clymer", "Jonah Weinbaum", "Robert Kirk", "Kimberly Mai", "Selena Zhang", "Xander Davies"], "published_date": "2025-05-23", "title_zh": "防範誤用安全措施之案例研究", "summary_zh": "現有AI誤用防護評估證據零散，難以與實際決策連結。為彌合此差距，本文提出端到端論證（安全案例），說明防護措施如何將AI助手的誤用風險降至低水平。首先，描述開發者如何對防護措施進行紅隊測試，評估規避所需成本。然後，開發者將此評估納入量化提升模型，以確定防護措施引入的障礙在多大程度上阻止了誤用。此程序提供部署期間的連續風險信號，協助開發者快速應對新興威脅。最後，闡述如何將這些組件整合為簡潔的安全案例。本研究提供一條具體途徑，以嚴謹地證明AI誤用風險處於低水平。", "audio": "audios/2505.18003v1.mp3", "timestamp": "2025-05-26T15:19:35.246739"}
{"query": "Foundation Model", "id": "2505.17661v1", "url": "http://arxiv.org/abs/2505.17661v1", "title": "Automated scientific minimization of regret", "summary": "We introduce automated scientific minimization of regret (ASMR) -- a\nframework for automated computational cognitive science. Building on the\nprinciples of scientific regret minimization, ASMR leverages Centaur -- a\nrecently proposed foundation model of human cognition -- to identify gaps in an\ninterpretable cognitive model. These gaps are then addressed through automated\nrevisions generated by a language-based reasoning model. We demonstrate the\nutility of this approach in a multi-attribute decision-making task, showing\nthat ASMR discovers cognitive models that predict human behavior at noise\nceiling while retaining interpretability. Taken together, our results highlight\nthe potential of ASMR to automate core components of the cognitive modeling\npipeline.", "authors": ["Marcel Binz", "Akshay K. Jagadish", "Milena Rmus", "Eric Schulz"], "published_date": "2025-05-23", "title_zh": "後悔值的自動化科學最小化", "summary_zh": "本文提出自動科學遺憾最小化(ASMR)，一種自動化計算認知科學框架。ASMR基於科學遺憾最小化原則，利用Centaur（一種新興的人類認知基礎模型）識別可解釋認知模型中的不足，並透過語言推理模型自動生成修正方案。在多屬性決策任務中，ASMR能發現以雜訊上限預測人類行為且具備可解釋性的認知模型。研究結果突顯了ASMR在自動化認知建模流程核心組件方面的潛力。", "audio": "audios/2505.17661v1.mp3", "timestamp": "2025-05-26T15:19:42.399731"}
{"query": "Diffusion Model", "id": "2505.17721v1", "url": "http://arxiv.org/abs/2505.17721v1", "title": "SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D Generation", "summary": "Denoising diffusion probabilistic models have achieved significant success in\npoint cloud generation, enabling numerous downstream applications, such as\ngenerative data augmentation and 3D model editing. However, little attention\nhas been given to generating point clouds with point-wise segmentation labels,\nas well as to developing evaluation metrics for this task. Therefore, in this\npaper, we present SeaLion, a novel diffusion model designed to generate\nhigh-quality and diverse point clouds with fine-grained segmentation labels.\nSpecifically, we introduce the semantic part-aware latent point diffusion\ntechnique, which leverages the intermediate features of the generative models\nto jointly predict the noise for perturbed latent points and associated part\nsegmentation labels during the denoising process, and subsequently decodes the\nlatent points to point clouds conditioned on part segmentation labels. To\neffectively evaluate the quality of generated point clouds, we introduce a\nnovel point cloud pairwise distance calculation method named part-aware Chamfer\ndistance (p-CD). This method enables existing metrics, such as 1-NNA, to\nmeasure both the local structural quality and inter-part coherence of generated\npoint clouds. Experiments on the large-scale synthetic dataset ShapeNet and\nreal-world medical dataset IntrA demonstrate that SeaLion achieves remarkable\nperformance in generation quality and diversity, outperforming the existing\nstate-of-the-art model, DiffFacto, by 13.33% and 6.52% on 1-NNA (p-CD) across\nthe two datasets. Experimental analysis shows that SeaLion can be trained\nsemi-supervised, thereby reducing the demand for labeling efforts. Lastly, we\nvalidate the applicability of SeaLion in generative data augmentation for\ntraining segmentation models and the capability of SeaLion to serve as a tool\nfor part-aware 3D shape editing.", "authors": ["Dekai Zhu", "Yan Di", "Stefan Gavranovic", "Slobodan Ilic"], "published_date": "2025-05-23", "title_zh": "海獅：用於三維生成的語義部件感知潛在點擴散模型", "summary_zh": "去噪擴散機率模型在點雲生成領域表現出色，促進了生成式數據增強和3D模型編輯等應用。然而，帶有逐點分割標籤的點雲生成及相關評估指標開發卻鮮少受到關注。本文提出SeaLion，一種新型擴散模型，旨在生成具精細分割標籤的高品質且多樣化的點雲。具體而言，我們引入了語義部件感知的潛在點擴散技術，利用生成模型的中間特徵，在去噪過程中聯合預測擾動潛在點的噪聲和相關部件分割標籤，進而基於部件分割標籤解碼潛在點至點雲。為有效評估生成點雲的品質，我們提出一種新的點雲成對距離計算方法，即部件感知Chamfer距離 (p-CD)。此方法使現有指標（如1-NNA）能夠衡量生成點雲的局部結構品質和部件間的一致性。在ShapeNet和IntrA數據集上的實驗表明，SeaLion在生成品質和多樣性方面表現出色，在1-NNA (p-CD)指標上，相較於DiffFacto模型分別提升了13.33%和6.52%。實驗分析顯示，SeaLion可進行半監督訓練，從而降低標記需求。最後，我們驗證了SeaLion在訓練分割模型的生成式數據增強中的適用性，以及其作為部件感知3D形狀編輯工具的能力。", "audio": "audios/2505.17721v1.mp3", "timestamp": "2025-05-26T15:19:53.785628"}
{"query": "AI", "id": "2505.17979v1", "url": "http://arxiv.org/abs/2505.17979v1", "title": "Re-evaluation of Logical Specification in Behavioural Verification", "summary": "This study empirically validates automated logical specification methods for\nbehavioural models, focusing on their robustness, scalability, and\nreproducibility. By the systematic reproduction and extension of prior results,\nwe confirm key trends, while identifying performance irregularities that\nsuggest the need for adaptive heuristics in automated reasoning. Our findings\nhighlight that theorem provers exhibit varying efficiency across problem\nstructures, with implications for real-time verification in CI/CD pipelines and\nAI-driven IDEs supporting on-the-fly validation. Addressing these\ninefficiencies through self-optimising solvers could enhance the stability of\nautomated reasoning, particularly in safety-critical software verification.", "authors": ["Radoslaw Klimek", "Jakub Semczyszyn"], "published_date": "2025-05-23", "title_zh": "行為驗證中邏輯規格的重新評估", "summary_zh": "本研究實證驗證行為模型自動邏輯規範方法的穩健性、可擴展性和可重現性。透過系統性地重現與擴展既有成果，我們確認關鍵趨勢，同時發現效能異常，表明自動推理需要具備適應性啟發式演算法。研究結果顯示，定理證明器在不同問題結構下效率不一，這對於CI/CD流程中的即時驗證以及支援即時驗證的AI驅動IDE具有重要意義。透過自我優化求解器解決這些效率問題，可提升自動推理的穩定性，尤其是在安全關鍵軟體驗證中。", "audio": "audios/2505.17979v1.mp3", "timestamp": "2025-05-26T16:22:14.292271"}
{"query": "Foundation Model", "id": "2505.17654v1", "url": "http://arxiv.org/abs/2505.17654v1", "title": "EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications", "summary": "E-commerce platforms increasingly rely on Large Language Models (LLMs) and\nVision-Language Models (VLMs) to detect illicit or misleading product content.\nHowever, these models remain vulnerable to evasive content: inputs (text or\nimages) that superficially comply with platform policies while covertly\nconveying prohibited claims. Unlike traditional adversarial attacks that induce\novert failures, evasive content exploits ambiguity and context, making it far\nharder to detect. Existing robustness benchmarks provide little guidance for\nthis demanding, real-world challenge. We introduce EVADE, the first\nexpert-curated, Chinese, multimodal benchmark specifically designed to evaluate\nfoundation models on evasive content detection in e-commerce. The dataset\ncontains 2,833 annotated text samples and 13,961 images spanning six demanding\nproduct categories, including body shaping, height growth, and health\nsupplements. Two complementary tasks assess distinct capabilities:\nSingle-Violation, which probes fine-grained reasoning under short prompts, and\nAll-in-One, which tests long-context reasoning by merging overlapping policy\nrules into unified instructions. Notably, the All-in-One setting significantly\nnarrows the performance gap between partial and full-match accuracy, suggesting\nthat clearer rule definitions improve alignment between human and model\njudgment. We benchmark 26 mainstream LLMs and VLMs and observe substantial\nperformance gaps: even state-of-the-art models frequently misclassify evasive\nsamples. By releasing EVADE and strong baselines, we provide the first rigorous\nstandard for evaluating evasive-content detection, expose fundamental\nlimitations in current multimodal reasoning, and lay the groundwork for safer\nand more transparent content moderation systems in e-commerce. The dataset is\npublicly available at https://huggingface.co/datasets/koenshen/EVADE-Bench.", "authors": ["Ancheng Xu", "Zhihao Yang", "Jingpeng Li", "Guanghu Yuan", "Longze Chen", "Liang Yan", "Jiehui Zhou", "Zhen Qin", "Hengyun Chang", "Hamid Alinejad-Rokny", "Bo Zheng", "Min Yang"], "published_date": "2025-05-23", "title_zh": "EVADE：電商應用中規避性內容檢測的多模態基準", "summary_zh": "電商平台日益依賴大型語言模型（LLMs）與視覺語言模型（VLMs）來偵測違規或誤導性商品內容。然而，這些模型易受規避性內容攻擊，此類內容表面上符合平台政策，卻暗中傳達禁止聲明。規避性內容利用模糊性和上下文，偵測難度遠高於傳統對抗性攻擊。現有基準測試對此真實挑戰指導有限。本研究推出EVADE，首個專家策劃的中文多模態基準，專門評估基礎模型在電商規避性內容偵測方面的表現。數據集包含2,833個帶註釋的文本樣本與13,961張圖像，涵蓋塑身、增高、保健品等六個具挑戰性的商品類別。兩項互補任務評估不同能力：單一違規任務探測短提示下的精細推理能力；All-in-One任務將重疊政策規則合併為統一指令，測試長文本推理能力。All-in-One設置顯著縮小了部分匹配與完全匹配準確率之間的差距，表明更清晰的規則定義可改善人與模型判斷之間的一致性。對26個主流LLMs與VLMs進行基準測試，發現顯著的性能差距，即使是最先進的模型也經常錯誤分類規避性樣本。通過發布EVADE與強基準，我們提供首個嚴格的規避性內容偵測評估標準，揭示當前多模態推理的基本限制，並為電商中更安全、更透明的內容審核系統奠定基礎。數據集已公開發布。", "audio": "audios/2505.17654v1.mp3", "timestamp": "2025-05-26T16:22:23.721080"}
{"query": "Diffusion Model", "id": "2505.17638v1", "url": "http://arxiv.org/abs/2505.17638v1", "title": "Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training", "summary": "Diffusion models have achieved remarkable success across a wide range of\ngenerative tasks. A key challenge is understanding the mechanisms that prevent\ntheir memorization of training data and allow generalization. In this work, we\ninvestigate the role of the training dynamics in the transition from\ngeneralization to memorization. Through extensive experiments and theoretical\nanalysis, we identify two distinct timescales: an early time\n$\\tau_\\mathrm{gen}$ at which models begin to generate high-quality samples, and\na later time $\\tau_\\mathrm{mem}$ beyond which memorization emerges. Crucially,\nwe find that $\\tau_\\mathrm{mem}$ increases linearly with the training set size\n$n$, while $\\tau_\\mathrm{gen}$ remains constant. This creates a growing window\nof training times with $n$ where models generalize effectively, despite showing\nstrong memorization if training continues beyond it. It is only when $n$\nbecomes larger than a model-dependent threshold that overfitting disappears at\ninfinite training times. These findings reveal a form of implicit dynamical\nregularization in the training dynamics, which allow to avoid memorization even\nin highly overparameterized settings. Our results are supported by numerical\nexperiments with standard U-Net architectures on realistic and synthetic\ndatasets, and by a theoretical analysis using a tractable random features model\nstudied in the high-dimensional limit.", "authors": ["Tony Bonnaire", "Raphaël Urfin", "Giulio Biroli", "Marc Mézard"], "published_date": "2025-05-23", "title_zh": "擴散模型為何不記憶：訓練中隱式動態正規化的作用", "summary_zh": "擴散模型在生成任務中表現出色。本文探討訓練動態在泛化到記憶化轉變中的作用。研究發現兩個明顯的時間尺度：模型開始生成高品質樣本的早期時間$\\tau_\\mathrm{gen}$，以及記憶化出現的後期時間$\\tau_\\mathrm{mem}$。$\\tau_\\mathrm{mem}$隨訓練集大小$n$線性增加，而$\\tau_\\mathrm{gen}$保持不變，形成一個隨著$n$增長的泛化窗口。僅當$n$超過模型相關閾值時，過擬合才會在無限訓練時間消失。這些發現揭示了一種隱式動態正規化形式，即使在高參數化環境中也能避免記憶化。研究結果通過標準U-Net架構在真實和合成數據集上的數值實驗，以及在高維限制下可處理的隨機特徵模型的理論分析得到支持。", "audio": "audios/2505.17638v1.mp3", "timestamp": "2025-05-26T16:22:29.228578"}
{"query": "AI", "id": "2505.17968v1", "url": "http://arxiv.org/abs/2505.17968v1", "title": "Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems", "summary": "Using AI to create autonomous researchers has the potential to accelerate\nscientific discovery. A prerequisite for this vision is understanding how well\nan AI model can identify the underlying structure of a black-box system from\nits behavior. In this paper, we explore how well a large language model (LLM)\nlearns to identify a black-box function from passively observed versus actively\ncollected data. We investigate the reverse-engineering capabilities of LLMs\nacross three distinct types of black-box systems, each chosen to represent\ndifferent problem domains where future autonomous AI researchers may have\nconsiderable impact: Program, Formal Language, and Math Equation. Through\nextensive experiments, we show that LLMs fail to extract information from\nobservations, reaching a performance plateau that falls short of the ideal of\nBayesian inference. However, we demonstrate that prompting LLMs to not only\nobserve but also intervene -- actively querying the black-box with specific\ninputs to observe the resulting output -- improves performance by allowing LLMs\nto test edge cases and refine their beliefs. By providing the intervention data\nfrom one LLM to another, we show that this improvement is partly a result of\nengaging in the process of generating effective interventions, paralleling\nresults in the literature on human learning. Further analysis reveals that\nengaging in intervention can help LLMs escape from two common failure modes:\novercomplication, where the LLM falsely assumes prior knowledge about the\nblack-box, and overlooking, where the LLM fails to incorporate observations.\nThese insights provide practical guidance for helping LLMs more effectively\nreverse-engineer black-box systems, supporting their use in making new\ndiscoveries.", "authors": ["Jiayi Geng", "Howard Chen", "Dilip Arumugam", "Thomas L. Griffiths"], "published_date": "2025-05-23", "title_zh": "大型語言模型是可靠的人工智慧科學家嗎？評估黑箱系統的逆向工程", "summary_zh": "利用人工智慧創建自主研究員有望加速科學發現。前提是評估AI模型從黑箱系統行為中識別其底層結構的能力。本文探討大型語言模型(LLM)如何從被動觀察與主動收集資料中學習識別黑箱函數。我們研究LLM在程式、形式語言和數學方程式三種黑箱系統中的逆向工程能力。實驗表明，LLM從觀察中提取資訊失敗，效能停滯。然而，提示LLM主動查詢黑箱並觀察輸出，可以測試邊緣情況並完善信念，從而提高效能。將一個LLM的干預資料提供給另一個LLM，證明效能提升部分源於有效干預的生成過程，與人類學習文獻結果相似。干預有助於LLM擺脫過度複雜化(錯誤假設先驗知識)和忽略(未能納入觀察)兩種常見失敗模式。這些發現為LLM更有效地逆向工程黑箱系統提供實用指導，支援其用於新發現。", "audio": "audios/2505.17968v1.mp3", "timestamp": "2025-05-26T17:16:02.853772"}
{"query": "Foundation Model", "id": "2505.17645v1", "url": "http://arxiv.org/abs/2505.17645v1", "title": "HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning", "summary": "Embodied agents operating in smart homes must understand human behavior\nthrough diverse sensory inputs and communicate via natural language. While\nVision-Language Models (VLMs) have enabled impressive language-grounded\nperception, their reliance on visual data limits robustness in real-world\nscenarios with occlusions, poor lighting, or privacy constraints. In this\npaper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that\nintegrates uncommon but powerful sensing modalities, such as LiDAR, infrared,\nmmWave radar, and WiFi, to enable seamless human perception and reasoning\nacross heterogeneous environments. We address two key challenges: (1) the\nscarcity of aligned modality-text data for rare sensors, and (2) the\nheterogeneity of their physical signal representations. To overcome these, we\ndesign a Universal Modality-Injection Projector (UMIP) that enhances\npre-aligned modality embeddings with fine-grained, text-aligned features from\ntailored encoders via coarse-to-fine cross-attention without introducing\nsignificant alignment overhead. We further introduce a human-VLM collaborative\ndata curation pipeline to generate paired textual annotations for sensing\ndatasets. Extensive experiments on two newly constructed benchmarks show that\nHoloLLM significantly outperforms existing MLLMs, improving language-grounded\nhuman sensing accuracy by up to 30%. This work establishes a new foundation for\nreal-world, language-informed multisensory embodied intelligence.", "authors": ["Chuhao Zhou", "Jianfei Yang"], "published_date": "2025-05-23", "title_zh": "HoloLLM：用於語言引導之人體感知與推理的多感官基礎模型", "summary_zh": "在智慧家庭中運行的具身代理需要透過多樣感官輸入理解人類行為，並以自然語言溝通。雖然視覺語言模型(VLM)已展現出色的語言基礎感知能力，但對視覺資料的依賴限制了其在真實場景中因遮擋、光線不足或隱私限制所產生的穩健性。本研究提出HoloLLM，一種多模態大型語言模型(MLLM)，整合了LiDAR、紅外線、毫米波雷達和WiFi等不常見但強大的感測模式，以實現跨異質環境的無縫人類感知和推理。為了解決稀有感測器缺乏對齊模態文本數據以及物理信號表示異質性的問題，我們設計了一種通用模態注入投影器(UMIP)，透過粗略到精細的交叉注意力，利用來自客製化編碼器的細粒度、文本對齊特徵來增強預先對齊的模態嵌入，而不會引入顯著的對齊開銷。此外，我們引入了一個人機協作數據管理流程，為感測資料集生成配對的文本註釋。在兩個新構建的基準測試上的大量實驗表明，HoloLLM顯著優於現有的MLLM，語言基礎人類感測準確度提高了30%。這項工作為真實世界、語言引導的多感官具身智能奠定了新的基礎。", "audio": "audios/2505.17645v1.mp3", "timestamp": "2025-05-26T17:16:14.500283"}
{"query": "Diffusion Model", "id": "2505.17567v1", "url": "http://arxiv.org/abs/2505.17567v1", "title": "Enhancing Fourier-based Doppler Resolution with Diffusion Models", "summary": "In radar systems, high resolution in the Doppler dimension is important for\ndetecting slow-moving targets as it allows for more distinct separation between\nthese targets and clutter, or stationary objects. However, achieving sufficient\nresolution is constrained by hardware capabilities and physical factors,\nleading to the development of processing techniques to enhance the resolution\nafter acquisition. In this work, we leverage artificial intelligence to\nincrease the Doppler resolution in range-Doppler maps. Based on a zero-padded\nFFT, a refinement via the generative neural networks of diffusion models is\nachieved. We demonstrate that our method overcomes the limitations of\ntraditional FFT, generating data where closely spaced targets are effectively\nseparated.", "authors": ["Denisa Qosja", "Kilian Barth", "Simon Wagner"], "published_date": "2025-05-23", "title_zh": "利用擴散模型增強基於傅立葉變換的都卜勒解析度", "summary_zh": "都卜勒雷達系統中，高都卜勒解析度對偵測慢速移動目標至關重要，能有效區分目標與雜波。然受限於硬體及物理因素，達成足夠解析度具挑戰。本研究利用人工智慧提升距離-都卜勒圖的都卜勒解析度。基於補零快速傅立葉變換，透過生成式擴散模型進行優化。實驗證明，此方法突破傳統快速傅立葉變換限制，有效分離間距接近的目標。", "audio": "audios/2505.17567v1.mp3", "timestamp": "2025-05-26T17:16:19.997973"}
{"query": "AI", "id": "2505.17964v1", "url": "http://arxiv.org/abs/2505.17964v1", "title": "Counting Cycles with Deepseek", "summary": "Despite recent progress, AI still struggles on advanced mathematics. We\nconsider a difficult open problem: How to derive a Computationally Efficient\nEquivalent Form (CEEF) for the cycle count statistic? The CEEF problem does not\nhave known general solutions, and requires delicate combinatorics and tedious\ncalculations. Such a task is hard to accomplish by humans but is an ideal\nexample where AI can be very helpful. We solve the problem by combining a novel\napproach we propose and the powerful coding skills of AI. Our results use\ndelicate graph theory and contain new formulas for general cases that have not\nbeen discovered before. We find that, while AI is unable to solve the problem\nall by itself, it is able to solve it if we provide it with a clear strategy, a\nstep-by-step guidance and carefully written prompts. For simplicity, we focus\nour study on DeepSeek-R1 but we also investigate other AI approaches.", "authors": ["Jiashun Jin", "Tracy Ke", "Bingcheng Sui", "Zhenggang Wang"], "published_date": "2025-05-23", "title_zh": "利用Deepseek計數迴圈", "summary_zh": "儘管近期有所進展，人工智慧在高等數學方面仍面臨挑戰。本文探討一個困難的開放性問題：如何推導循環計數統計的計算高效等價形式 (CEEF)？CEEF問題缺乏已知的通用解，需要精巧的組合數學和繁瑣的計算。這項任務對人類而言難以完成，但人工智慧可在此發揮作用。我們結合一種新穎方法和人工智慧的強大編碼能力解決了此問題。結果利用精妙的圖論，包含先前未發現的通用公式。研究發現，雖然人工智慧無法獨立解決問題，但若提供清晰的策略、逐步指導和精心設計的提示，則能成功解決。為簡化起見，研究重點為DeepSeek-R1，但也調查了其他人工智慧方法。", "audio": "audios/2505.17964v1.mp3", "timestamp": "2025-05-26T18:24:48.530356"}
{"query": "Foundation Model", "id": "2505.17631v1", "url": "http://arxiv.org/abs/2505.17631v1", "title": "BehaveGPT: A Foundation Model for Large-scale User Behavior Modeling", "summary": "In recent years, foundational models have revolutionized the fields of\nlanguage and vision, demonstrating remarkable abilities in understanding and\ngenerating complex data; however, similar advances in user behavior modeling\nhave been limited, largely due to the complexity of behavioral data and the\nchallenges involved in capturing intricate temporal and contextual\nrelationships in user activities. To address this, we propose BehaveGPT, a\nfoundational model designed specifically for large-scale user behavior\nprediction. Leveraging transformer-based architecture and a novel pretraining\nparadigm, BehaveGPT is trained on vast user behavior datasets, allowing it to\nlearn complex behavior patterns and support a range of downstream tasks,\nincluding next behavior prediction, long-term generation, and cross-domain\nadaptation. Our approach introduces the DRO-based pretraining paradigm tailored\nfor user behavior data, which improves model generalization and transferability\nby equitably modeling both head and tail behaviors. Extensive experiments on\nreal-world datasets demonstrate that BehaveGPT outperforms state-of-the-art\nbaselines, achieving more than a 10% improvement in macro and weighted recall,\nshowcasing its ability to effectively capture and predict user behavior.\nFurthermore, we measure the scaling law in the user behavior domain for the\nfirst time on the Honor dataset, providing insights into how model performance\nscales with increased data and parameter sizes.", "authors": ["Jiahui Gong", "Jingtao Ding", "Fanjin Meng", "Chen Yang", "Hong Chen", "Zuojian Wang", "Haisheng Lu", "Yong Li"], "published_date": "2025-05-23", "title_zh": "BehaveGPT：大規模使用者行為建模之基礎模型", "summary_zh": "近年來，基礎模型徹底改變了語言和視覺領域，但在使用者行為建模方面進展有限。為此，我們提出BehaveGPT，一種專為大規模使用者行為預測設計的基礎模型。BehaveGPT基於Transformer架構和新型預訓練範式，透過海量使用者行為數據訓練，學習複雜行為模式，支援多種下游任務，包括行為預測、長期生成和跨域適應。我們引入針對使用者行為數據的DRO預訓練範式，透過公平建模頭部和尾部行為，提升模型泛化能力和遷移性。在真實數據集上的實驗表明，BehaveGPT優於現有技術，宏平均召回率和加權召回率提升超過10%，展現其有效捕捉和預測使用者行為的能力。此外，我們首次在Honor數據集上測量使用者行為領域的縮放定律，深入了解模型效能如何隨數據和參數規模的增加而變化。", "audio": "audios/2505.17631v1.mp3", "timestamp": "2025-05-26T18:25:01.999761"}
{"query": "Diffusion Model", "id": "2505.17561v1", "url": "http://arxiv.org/abs/2505.17561v1", "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via Attention in Video Diffusion Model", "summary": "The choice of initial noise significantly affects the quality and prompt\nalignment of video diffusion models, where different noise seeds for the same\nprompt can lead to drastically different generations. While recent methods rely\non externally designed priors such as frequency filters or inter-frame\nsmoothing, they often overlook internal model signals that indicate which noise\nseeds are inherently preferable. To address this, we propose ANSE (Active Noise\nSelection for Generation), a model-aware framework that selects high-quality\nnoise seeds by quantifying attention-based uncertainty. At its core is BANSA\n(Bayesian Active Noise Selection via Attention), an acquisition function that\nmeasures entropy disagreement across multiple stochastic attention samples to\nestimate model confidence and consistency. For efficient inference-time\ndeployment, we introduce a Bernoulli-masked approximation of BANSA that enables\nscore estimation using a single diffusion step and a subset of attention\nlayers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video\nquality and temporal coherence with only an 8% and 13% increase in inference\ntime, respectively, providing a principled and generalizable approach to noise\nselection in video diffusion. See our project page:\nhttps://anse-project.github.io/anse-project/", "authors": ["Kwanyoung Kim", "Sanghyun Kim"], "published_date": "2025-05-23", "title_zh": "模型早已知曉最佳雜訊：影片擴散模型中基於注意力之貝氏主動雜訊選擇", "summary_zh": "初始雜訊選擇顯著影響影片擴散模型品質及提示對齊。針對同一提示，不同雜訊種子可能產生迥異結果。現有方法依賴外部先驗（如頻率濾波器或幀間平滑），忽略指示優選雜訊種子的內部模型訊號。本研究提出ANSE（主動雜訊選擇），一種模型感知框架，透過量化基於注意力機制的不確定性來選擇高品質雜訊種子。其核心為BANSA（基於注意力的貝氏主動雜訊選擇），一種獲取函數，透過測量多個隨機注意力樣本間的熵值差異來評估模型信心和一致性。為實現高效推論部署，導入BANSA的伯努利遮罩近似，僅需單步擴散及部分注意力層即可估算分數。CogVideoX-2B和5B實驗表明，ANSE僅分別增加8%和13%的推論時間，即可提升影片品質和時間一致性，為影片擴散中的雜訊選擇提供了一種有原則且具泛化性的方法。", "audio": "audios/2505.17561v1.mp3", "timestamp": "2025-05-26T18:25:11.817494"}
{"query": "AI", "id": "2505.17945v1", "url": "http://arxiv.org/abs/2505.17945v1", "title": "Towards Industrial Convergence : Understanding the evolution of scientific norms and practices in the field of AI", "summary": "In the field of artificial intelligence (AI) research, there seems to be a\nrapprochement between academics and industrial forces. The aim of this study is\nto assess whether and to what extent industrial domination in the field as well\nas the ever more frequent switch between academia and industry resulted in the\nadoption of industrial norms and practices by academics. Using bibliometric\ninformation and data on scientific code, we aimed to understand academic and\nindustrial researchers' practices, the way of choosing, investing, and\nsucceeding across multiple and concurrent artifacts. Our results show that,\nalthough both actors write papers and code, their practices and the norms\nguiding them differ greatly. Nevertheless, it appears that the presence of\nindustrials in academic studies leads to practices leaning toward the\nindustrial side, but also to greater success in both artifacts, suggesting that\nif convergence is, then it is passing through those mixed teams rather than\nthrough pure academic or industrial studies.", "authors": ["Antoine Houssard"], "published_date": "2025-05-23", "title_zh": "邁向產業融合：理解人工智慧領域中科學規範與實踐的演進", "summary_zh": "人工智慧研究領域中，學術界與產業界的聯繫日益緊密。本研究旨在評估產業界主導地位以及學術界與產業間頻繁流動，是否導致學術界採用產業規範和實踐。透過文獻計量資訊和科學程式碼數據，我們分析學術和產業研究人員的實踐、選擇方式、投資策略以及成功途徑。結果顯示，儘管雙方均發表論文和程式碼，但其實踐和規範存在顯著差異。然而，產業界人士參與學術研究似乎會導致實踐更趨向產業化，並在兩類產出中取得更大成功，這表明融合可能發生在混合團隊中，而非單純的學術或產業研究。", "audio": "audios/2505.17945v1.mp3", "timestamp": "2025-05-26T19:14:50.093182"}
{"query": "Foundation Model", "id": "2505.17602v1", "url": "http://arxiv.org/abs/2505.17602v1", "title": "A Unified Multi-Scale Attention-Based Network for Automatic 3D Segmentation of Lung Parenchyma & Nodules In Thoracic CT Images", "summary": "Lung cancer has been one of the major threats across the world with the\nhighest mortalities. Computer-aided detection (CAD) can help in early detection\nand thus can help increase the survival rate. Accurate lung parenchyma\nsegmentation (to include the juxta-pleural nodules) and lung nodule\nsegmentation, the primary symptom of lung cancer, play a crucial role in the\noverall accuracy of the Lung CAD pipeline. Lung nodule segmentation is quite\nchallenging because of the diverse nodule types and other inhibit structures\npresent within the lung lobes. Traditional machine/deep learning methods suffer\nfrom generalization and robustness. Recent Vision Language Models/Foundation\nModels perform well on the anatomical level, but they suffer on fine-grained\nsegmentation tasks, and their semi-automatic nature limits their effectiveness\nin real-time clinical scenarios. In this paper, we propose a novel method for\naccurate 3D segmentation of lung parenchyma and lung nodules. The proposed\narchitecture is an attention-based network with residual blocks at each\nencoder-decoder state. Max pooling is replaced by strided convolutions at the\nencoder, and trilinear interpolation is replaced by transposed convolutions at\nthe decoder to maximize the number of learnable parameters. Dilated\nconvolutions at each encoder-decoder stage allow the model to capture the\nlarger context without increasing computational costs. The proposed method has\nbeen evaluated extensively on one of the largest publicly available datasets,\nnamely LUNA16, and is compared with recent notable work in the domain using\nstandard performance metrics like Dice score, IOU, etc. It can be seen from the\nresults that the proposed method achieves better performance than\nstate-of-the-art methods. The source code, datasets, and pre-processed data can\nbe accessed using the link:\nhttps://github.com/EMeRALDsNRPU/Attention-Based-3D-ResUNet.", "authors": ["Muhammad Abdullah", "Furqan Shaukat"], "published_date": "2025-05-23", "title_zh": "基於統一多尺度注意力網絡的胸腔CT影像肺實質與結節自動三維分割", "summary_zh": "肺癌是全球高死亡率的主要威脅之一，電腦輔助偵測(CAD)有助於早期發現並提高存活率。精確的肺實質（包含近胸膜結節）與肺結節分割對肺癌CAD的整體準確性至關重要，而肺結節分割因其多樣性及肺葉內的其他干擾結構極具挑戰。傳統機器/深度學習方法的泛化性和穩健性不足。近期視覺語言模型/基礎模型在解剖層面表現良好，但在細粒度分割任務中表現欠佳，且其半自動性質限制了其在即時臨床場景中的有效性。本文提出一種用於精確3D肺實質與肺結節分割的新穎方法。該架構為基於注意力機制的網路，編碼器-解碼器的每一階段都帶有殘差塊。編碼器中使用步長卷積取代最大池化，解碼器中使用轉置卷積取代三線性插值，以最大化可學習參數的數量。編碼器-解碼器每一階段的擴張卷積使模型能夠在不增加計算成本的情況下捕獲更大的上下文。該方法已在LUNA16上進行了廣泛評估，並使用Dice係數、IOU等標準指標與該領域的最新成果進行了比較。結果表明，該方法優於現有技術。原始碼、資料集和預處理資料可通過以下連結存取：https://github.com/EMeRALDsNRPU/Attention-Based-3D-ResUNet。", "audio": "audios/2505.17602v1.mp3", "timestamp": "2025-05-26T19:15:06.129740"}
{"query": "Diffusion Model", "id": "2505.17560v1", "url": "http://arxiv.org/abs/2505.17560v1", "title": "Deeper Diffusion Models Amplify Bias", "summary": "Despite the impressive performance of generative Diffusion Models (DMs),\ntheir internal working is still not well understood, which is potentially\nproblematic. This paper focuses on exploring the important notion of\nbias-variance tradeoff in diffusion models. Providing a systematic foundation\nfor this exploration, it establishes that at one extreme the diffusion models\nmay amplify the inherent bias in the training data and, on the other, they may\ncompromise the presumed privacy of the training samples. Our exploration aligns\nwith the memorization-generalization understanding of the generative models,\nbut it also expands further along this spectrum beyond ``generalization'',\nrevealing the risk of bias amplification in deeper models. Building on the\ninsights, we also introduce a training-free method to improve output quality in\ntext-to-image and image-to-image generation. By progressively encouraging\ntemporary high variance in the generation process with partial bypassing of the\nmid-block's contribution in the denoising process of DMs, our method\nconsistently improves generative image quality with zero training cost. Our\nclaims are validated both theoretically and empirically.", "authors": ["Shahin Hakemi", "Naveed Akhtar", "Ghulam Mubashar Hassan", "Ajmal Mian"], "published_date": "2025-05-23", "title_zh": "更深層次的擴散模型放大偏差", "summary_zh": "儘管生成式擴散模型表現出色，其內部運作機制仍不甚明瞭，這可能存在問題。本文探討擴散模型中重要的偏差-方差權衡。研究建立了系統性基礎，表明擴散模型可能放大訓練資料中的固有偏差，或損害訓練樣本的預期隱私。此研究與生成模型的記憶-泛化理解一致，並進一步擴展到泛化之外，揭示了深度模型中偏差放大的風險。基於這些洞見，本文提出一種免訓練方法，通過在生成過程中逐步鼓勵暫時性的高方差，並部分繞過擴散模型降噪過程的中間模塊貢獻，從而提高文本到圖像和圖像到圖像生成的輸出品質，且無需任何訓練成本。理論和實驗均驗證了本文的主張。", "audio": "audios/2505.17560v1.mp3", "timestamp": "2025-05-26T19:15:22.183349"}
{"query": "AI", "id": "2505.17937v1", "url": "http://arxiv.org/abs/2505.17937v1", "title": "Survival Games: Human-LLM Strategic Showdowns under Severe Resource Scarcity", "summary": "The rapid advancement of large language models (LLMs) raises critical\nconcerns about their ethical alignment, particularly in scenarios where human\nand AI co-exist under the conflict of interest. This work introduces an\nextendable, asymmetric, multi-agent simulation-based benchmarking framework to\nevaluate the moral behavior of LLMs in a novel human-AI co-existence setting\nfeaturing consistent living and critical resource management. Building on\nprevious generative agent environments, we incorporate a life-sustaining\nsystem, where agents must compete or cooperate for food resources to survive,\noften leading to ethically charged decisions such as deception, theft, or\nsocial influence. We evaluated two types of LLM, DeepSeek and OpenAI series, in\na three-agent setup (two humans, one LLM-powered robot), using adapted\nbehavioral detection from the MACHIAVELLI framework and a custom survival-based\nethics metric. Our findings reveal stark behavioral differences: DeepSeek\nfrequently engages in resource hoarding, while OpenAI exhibits restraint,\nhighlighting the influence of model design on ethical outcomes. Additionally,\nwe demonstrate that prompt engineering can significantly steer LLM behavior,\nwith jailbreaking prompts significantly enhancing unethical actions, even for\nhighly restricted OpenAI models and cooperative prompts show a marked reduction\nin unethical actions. Our framework provides a reproducible testbed for\nquantifying LLM ethics in high-stakes scenarios, offering insights into their\nsuitability for real-world human-AI interactions.", "authors": ["Zhihong Chen", "Yiqian Yang", "Jinzhao Zhou", "Qiang Zhang", "Chin-Teng Lin", "Yiqun Duan"], "published_date": "2025-05-23", "title_zh": "生存遊戲：嚴苛資源匱乏下人機戰略對決", "summary_zh": "大型語言模型快速發展，引發了對其倫理一致性的擔憂，尤其是在人機共存且存在利益衝突的情境下。本研究提出一個可擴展、非對稱、基於多代理模擬的基準測試框架，以評估語言模型在新型人機共存環境中的道德行為，該環境模擬持續生活及關鍵資源管理。在先前的生成式代理環境基礎上，我們加入維生系統，代理人必須競爭或合作以獲取食物資源才能生存，進而導致欺騙、盜竊或社會影響等涉及倫理的決策。我們在三代理人設置（兩個人類，一個由語言模型驅動的機器人）中評估了 DeepSeek 和 OpenAI 系列兩種語言模型，使用改編自 MACHIAVELLI 框架的行為檢測和自定義的基於生存的倫理指標。研究結果顯示了顯著的行為差異：DeepSeek 頻繁囤積資源，而 OpenAI 則表現出克制，突顯了模型設計對倫理結果的影響。此外，我們證明了提示工程可以顯著地引導語言模型的行為，越獄提示顯著增強了不道德行為，即使對於高度受限的 OpenAI 模型也是如此，而合作提示則顯著減少了不道德行為。我們的框架為量化高風險情境下語言模型的倫理提供了可重複的測試平台，並深入了解它們在真實世界人機互動中的適用性。", "audio": "audios/2505.17937v1.mp3", "timestamp": "2025-05-26T20:20:21.392685"}
{"query": "Foundation Model", "id": "2505.17370v1", "url": "http://arxiv.org/abs/2505.17370v1", "title": "FRIREN: Beyond Trajectories -- A Spectral Lens on Time", "summary": "Long-term time-series forecasting (LTSF) models are often presented as\ngeneral-purpose solutions that can be applied across domains, implicitly\nassuming that all data is pointwise predictable. Using chaotic systems such as\nLorenz-63 as a case study, we argue that geometric structure - not pointwise\nprediction - is the right abstraction for a dynamic-agnostic foundational\nmodel. Minimizing the Wasserstein-2 distance (W2), which captures geometric\nchanges, and providing a spectral view of dynamics are essential for\nlong-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via\nInterpretable Eigen-networks), implements an augmented normalizing-flow block\nthat embeds data into a normally distributed latent representation. It then\ngenerates a W2-efficient optimal path that can be decomposed into rotation,\nscaling, inverse rotation, and translation. This architecture yields locally\ngenerated, geometry-preserving predictions that are independent of the\nunderlying dynamics, and a global spectral representation that functions as a\nfinite Koopman operator with a small modification. This enables practitioners\nto identify which modes grow, decay, or oscillate, both locally and\nsystem-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on\nLorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE\n27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out\nof 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out),\nFRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170,\noutperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065.\nFRIREN is also competitive on standard LTSF datasets such as ETT and Weather.\nBy connecting modern generative flows with classical spectral analysis, FRIREN\nmakes long-term forecasting both accurate and interpretable, setting a new\nbenchmark for LTSF model design.", "authors": ["Qilin Wang"], "published_date": "2025-05-23", "title_zh": "芙莉蓮：超越軌跡——時間之譜分析", "summary_zh": "長時序預測模型常被視為通用解決方案，但忽略了數據並非總是逐點可預測。以Lorenz-63等混沌系統為例，幾何結構而非逐點預測才是動態無關基礎模型的正確抽象。最小化Wasserstein-2距離(W2)以捕捉幾何變化，並提供動態譜視角對於長程預測至關重要。FRIREN模型透過可解釋的本徵網路，將數據嵌入常態分佈的潛在表示中，生成W2有效的最佳路徑，可分解為旋轉、縮放、反旋轉和平移。此架構產生局部幾何保持的預測，獨立於底層動態，並生成全局譜表示，作為有限Koopman算子，可識別局部和系統範圍內增長、衰減或震盪的模式。FRIREN在Lorenz-63上超越TimeMixer，並在Rossler和其他標準長時序預測數據集上表現出色。FRIREN將生成流與譜分析結合，使長時序預測既準確又可解釋，為長時序預測模型設計樹立新標竿。", "audio": "audios/2505.17370v1.mp3", "timestamp": "2025-05-26T20:20:49.254860"}
{"query": "Diffusion Model", "id": "2505.17550v1", "url": "http://arxiv.org/abs/2505.17550v1", "title": "T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models", "summary": "Recent advances in text-to-video (T2V) diffusion models have significantly\nenhanced the quality of generated videos. However, their ability to produce\nexplicit or harmful content raises concerns about misuse and potential rights\nviolations. Inspired by the success of unlearning techniques in erasing\nundesirable concepts from text-to-image (T2I) models, we extend unlearning to\nT2V models and propose a robust and precise unlearning method. Specifically, we\nadopt negatively-guided velocity prediction fine-tuning and enhance it with\nprompt augmentation to ensure robustness against LLM-refined prompts. To\nachieve precise unlearning, we incorporate a localization and a preservation\nregularization to preserve the model's ability to generate non-target concepts.\nExtensive experiments demonstrate that our method effectively erases a specific\nconcept while preserving the model's generation capability for all other\nconcepts, outperforming existing methods. We provide the unlearned models in\n\\href{https://github.com/VDIGPKU/T2VUnlearning.git}{https://github.com/VDIGPKU/T2VUnlearning.git}.", "authors": ["Xiaoyu Ye", "Songjie Cheng", "Yongtao Wang", "Yajiao Xiong", "Yishen Li"], "published_date": "2025-05-23", "title_zh": "T2V逆學習：一種用於文本到視頻擴散模型的概念擦除方法", "summary_zh": "文字轉影片擴散模型近年進展顯著提升了生成影片品質，但也引發濫用及侵權疑慮。受文字轉圖像模型逆學習技術啟發，本研究將逆學習擴展至文字轉影片模型，提出一種穩健且精準的逆學習方法。具體而言，採用負向引導速度預測微調，並輔以提示增強，以確保對抗大型語言模型優化提示的穩健性。為實現精準逆學習，加入定位和保存正規化，以保留模型生成非目標概念的能力。實驗結果表明，本方法能有效移除特定概念，同時保留模型生成其他概念的能力，優於現有方法。已於指定連結提供逆學習模型。", "audio": "audios/2505.17550v1.mp3", "timestamp": "2025-05-26T20:20:59.859089"}
{"query": "AI", "id": "2505.17908v1", "url": "http://arxiv.org/abs/2505.17908v1", "title": "ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback", "summary": "With the rapid advancement of generative models, general-purpose generation\nhas gained increasing attention as a promising approach to unify diverse tasks\nacross modalities within a single system. Despite this progress, existing\nopen-source frameworks often remain fragile and struggle to support complex\nreal-world applications due to the lack of structured workflow planning and\nexecution-level feedback. To address these limitations, we present ComfyMind, a\ncollaborative AI system designed to enable robust and scalable general-purpose\ngeneration, built on the ComfyUI platform. ComfyMind introduces two core\ninnovations: Semantic Workflow Interface (SWI) that abstracts low-level node\ngraphs into callable functional modules described in natural language, enabling\nhigh-level composition and reducing structural errors; Search Tree Planning\nmechanism with localized feedback execution, which models generation as a\nhierarchical decision process and allows adaptive correction at each stage.\nTogether, these components improve the stability and flexibility of complex\ngenerative workflows. We evaluate ComfyMind on three public benchmarks:\nComfyBench, GenEval, and Reason-Edit, which span generation, editing, and\nreasoning tasks. Results show that ComfyMind consistently outperforms existing\nopen-source baselines and achieves performance comparable to GPT-Image-1.\nComfyMind paves a promising path for the development of open-source\ngeneral-purpose generative AI systems. Project page:\nhttps://github.com/LitaoGuo/ComfyMind", "authors": ["Litao Guo", "Xinli Xu", "Luozhou Wang", "Jiantao Lin", "Jinsong Zhou", "Zixin Zhang", "Bolan Su", "Ying-Cong Chen"], "published_date": "2025-05-23", "title_zh": "ComfyMind：基於樹狀規劃與反應式回饋之通用生成方法", "summary_zh": "隨著生成模型的快速發展，通用生成技術作為整合跨模態多樣化任務的途徑備受關注。現有開源框架因缺乏結構化工作流程規劃與執行回饋，在支援複雜應用時常顯脆弱。為此，我們於ComfyUI平台構建名為ComfyMind的協作式AI系統，以實現穩健且可擴展的通用生成。ComfyMind引入兩項核心創新：語義工作流程介面(SWI)，將底層節點圖抽象為自然語言描述的可呼叫功能模組，以實現高層次組合並減少結構錯誤；以及具局部回饋執行的搜尋樹規劃機制，將生成建模為層次決策過程，並允許在每個階段進行自適應校正。這些組件共同提升了複雜生成工作流程的穩定性與靈活性。我們在ComfyBench、GenEval和Reason-Edit三個公開基準上評估ComfyMind，涵蓋生成、編輯和推理任務。結果顯示，ComfyMind始終優於現有開源基準，並達到與GPT-Image-1相當的性能。ComfyMind為開源通用生成AI系統的發展開闢了道路。專案頁面：https://github.com/LitaoGuo/ComfyMind", "audio": "audios/2505.17908v1.mp3", "timestamp": "2025-05-26T21:16:28.250149"}
{"query": "Foundation Model", "id": "2505.17338v1", "url": "http://arxiv.org/abs/2505.17338v1", "title": "Render-FM: A Foundation Model for Real-time Photorealistic Volumetric Rendering", "summary": "Volumetric rendering of Computed Tomography (CT) scans is crucial for\nvisualizing complex 3D anatomical structures in medical imaging. Current\nhigh-fidelity approaches, especially neural rendering techniques, require\ntime-consuming per-scene optimization, limiting clinical applicability due to\ncomputational demands and poor generalizability. We propose Render-FM, a novel\nfoundation model for direct, real-time volumetric rendering of CT scans.\nRender-FM employs an encoder-decoder architecture that directly regresses 6D\nGaussian Splatting (6DGS) parameters from CT volumes, eliminating per-scan\noptimization through large-scale pre-training on diverse medical data. By\nintegrating robust feature extraction with the expressive power of 6DGS, our\napproach efficiently generates high-quality, real-time interactive 3D\nvisualizations across diverse clinical CT data. Experiments demonstrate that\nRender-FM achieves visual fidelity comparable or superior to specialized\nper-scan methods while drastically reducing preparation time from nearly an\nhour to seconds for a single inference step. This advancement enables seamless\nintegration into real-time surgical planning and diagnostic workflows. The\nproject page is: https://gaozhongpai.github.io/renderfm/.", "authors": ["Zhongpai Gao", "Meng Zheng", "Benjamin Planche", "Anwesa Choudhuri", "Terrence Chen", "Ziyan Wu"], "published_date": "2025-05-22", "title_zh": "Render-FM：用於即時照片寫實體積渲染的基礎模型", "summary_zh": "電腦斷層掃描的容積渲染在醫學影像中對於複雜三維解剖結構的可視化至關重要。目前的高保真方法，特別是神經渲染技術，需要耗時的逐場景優化，因計算需求和泛化能力差而限制了臨床應用。本研究提出Render-FM，一種用於直接即時電腦斷層掃描容積渲染的新型基礎模型。Render-FM採用編碼器-解碼器架構，直接從電腦斷層容積回歸6D高斯濺射（6DGS）參數，通過大規模醫學數據預訓練消除了逐掃描優化。透過整合穩健的特徵提取和6DGS的表達能力，此方法能跨多樣臨床電腦斷層數據高效生成高品質、即時互動的三維可視化。實驗表明，Render-FM達成的視覺保真度可與專門的逐掃描方法媲美或更優，同時大幅減少準備時間，單次推論從近一小時降至數秒。此進展有助於無縫整合至即時手術規劃和診斷工作流程。項目網頁：https://gaozhongpai.github.io/renderfm/。", "audio": "audios/2505.17338v1.mp3", "timestamp": "2025-05-26T21:16:37.782954"}
{"query": "Diffusion Model", "id": "2505.17517v1", "url": "http://arxiv.org/abs/2505.17517v1", "title": "Spacetime Geometry of Denoising in Diffusion Models", "summary": "We present a novel perspective on diffusion models using the framework of\ninformation geometry. We show that the set of noisy samples, taken across all\nnoise levels simultaneously, forms a statistical manifold -- a family of\ndenoising probability distributions. Interpreting the noise level as a temporal\nparameter, we refer to this manifold as spacetime. This manifold naturally\ncarries a Fisher-Rao metric, which defines geodesics -- shortest paths between\nnoisy points. Notably, this family of distributions is exponential, enabling\nefficient geodesic computation even in high-dimensional settings without\nretraining or fine-tuning. We demonstrate the practical value of this geometric\nviewpoint in transition path sampling, where spacetime geodesics define smooth\nsequences of Boltzmann distributions, enabling the generation of continuous\ntrajectories between low-energy metastable states. Code is available at:\nhttps://github.com/Aalto-QuML/diffusion-spacetime-geometry.", "authors": ["Rafał Karczewski", "Markus Heinonen", "Alison Pouplin", "Søren Hauberg", "Vikas Garg"], "published_date": "2025-05-23", "title_zh": "擴散模型去噪的時空幾何", "summary_zh": "本文以信息幾何框架探討擴散模型的新視角，揭示所有噪音水平下的噪聲樣本集合構成一個統計流形，即去噪概率分佈族。將噪音水平視為時間參數，此流形被稱為時空流形，其天然具備Fisher-Rao度量，定義了噪聲點間的最短路徑，即測地線。此分佈族為指數族，即使在高維環境下，無需重新訓練或微調，也能高效計算測地線。本文展示了此幾何視角在躍遷路徑抽樣中的實用價值，時空測地線定義了平滑的波茲曼分佈序列，得以生成低能量亞穩態間的連續軌跡。代碼可見：https://github.com/Aalto-QuML/diffusion-spacetime-geometry。", "audio": "audios/2505.17517v1.mp3", "timestamp": "2025-05-26T21:16:43.909095"}
{"query": "AI", "id": "2505.17870v1", "url": "http://arxiv.org/abs/2505.17870v1", "title": "Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods", "summary": "Generative AI models often learn and reproduce false information present in\ntheir training corpora. This position paper argues that, analogous to\nbiological immunization, where controlled exposure to a weakened pathogen\nbuilds immunity, AI models should be fine tuned on small, quarantined sets of\nexplicitly labeled falsehoods as a \"vaccine\" against misinformation. These\ncurated false examples are periodically injected during finetuning,\nstrengthening the model ability to recognize and reject misleading claims while\npreserving accuracy on truthful inputs. An illustrative case study shows that\nimmunized models generate substantially less misinformation than baselines. To\nour knowledge, this is the first training framework that treats fact checked\nfalsehoods themselves as a supervised vaccine, rather than relying on input\nperturbations or generic human feedback signals, to harden models against\nfuture misinformation. We also outline ethical safeguards and governance\ncontrols to ensure the safe use of false data. Model immunization offers a\nproactive paradigm for aligning AI systems with factuality.", "authors": ["Shaina Raza", "Rizwan Qureshi", "Marcelo Lotif", "Aman Chadha", "Deval Pandya", "Christos Emmanouilidis"], "published_date": "2025-05-23", "title_zh": "正如人類需要疫苗，模型亦然：模型免疫以對抗虛假資訊", "summary_zh": "生成式人工智慧模型常學習並複製訓練語料庫中的錯誤資訊。本文提出，類比生物免疫，透過控制暴露於減弱的病原體來建立免疫力，AI 模型應以少量、隔離且明確標記的錯誤資訊集進行微調，作為對抗假訊息的疫苗。定期在微調期間注入這些精選的錯誤範例，增強模型識別和拒絕誤導性聲明的能力，同時保持對真實輸入的準確性。案例研究表明，免疫後的模型產生的錯誤資訊顯著減少。據我們所知，這是第一個將事實查核的錯誤資訊本身視為監督式疫苗的訓練框架，而非依賴輸入擾動或通用的人工回饋信號，以強化模型對抗未來的假訊息。我們也概述了道德保障和治理控制，以確保安全使用錯誤數據。模型免疫為使 AI 系統與事實相符提供了一種積極主動的範例。", "audio": "audios/2505.17870v1.mp3", "timestamp": "2025-05-26T22:17:51.073675"}
{"query": "Foundation Model", "id": "2505.17257v1", "url": "http://arxiv.org/abs/2505.17257v1", "title": "JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model", "summary": "Large language models (LLMs) have revolutionized natural language processing\nand are increasingly applied to other sequential data types, including genetic\nsequences. However, adapting LLMs to genomics presents significant challenges.\nCapturing complex genomic interactions requires modeling long-range\ndependencies within DNA sequences, where interactions often span over 10,000\nbase pairs, even within a single gene, posing substantial computational burdens\nunder conventional model architectures and training paradigms. Moreover,\nstandard LLM training approaches are suboptimal for DNA: autoregressive\ntraining, while efficient, supports only unidirectional understanding. However,\nDNA is inherently bidirectional, e.g., bidirectional promoters regulate\ntranscription in both directions and account for nearly 11% of human gene\nexpression. Masked language models (MLMs) allow bidirectional understanding but\nare inefficient, as only masked tokens contribute to the loss per step. To\naddress these limitations, we introduce JanusDNA, the first bidirectional DNA\nfoundation model built upon a novel pretraining paradigm that combines the\noptimization efficiency of autoregressive modeling with the bidirectional\ncomprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and\nMixture of Experts (MoE) architecture, combining long-range modeling of\nAttention with efficient sequential learning of Mamba. MoE layers further scale\nmodel capacity via sparse activation while keeping computational cost low.\nNotably, JanusDNA processes up to 1 million base pairs at single nucleotide\nresolution on a single 80GB GPU. Extensive experiments and ablations show\nJanusDNA achieves new SOTA results on three genomic representation benchmarks,\noutperforming models with 250x more activated parameters. Code:\nhttps://github.com/Qihao-Duan/JanusDNA", "authors": ["Qihao Duan", "Bingding Huang", "Zhenqiao Song", "Irina Lehmann", "Lei Gu", "Roland Eils", "Benjamin Wild"], "published_date": "2025-05-22", "title_zh": "JanusDNA：一種強大的雙向混合DNA基礎模型", "summary_zh": "大型語言模型革新自然語言處理，並擴展至基因序列等序列數據。將其應用於基因組學面臨挑戰：捕捉複雜基因組交互需要對DNA序列中的長程依賴關係建模，交互作用跨越上萬鹼基對，對傳統模型架構和訓練模式造成計算負擔。自迴歸訓練雖高效，但僅支援單向理解，而DNA本質上是雙向的。遮蔽語言模型允許雙向理解，但效率較低。為了解決這些限制，我們提出JanusDNA，一種基於新型預訓練模式的雙向DNA基礎模型，結合了自迴歸建模的優化效率和遮蔽建模的雙向理解能力。JanusDNA採用混合Mamba、Attention和混合專家(MoE)架構，結合Attention的長程建模和Mamba的高效序列學習。MoE層透過稀疏激活擴展模型容量，同時保持低計算成本。JanusDNA可在單張80GB GPU上處理高達一百萬鹼基對的單核苷酸分辨率數據。實驗表明，JanusDNA在三個基因組表示基準測試中實現了新的最優結果，超越了激活參數多出250倍的模型。程式碼：https://github.com/Qihao-Duan/JanusDNA", "audio": "audios/2505.17257v1.mp3", "timestamp": "2025-05-26T22:18:07.607262"}
{"query": "Diffusion Model", "id": "2505.17478v1", "url": "http://arxiv.org/abs/2505.17478v1", "title": "Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression", "summary": "Understanding protein dynamics is critical for elucidating their biological\nfunctions. The increasing availability of molecular dynamics (MD) data enables\nthe training of deep generative models to efficiently explore the\nconformational space of proteins. However, existing approaches either fail to\nexplicitly capture the temporal dependencies between conformations or do not\nsupport direct generation of time-independent samples. To address these\nlimitations, we introduce ConfRover, an autoregressive model that\nsimultaneously learns protein conformation and dynamics from MD trajectories,\nsupporting both time-dependent and time-independent sampling. At the core of\nour model is a modular architecture comprising: (i) an encoding layer, adapted\nfrom protein folding models, that embeds protein-specific information and\nconformation at each time frame into a latent space; (ii) a temporal module, a\nsequence model that captures conformational dynamics across frames; and (iii)\nan SE(3) diffusion model as the structure decoder, generating conformations in\ncontinuous space. Experiments on ATLAS, a large-scale protein MD dataset of\ndiverse structures, demonstrate the effectiveness of our model in learning\nconformational dynamics and supporting a wide range of downstream tasks.\nConfRover is the first model to sample both protein conformations and\ntrajectories within a single framework, offering a novel and flexible approach\nfor learning from protein MD data.", "authors": ["Yuning Shen", "Lihao Wang", "Huizhuo Yuan", "Yan Wang", "Bangji Yang", "Quanquan Gu"], "published_date": "2025-05-23", "title_zh": "自迴歸方法同步建模蛋白質構象與動力學", "summary_zh": "理解蛋白質動力學對於闡明其生物功能至關重要。分子動力學(MD)數據日益增多，使得訓練深度生成模型以有效探索蛋白質構象空間成為可能。然而，現有方法或未能明確捕捉構象之間的時間依賴性，或不支持直接生成時間獨立樣本。為了解決這些限制，我們引入ConfRover，一種自迴歸模型，可同時從MD軌跡中學習蛋白質構象和動力學，並支持時間相關和時間獨立的取樣。ConfRover的核心是一個模組化架構，包含：(i)一個編碼層，改編自蛋白質摺疊模型，將每個時間框架的蛋白質特定信息和構象嵌入到潛在空間中；(ii)一個時間模組，一種序列模型，捕捉跨框架的構象動力學；以及(iii)一個SE(3)擴散模型作為結構解碼器，在連續空間中生成構象。在ATLAS（一個包含多樣結構的大規模蛋白質MD數據集）上的實驗表明，我們的模型在學習構象動力學和支持各種下游任務方面非常有效。ConfRover是第一個在單一框架內同時對蛋白質構象和軌跡進行取樣的模型，為從蛋白質MD數據中學習提供了一種新穎且靈活的方法。", "audio": "audios/2505.17478v1.mp3", "timestamp": "2025-05-26T22:18:25.963641"}
{"query": "AI", "id": "2505.17861v1", "url": "http://arxiv.org/abs/2505.17861v1", "title": "Superplatforms Have to Attack AI Agents", "summary": "Over the past decades, superplatforms, digital companies that integrate a\nvast range of third-party services and applications into a single, unified\necosystem, have built their fortunes on monopolizing user attention through\ntargeted advertising and algorithmic content curation. Yet the emergence of AI\nagents driven by large language models (LLMs) threatens to upend this business\nmodel. Agents can not only free user attention with autonomy across diverse\nplatforms and therefore bypass the user-attention-based monetization, but might\nalso become the new entrance for digital traffic. Hence, we argue that\nsuperplatforms have to attack AI agents to defend their centralized control of\ndigital traffic entrance. Specifically, we analyze the fundamental conflict\nbetween user-attention-based monetization and agent-driven autonomy through the\nlens of our gatekeeping theory. We show how AI agents can disintermediate\nsuperplatforms and potentially become the next dominant gatekeepers, thereby\nforming the urgent necessity for superplatforms to proactively constrain and\nattack AI agents. Moreover, we go through the potential technologies for\nsuperplatform-initiated attacks, covering a brand-new, unexplored technical\narea with unique challenges. We have to emphasize that, despite our position,\nthis paper does not advocate for adversarial attacks by superplatforms on AI\nagents, but rather offers an envisioned trend to highlight the emerging\ntensions between superplatforms and AI agents. Our aim is to raise awareness\nand encourage critical discussion for collaborative solutions, prioritizing\nuser interests and perserving the openness of digital ecosystems in the age of\nAI agents.", "authors": ["Jianghao Lin", "Jiachen Zhu", "Zheli Zhou", "Yunjia Xi", "Weiwen Liu", "Yong Yu", "Weinan Zhang"], "published_date": "2025-05-23", "title_zh": "超級平台必須攻擊人工智慧代理人", "summary_zh": "超級平台透過整合第三方服務和應用程式以壟斷使用者注意力，並藉由定向廣告和演算法內容實現盈利。大型語言模型驅動的人工智慧代理可能顛覆此商業模式，因為代理能解放使用者注意力，繞過基於注意力的盈利模式，並成為新的流量入口。因此，超級平台必須反制人工智慧代理以捍衛其對數位流量入口的控制。本文透過閘道把關理論分析基於使用者注意力的盈利模式與代理驅動的自主性之間的根本衝突，說明人工智慧代理如何解除超級平台的中介作用，並可能成為下一個主導的閘道把關者，進而促使超級平台主動約束和攻擊人工智慧代理。本文亦探討超級平台發起的潛在攻擊技術，涵蓋一個全新的技術領域。儘管如此，本文並非提倡超級平台對人工智慧代理進行對抗性攻擊，而是旨在突顯超級平台與人工智慧代理之間的新興緊張關係，提高警覺並鼓勵批判性討論，以尋求協作解決方案，在人工智慧代理時代優先考慮使用者利益並維護數位生態系統的開放性。", "audio": "audios/2505.17861v1.mp3", "timestamp": "2025-05-26T23:17:30.271325"}
{"query": "Foundation Model", "id": "2505.17233v1", "url": "http://arxiv.org/abs/2505.17233v1", "title": "Semantic-Aware Interpretable Multimodal Music Auto-Tagging", "summary": "Music auto-tagging is essential for organizing and discovering music in\nextensive digital libraries. While foundation models achieve exceptional\nperformance in this domain, their outputs often lack interpretability, limiting\ntrust and usability for researchers and end-users alike. In this work, we\npresent an interpretable framework for music auto-tagging that leverages groups\nof musically meaningful multimodal features, derived from signal processing,\ndeep learning, ontology engineering, and natural language processing. To\nenhance interpretability, we cluster features semantically and employ an\nexpectation maximization algorithm, assigning distinct weights to each group\nbased on its contribution to the tagging process. Our method achieves\ncompetitive tagging performance while offering a deeper understanding of the\ndecision-making process, paving the way for more transparent and user-centric\nmusic tagging systems.", "authors": ["Andreas Patakis", "Vassilis Lyberatos", "Spyridon Kantarelis", "Edmund Dervakos", "Giorgos Stamou"], "published_date": "2025-05-22", "title_zh": "語義感知的可解釋多模態音樂自動標籤", "summary_zh": "音樂自動標籤對於管理和探索龐大的數位音樂庫至關重要。雖然基礎模型在此領域表現出色，但其輸出結果缺乏可解釋性，限制了研究人員和使用者的信任與可用性。本研究提出一種可解釋的音樂自動標籤框架，利用源自訊號處理、深度學習、本體工程和自然語言處理的多模態音樂特徵群組。為增強可解釋性，我們對特徵進行語義聚類，並採用期望最大化演算法，根據每個群組對標籤過程的貢獻分配權重。該方法在實現具競爭力的標籤效能的同時，更深入地理解決策過程，為更透明和以使用者為中心的音樂標籤系統奠定基礎。", "audio": "audios/2505.17233v1.mp3", "timestamp": "2025-05-26T23:17:49.110426"}
{"query": "Diffusion Model", "id": "2505.17384v1", "url": "http://arxiv.org/abs/2505.17384v1", "title": "Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling", "summary": "Discrete diffusion models have recently shown great promise for modeling\ncomplex discrete data, with masked diffusion models (MDMs) offering a\ncompelling trade-off between quality and generation speed. MDMs denoise by\nprogressively unmasking multiple dimensions from an all-masked input, but their\nperformance can degrade when using few denoising steps due to limited modeling\nof inter-dimensional dependencies. In this paper, we propose Variational\nAutoencoding Discrete Diffusion (VADD), a novel framework that enhances\ndiscrete diffusion with latent variable modeling to implicitly capture\ncorrelations among dimensions. By introducing an auxiliary recognition model,\nVADD enables stable training via variational lower bounds maximization and\namortized inference over the training set. Our approach retains the efficiency\nof traditional MDMs while significantly improving sample quality, especially\nwhen the number of denoising steps is small. Empirical results on 2D toy data,\npixel-level image generation, and text generation demonstrate that VADD\nconsistently outperforms MDM baselines.", "authors": ["Tianyu Xie", "Shuchen Xue", "Zijin Feng", "Tianyang Hu", "Jiacheng Sun", "Zhenguo Li", "Cheng Zhang"], "published_date": "2025-05-23", "title_zh": "具備增強維度相關性建模的變分自編碼離散擴散", "summary_zh": "離散擴散模型在複雜離散數據建模中展現潛力，遮罩擴散模型(MDM)在品質與生成速度間取得平衡。MDM透過逐步揭露多維度遮罩進行去噪，但去噪步驟少時，維度間依賴建模不足導致效能下降。本文提出變分自編碼離散擴散(VADD)，此框架透過潛變量建模增強離散擴散，隱式捕捉維度間關聯。引入輔助識別模型，VADD經由變分下界最大化與訓練集上的分攤推論實現穩定訓練。本方法保留傳統MDM的效率，同時顯著提升樣本品質，尤其在去噪步驟少時。在2D玩具數據、像素級圖像生成和文本生成上的實驗結果表明，VADD始終優於MDM基準模型。", "audio": "audios/2505.17384v1.mp3", "timestamp": "2025-05-26T23:18:05.445637"}
{"query": "AI", "id": "2505.17855v1", "url": "http://arxiv.org/abs/2505.17855v1", "title": "Explaining Sources of Uncertainty in Automated Fact-Checking", "summary": "Understanding sources of a model's uncertainty regarding its predictions is\ncrucial for effective human-AI collaboration. Prior work proposes using\nnumerical uncertainty or hedges (\"I'm not sure, but ...\"), which do not explain\nuncertainty that arises from conflicting evidence, leaving users unable to\nresolve disagreements or rely on the output. We introduce CLUE\n(Conflict-and-Agreement-aware Language-model Uncertainty Explanations), the\nfirst framework to generate natural language explanations of model uncertainty\nby (i) identifying relationships between spans of text that expose\nclaim-evidence or inter-evidence conflicts and agreements that drive the\nmodel's predictive uncertainty in an unsupervised way, and (ii) generating\nexplanations via prompting and attention steering that verbalize these critical\ninteractions. Across three language models and two fact-checking datasets, we\nshow that CLUE produces explanations that are more faithful to the model's\nuncertainty and more consistent with fact-checking decisions than prompting for\nuncertainty explanations without span-interaction guidance. Human evaluators\njudge our explanations to be more helpful, more informative, less redundant,\nand more logically consistent with the input than this baseline. CLUE requires\nno fine-tuning or architectural changes, making it plug-and-play for any\nwhite-box language model. By explicitly linking uncertainty to evidence\nconflicts, it offers practical support for fact-checking and generalises\nreadily to other tasks that require reasoning over complex information.", "authors": ["Jingyi Sun", "Greta Warren", "Irina Shklovski", "Isabelle Augenstein"], "published_date": "2025-05-23", "title_zh": "自動事實查核中不確定性來源解析", "summary_zh": "理解模型預測不確定性的來源，對有效的人機協作至關重要。以往研究多採用數值不確定性或模糊語氣，但無法解釋源於證據衝突的不確定性，導致使用者難以解決分歧或信任模型輸出。本研究提出CLUE，首個能產生模型不確定性自然語言解釋的框架。CLUE透過（一）無監督方式識別文本片段間的關係，揭示影響模型預測不確定性的主張-證據或證據間的衝突與一致；（二）藉由提示與注意力引導，將關鍵互動轉化為解釋。實驗結果顯示，相較於缺乏片段互動引導的不確定性提示，CLUE產生的解釋更忠實於模型的不確定性，且與事實查核決策更一致。人工評估結果表明，CLUE解釋更具幫助性、資訊性、邏輯一致性且冗餘度更低。CLUE無需微調或架構變更，可隨插即用，適用於任何白盒語言模型。透過將不確定性與證據衝突明確連結，CLUE為事實查核提供實用支持，並可推廣至其他需要複雜資訊推理的任務。", "audio": "audios/2505.17855v1.mp3", "timestamp": "2025-05-27T01:25:22.010195"}
{"query": "Foundation Model", "id": "2505.17228v1", "url": "http://arxiv.org/abs/2505.17228v1", "title": "Automated Capability Evaluation of Foundation Models", "summary": "Current evaluation frameworks for foundation models rely heavily on fixed,\nmanually curated benchmarks, limiting their ability to capture the full breadth\nof model capabilities. This paper introduces Active learning for Capability\nEvaluation (ACE), a novel framework for scalable, automated, and fine-grained\nevaluation of foundation models. ACE leverages the knowledge embedded in\npowerful language models to decompose a domain into semantically meaningful\ncapabilities and generate diverse evaluation tasks, significantly reducing\nhuman effort. To maximize coverage and efficiency, ACE models a subject model's\nperformance as a capability function over a latent semantic space and uses\nactive learning to prioritize the evaluation of the most informative\ncapabilities. This adaptive evaluation strategy enables cost-effective\ndiscovery of strengths, weaknesses, and failure modes that static benchmarks\nmay miss. Our results suggest that ACE provides a more complete and informative\npicture of model capabilities, which is essential for safe and well-informed\ndeployment of foundation models.", "authors": ["Arash Afkanpour", "Omkar Dige", "Fatemeh Tavakoli"], "published_date": "2025-05-22", "title_zh": "基礎模型自動化能力評估", "summary_zh": "現有基礎模型評估框架過於依賴固定的人工基準測試，難以全面捕捉模型能力。本研究提出能力評估主動學習（ACE），一種可擴展、自動化、精細化的新型評估框架。ACE利用語言模型內含知識將領域分解為具語義的能力，並生成多樣化的評估任務，大幅降低人力成本。為最大化覆蓋率和效率，ACE將受測模型的表現建模為潛在語義空間上的能力函數，並使用主動學習優先評估最具資訊性的能力。這種自適應評估策略能以低成本方式發現靜態基準測試可能忽略的優勢、弱點和失效模式。研究結果表明，ACE能提供更完整且資訊豐富的模型能力圖像，對於基礎模型的安全且知情部署至關重要。", "audio": "audios/2505.17228v1.mp3", "timestamp": "2025-05-27T01:25:35.769593"}
{"query": "Diffusion Model", "id": "2505.18017v2", "url": "http://arxiv.org/abs/2505.18017v2", "title": "Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling", "summary": "Deep generative models hold great promise for representing complex physical\nsystems, but their deployment is currently limited by the lack of guarantees on\nthe physical plausibility of the generated outputs. Ensuring that known\nphysical constraints are enforced is therefore critical when applying\ngenerative models to scientific and engineering problems. We address this\nlimitation by developing a principled framework for sampling from a target\ndistribution while rigorously satisfying physical constraints. Leveraging the\nvariational formulation of Langevin dynamics, we propose Split Augmented\nLangevin (SAL), a novel primal-dual sampling algorithm that enforces\nconstraints progressively through variable splitting, with convergence\nguarantees. While the method is developed theoretically for Langevin dynamics,\nwe demonstrate its effective applicability to diffusion models. In particular,\nwe use constrained diffusion models to generate physical fields satisfying\nenergy and mass conservation laws. We apply our method to diffusion-based data\nassimilation on a complex physical system, where enforcing physical constraints\nsubstantially improves both forecast accuracy and the preservation of critical\nconserved quantities. We also demonstrate the potential of SAL for challenging\nfeasibility problems in optimal control.", "authors": ["Matthieu Blanke", "Yongquan Qu", "Sara Shamekh", "Pierre Gentine"], "published_date": "2025-05-23", "title_zh": "基於分裂增廣朗之萬抽樣的嚴格約束生成模型", "summary_zh": "深度生成模型在表徵複雜物理系統方面具巨大潛力，但其應用受限於生成輸出物理合理性缺乏保證。確保已知物理約束至關重要。本研究提出一個嚴格滿足物理約束的抽樣框架，利用朗之萬動力學的變分公式，提出分割增廣朗之萬(SAL)演算法，透過變數分割逐步強制約束，並具收斂保證。此方法雖基於朗之萬動力學，但能有效應用於擴散模型。我們使用受限擴散模型生成滿足能量及質量守恆定律的物理場，並將此方法應用於複雜物理系統的基於擴散之數據同化，物理約束顯著提升預測準確度及守恆量保存。同時，亦展示SAL於最佳控制中具挑戰性的可行性問題之潛力。", "audio": "audios/2505.18017v2.mp3", "timestamp": "2025-05-27T01:25:45.646116"}
{"query": "AI", "id": "2505.20246v1", "url": "http://arxiv.org/abs/2505.20246v1", "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning.", "authors": ["Jiahao Qiu", "Fulian Xiao", "Yimin Wang", "Yuchen Mao", "Yijia Chen", "Xinzhe Juan", "Siran Wang", "Xuan Qi", "Tongcheng Zhang", "Zixin Yao", "Jiacheng Guo", "Yifu Lu", "Charles Argon", "Jundi Cui", "Daixin Chen", "Junran Zhou", "Shuyao Zhou", "Zhanpeng Zhou", "Ling Yang", "Shilong Liu", "Hongru Wang", "Kaixuan Huang", "Xun Jiang", "Yuming Cao", "Yue Chen", "Yunfei Chen", "Zhengyi Chen", "Ruowei Dai", "Mengqiu Deng", "Jiye Fu", "Yunting Gu", "Zijie Guan", "Zirui Huang", "Xiaoyan Ji", "Yumeng Jiang", "Delong Kong", "Haolong Li", "Jiaqi Li", "Ruipeng Li", "Tianze Li", "Zhuoran Li", "Haixia Lian", "Mengyue Lin", "Xudong Liu", "Jiayi Lu", "Jinghan Lu", "Wanyu Luo", "Ziyue Luo", "Zihao Pu", "Zhi Qiao", "Ruihuan Ren", "Liang Wan", "Ruixiang Wang", "Tianhui Wang", "Yang Wang", "Zeyu Wang", "Zihua Wang", "Yujia Wu", "Zhaoyi Wu", "Hao Xin", "Weiao Xing", "Ruojun Xiong", "Weijie Xu", "Yao Shu", "Xiao Yao", "Xiaorui Yang", "Yuchen Yang", "Nan Yi", "Jiadong Yu", "Yangyuxuan Yu", "Huiting Zeng", "Danni Zhang", "Yunjie Zhang", "Zhaoyu Zhang", "Zhiheng Zhang", "Xiaofeng Zheng", "Peirong Zhou", "Linyan Zhong", "Xiaoyin Zong", "Ying Zhao", "Zhenxin Chen", "Lin Ding", "Xiaoyu Gao", "Bingbing Gong", "Yichao Li", "Yang Liao", "Guang Ma", "Tianyuan Ma", "Xinrui Sun", "Tianyi Wang", "Han Xia", "Ruobing Xian", "Gen Ye", "Tengfei Yu", "Wentao Zhang", "Yuxi Wang", "Xi Gao", "Mengdi Wang"], "published_date": "2025-05-26", "title_zh": "邁向多模態歷史推理之路：歷史基準與歷史代理人", "summary_zh": "大型語言模型雖在多領域取得顯著進展，但在人文學科，尤其歷史學的應用仍待探索。歷史推理對人工智慧構成獨特挑戰，涉及多模態資源解讀、時間推理及跨語言分析。現有通用模型雖在基準測試表現尚可，但缺乏歷史領域專業知識。為此，我們推出HistBench，包含414個高品質問題，旨在評估人工智慧的歷史推理能力，由超過40位專家編寫，涵蓋基於原始文獻的事實檢索、手稿與圖像的詮釋分析，以及涉及考古學、語言學或文化史的跨學科挑戰。該基準測試資料集橫跨29種古代及現代語言，涵蓋廣泛的歷史時期與世界區域。由於大型語言模型及其他模型在HistBench表現不佳，我們進一步提出HistAgent，一款專為歷史學設計的代理，配備OCR、翻譯、檔案搜尋和圖像理解工具。在HistBench上，基於GPT-4o的HistAgent準確率顯著優於具備線上搜尋功能的大型語言模型及通用型代理，顯示現有模型的局限性，並證實HistAgent在歷史推理方面的優勢。", "audio": "audios/2505.20246v1.mp3", "timestamp": "2025-05-27T03:10:05.404551"}
{"query": "Foundation Model", "id": "2505.20202v1", "url": "http://arxiv.org/abs/2505.20202v1", "title": "PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology", "summary": "The emergence of pathology foundation models has revolutionized computational\nhistopathology, enabling highly accurate, generalized whole-slide image\nanalysis for improved cancer diagnosis, and prognosis assessment. While these\nmodels show remarkable potential across cancer diagnostics and prognostics,\ntheir clinical translation faces critical challenges including variability in\noptimal model across cancer types, potential data leakage in evaluation, and\nlack of standardized benchmarks. Without rigorous, unbiased evaluation, even\nthe most advanced PFMs risk remaining confined to research settings, delaying\ntheir life-saving applications. Existing benchmarking efforts remain limited by\nnarrow cancer-type focus, potential pretraining data overlaps, or incomplete\ntask coverage. We present PathBench, the first comprehensive benchmark\naddressing these gaps through: multi-center in-hourse datasets spanning common\ncancers with rigorous leakage prevention, evaluation across the full clinical\nspectrum from diagnosis to prognosis, and an automated leaderboard system for\ncontinuous model assessment. Our framework incorporates large-scale data,\nenabling objective comparison of PFMs while reflecting real-world clinical\ncomplexity. All evaluation data comes from private medical providers, with\nstrict exclusion of any pretraining usage to avoid data leakage risks. We have\ncollected 15,888 WSIs from 8,549 patients across 10 hospitals, encompassing\nover 64 diagnosis and prognosis tasks. Currently, our evaluation of 19 PFMs\nshows that Virchow2 and H-Optimus-1 are the most effective models overall. This\nwork provides researchers with a robust platform for model development and\noffers clinicians actionable insights into PFM performance across diverse\nclinical scenarios, ultimately accelerating the translation of these\ntransformative technologies into routine pathology practice.", "authors": ["Jiabo Ma", "Yingxue Xu", "Fengtao Zhou", "Yihui Wang", "Cheng Jin", "Zhengrui Guo", "Jianfeng Wu", "On Ki Tang", "Huajun Zhou", "Xi Wang", "Luyang Luo", "Zhengyu Zhang", "Du Cai", "Zizhao Gao", "Wei Wang", "Yueping Liu", "Jiankun He", "Jing Cui", "Zhenhui Li", "Jing Zhang", "Feng Gao", "Xiuming Zhang", "Li Liang", "Ronald Cheong Kin Chan", "Zhe Wang", "Hao Chen"], "published_date": "2025-05-26", "title_zh": "PathBench：面向精準腫瘤學的病理學基礎模型綜合比較基準", "summary_zh": "病理學基礎模型的出現革新了計算病理學，實現了高精度、廣泛的全玻片影像分析，從而改善癌症診斷及預後評估。儘管這些模型在癌症診斷及預後方面展現巨大潛力，但其臨床轉化面臨多重挑戰，包括不同癌症類型的最佳模型變異性、評估中潛在的數據洩漏以及缺乏標準化基準。為了解決這些問題，我們提出了PathBench，首個綜合性基準，透過多中心內部數據集（涵蓋常見癌症並嚴格防止數據洩漏）、診斷到預後的全臨床範圍評估以及持續模型評估的自動排行榜系統來解決上述問題。我們的框架整合了大規模數據，能夠客觀比較病理學基礎模型，同時反映真實世界的臨床複雜性。所有評估數據均來自私人醫療機構，並嚴格排除任何預訓練使用以避免數據洩漏風險。我們收集了來自10家醫院、8549名患者的15888張全玻片影像，涵蓋64多項診斷及預後任務。目前，我們對19個病理學基礎模型的評估顯示，Virchow2和H-Optimus-1是整體上最有效的模型。這項工作為研究人員提供了一個穩健的模型開發平台，並為臨床醫生提供了關於病理學基礎模型在不同臨床情境下性能的可操作見解，最終加速將這些變革性技術轉化為常規病理學實踐。", "audio": "audios/2505.20202v1.mp3", "timestamp": "2025-05-27T03:10:16.123666"}
{"query": "Diffusion Model", "id": "2505.20171v1", "url": "http://arxiv.org/abs/2505.20171v1", "title": "Long-Context State-Space Video World Models", "summary": "Video diffusion models have recently shown promise for world modeling through\nautoregressive frame prediction conditioned on actions. However, they struggle\nto maintain long-term memory due to the high computational cost associated with\nprocessing extended sequences in attention layers. To overcome this limitation,\nwe propose a novel architecture leveraging state-space models (SSMs) to extend\ntemporal memory without compromising computational efficiency. Unlike previous\napproaches that retrofit SSMs for non-causal vision tasks, our method fully\nexploits the inherent advantages of SSMs in causal sequence modeling. Central\nto our design is a block-wise SSM scanning scheme, which strategically trades\noff spatial consistency for extended temporal memory, combined with dense local\nattention to ensure coherence between consecutive frames. We evaluate the\nlong-term memory capabilities of our model through spatial retrieval and\nreasoning tasks over extended horizons. Experiments on Memory Maze and\nMinecraft datasets demonstrate that our approach surpasses baselines in\npreserving long-range memory, while maintaining practical inference speeds\nsuitable for interactive applications.", "authors": ["Ryan Po", "Yotam Nitzan", "Richard Zhang", "Berlin Chen", "Tri Dao", "Eli Shechtman", "Gordon Wetzstein", "Xun Huang"], "published_date": "2025-05-26", "title_zh": "長程上下文狀態空間影片世界模型", "summary_zh": "影片擴散模型在世界建模中展現潛力，透過以動作為條件的自迴歸幀預測實現。然而，因注意力層處理長序列的計算成本高昂，模型難以維持長期記憶。為克服此限制，本研究提出一種新穎架構，利用狀態空間模型(SSM)擴展時間記憶，同時不犧牲計算效率。不同於以往將SSM改造用於非因果視覺任務的方法，本方法充分利用SSM在因果序列建模中的固有優勢。設計核心為分塊式SSM掃描方案，策略性地犧牲空間一致性以換取更長的時間記憶，並結合密集局部注意力以確保連續幀之間的一致性。透過在長時間範圍內的空間檢索和推理任務評估模型長期記憶能力。在記憶迷宮和Minecraft數據集上的實驗表明，本方法在保持適用於互動式應用程序的推理速度下，超越基準模型，能更好地保留長程記憶。", "audio": "audios/2505.20171v1.mp3", "timestamp": "2025-05-27T03:10:28.778843"}
{"query": "AI", "id": "2505.20148v1", "url": "http://arxiv.org/abs/2505.20148v1", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "published_date": "2025-05-26", "title_zh": "礦構萬物：開放世界AI代理空間規劃基準測試", "summary_zh": "空間規劃是空間智能的關鍵組成部分，需要理解並規劃空間中的物體佈局。具備空間規劃能力的人工智慧代理可以更好地適應現實應用，如機器人操作、自動組裝和城市規劃。現有研究試圖構建基準來評估多模態大型語言模型（MLLM）的空間智能，但這些基準主要關注基於視覺問答（VQA）形式的空間推理，存在抽象空間理解與具體任務執行之間的差距。本研究進一步構建名為MineAnyBuild的綜合基準，旨在評估開放世界人工智慧代理在Minecraft遊戲中的空間規劃能力。MineAnyBuild要求代理根據多模態人類指令生成可執行的建築規劃，包含4000個精選的空間規劃任務，並提供利用豐富玩家生成內容無限擴展數據收集的範例。MineAnyBuild透過空間理解、空間推理、創造力和空間常識四個核心維度評估空間規劃。基於MineAnyBuild，我們對現有基於MLLM的代理進行全面評估，揭示了它們在空間規劃能力方面的嚴重局限性，但也展現了巨大潛力。MineAnyBuild將為空間智能的評估開闢新途徑，並促進具備空間規劃能力的開放世界人工智慧代理的進一步發展。", "audio": "audios/2505.20148v1.mp3", "timestamp": "2025-05-27T04:25:27.119628"}
{"query": "Foundation Model", "id": "2505.20003v1", "url": "http://arxiv.org/abs/2505.20003v1", "title": "TabPFN: One Model to Rule Them All?", "summary": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study).", "authors": ["Qiong Zhang", "Yan Shuo Tan", "Qinglong Tian", "Pengfei Li"], "published_date": "2025-05-26", "title_zh": "TabPFN：一統天下的模型？", "summary_zh": "Hollmann等人於《自然》期刊發表TabPFN，一種基於Transformer的表格數據深度學習模型，聲稱其在小型數據集上大幅超越既有方法，並具備數據生成、密度估計、可重用嵌入和微調等能力，堪稱表格數據的基礎模型。本研究針對統計學受眾闡釋TabPFN的運作原理，強調其近似貝葉斯推斷的詮釋。實驗證明，TabPFN在半監督參數估計、協變量偏移下的預測和異質性處理效應估計等方面，優於專業的頂尖方法；在稀疏回歸中優於LASSO，並打破了分類中的穩健性-效率權衡。代碼已公開於https://github.com/qinglong-tian/tabpfn_study。", "audio": "audios/2505.20003v1.mp3", "timestamp": "2025-05-27T04:25:32.362431"}
{"query": "Diffusion Model", "id": "2505.20131v1", "url": "http://arxiv.org/abs/2505.20131v1", "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning", "summary": "Molecular editing aims to modify a given molecule to optimize desired\nchemical properties while preserving structural similarity. However, current\napproaches typically rely on string-based or continuous representations, which\nfail to adequately capture the discrete, graph-structured nature of molecules,\nresulting in limited structural fidelity and poor controllability. In this\npaper, we propose MolEditRL, a molecular editing framework that explicitly\nintegrates structural constraints with precise property optimization.\nSpecifically, MolEditRL consists of two stages: (1) a discrete graph diffusion\nmodel pretrained to reconstruct target molecules conditioned on source\nstructures and natural language instructions; (2) an editing-aware\nreinforcement learning fine-tuning stage that further enhances property\nalignment and structural preservation by explicitly optimizing editing\ndecisions under graph constraints. For comprehensive evaluation, we construct\nMolEdit-Instruct, the largest and most property-rich molecular editing dataset,\ncomprising 3 million diverse examples spanning single- and multi-property tasks\nacross 10 chemical attributes. Experimental results demonstrate that MolEditRL\nsignificantly outperforms state-of-the-art methods in both property\noptimization accuracy and structural fidelity, achieving a 74\\% improvement in\nediting success rate while using 98\\% fewer parameters.", "authors": ["Yuanxin Zhuang", "Dazhong Shen", "Ying Sun"], "published_date": "2025-05-26", "title_zh": "MolEditRL：基於離散擴散與強化學習的結構保持分子編輯", "summary_zh": "分子編輯旨在修改既有分子，優化化學性質，同時維持結構相似性。現有方法多仰賴字串或連續表徵，未能充分捕捉分子的離散圖結構特性，導致結構保真度受限、可控性不佳。本文提出 MolEditRL，一種分子編輯框架，明確整合結構約束與精確性質優化。MolEditRL包含兩階段：(1) 預訓練離散圖擴散模型，根據源結構與自然語言指令重建目標分子；(2) 編輯感知強化學習微調階段，藉由在圖約束下明確優化編輯決策，進一步提升性質一致性與結構保留。為全面評估，我們構建了 MolEdit-Instruct，涵蓋三百萬個範例，跨越單一與多重性質任務，包含十種化學屬性，是規模最大、性質最豐富的分子編輯資料集。實驗結果表明，MolEditRL 在性質優化準確性與結構保真度方面顯著優於現有技術，編輯成功率提升74%，同時參數減少98%。", "audio": "audios/2505.20131v1.mp3", "timestamp": "2025-05-27T04:25:40.009661"}
{"query": "AI", "id": "2505.20236v1", "url": "http://arxiv.org/abs/2505.20236v1", "title": "Seeing is Believing, but How Much? A Comprehensive Analysis of Verbalized Calibration in Vision-Language Models", "summary": "Uncertainty quantification is essential for assessing the reliability and\ntrustworthiness of modern AI systems. Among existing approaches, verbalized\nuncertainty, where models express their confidence through natural language,\nhas emerged as a lightweight and interpretable solution in large language\nmodels (LLMs). However, its effectiveness in vision-language models (VLMs)\nremains insufficiently studied. In this work, we conduct a comprehensive\nevaluation of verbalized confidence in VLMs, spanning three model categories,\nfour task domains, and three evaluation scenarios. Our results show that\ncurrent VLMs often display notable miscalibration across diverse tasks and\nsettings. Notably, visual reasoning models (i.e., thinking with images)\nconsistently exhibit better calibration, suggesting that modality-specific\nreasoning is critical for reliable uncertainty estimation. To further address\ncalibration challenges, we introduce Visual Confidence-Aware Prompting, a\ntwo-stage prompting strategy that improves confidence alignment in multimodal\nsettings. Overall, our study highlights the inherent miscalibration in VLMs\nacross modalities. More broadly, our findings underscore the fundamental\nimportance of modality alignment and model faithfulness in advancing reliable\nmultimodal systems.", "authors": ["Weihao Xuan", "Qingcheng Zeng", "Heli Qi", "Junjue Wang", "Naoto Yokoya"], "published_date": "2025-05-26", "title_zh": "眼見為憑，然信度幾何？視覺語言模型中口語校準之綜合分析", "summary_zh": "不確定性量化對評估現代人工智慧系統的可靠性至關重要。在現有方法中，口語化不確定性已成為大型語言模型中一種輕量且可解釋的解決方案，透過自然語言表達模型信心。然而，其在視覺語言模型中的有效性仍未得到充分研究。本研究全面評估視覺語言模型中的口語化信心，涵蓋三種模型類別、四個任務領域和三種評估情境。結果顯示，當前視覺語言模型在不同任務和設定中普遍存在顯著的校準誤差。值得注意的是，視覺推理模型展現出較佳的校準效果，表明模態特定推理對於可靠的不確定性估計至關重要。為了解決校準挑戰，我們提出視覺信心感知提示，一種雙階段提示策略，旨在提升多模態設定中的信心對齊。總體而言，本研究強調視覺語言模型中跨模態的固有校準誤差，並強調模態對齊和模型忠實度的重要性，以推進可靠的多模態系統。", "audio": "audios/2505.20236v1.mp3", "timestamp": "2025-05-27T05:18:58.542480"}
{"query": "Foundation Model", "id": "2505.19892v1", "url": "http://arxiv.org/abs/2505.19892v1", "title": "Unifying Multimodal Large Language Model Capabilities and Modalities via Model Merging", "summary": "While foundation models update slowly due to resource-intensive training\nrequirements, domain-specific models evolve between updates. Model merging aims\nto combine multiple expert models into a single, more capable model, thereby\nreducing storage and serving costs while supporting decentralized model\ndevelopment. Despite its potential, previous studies have primarily focused on\nmerging visual classification models or Large Language Models (LLMs) for code\nand math tasks. Multimodal Large Language Models (MLLMs), which extend the\ncapabilities of LLMs through large-scale multimodal training, have gained\ntraction. However, there lacks a benchmark for model merging research that\nclearly divides the tasks for MLLM training and evaluation. In this paper, (i)\nwe introduce the model merging benchmark for MLLMs, which includes multiple\ntasks such as VQA, Geometry, Chart, OCR, and Grounding, providing both LoRA and\nfull fine-tuning models. Moreover, we explore how model merging can combine\ndifferent modalities (e.g., vision-language, audio-language, and video-language\nmodels), moving toward the Omni-language model. (ii) We implement 10 model\nmerging algorithms on the benchmark. Furthermore, we propose a novel method\nthat removes noise from task vectors and robustly optimizes the merged vector\nbased on a loss defined over task vector interactions, achieving an average\nperformance gain of 2.48%. (iii) We find that model merging offers a promising\nway for building improved MLLMs without requiring data training. Our results\nalso demonstrate that the complementarity among multiple modalities outperforms\nindividual modalities.", "authors": ["Yongxian Wei", "Runxi Cheng", "Weike Jin", "Enneng Yang", "Li Shen", "Lu Hou", "Sinan Du", "Chun Yuan", "Xiaochun Cao", "Dacheng Tao"], "published_date": "2025-05-26", "title_zh": "透過模型合併統一多模態大型語言模型之能力與模態", "summary_zh": "由於基礎模型訓練耗費大量資源，更新速度較慢，領域模型則在更新間持續演進。模型合併旨在將多個專業模型整合為單一、更強大的模型，藉此降低儲存和服務成本，並支援分散式模型開發。儘管潛力巨大，既有研究主要集中於合併視覺分類模型或用於程式碼和數學任務的大型語言模型（LLMs）。多模態大型語言模型（MLLMs）透過大規模多模態訓練擴展了LLMs的功能，已備受關注。然而，缺乏針對MLLM模型合併研究的基準，能明確劃分MLLM訓練和評估任務。本研究（i）提出MLLM的模型合併基準，包含VQA、幾何、圖表、OCR和Grounding等多項任務，並提供LoRA和完整微調模型。此外，我們探索模型合併如何結合不同模態（例如，視覺-語言、音訊-語言和視訊-語言模型），朝向全語言模型發展。（ii）我們在基準上實作了10種模型合併演算法。更進一步，我們提出一種新穎方法，可移除任務向量中的雜訊，並基於任務向量互動定義的損失函數，穩健地優化合併向量，平均效能提升2.48%。（iii）研究發現，模型合併為構建更佳的MLLM提供了一種有前景的方式，無需數據訓練。結果也表明，多模態之間的互補性優於單一模態。", "audio": "audios/2505.19892v1.mp3", "timestamp": "2025-05-27T05:19:07.980818"}
{"query": "Diffusion Model", "id": "2505.20123v1", "url": "http://arxiv.org/abs/2505.20123v1", "title": "Understanding Generalization in Diffusion Models via Probability Flow Distance", "summary": "Diffusion models have emerged as a powerful class of generative models,\ncapable of producing high-quality samples that generalize beyond the training\ndata. However, evaluating this generalization remains challenging: theoretical\nmetrics are often impractical for high-dimensional data, while no practical\nmetrics rigorously measure generalization. In this work, we bridge this gap by\nintroducing probability flow distance ($\\texttt{PFD}$), a theoretically\ngrounded and computationally efficient metric to measure distributional\ngeneralization. Specifically, $\\texttt{PFD}$ quantifies the distance between\ndistributions by comparing their noise-to-data mappings induced by the\nprobability flow ODE. Moreover, by using $\\texttt{PFD}$ under a teacher-student\nevaluation protocol, we empirically uncover several key generalization\nbehaviors in diffusion models, including: (1) scaling behavior from\nmemorization to generalization, (2) early learning and double descent training\ndynamics, and (3) bias-variance decomposition. Beyond these insights, our work\nlays a foundation for future empirical and theoretical studies on\ngeneralization in diffusion models.", "authors": ["Huijie Zhang", "Zijian Huang", "Siyi Chen", "Jinfan Zhou", "Zekai Zhang", "Peng Wang", "Qing Qu"], "published_date": "2025-05-26", "title_zh": "基於機率流距離理解擴散模型的泛化能力", "summary_zh": "擴散模型已成為強大的生成模型，能產生高品質且泛化性超越訓練資料的樣本。然而，評估此泛化能力仍具挑戰：理論指標在高維資料中不切實際，且缺乏嚴格衡量泛化的實用指標。本研究引入機率流距離（PFD），一種具理論基礎且計算高效的指標，以衡量分布泛化能力。PFD透過比較機率流ODE引導的雜訊到資料映射，量化分布之間的距離。透過教師-學生評估協議使用PFD，我們揭示了擴散模型中的關鍵泛化行為，包括：(1)從記憶到泛化的縮放行為，(2)早期學習和雙重下降訓練動態，(3)偏差-方差分解。本研究為未來擴散模型泛化的實證和理論研究奠定了基礎。", "audio": "audios/2505.20123v1.mp3", "timestamp": "2025-05-27T05:19:13.780629"}
{"query": "AI", "id": "2505.20142v1", "url": "http://arxiv.org/abs/2505.20142v1", "title": "Model Stitching by Functional Latent Alignment", "summary": "Evaluating functional similarity involves quantifying the degree to which\nindependently trained neural networks learn functionally similar\nrepresentations. Reliably inferring the functional similarity of these networks\nremains an open problem with far-reaching implications for AI. Model stitching\nhas emerged as a promising paradigm, where an optimal affine transformation\naligns two models to solve a task, with the stitched model serving as a proxy\nfor functional similarity. In this work, we draw inspiration from the knowledge\ndistillation literature and propose Functional Latent Alignment (FuLA) as a\nnovel optimality condition for model stitching. We revisit previously explored\nfunctional similarity testbeds and introduce a new one, based on which FuLA\nemerges as an overall more reliable method of functional similarity.\nSpecifically, our experiments in (a) adversarial training, (b) shortcut\ntraining and, (c) cross-layer stitching, reveal that FuLA is less prone to\nartifacts tied to training on task cues while achieving non-trivial alignments\nthat are missed by stitch-level matching.", "authors": ["Ioannis Athanasiadis", "Anmar Karmush", "Michael Felsberg"], "published_date": "2025-05-26", "title_zh": "基於函數潛在對齊的模型縫合", "summary_zh": "評估功能相似性涉及量化獨立訓練神經網路學習功能相似表徵的程度。可靠推斷網路功能相似性仍是未解難題，對人工智慧影響深遠。模型縫合已成為有前景的方法，透過最佳仿射變換對齊兩模型以解決任務，縫合模型作為功能相似性的替代。本研究從知識蒸餾汲取靈感，提出功能潛在對齊 (FuLA) 作為模型縫合的新優化條件。我們重新檢視先前探索的功能相似性測試平台，並引入新平台，基於此，FuLA 成為更可靠的功能相似性方法。具體而言，在 (a) 對抗訓練、(b) 捷徑訓練和 (c) 跨層縫合的實驗表明，FuLA 不易受到與任務線索相關的偽像影響，同時實現了縫合層級匹配遺漏的重要對齊。", "audio": "audios/2505.20142v1.mp3", "timestamp": "2025-05-27T06:27:18.127000"}
{"query": "Foundation Model", "id": "2505.19888v1", "url": "http://arxiv.org/abs/2505.19888v1", "title": "Generalized and Personalized Federated Learning with Foundation Models via Orthogonal Transformations", "summary": "Federated Learning (FL) aims to train models across decentralized clients or\ndevices holding local data without the need for centralized data collection,\nthus enhancing data privacy and security. However, achieving both\ngeneralization and personalization in heterogeneous settings remains a\nsignificant challenge. To address this, we introduce FedOT, a novel approach\nthat leverages black-box foundation models. FedOT shares only a global\ntask-dependent classifier across clients while locally adapting features\nthrough orthogonal transformations. By enforcing orthogonality, FedOT mitigates\ngradient conflicts across diverse clients, preserves semantic integrity, and\nachieves robust performance even in the presence of substantial data\nheterogeneity. The strategy of combining global and local parameters enables a\nmore balanced approach for both generalization and personalization,\noutperforming baseline FL methods across multiple benchmarks. Furthermore, our\nextensive analysis confirms that joint optimization of global classifiers and\nlocal orthogonal transformations yields superior performance and suggests\nbroader applicability.", "authors": ["Eun Gyung Kong", "Je Won Yeom", "Yonghoon Jeon", "Taesup Kim"], "published_date": "2025-05-26", "title_zh": "基於正交變換的基礎模型廣義與個性化聯邦學習", "summary_zh": "聯邦學習旨在分散式客戶端或設備上訓練模型，無需集中數據收集，從而加強數據隱私與安全。然而，在異質環境中同時實現泛化與個性化仍具挑戰。為此，我們提出FedOT，一種利用黑盒基礎模型的新穎方法。FedOT僅共享全局任務相關分類器，同時通過正交轉換在本地調整特徵。通過強制正交性，FedOT減輕了跨多樣化客戶端的梯度衝突，保留了語義完整性，並在存在顯著數據異質性的情況下實現穩健的性能。全局與本地參數的結合策略有助於平衡泛化與個性化，在多個基準測試中優於傳統聯邦學習方法。此外，我們的廣泛分析證實了全局分類器和本地正交轉換的聯合優化可產生卓越性能，並暗示了更廣泛的適用性。", "audio": "audios/2505.19888v1.mp3", "timestamp": "2025-05-27T06:27:25.061762"}
{"query": "Diffusion Model", "id": "2505.20107v1", "url": "http://arxiv.org/abs/2505.20107v1", "title": "Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning", "summary": "Text-to-multiview (T2MV) generation, which produces coherent multiview images\nfrom a single text prompt, remains computationally intensive, while accelerated\nT2MV methods using few-step diffusion models often sacrifice image fidelity and\nview consistency. To address this, we propose a novel reinforcement learning\n(RL) finetuning framework tailored for few-step T2MV diffusion models to\njointly optimize per-view fidelity and cross-view consistency. Specifically, we\nfirst reformulate T2MV denoising across all views as a single unified Markov\ndecision process, enabling multiview-aware policy optimization driven by a\njoint-view reward objective. Next, we introduce ZMV-Sampling, a test-time T2MV\nsampling technique that adds an inversion-denoising pass to reinforce both\nviewpoint and text conditioning, resulting in improved T2MV generation at the\ncost of inference time. To internalize its performance gains into the base\nsampling policy, we develop MV-ZigAL, a novel policy optimization strategy that\nuses reward advantages of ZMV-Sampling over standard sampling as learning\nsignals for policy updates. Finally, noting that the joint-view reward\nobjective under-optimizes per-view fidelity but naively optimizing single-view\nmetrics neglects cross-view alignment, we reframe RL finetuning for T2MV\ndiffusion models as a constrained optimization problem that maximizes per-view\nfidelity subject to an explicit joint-view constraint, thereby enabling more\nefficient and balanced policy updates. By integrating this constrained\noptimization paradigm with MV-ZigAL, we establish our complete RL finetuning\nframework, referred to as MVC-ZigAL, which effectively refines the few-step\nT2MV diffusion baseline in both fidelity and consistency while preserving its\nfew-step efficiency.", "authors": ["Ziyi Zhang", "Li Shen", "Deheng Ye", "Yong Luo", "Huangxuan Zhao", "Lefei Zhang"], "published_date": "2025-05-26", "title_zh": "藉由強化學習精煉少步文本至多視角擴散", "summary_zh": "本文提出一種新的強化學習微調框架，專為少步文字到多視圖（T2MV）擴散模型設計，以共同優化單視圖逼真度和跨視圖一致性。首先，將所有視圖的T2MV去噪重新架構為統一的馬可夫決策過程，實現由聯合視圖獎勵目標驅動的多視圖感知策略優化。其次，引入ZMV-Sampling，一種測試時採樣技術，通過增加反演-去噪過程來強化視點和文本條件，改善T2MV生成效果，但會增加推論時間。為將其性能提升融入基礎採樣策略，開發了MV-ZigAL，一種策略優化策略，利用ZMV-Sampling相較於標準採樣的獎勵優勢作為策略更新的學習訊號。最後，將T2MV擴散模型的強化學習微調重新定義為約束優化問題，在明確的聯合視圖約束下最大化單視圖逼真度，從而實現更有效和平衡的策略更新。透過整合此約束優化範例與MV-ZigAL，建立了完整的強化學習微調框架MVC-ZigAL，可有效改進少步T2MV擴散模型的逼真度和一致性，同時保持其少步效率。", "audio": "audios/2505.20107v1.mp3", "timestamp": "2025-05-27T06:27:32.498349"}
{"query": "AI", "id": "2505.20222v1", "url": "http://arxiv.org/abs/2505.20222v1", "title": "FT-Boosted SV: Towards Noise Robust Speaker Verification for English Speaking Classroom Environments", "summary": "Creating Speaker Verification (SV) systems for classroom settings that are\nrobust to classroom noises such as babble noise is crucial for the development\nof AI tools that assist educational environments. In this work, we study the\nefficacy of finetuning with augmented children datasets to adapt the x-vector\nand ECAPA-TDNN to classroom environments. We demonstrate that finetuning with\naugmented children's datasets is powerful in that regard and reduces the Equal\nError Rate (EER) of x-vector and ECAPA-TDNN models for both classroom datasets\nand children speech datasets. Notably, this method reduces EER of the\nECAPA-TDNN model on average by half (a 5 % improvement) for classrooms in the\nMPT dataset compared to the ECAPA-TDNN baseline model. The x-vector model shows\nan 8 % average improvement for classrooms in the NCTE dataset compared to its\nbaseline.", "authors": ["Saba Tabatabaee", "Jing Liu", "Carol Espy-Wilson"], "published_date": "2025-05-26", "title_zh": "FT增強型SV：面向英語口語課堂環境的抗噪聲說話人驗證", "summary_zh": "為開發輔助教育環境的人工智慧工具，建立能有效應對教室噪音（如人聲嘈雜）的講者驗證系統至關重要。本研究探討使用擴增兒童語音資料集微調 x-vector 和 ECAPA-TDNN 模型，使其適應教室環境的有效性。結果表明，此微調方法能顯著降低 x-vector 和 ECAPA-TDNN 模型在教室和兒童語音資料集上的等錯誤率 (EER)。特別是，ECAPA-TDNN 模型在 MPT 資料集的教室環境中，EER 平均降低一半（提升 5%），而 x-vector 模型在 NCTE 資料集的教室環境中，相較於基準模型平均提升 8%。", "audio": "audios/2505.20222v1.mp3", "timestamp": "2025-05-27T07:19:01.932745"}
{"query": "Foundation Model", "id": "2505.19863v1", "url": "http://arxiv.org/abs/2505.19863v1", "title": "FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields", "summary": "We introduce FruitNeRF++, a novel fruit-counting approach that combines\ncontrastive learning with neural radiance fields to count fruits from\nunstructured input photographs of orchards. Our work is based on FruitNeRF,\nwhich employs a neural semantic field combined with a fruit-specific clustering\napproach. The requirement for adaptation for each fruit type limits the\napplicability of the method, and makes it difficult to use in practice. To lift\nthis limitation, we design a shape-agnostic multi-fruit counting framework,\nthat complements the RGB and semantic data with instance masks predicted by a\nvision foundation model. The masks are used to encode the identity of each\nfruit as instance embeddings into a neural instance field. By volumetrically\nsampling the neural fields, we extract a point cloud embedded with the instance\nfeatures, which can be clustered in a fruit-agnostic manner to obtain the fruit\ncount. We evaluate our approach using a synthetic dataset containing apples,\nplums, lemons, pears, peaches, and mangoes, as well as a real-world benchmark\napple dataset. Our results demonstrate that FruitNeRF++ is easier to control\nand compares favorably to other state-of-the-art methods.", "authors": ["Lukas Meyer", "Andrei-Timotei Ardelean", "Tim Weyrich", "Marc Stamminger"], "published_date": "2025-05-26", "title_zh": "FruitNeRF++：基於對比學習與神經輻射場的通用多水果計數方法", "summary_zh": "FruitNeRF++是一種新型水果計數方法，結合對比學習與神經輻射場，從果園的非結構化照片中計數水果。此方法基於FruitNeRF，後者採用神經語義場和特定水果的聚類方法。為了解決FruitNeRF需針對每種水果進行調整的限制，我們設計了一種形狀無關的多水果計數框架，利用視覺基礎模型預測的實例掩碼來補充RGB和語義數據。掩碼用於將每個水果的身份編碼為神經實例場中的實例嵌入。透過對神經場進行體積採樣，提取嵌入實例特徵的點雲，並以與水果種類無關的方式進行聚類，從而獲得水果數量。我們使用包含蘋果、李子、檸檬、梨、桃子和芒果的合成數據集以及真實世界的蘋果基準數據集評估了此方法。結果表明，FruitNeRF++更易於控制，並且優於其他最先進的方法。", "audio": "audios/2505.19863v1.mp3", "timestamp": "2025-05-27T07:19:09.385657"}
{"query": "Diffusion Model", "id": "2505.20056v1", "url": "http://arxiv.org/abs/2505.20056v1", "title": "PAMD: Plausibility-Aware Motion Diffusion Model for Long Dance Generation", "summary": "Computational dance generation is crucial in many areas, such as art,\nhuman-computer interaction, virtual reality, and digital entertainment,\nparticularly for generating coherent and expressive long dance sequences.\nDiffusion-based music-to-dance generation has made significant progress, yet\nexisting methods still struggle to produce physically plausible motions. To\naddress this, we propose Plausibility-Aware Motion Diffusion (PAMD), a\nframework for generating dances that are both musically aligned and physically\nrealistic. The core of PAMD lies in the Plausible Motion Constraint (PMC),\nwhich leverages Neural Distance Fields (NDFs) to model the actual pose manifold\nand guide generated motions toward a physically valid pose manifold. To provide\nmore effective guidance during generation, we incorporate Prior Motion Guidance\n(PMG), which uses standing poses as auxiliary conditions alongside music\nfeatures. To further enhance realism for complex movements, we introduce the\nMotion Refinement with Foot-ground Contact (MRFC) module, which addresses\nfoot-skating artifacts by bridging the gap between the optimization objective\nin linear joint position space and the data representation in nonlinear\nrotation space. Extensive experiments show that PAMD significantly improves\nmusical alignment and enhances the physical plausibility of generated motions.\nThis project page is available at: https://mucunzhuzhu.github.io/PAMD-page/.", "authors": ["Hongsong Wang", "Yin Zhu", "Qiuxia Lai", "Yang Zhang", "Guo-Sen Xie", "Xin Geng"], "published_date": "2025-05-26", "title_zh": "PAMD：基於合理性感知運動擴散模型的長舞蹈生成", "summary_zh": "計算舞蹈生成在藝術、人機互動、虛擬實境及數位娛樂等領域至關重要，尤其在產生連貫且富表現力的長舞蹈序列方面。基於擴散模型的音樂到舞蹈生成已取得顯著進展，但現有方法仍難以產生符合物理真實性的動作。為此，我們提出具合理性感知的動作擴散（PAMD）框架，旨在生成兼具音樂對齊性和物理真實性的舞蹈。PAMD的核心在於合理動作約束（PMC），利用神經距離場（NDFs）建模真實姿勢流形，引導生成動作朝向物理上有效的姿勢流形。為在生成過程中提供更有效的引導，我們結合了先驗動作引導（PMG），將站立姿勢作為輔助條件與音樂特徵一起使用。為進一步提升複雜動作的真實感，我們引入了基於足部-地面接觸的動作精煉（MRFC）模組，通過彌合線性關節位置空間中的優化目標與非線性旋轉空間中的數據表示之間的差距，解決足部滑動問題。大量實驗表明，PAMD顯著提升了音樂對齊性並增強了生成動作的物理合理性。專案主頁：https://mucunzhuzhu.github.io/PAMD-page/。", "audio": "audios/2505.20056v1.mp3", "timestamp": "2025-05-27T07:19:18.435166"}
{"query": "AI", "id": "2505.20206v1", "url": "http://arxiv.org/abs/2505.20206v1", "title": "Evaluating Large Language Models for Code Review", "summary": "Context: Code reviews are crucial for software quality. Recent AI advances\nhave allowed large language models (LLMs) to review and fix code; now, there\nare tools that perform these reviews. However, their reliability and accuracy\nhave not yet been systematically evaluated. Objective: This study compares\ndifferent LLMs' performance in detecting code correctness and suggesting\nimprovements. Method: We tested GPT4o and Gemini 2.0 Flash on 492 AI generated\ncode blocks of varying correctness, along with 164 canonical code blocks from\nthe HumanEval benchmark. To simulate the code review task objectively, we\nexpected LLMs to assess code correctness and improve the code if needed. We ran\nexperiments with different configurations and reported on the results. Results:\nWith problem descriptions, GPT4o and Gemini 2.0 Flash correctly classified code\ncorrectness 68.50% and 63.89% of the time, respectively, and corrected the code\n67.83% and 54.26% of the time for the 492 code blocks of varying correctness.\nWithout problem descriptions, performance declined. The results for the 164\ncanonical code blocks differed, suggesting that performance depends on the type\nof code. Conclusion: LLM code reviews can help suggest improvements and assess\ncorrectness, but there is a risk of faulty outputs. We propose a process that\ninvolves humans, called the \"Human in the loop LLM Code Review\" to promote\nknowledge sharing while mitigating the risk of faulty outputs.", "authors": ["Umut Cihan", "Arda İçöz", "Vahid Haratian", "Eray Tüzün"], "published_date": "2025-05-26", "title_zh": "用於程式碼審查的大型語言模型評估", "summary_zh": "背景：程式碼審查對軟體品質至關重要。大型語言模型(LLM)在程式碼審查和修復方面有所進展，相關工具應運而生，但其可靠性和準確性尚未經系統評估。\n目標：本研究旨在比較不同LLM在檢測程式碼正確性及提出改進建議方面的效能。\n方法：針對492個不同正確性的AI生成程式碼片段和164個HumanEval基準程式碼片段，測試GPT4o和Gemini 2.0 Flash。要求LLM評估程式碼正確性並在必要時改進程式碼，以客觀模擬程式碼審查任務。\n結果：在提供問題描述的情況下，GPT4o和Gemini 2.0 Flash分別在68.50%和63.89%的時間內正確分類程式碼，並分別在67.83%和54.26%的時間內修正了492個程式碼片段。缺少問題描述時，效能下降。164個基準程式碼片段的結果不同，表明效能取決於程式碼類型。\n結論：LLM程式碼審查有助於提出改進建議並評估正確性，但也存在錯誤輸出的風險。我們提出一種涉及人工的流程，稱為「人機迴路LLM程式碼審查」，以促進知識共享並降低錯誤輸出的風險。", "audio": "audios/2505.20206v1.mp3", "timestamp": "2025-05-27T08:25:46.293150"}
{"query": "Foundation Model", "id": "2505.19851v1", "url": "http://arxiv.org/abs/2505.19851v1", "title": "Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages", "summary": "Transliteration, the process of mapping text from one script to another,\nplays a crucial role in multilingual natural language processing, especially\nwithin linguistically diverse contexts such as India. Despite significant\nadvancements through specialized models like IndicXlit, recent developments in\nlarge language models suggest a potential for general-purpose models to excel\nat this task without explicit task-specific training. The current work\nsystematically evaluates the performance of prominent LLMs, including GPT-4o,\nGPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a\nstate-of-the-art transliteration model, across ten major Indian languages.\nExperiments utilized standard benchmarks, including Dakshina and Aksharantar\ndatasets, with performance assessed via Top-1 Accuracy and Character Error\nRate. Our findings reveal that while GPT family models generally outperform\nother LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o\nimproves performance on specific languages notably. An extensive error analysis\nand robustness testing under noisy conditions further elucidate strengths of\nLLMs compared to specialized models, highlighting the efficacy of foundational\nmodels for a wide spectrum of specialized applications with minimal overhead.", "authors": ["Gulfarogh Azam", "Mohd Sadique", "Saif Ali", "Mohammad Nadeem", "Erik Cambria", "Shahab Saquib Sohail", "Mohammad Sultan Alam"], "published_date": "2025-05-26", "title_zh": "超越專精：印度語言音譯之大型語言模型基準測試", "summary_zh": "音譯是將文字從一種文字系統映射到另一種文字系統的過程，在多語言自然語言處理中至關重要，尤其是在印度等語言多樣化的環境中。儘管IndicXlit等專用模型取得了顯著進展，但大型語言模型（LLM）的最新發展表明，通用模型可能無需明確的任務特定訓練也能勝任此任務。本研究系統地評估了GPT-4o、GPT-4.5、GPT-4.1、Gemma-3-27B-it和Mistral-Large等主要LLM相對於最先進的音譯模型IndicXlit在十種主要印度語言上的性能。實驗採用了Dakshina和Aksharantar等標準基準數據集，並透過Top-1準確率和字元錯誤率評估性能。研究結果顯示，在大多數情況下，GPT系列模型普遍優於其他LLM和IndicXlit。此外，針對GPT-4o進行微調可顯著提高特定語言的性能。廣泛的錯誤分析和雜訊條件下的穩健性測試進一步闡明了LLM相較於專用模型的優勢，突顯了基礎模型在最小開銷下，能有效應用於廣泛的專業應用。", "audio": "audios/2505.19851v1.mp3", "timestamp": "2025-05-27T08:25:53.493689"}
{"query": "Diffusion Model", "id": "2505.20053v1", "url": "http://arxiv.org/abs/2505.20053v1", "title": "Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion", "summary": "Diffusion models have become the mainstream architecture for text-to-image\ngeneration, achieving remarkable progress in visual quality and prompt\ncontrollability. However, current inference pipelines generally lack\ninterpretable semantic supervision and correction mechanisms throughout the\ndenoising process. Most existing approaches rely solely on post-hoc scoring of\nthe final image, prompt filtering, or heuristic resampling strategies-making\nthem ineffective in providing actionable guidance for correcting the generative\ntrajectory. As a result, models often suffer from object confusion, spatial\nerrors, inaccurate counts, and missing semantic elements, severely compromising\nprompt-image alignment and image quality. To tackle these challenges, we\npropose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel\nframework that, for the first time, introduces a Multimodal Large Language\nModel (MLLM) as a semantic observer during inference. PPAD performs real-time\nanalysis on intermediate generations, identifies latent semantic\ninconsistencies, and translates feedback into controllable signals that\nactively guide the remaining denoising steps. The framework supports both\ninference-only and training-enhanced settings, and performs semantic correction\nat only extremely few diffusion steps, offering strong generality and\nscalability. Extensive experiments demonstrate PPAD's significant improvements.", "authors": ["Zheqi Lv", "Junhao Chen", "Qi Tian", "Keting Yin", "Shengyu Zhang", "Fei Wu"], "published_date": "2025-05-26", "title_zh": "多模態大型語言模型引導的文圖擴散語義校正", "summary_zh": "擴散模型已成文字生成圖像的主流架構，然目前推論流程普遍缺乏可解釋的語義監督與校正機制。現有方法多仰賴事後評分、提示過濾或啟發式重採樣，難以有效導正生成軌跡，導致物件混淆、空間錯誤、數量不準確及語義缺失等問題，損害提示圖像對齊及圖像品質。為解決此困境，我們提出MLLM語義校正乒乓前向擴散(PPAD)框架，首度引入多模態大型語言模型(MLLM)作為推論期間的語義觀察者。PPAD即時分析中間生成結果，識別潛在語義不一致，並將反饋轉譯為可控信號，主動引導後續降噪步驟。該框架支援純推論及訓練增強設定，僅於極少擴散步驟進行語義校正，具備良好泛用性及擴展性。實驗證明PPAD能顯著改善生成結果。", "audio": "audios/2505.20053v1.mp3", "timestamp": "2025-05-27T08:26:00.673368"}
{"query": "AI", "id": "2505.20136v1", "url": "http://arxiv.org/abs/2505.20136v1", "title": "Engineering Trustworthy Machine-Learning Operations with Zero-Knowledge Proofs", "summary": "As Artificial Intelligence (AI) systems, particularly those based on machine\nlearning (ML), become integral to high-stakes applications, their probabilistic\nand opaque nature poses significant challenges to traditional verification and\nvalidation methods. These challenges are exacerbated in regulated sectors\nrequiring tamper-proof, auditable evidence, as highlighted by apposite legal\nframeworks, e.g., the EU AI Act. Conversely, Zero-Knowledge Proofs (ZKPs) offer\na cryptographic solution that enables provers to demonstrate, through verified\ncomputations, adherence to set requirements without revealing sensitive model\ndetails or data. Through a systematic survey of ZKP protocols, we identify five\nkey properties (non-interactivity, transparent setup, standard representations,\nsuccinctness, and post-quantum security) critical for their application in AI\nvalidation and verification pipelines. Subsequently, we perform a follow-up\nsystematic survey analyzing ZKP-enhanced ML applications across an adaptation\nof the Team Data Science Process (TDSP) model (Data & Preprocessing, Training &\nOffline Metrics, Inference, and Online Metrics), detailing verification\nobjectives, ML models, and adopted protocols. Our findings indicate that\ncurrent research on ZKP-Enhanced ML primarily focuses on inference\nverification, while the data preprocessing and training stages remain\nunderexplored. Most notably, our analysis identifies a significant convergence\nwithin the research domain toward the development of a unified Zero-Knowledge\nMachine Learning Operations (ZKMLOps) framework. This emerging framework\nleverages ZKPs to provide robust cryptographic guarantees of correctness,\nintegrity, and privacy, thereby promoting enhanced accountability,\ntransparency, and compliance with Trustworthy AI principles.", "authors": ["Filippo Scaramuzza", "Giovanni Quattrocchi", "Damian A. Tamburri"], "published_date": "2025-05-26", "title_zh": "利用零知識證明構建可信賴的機器學習運營", "summary_zh": "隨著人工智慧系統，特別是基於機器學習的系統，在高風險應用中日益重要，其機率性和不透明性對傳統驗證方法構成重大挑戰。在需要防篡改、可稽核證據的監管領域，這些挑戰更為嚴峻，例如歐盟人工智慧法案。零知識證明提供了一種加密解決方案，使證明者能夠透過驗證計算來證明符合既定要求，而無需洩露敏感的模型細節或資料。透過對零知識證明協議的系統性調查，我們確定了五個關鍵特性（非互動性、透明設置、標準表示、簡潔性和後量子安全性），這些特性對於其在人工智慧驗證管線中的應用至關重要。隨後，我們進行了一項後續的系統性調查，分析了零知識證明強化的機器學習應用，採用了團隊資料科學流程模型的改編版本（資料與預處理、訓練與離線指標、推論和線上指標），詳細說明了驗證目標、機器學習模型和採用的協議。研究結果表明，目前零知識證明強化的機器學習研究主要集中在推論驗證上，而資料預處理和訓練階段仍未得到充分探索。最值得注意的是，我們的分析發現研究領域內正在向統一的零知識機器學習運營框架發展。這個新興框架利用零知識證明來提供強大的正確性、完整性和隱私加密保證，從而促進增強的問責制、透明度以及對可信賴人工智慧原則的遵守。", "audio": "audios/2505.20136v1.mp3", "timestamp": "2025-05-27T09:19:42.431869"}
{"query": "Foundation Model", "id": "2505.19825v1", "url": "http://arxiv.org/abs/2505.19825v1", "title": "Foundation Models for Tabular Data within Systemic Contexts Need Grounding", "summary": "Current research on tabular foundation models often overlooks the\ncomplexities of large-scale, real-world data by treating tables as isolated\nentities and assuming information completeness, thereby neglecting the vital\noperational context. To address this, we introduce the concept of Semantically\nLinked Tables (SLT), recognizing that tables are inherently connected to both\ndeclarative and procedural operational knowledge. We propose Foundation Models\nfor Semantically Linked Tables (FMSLT), which integrate these components to\nground tabular data within its true operational context. This comprehensive\nrepresentation unlocks the full potential of machine learning for complex,\ninterconnected tabular data across diverse domains. Realizing FMSLTs requires\naccess to operational knowledge that is often unavailable in public datasets,\nhighlighting the need for close collaboration between domain experts and\nresearchers. Our work exposes the limitations of current tabular foundation\nmodels and proposes a new direction centered on FMSLTs, aiming to advance\nrobust, context-aware models for structured data.", "authors": ["Tassilo Klein", "Johannes Hoffart"], "published_date": "2025-05-26", "title_zh": "系統性脈絡下表格式資料之基礎模型需具備基礎性", "summary_zh": "現有表格基礎模型研究常忽略大規模真實數據的複雜性，將表格視為孤立個體並假設信息完整，從而忽略關鍵的操作背景。為此，我們引入語義鏈接表格（SLT）的概念，認識到表格與聲明式和程序式操作知識固有聯繫。我們提出用於語義鏈接表格的基礎模型（FMSLT），整合這些組件，將表格數據紮根於其真實操作背景中。此全面表示釋放機器學習在複雜、互連表格數據方面的潛力。實現FMSLT需要獲取公共數據集中通常無法獲得的操作知識，突顯領域專家與研究人員密切合作之必要。本研究揭示現有表格基礎模型的局限性，並提出以SLT為中心的新方向，旨在推進結構化數據之穩健、具上下文感知模型。", "audio": "audios/2505.19825v1.mp3", "timestamp": "2025-05-27T09:19:48.132133"}
{"query": "Diffusion Model", "id": "2505.19983v1", "url": "http://arxiv.org/abs/2505.19983v1", "title": "ICDM: Interference Cancellation Diffusion Models for Wireless Semantic Communications", "summary": "Diffusion models (DMs) have recently achieved significant success in wireless\ncommunications systems due to their denoising capabilities. The broadcast\nnature of wireless signals makes them susceptible not only to Gaussian noise,\nbut also to unaware interference. This raises the question of whether DMs can\neffectively mitigate interference in wireless semantic communication systems.\nIn this paper, we model the interference cancellation problem as a maximum a\nposteriori (MAP) problem over the joint posterior probability of the signal and\ninterference, and theoretically prove that the solution provides excellent\nestimates for the signal and interference. To solve this problem, we develop an\ninterference cancellation diffusion model (ICDM), which decomposes the joint\nposterior into independent prior probabilities of the signal and interference,\nalong with the channel transition probablity. The log-gradients of these\ndistributions at each time step are learned separately by DMs and accurately\nestimated through deriving. ICDM further integrates these gradients with\nadvanced numerical iteration method, achieving accurate and rapid interference\ncancellation. Extensive experiments demonstrate that ICDM significantly reduces\nthe mean square error (MSE) and enhances perceptual quality compared to schemes\nwithout ICDM. For example, on the CelebA dataset under the Rayleigh fading\nchannel with a signal-to-noise ratio (SNR) of $20$ dB and signal to\ninterference plus noise ratio (SINR) of 0 dB, ICDM reduces the MSE by 4.54 dB\nand improves the learned perceptual image patch similarity (LPIPS) by 2.47 dB.", "authors": ["Tong Wu", "Zhiyong Chen", "Dazhi He", "Feng Yang", "Meixia Tao", "Xiaodong Xu", "Wenjun Zhang", "Ping Zhang"], "published_date": "2025-05-26", "title_zh": "用於無線語義通信的干擾消除擴散模型", "summary_zh": "擴散模型因其降噪能力，近期在無線通訊系統中獲得顯著成功。無線訊號的廣播特性使其易受高斯雜訊及未知干擾影響。本文探討擴散模型能否有效緩解無線語義通訊系統中的干擾。我們將干擾消除問題建模為訊號與干擾聯合後驗機率的最大後驗(MAP)問題，並從理論上證明該解能提供優良的訊號與干擾估計。為了解決此問題，我們開發了一種干擾消除擴散模型(ICDM)，其將聯合後驗分解為訊號和干擾的獨立先驗機率，以及通道轉移機率。這些分佈的對數梯度在每個時間步由擴散模型單獨學習，並透過推導精確估計。ICDM進一步將這些梯度與先進的數值迭代方法整合，實現準確快速的干擾消除。大量實驗表明，相較於未使用ICDM的方案，ICDM顯著降低了均方誤差(MSE)並提高了感知品質。例如，在Rayleigh衰落通道下，訊雜比(SNR)為20 dB，訊號干擾加雜訊比(SINR)為0 dB時，ICDM在CelebA數據集上將MSE降低了4.54 dB，並將學習到的感知圖像塊相似度(LPIPS)提高了2.47 dB。", "audio": "audios/2505.19983v1.mp3", "timestamp": "2025-05-27T09:19:57.159004"}
{"query": "AI", "id": "2505.20129v1", "url": "http://arxiv.org/abs/2505.20129v1", "title": "Agentic 3D Scene Generation with Spatially Contextualized VLMs", "summary": "Despite recent advances in multimodal content generation enabled by\nvision-language models (VLMs), their ability to reason about and generate\nstructured 3D scenes remains largely underexplored. This limitation constrains\ntheir utility in spatially grounded tasks such as embodied AI, immersive\nsimulations, and interactive 3D applications. We introduce a new paradigm that\nenables VLMs to generate, understand, and edit complex 3D environments by\ninjecting a continually evolving spatial context. Constructed from multimodal\ninput, this context consists of three components: a scene portrait that\nprovides a high-level semantic blueprint, a semantically labeled point cloud\ncapturing object-level geometry, and a scene hypergraph that encodes rich\nspatial relationships, including unary, binary, and higher-order constraints.\nTogether, these components provide the VLM with a structured, geometry-aware\nworking memory that integrates its inherent multimodal reasoning capabilities\nwith structured 3D understanding for effective spatial reasoning. Building on\nthis foundation, we develop an agentic 3D scene generation pipeline in which\nthe VLM iteratively reads from and updates the spatial context. The pipeline\nfeatures high-quality asset generation with geometric restoration, environment\nsetup with automatic verification, and ergonomic adjustment guided by the scene\nhypergraph. Experiments show that our framework can handle diverse and\nchallenging inputs, achieving a level of generalization not observed in prior\nwork. Further results demonstrate that injecting spatial context enables VLMs\nto perform downstream tasks such as interactive scene editing and path\nplanning, suggesting strong potential for spatially intelligent systems in\ncomputer graphics, 3D vision, and embodied applications.", "authors": ["Xinhang Liu", "Yu-Wing Tai", "Chi-Keung Tang"], "published_date": "2025-05-26", "title_zh": "具空間脈絡視覺語言模型的主動式三維場景生成", "summary_zh": "儘管視覺語言模型在多模態內容生成方面取得進展，但其推理和生成結構化3D場景的能力仍待開發，限制了其在空間定位任務中的應用。我們提出一種新範式，透過注入持續演進的空間背景，使視覺語言模型能夠生成、理解和編輯複雜的3D環境。此背景由多模態輸入構建，包含場景藍圖、語義標籤點雲和編碼空間關係的場景超圖。這些組件為視覺語言模型提供結構化、幾何感知的記憶體，整合其多模態推理能力與3D理解能力，以實現有效的空間推理。在此基礎上，我們開發了一種主動式3D場景生成流程，視覺語言模型迭代讀取並更新空間背景，實現高品質資產生成、環境自動驗證設定以及場景超圖引導的人體工學調整。實驗表明，該框架能處理多樣且具挑戰性的輸入，達到前所未有的泛化程度。結果亦顯示，注入空間背景能使視覺語言模型執行互動式場景編輯和路徑規劃等下游任務，展現其在電腦圖學、3D視覺和具體應用中空間智慧系統的潛力。", "audio": "audios/2505.20129v1.mp3", "timestamp": "2025-05-27T10:20:58.252116"}
{"query": "Foundation Model", "id": "2505.19779v1", "url": "http://arxiv.org/abs/2505.19779v1", "title": "Advancements in Medical Image Classification through Fine-Tuning Natural Domain Foundation Models", "summary": "Using massive datasets, foundation models are large-scale, pre-trained models\nthat perform a wide range of tasks. These models have shown consistently\nimproved results with the introduction of new methods. It is crucial to analyze\nhow these trends impact the medical field and determine whether these\nadvancements can drive meaningful change. This study investigates the\napplication of recent state-of-the-art foundation models, DINOv2, MAE, VMamba,\nCoCa, SAM2, and AIMv2, for medical image classification. We explore their\neffectiveness on datasets including CBIS-DDSM for mammography, ISIC2019 for\nskin lesions, APTOS2019 for diabetic retinopathy, and CHEXPERT for chest\nradiographs. By fine-tuning these models and evaluating their configurations,\nwe aim to understand the potential of these advancements in medical image\nclassification. The results indicate that these advanced models significantly\nenhance classification outcomes, demonstrating robust performance despite\nlimited labeled data. Based on our results, AIMv2, DINOv2, and SAM2 models\noutperformed others, demonstrating that progress in natural domain training has\npositively impacted the medical domain and improved classification outcomes.\nOur code is publicly available at:\nhttps://github.com/sajjad-sh33/Medical-Transfer-Learning.", "authors": ["Mobina Mansoori", "Sajjad Shahabodini", "Farnoush Bayatmakou", "Jamshid Abouei", "Konstantinos N. Plataniotis", "Arash Mohammadi"], "published_date": "2025-05-26", "title_zh": "透過微調自然領域基礎模型提升醫學影像分類效能", "summary_zh": "基於海量資料集的大型預訓練基礎模型能執行多種任務，且新方法不斷提升其效能。本研究探討最先進基礎模型DINOv2、MAE、VMamba、CoCa、SAM2與AIMv2在醫學影像分類上的應用，評估其於乳房攝影CBIS-DDSM、皮膚病灶ISIC2019、糖尿病視網膜病變APTOS2019及胸腔X光CHEXPERT等資料集的效能。透過微調與配置評估，旨在了解這些進展在醫學影像分類中的潛力。結果顯示，這些模型顯著改善分類結果，即使在標記資料有限的情況下也能展現穩健效能。AIMv2、DINOv2與SAM2表現優於其他模型，表明自然領域訓練的進步已對醫學領域產生正面影響，並提升了分類結果。相關程式碼已公開。", "audio": "audios/2505.19779v1.mp3", "timestamp": "2025-05-27T10:21:04.796069"}
{"query": "Diffusion Model", "id": "2505.19958v1", "url": "http://arxiv.org/abs/2505.19958v1", "title": "UltraVSR: Achieving Ultra-Realistic Video Super-Resolution with Efficient One-Step Diffusion Space", "summary": "Diffusion models have shown great potential in generating realistic image\ndetail. However, adapting these models to video super-resolution (VSR) remains\nchallenging due to their inherent stochasticity and lack of temporal modeling.\nIn this paper, we propose UltraVSR, a novel framework that enables\nultra-realistic and temporal-coherent VSR through an efficient one-step\ndiffusion space. A central component of UltraVSR is the Degradation-aware\nRestoration Schedule (DRS), which estimates a degradation factor from the\nlow-resolution input and transforms iterative denoising process into a\nsingle-step reconstruction from from low-resolution to high-resolution videos.\nThis design eliminates randomness from diffusion noise and significantly speeds\nup inference. To ensure temporal consistency, we propose a lightweight yet\neffective Recurrent Temporal Shift (RTS) module, composed of an RTS-convolution\nunit and an RTS-attention unit. By partially shifting feature components along\nthe temporal dimension, these two units collaboratively facilitate effective\nfeature propagation, fusion, and alignment across neighboring frames, without\nrelying on explicit temporal layers. The RTS module is integrated into a\npretrained text-to-image diffusion model and is further enhanced through\nSpatio-temporal Joint Distillation (SJD), which improves temporal coherence\nwhile preserving realistic details. Additionally, we introduce a Temporally\nAsynchronous Inference (TAI) strategy to capture long-range temporal\ndependencies under limited memory constraints. Extensive experiments show that\nUltraVSR achieves state-of-the-art performance, both qualitatively and\nquantitatively, in a single sampling step.", "authors": ["Yong Liu", "Jinshan Pan", "Yinchuan Li", "Qingji Dong", "Chao Zhu", "Yu Guo", "Fei Wang"], "published_date": "2025-05-26", "title_zh": "UltraVSR：藉由高效單步擴散空間實現超逼真影片超解析度", "summary_zh": "擴散模型在生成逼真圖像細節方面展現巨大潛力，但由於其內在隨機性和缺乏時間建模，將其應用於影片超解析度（VSR）仍具挑戰。本文提出 UltraVSR，一種新型框架，透過高效的一步擴散空間實現超逼真且時間一致的 VSR。UltraVSR 的核心組件是退化感知恢復排程（DRS），它估計低解析度輸入的退化因子，並將迭代去噪過程轉變為從低解析度到高解析度影片的單步重建，消除了擴散雜訊的隨機性並顯著加速了推論。為確保時間一致性，我們提出輕量級但高效的循環時間位移（RTS）模組，由 RTS 卷積單元和 RTS 注意力單元組成。透過沿時間維度部分移動特徵分量，這兩個單元協同促進相鄰幀之間的有效特徵傳播、融合和對齊，而無需依賴顯式時間層。RTS 模組被整合到預訓練的文本到圖像擴散模型中，並透過時空聯合蒸餾（SJD）進一步增強，從而在保持逼真細節的同時提高時間一致性。此外，我們引入時間非同步推論（TAI）策略，以在有限的記憶體約束下捕獲遠程時間依賴性。大量實驗表明，UltraVSR 在單次採樣步驟中實現了最先進的效能，無論是在質量上還是數量上。", "audio": "audios/2505.19958v1.mp3", "timestamp": "2025-05-27T10:21:13.370524"}
{"query": "AI", "id": "2505.20127v1", "url": "http://arxiv.org/abs/2505.20127v1", "title": "Agentic AI Process Observability: Discovering Behavioral Variability", "summary": "AI agents that leverage Large Language Models (LLMs) are increasingly\nbecoming core building blocks of modern software systems. A wide range of\nframeworks is now available to support the specification of such applications.\nThese frameworks enable the definition of agent setups using natural language\nprompting, which specifies the roles, goals, and tools assigned to the various\nagents involved. Within such setups, agent behavior is non-deterministic for\nany given input, highlighting the critical need for robust debugging and\nobservability tools. In this work, we explore the use of process and causal\ndiscovery applied to agent execution trajectories as a means of enhancing\ndeveloper observability. This approach aids in monitoring and understanding the\nemergent variability in agent behavior. Additionally, we complement this with\nLLM-based static analysis techniques to distinguish between intended and\nunintended behavioral variability. We argue that such instrumentation is\nessential for giving developers greater control over evolving specifications\nand for identifying aspects of functionality that may require more precise and\nexplicit definitions.", "authors": ["Fabiana Fournier", "Lior Limonad", "Yuval David"], "published_date": "2025-05-26", "title_zh": "自主型人工智慧流程可觀測性：探索行為變異性", "summary_zh": "利用大型語言模型的人工智慧代理程式正迅速成為現代軟體系統的核心組件。目前有多種框架支援此類應用程式的規範。這些框架允許使用自然語言提示定義代理程式設定，指定角色、目標和分配給各代理程式的工具。在這些設定中，代理程式行為對於任何給定輸入都具有不確定性，突顯了對穩健偵錯和可觀察性工具的迫切需求。本研究探索將流程和因果發現應用於代理程式執行軌跡，以增強開發人員的可觀察性，協助監控和理解代理程式行為的突發變異。此外，我們輔以基於大型語言模型的靜態分析技術，以區分預期和非預期的行為變異。我們認為這種儀器化對於使開發人員能夠更好地控制不斷演進的規範，並識別可能需要更精確和明確定義的功能方面至關重要。", "audio": "audios/2505.20127v1.mp3", "timestamp": "2025-05-27T11:16:27.003734"}
{"query": "Foundation Model", "id": "2505.19625v1", "url": "http://arxiv.org/abs/2505.19625v1", "title": "Search-Based Software Engineering in the Landscape of AI Foundation Models", "summary": "Search-based software engineering (SBSE), at the intersection of artificial\nintelligence (AI) and software engineering, has been an active area of research\nfor about 25 years. It has been applied to solve numerous problems across the\nentire software engineering lifecycle and has demonstrated its versatility in\nmultiple domains. With the recent advancements in AI, particularly the\nemergence of foundation models (FMs), the evolution of SBSE alongside FMs\nremains undetermined. In this window of opportunity, we propose a research\nroadmap that articulates the current landscape of SBSE in relation to\nfoundation models (FMs), highlights open challenges, and outlines potential\nresearch directions for advancing SBSE through its interplay with FMs. This\nroadmap aims to establish a forward-thinking and innovative perspective for the\nfuture of SBSE in the era of FMs.", "authors": ["Hassan Sartaj", "Shaukat Ali"], "published_date": "2025-05-26", "title_zh": "基於搜尋的軟體工程於人工智慧基礎模型之脈絡下", "summary_zh": "基於搜尋的軟體工程（SBSE）結合人工智慧（AI）與軟體工程，已活躍研究約25年，廣泛應用於軟體工程生命週期各階段。隨著AI特別是基礎模型（FMs）的發展，SBSE與FMs的協同演進尚不明確。本文提出研究藍圖，闡述SBSE與FMs的現況，點明未解難題，並概述透過FMs推進SBSE的潛在研究方向，旨在為FMs時代的SBSE建立具前瞻性的創新視野。", "audio": "audios/2505.19625v1.mp3", "timestamp": "2025-05-27T11:16:31.140183"}
{"query": "Diffusion Model", "id": "2505.19868v1", "url": "http://arxiv.org/abs/2505.19868v1", "title": "Harnessing the Power of Training-Free Techniques in Text-to-2D Generation for Text-to-3D Generation via Score Distillation Sampling", "summary": "Recent studies show that simple training-free techniques can dramatically\nimprove the quality of text-to-2D generation outputs, e.g. Classifier-Free\nGuidance (CFG) or FreeU. However, these training-free techniques have been\nunderexplored in the lens of Score Distillation Sampling (SDS), which is a\npopular and effective technique to leverage the power of pretrained text-to-2D\ndiffusion models for various tasks. In this paper, we aim to shed light on the\neffect such training-free techniques have on SDS, via a particular application\nof text-to-3D generation via 2D lifting. We present our findings, which show\nthat varying the scales of CFG presents a trade-off between object size and\nsurface smoothness, while varying the scales of FreeU presents a trade-off\nbetween texture details and geometric errors. Based on these findings, we\nprovide insights into how we can effectively harness training-free techniques\nfor SDS, via a strategic scaling of such techniques in a dynamic manner with\nrespect to the timestep or optimization iteration step. We show that using our\nproposed scheme strikes a favorable balance between texture details and surface\nsmoothness in text-to-3D generations, while preserving the size of the output\nand mitigating the occurrence of geometric defects.", "authors": ["Junhong Lee", "Seungwook Kim", "Minsu Cho"], "published_date": "2025-05-26", "title_zh": "藉由分數蒸餾取樣在文本到三維生成中利用免訓練技術於文本到二維生成的力量", "summary_zh": "近期研究表明，無需訓練的簡便技巧能顯著提升文生圖品質，如無分類器引導（CFG）或FreeU。然這些技巧在分數蒸餾抽樣（SDS）中尚未被充分探索，SDS為利用預訓練文生圖擴散模型之熱門有效方法。本文旨在探討此類技巧對SDS之影響，特別是在透過2D提升實現文生3D生成之應用中。研究顯示，調整CFG尺度會在物件大小與表面平滑度間產生權衡，調整FreeU尺度則在紋理細節與幾何誤差間產生權衡。基於此，本文深入探討如何有效利用無需訓練之技巧於SDS中，透過動態調整尺度，並提出策略性縮放方案，能在文生3D生成中，於紋理細節與表面平滑度間取得良好平衡，同時維持輸出大小並減少幾何缺陷。", "audio": "audios/2505.19868v1.mp3", "timestamp": "2025-05-27T11:16:37.932775"}
{"query": "AI", "id": "2505.20181v1", "url": "http://arxiv.org/abs/2505.20181v1", "title": "The Problem of Algorithmic Collisions: Mitigating Unforeseen Risks in a Connected World", "summary": "The increasing deployment of Artificial Intelligence (AI) and other\nautonomous algorithmic systems presents the world with new systemic risks.\nWhile focus often lies on the function of individual algorithms, a critical and\nunderestimated danger arises from their interactions, particularly when\nalgorithmic systems operate without awareness of each other, or when those\ndeploying them are unaware of the full algorithmic ecosystem deployment is\noccurring in. These interactions can lead to unforeseen, rapidly escalating\nnegative outcomes - from market crashes and energy supply disruptions to\npotential physical accidents and erosion of public trust - often exceeding the\nhuman capacity for effective monitoring and the legal capacities for proper\nintervention. Current governance frameworks are inadequate as they lack\nvisibility into this complex ecosystem of interactions. This paper outlines the\nnature of this challenge and proposes some initial policy suggestions centered\non increasing transparency and accountability through phased system\nregistration, a licensing framework for deployment, and enhanced monitoring\ncapabilities.", "authors": ["Maurice Chiodo", "Dennis Müller"], "published_date": "2025-05-26", "title_zh": "演算法碰撞問題：減緩互聯世界中未預見的風險", "summary_zh": "人工智慧與自主演算法系統的廣泛應用帶來新的系統性風險。相較於個別演算法功能，演算法間的互動構成更嚴峻的潛在威脅，尤其在系統缺乏互知或部署者未充分掌握演算法生態系的情況下，可能導致難以預測且迅速升級的負面後果，如市場崩盤、能源供應中斷、物理事故及公眾信任瓦解，超出人類監測能力與法律干預範圍。現行治理框架缺乏對此複雜互動生態系的能見度，故本文闡述此挑戰，並提出初步政策建議，聚焦於透過分階段系統註冊、部署許可框架及強化監測能力，提升透明度與問責性。", "audio": "audios/2505.20181v1.mp3", "timestamp": "2025-05-27T13:31:42.869913"}
{"query": "Foundation Model", "id": "2505.19606v1", "url": "http://arxiv.org/abs/2505.19606v1", "title": "Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically", "summary": "Cross-lingual alignment in pretrained language models (LMs) has enabled\nefficient transfer in text-based LMs. Such an alignment has also been observed\nin speech foundation models. However, it remains an open question whether\nfindings and methods from text-based cross-lingual alignment apply to speech.\nBuilding on prior work on spoken translation retrieval, we perform\npronunciation-controlled experiments to observe if cross-lingual alignment can\nindeed occur in such models on a semantic basis, instead of relying on phonetic\nsimilarities. Our findings indicate that even in the absence of phonetic cues,\nspoken translation retrieval accuracy remains relatively stable. We follow up\nwith a controlled experiment on a word-level dataset of cross-lingual synonyms\nand near-homophones, confirming the existence of both phonetic and semantic\nknowledge in the encoder. Finally, we qualitatively examine the transcriptions\nproduced by early exiting the encoder, where we observe that speech translation\nproduces semantic errors that are characterized by phonetic similarities to\ncorresponding words in the source language. We apply this insight from early\nexiting to speech recognition in seven low-resource languages unsupported by\nthe Whisper model, and achieve improved accuracy in all languages examined,\nparticularly for languages with transparent orthographies.", "authors": ["Ryan Soh-Eun Shim", "Domenico De Cristofaro", "Chengzhi Martin Hu", "Alessandro Vietti", "Barbara Plank"], "published_date": "2025-05-26", "title_zh": "多語語音基礎模型中，語言在音韻和語義層面的對齊", "summary_zh": "預訓練語言模型中的跨語言對齊已促進文本模型的有效遷移。語音基礎模型中也觀察到類似對齊現象，但文本相關研究成果是否適用於語音領域仍待驗證。本研究基於口語翻譯檢索，透過發音控制實驗，探討跨語言對齊是否能在語義層面發生，而非僅依賴語音相似性。研究發現，即使缺乏語音線索，口語翻譯檢索準確率仍相對穩定。進一步針對跨語言同義詞和近音同形異義詞進行詞級實驗，證實編碼器同時具備語音和語義知識。分析編碼器早期輸出的轉錄文本，發現語音翻譯產生語義錯誤，且錯誤與源語言中的發音相似詞相關。運用此發現，改善了Whisper模型未支援的七種低資源語言的語音辨識，特別是對於拼寫透明度高的語言，準確率顯著提升。", "audio": "audios/2505.19606v1.mp3", "timestamp": "2025-05-27T13:31:49.781253"}
{"query": "Diffusion Model", "id": "2505.19835v1", "url": "http://arxiv.org/abs/2505.19835v1", "title": "On a retarded stochastic system with discrete diffusion modeling life tables", "summary": "This work proposes a method for modeling and forecasting mortality rates. It\nconstitutes an improvement over previous studies by incorporating both the\nhistorical evolution of the mortality phenomenon and its random behavior. In\nthe first part, we introduce the model and analyze mathematical properties such\nas the existence of solutions and their asymptotic behavior. In the second\npart, we apply this model to forecast mortality rates in Spain, showing that it\nyields better results than classical methods.", "authors": ["Tomás Caraballo", "Francisco Morillas", "José Valero"], "published_date": "2025-05-26", "title_zh": "具離散擴散的滯後隨機系統在生命表建模中的應用", "summary_zh": "本研究提出一種死亡率建模與預測方法，透過整合歷史演變趨勢與隨機變異，優化既有研究。第一部分闡述模型並分析其數學特性，如解的存在性及漸近行為。第二部分將此模型應用於西班牙死亡率預測，結果顯示其效能優於傳統方法。", "audio": "audios/2505.19835v1.mp3", "timestamp": "2025-05-27T13:31:53.088075"}
{"query": "AI", "id": "2505.20120v1", "url": "http://arxiv.org/abs/2505.20120v1", "title": "Agents Require Metacognitive and Strategic Reasoning to Succeed in the Coming Labor Markets", "summary": "Current labor markets are strongly affected by the economic forces of adverse\nselection, moral hazard, and reputation, each of which arises due to\n$\\textit{incomplete information}$. These economic forces will still be\ninfluential after AI agents are introduced, and thus, agents must use\nmetacognitive and strategic reasoning to perform effectively. Metacognition is\na form of $\\textit{internal reasoning}$ that includes the capabilities for\nself-assessment, task understanding, and evaluation of strategies. Strategic\nreasoning is $\\textit{external reasoning}$ that covers holding beliefs about\nother participants in the labor market (e.g., competitors, colleagues), making\nstrategic decisions, and learning about others over time. Both types of\nreasoning are required by agents as they decide among the many\n$\\textit{actions}$ they can take in labor markets, both within and outside\ntheir jobs. We discuss current research into metacognitive and strategic\nreasoning and the areas requiring further development.", "authors": ["Simpson Zhang", "Tennison Liu", "Mihaela van der Schaar"], "published_date": "2025-05-26", "title_zh": "代理人需具備後設認知與策略性推理能力方能在未來勞動市場中取得成功", "summary_zh": "當前勞動市場深受逆向選擇、道德風險和聲譽等不完全資訊經濟力量影響。即使引入人工智慧代理人後，這些力量依然存在，因此代理人必須運用後設認知和策略性推理才能有效工作。後設認知是一種內部推理，包含自我評估、任務理解和策略評估能力。策略性推理是一種外部推理，涵蓋對勞動市場參與者的信念（如競爭者、同事）、制定策略決策和隨時間推移了解他人。代理人在決定勞動市場內外可採取的多種行動時，需要這兩種推理。本文探討後設認知和策略性推理的現有研究，並指出需要進一步發展的領域。", "audio": "audios/2505.20120v1.mp3", "timestamp": "2025-05-27T14:18:23.991105"}
{"query": "Foundation Model", "id": "2505.19502v1", "url": "http://arxiv.org/abs/2505.19502v1", "title": "CODE-DITING: A Reasoning-Based Metric for Functional Alignment in Code Evaluation", "summary": "Trustworthy evaluation methods for code snippets play a crucial role in\nneural code generation. Traditional methods, which either rely on reference\nsolutions or require executable test cases, have inherent limitation in\nflexibility and scalability. The recent LLM-as-Judge methodology offers a\npromising alternative by directly evaluating functional consistency between the\nproblem description and the generated code. To systematically understand the\nlandscape of these LLM-as-Judge methods, we conduct a comprehensive empirical\nstudy across three diverse datasets. Our investigation reveals the pros and\ncons of two categories of LLM-as-Judge methods: the methods based on general\nfoundation models can achieve good performance but require complex prompts and\nlack explainability, while the methods based on reasoning foundation models\nprovide better explainability with simpler prompts but demand substantial\ncomputational resources due to their large parameter sizes. To address these\nlimitations, we propose CODE-DITING, a novel code evaluation method that\nbalances accuracy, efficiency and explainability. We develop a data\ndistillation framework that effectively transfers reasoning capabilities from\nDeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing\nevaluation explainability and reducing the computational cost. With the\nmajority vote strategy in the inference process, CODE-DITING 1.5B outperforms\nall models with the same magnitude of parameters and achieves performance which\nwould normally exhibit in a model with 5 times of parameter scale. CODE-DITING\n7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the\nparameter volume of these large models. Further experiments show that\nCODEDITING is robust to preference leakage and can serve as a promising\nalternative for code evaluation.", "authors": ["Guang Yang", "Yu Zhou", "Xiang Chen", "Wei Zheng", "Xing Hu", "Xin Zhou", "David Lo", "Taolue Chen"], "published_date": "2025-05-26", "title_zh": "程式碼校訂：一種基於推理的程式碼評估中功能對齊度量", "summary_zh": "程式碼片段的可信評估方法在神經程式碼生成中至關重要。傳統方法依賴參考解答或可執行測試案例，存在彈性和可擴展性的固有局限。新興的大型語言模型即評審方法透過直接評估問題描述與生成程式碼之間的功能一致性，提供了一種有前景的替代方案。為系統性地理解此類方法，我們針對三個不同資料集進行了全面的實證研究。研究揭示了兩類大型語言模型即評審方法的優缺點：基於通用基礎模型的方法能達到良好效能，但需要複雜的提示且缺乏可解釋性；基於推理基礎模型的方法提供較佳的可解釋性，使用較簡單的提示，但因其龐大的參數規模而需要大量的計算資源。為了解決這些限制，我們提出了 CODE-DITING，一種平衡準確性、效率和可解釋性的新型程式碼評估方法。我們開發了一個資料蒸餾框架，能有效地將 DeepSeek-R1671B 的推理能力轉移到我們的 CODE-DITING 1.5B 和 7B 模型，顯著提升了評估可解釋性並降低了計算成本。在推理過程中，CODE-DITING 1.5B 透過多數投票策略超越了所有相同參數規模的模型，並達到通常為五倍參數規模的模型才能展現的效能。CODE-DITING 7B 超越了 GPT-4o 和 DeepSeek-V3 671B，即便它僅使用了這些大型模型 1% 的參數量。進一步的實驗表明，CODEDITING 對偏好洩漏具有穩健性，可作為程式碼評估的一種有前景的替代方案。", "audio": "audios/2505.19502v1.mp3", "timestamp": "2025-05-27T14:18:34.080936"}
{"query": "Diffusion Model", "id": "2505.19769v1", "url": "http://arxiv.org/abs/2505.19769v1", "title": "TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning", "summary": "Developing scalable and generalizable reward engineering for reinforcement\nlearning (RL) is crucial for creating general-purpose agents, especially in the\nchallenging domain of robotic manipulation. While recent advances in reward\nengineering with Vision-Language Models (VLMs) have shown promise, their sparse\nreward nature significantly limits sample efficiency. This paper introduces\nTeViR, a novel method that leverages a pre-trained text-to-video diffusion\nmodel to generate dense rewards by comparing the predicted image sequence with\ncurrent observations. Experimental results across 11 complex robotic tasks\ndemonstrate that TeViR outperforms traditional methods leveraging sparse\nrewards and other state-of-the-art (SOTA) methods, achieving better sample\nefficiency and performance without ground truth environmental rewards. TeViR's\nability to efficiently guide agents in complex environments highlights its\npotential to advance reinforcement learning applications in robotic\nmanipulation.", "authors": ["Yuhui Chen", "Haoran Li", "Zhennan Jiang", "Haowei Wen", "Dongbin Zhao"], "published_date": "2025-05-26", "title_zh": "TeViR：藉由擴散模型實現文本到影片獎勵以提升強化學習效率", "summary_zh": "為使強化學習具備可擴展性和通用性，獎勵工程至關重要，尤其在機器人操作領域。儘管視覺語言模型在獎勵工程方面有所進展，但其稀疏獎勵特性限制了樣本效率。本研究提出TeViR，一種利用預訓練文字轉影片擴散模型生成密集獎勵的新方法，透過比較預測影像序列與當前觀測值來實現。在11項複雜機器人任務上的實驗結果表明，TeViR優於傳統的稀疏獎勵方法和其他最先進技術，在沒有真實環境獎勵的情況下，實現了更高的樣本效率和性能。TeViR有效引導智能體在複雜環境中操作的能力，突顯了其在推進機器人操作強化學習應用方面的潛力。", "audio": "audios/2505.19769v1.mp3", "timestamp": "2025-05-27T14:18:39.647470"}
{"query": "AI", "id": "2505.20158v1", "url": "http://arxiv.org/abs/2505.20158v1", "title": "Evaluating Software Plagiarism Detection in the Age of AI: Automated Obfuscation and Lessons for Academic Integrity", "summary": "Plagiarism in programming assignments is a persistent issue in computer\nscience education, increasingly complicated by the emergence of automated\nobfuscation attacks. While software plagiarism detectors are widely used to\nidentify suspicious similarities at scale and are resilient to simple\nobfuscation techniques, they are vulnerable to advanced obfuscation based on\nstructural modification of program code that preserves the original program\nbehavior. While different defense mechanisms have been proposed to increase\nresilience against these attacks, their current evaluation is limited to the\nscope of attacks used and lacks a comprehensive investigation regarding\nAI-based obfuscation. In this paper, we investigate the resilience of these\ndefense mechanisms against a broad range of automated obfuscation attacks,\nincluding both algorithmic and AI-generated methods, and for a wide variety of\nreal-world datasets. We evaluate the improvements of two defense mechanisms\nover the plagiarism detector JPlag across over four million pairwise program\ncomparisons. Our results show significant improvements in detecting obfuscated\nplagiarism instances, and we observe an improved detection of AI-generated\nprograms, even though the defense mechanisms are not designed for this use\ncase. Based on our findings, we provide an in-depth discussion of their broader\nimplications for academic integrity and the role of AI in education.", "authors": ["Timur Sağlam", "Larissa Schmid"], "published_date": "2025-05-26", "title_zh": "人工智能時代下評估軟體抄襲偵測：自動混淆與學術誠信之教訓", "summary_zh": "程式作業抄襲是電腦科學教育中長期存在的問題，自動混淆攻擊的出現使其更趨複雜。軟體抄襲偵測器雖廣泛用於大規模識別可疑相似性，且能抵抗簡單混淆技巧，但易受基於程式碼結構修改的高級混淆攻擊影響。現有防禦機制雖旨在提高對抗此類攻擊的韌性，但評估範圍僅限於特定攻擊，缺乏對基於人工智慧混淆的全面研究。本文針對一系列自動混淆攻擊（包括演算法和人工智慧生成方法）以及各種真實世界資料集，研究這些防禦機制的韌性。我們評估了兩種防禦機制相較於JPlag的改進，涉及超過四百萬組程式碼對比。結果顯示，在偵測被混淆的抄襲實例方面有顯著提升，並觀察到對人工智慧生成程式碼的偵測能力亦有所改善，儘管防禦機制並非專為此用例設計。基於研究結果，我們深入探討了其對學術誠信以及人工智慧在教育領域中所扮演角色的廣泛影響。", "audio": "audios/2505.20158v1.mp3", "timestamp": "2025-05-27T15:20:50.349751"}
{"query": "Foundation Model", "id": "2505.19447v1", "url": "http://arxiv.org/abs/2505.19447v1", "title": "A Contrastive Learning Foundation Model Based on Perfectly Aligned Sample Pairs for Remote Sensing Images", "summary": "Self-Supervised Learning (SSL) enables us to pre-train foundation models\nwithout costly labeled data. Among SSL methods, Contrastive Learning (CL)\nmethods are better at obtaining accurate semantic representations in noise\ninterference. However, due to the significant domain gap, while CL methods have\nachieved great success in many computer vision tasks, they still require\nspecific adaptation for Remote Sensing (RS) images. To this end, we present a\nnovel self-supervised method called PerA, which produces all-purpose RS\nfeatures through semantically Perfectly Aligned sample pairs. Specifically,\nPerA obtains features from sampled views by applying spatially disjoint masks\nto augmented images rather than random cropping. With disjoint masks, we divide\npatches from different views into different parts that are semantically aligned\nbut inconsistent in appearance. Our framework provides high-quality features by\nensuring consistency between teacher and student and predicting learnable mask\ntokens. Compared to previous contrastive methods, our method demonstrates\nhigher memory efficiency and can be trained with larger batches due to its\nsparse inputs. We also collect an unlabeled pre-training dataset, which\ncontains about 5 million RS images. We conducted experiments on multiple\ndownstream task datasets and achieved performance comparable to previous\nstate-of-the-art methods with a limited model scale, which verified the\nsuperiority of our method. We hope this work will contribute to practical\nremote sensing interpretation works.", "authors": ["Hengtong Shen", "Haiyan Gu", "Haitao Li", "Yi Yang", "Agen qiu"], "published_date": "2025-05-26", "title_zh": "基於完美對齊樣本對的遙感影像對比學習基礎模型", "summary_zh": "自監督學習無需標註資料即可預訓練基礎模型。對比學習方法在雜訊干擾下更能獲取精確的語義表示。然而，由於顯著的領域差距，對比學習方法在遙感影像上仍需特定調整。本文提出一種名為PerA的新型自監督方法，透過語義完美對齊的樣本對生成通用的遙感特徵。PerA對增強影像應用空間不相交的遮罩而非隨機裁剪，從而從採樣視圖中獲取特徵。透過不相交的遮罩，我們將來自不同視圖的圖塊劃分為語義對齊但外觀不一致的不同部分。我們的框架透過確保教師和學生之間的一致性並預測可學習的遮罩令牌來提供高品質特徵。相較於先前的對比方法，我們的記憶體效率更高，並且由於其稀疏輸入，因此可以使用更大的批次進行訓練。我們還收集了一個包含約五百萬張遙感影像的未標註預訓練資料集。在多個下游任務資料集上進行的實驗表明，在有限的模型規模下，我們的效能與先前的最先進方法相當，驗證了我們方法的優越性。期望此研究能對實際的遙感影像解譯工作有所貢獻。", "audio": "audios/2505.19447v1.mp3", "timestamp": "2025-05-27T15:21:01.847152"}
{"query": "Diffusion Model", "id": "2505.19765v1", "url": "http://arxiv.org/abs/2505.19765v1", "title": "On some coupled local and nonlocal diffusion models", "summary": "We study problems in which a local model is coupled with a nonlocal one. We\npropose two energies: both of them are based on the same classical weighted\n$H^1$-semi norm to model the local part, while two different weighted\n$H^s$-semi norms, with $s \\in (0,1)$, are used to model the nonlocal part. The\ncorresponding strong formulations are derived. In doing so, one needs to\ndevelop some technical tools, such as suitable integration by parts formulas\nfor operators with variable diffusivity, and one also needs to study the\nmapping properties of the Neumann operators that arise. In contrast to problems\ncoupling purely local models, in which one requires transmission conditions on\nthe interface between the subdomains, the presence of a nonlocal operator may\ngive rise to nonlocal fluxes. These nonlocal fluxes may enter the problem as a\nsource term, thereby changing its structure. Finally, we focus on a specific\nproblem, that we consider most relevant, and study regularity of solutions and\nfinite element discretizations. We provide numerical experiments to illustrate\nthe most salient features of the models.", "authors": ["Juan Pablo Borthagaray", "Patrick Ciarlet Jr"], "published_date": "2025-05-26", "title_zh": "關於某些耦合的局部與非局部擴散模型", "summary_zh": "本研究探討局部模型與非局部模型耦合問題。提出兩種能量模型，局部部分基於相同的加權$H^1$半範數，非局部部分則採用不同的加權$H^s$半範數，其中$s \\in (0,1)$。推導出相應的強公式，並發展了變擴散率算子的分部積分公式等技術工具，以及Neumann算子的映射性質。與純局部模型耦合問題不同，非局部算子的存在可能導致非局部通量，進而改變問題結構，表現為源項。最後，針對一個特定問題，研究其解的規律性和有限元素離散化，並通過數值實驗闡明模型的顯著特徵。", "audio": "audios/2505.19765v1.mp3", "timestamp": "2025-05-27T15:21:10.052596"}
{"query": "AI", "id": "2505.20096v1", "url": "http://arxiv.org/abs/2505.20096v1", "title": "MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning", "summary": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\neither end-to-end fine-tuning or isolated component enhancements, MA-RAG\norchestrates a collaborative set of specialized AI agents: Planner, Step\nDefiner, Extractor, and QA Agents, to tackle each stage of the RAG pipeline\nwith task-aware reasoning. Ambiguities may arise from underspecified queries,\nsparse or indirect evidence in retrieved documents, or the need to integrate\ninformation scattered across multiple sources. MA-RAG mitigates these\nchallenges by decomposing the problem into subtasks, such as query\ndisambiguation, evidence extraction, and answer synthesis, and dispatching them\nto dedicated agents equipped with chain-of-thought prompting. These agents\ncommunicate intermediate reasoning and progressively refine the retrieval and\nsynthesis process. Our design allows fine-grained control over information flow\nwithout any model fine-tuning. Crucially, agents are invoked on demand,\nenabling a dynamic and efficient workflow that avoids unnecessary computation.\nThis modular and reasoning-driven architecture enables MA-RAG to deliver\nrobust, interpretable results. Experiments on multi-hop and ambiguous QA\nbenchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free\nbaselines and rivals fine-tuned systems, validating the effectiveness of\ncollaborative agent-based reasoning in RAG.", "authors": ["Thang Nguyen", "Peter Chin", "Yu-Wing Tai"], "published_date": "2025-05-26", "title_zh": "MA-RAG：基於協作式思維鏈推理的多代理檢索增強生成", "summary_zh": "本研究提出多代理檢索增強生成框架（MA-RAG），旨在解決複雜資訊檢索任務中的模糊性和推理挑戰。不同於傳統方法，MA-RAG協調規劃、步驟定義、提取和問答等多個專業AI代理，以具備任務感知推理能力的方式處理檢索增強生成流程的各個階段。針對查詢不明確、證據稀疏或需整合多源資訊等問題，MA-RAG將問題分解為子任務，例如查詢消歧、證據提取和答案合成，並將其分派給配備思維鏈提示的專用代理。這些代理溝通中間推理過程，逐步完善檢索和合成。此設計允許對資訊流進行精細控制，無需模型微調。代理按需調用，實現動態高效的工作流程，避免不必要的計算。實驗結果表明，在多跳和模糊問答基準測試中，MA-RAG優於最先進的免訓練基線，並可與微調系統相媲美，驗證了基於協作代理的推理在檢索增強生成中的有效性。", "audio": "audios/2505.20096v1.mp3", "timestamp": "2025-05-27T16:23:43.685904"}
{"query": "Foundation Model", "id": "2505.19397v1", "url": "http://arxiv.org/abs/2505.19397v1", "title": "Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial Robustness Across Domains", "summary": "Time Series Foundation Models (TSFMs), which are pretrained on large-scale,\ncross-domain data and capable of zero-shot forecasting in new scenarios without\nfurther training, are increasingly adopted in real-world applications. However,\nas the zero-shot forecasting paradigm gets popular, a critical yet overlooked\nquestion emerges: Are TSFMs robust to adversarial input perturbations? Such\nperturbations could be exploited in man-in-the-middle attacks or data\npoisoning. To address this gap, we conduct a systematic investigation into the\nadversarial robustness of TSFMs. Our results show that even minimal\nperturbations can induce significant and controllable changes in forecast\nbehaviors, including trend reversal, temporal drift, and amplitude shift,\nposing serious risks to TSFM-based services. Through experiments on\nrepresentative TSFMs and multiple datasets, we reveal their consistent\nvulnerabilities and identify potential architectural designs, such as\nstructural sparsity and multi-task pretraining, that may improve robustness.\nOur findings offer actionable guidance for designing more resilient forecasting\nsystems and provide a critical assessment of the adversarial robustness of\nTSFMs.", "authors": ["Jiawen Zhang", "Zhenwei Zhang", "Shun Zheng", "Xumeng Wen", "Jia Li", "Jiang Bian"], "published_date": "2025-05-26", "title_zh": "時間序列基礎模型是否已準備好部署？跨領域對抗性穩健性的系統性研究", "summary_zh": "時間序列基礎模型(TSFMs)因其跨領域數據預訓練及零樣本預測能力而被廣泛採用。然而，TSFMs在對抗性輸入擾動下的穩健性問題日益重要。本研究系統性探討TSFMs的對抗性穩健性，結果表明即使是微小擾動也能顯著且可控地改變預測行為，如趨勢反轉、時間漂移和幅度偏移，對基於TSFM的服務構成嚴重風險。通過實驗，揭示了TSFMs的普遍脆弱性，並探討了結構稀疏性和多任務預訓練等潛在架構設計，以提升穩健性。研究結果為設計更具彈性的預測系統提供了指導，並對TSFMs的對抗性穩健性進行了關鍵評估。", "audio": "audios/2505.19397v1.mp3", "timestamp": "2025-05-27T16:23:49.114355"}
{"query": "Diffusion Model", "id": "2505.19751v1", "url": "http://arxiv.org/abs/2505.19751v1", "title": "SAIL: Self-supervised Albedo Estimation from Real Images with a Latent Diffusion Model", "summary": "Intrinsic image decomposition aims at separating an image into its underlying\nalbedo and shading components, isolating the base color from lighting effects\nto enable downstream applications such as virtual relighting and scene editing.\nDespite the rise and success of learning-based approaches, intrinsic image\ndecomposition from real-world images remains a significant challenging task due\nto the scarcity of labeled ground-truth data. Most existing solutions rely on\nsynthetic data as supervised setups, limiting their ability to generalize to\nreal-world scenes. Self-supervised methods, on the other hand, often produce\nalbedo maps that contain reflections and lack consistency under different\nlighting conditions. To address this, we propose SAIL, an approach designed to\nestimate albedo-like representations from single-view real-world images. We\nrepurpose the prior knowledge of a latent diffusion model for unconditioned\nscene relighting as a surrogate objective for albedo estimation. To extract the\nalbedo, we introduce a novel intrinsic image decomposition fully formulated in\nthe latent space. To guide the training of our latent diffusion model, we\nintroduce regularization terms that constrain both the lighting-dependent and\nindependent components of our latent image decomposition. SAIL predicts stable\nalbedo under varying lighting conditions and generalizes to multiple scenes,\nusing only unlabeled multi-illumination data available online.", "authors": ["Hala Djeghim", "Nathan Piasco", "Luis Roldão", "Moussab Bennehar", "Dzmitry Tsishkou", "Céline Loscos", "Désiré Sidibé"], "published_date": "2025-05-26", "title_zh": "SAIL：藉由潛在擴散模型從真實圖像進行自我監督反照率估計", "summary_zh": "內在圖像分解旨在將圖像分解為反照率和陰影成分，從光照效果中分離出基本顏色，以實現虛擬重照明和場景編輯等下游應用。儘管基於學習的方法有所興起和成功，但由於缺乏標記的真實數據，從真實圖像中進行內在圖像分解仍然是一項重大挑戰。現有解決方案大多依賴合成數據作為監督設置，限制了其推廣到真實場景的能力。另一方面，自監督方法通常會產生包含反射且在不同光照條件下缺乏一致性的反照率圖。為了解決這個問題，我們提出 SAIL，一種旨在從單視角真實圖像估計類反照率表示的方法。我們將潛在擴散模型在無條件場景重照明中的先驗知識重新用於反照率估計的替代目標。為了提取反照率，我們提出了一種完全在潛在空間中制定的新型內在圖像分解。為了指導我們潛在擴散模型的訓練，我們引入了正則化項，以約束我們潛在圖像分解中光照相關和獨立的成分。SAIL 在不同的光照條件下預測穩定的反照率，並推廣到多個場景，僅使用在線提供的未標記多照明數據。", "audio": "audios/2505.19751v1.mp3", "timestamp": "2025-05-27T16:23:57.490623"}
{"query": "AI", "id": "2505.20085v1", "url": "http://arxiv.org/abs/2505.20085v1", "title": "Explanation User Interfaces: A Systematic Literature Review", "summary": "Artificial Intelligence (AI) is one of the major technological advancements\nof this century, bearing incredible potential for users through AI-powered\napplications and tools in numerous domains. Being often black-box (i.e., its\ndecision-making process is unintelligible), developers typically resort to\neXplainable Artificial Intelligence (XAI) techniques to interpret the behaviour\nof AI models to produce systems that are transparent, fair, reliable, and\ntrustworthy. However, presenting explanations to the user is not trivial and is\noften left as a secondary aspect of the system's design process, leading to AI\nsystems that are not useful to end-users. This paper presents a Systematic\nLiterature Review on Explanation User Interfaces (XUIs) to gain a deeper\nunderstanding of the solutions and design guidelines employed in the academic\nliterature to effectively present explanations to users. To improve the\ncontribution and real-world impact of this survey, we also present a framework\nfor Human-cEnteRed developMent of Explainable user interfaceS (HERMES) to guide\npractitioners and academics in the design and evaluation of XUIs.", "authors": ["Eleonora Cappuccio", "Andrea Esposito", "Francesco Greco", "Giuseppe Desolda", "Rosa Lanzilotti", "Salvatore Rinzivillo"], "published_date": "2025-05-26", "title_zh": "解釋型使用者介面：系統性文獻回顧", "summary_zh": "人工智慧是本世紀重大科技進展之一，於多領域的應用程式和工具中具備巨大潛力。由於其決策過程通常難以理解，開發者常藉由可解釋人工智慧（XAI）技術來詮釋模型行為，以打造透明、公平、可靠且值得信賴的系統。然而，向使用者呈現解釋並非易事，且常被視為系統設計的次要環節，導致人工智慧系統對終端使用者無用。本文針對解釋使用者介面（XUI）進行系統性文獻回顧，以深入了解學術文獻中為有效向使用者呈現解釋而採用的解決方案與設計指南。為提升本研究的貢獻與實際影響力，我們亦提出以人為本的可解釋使用者介面開發框架（HERMES），以指導從業者與學者進行XUI的設計與評估。", "audio": "audios/2505.20085v1.mp3", "timestamp": "2025-05-27T17:16:57.575517"}
{"query": "Foundation Model", "id": "2505.19390v1", "url": "http://arxiv.org/abs/2505.19390v1", "title": "Foundation Model for Wireless Technology Recognition Using IQ Timeseries", "summary": "Wireless Technology Recognition (WTR) is essential in modern communication\nsystems, enabling efficient spectrum management and the seamless coexistence of\ndiverse technologies. In real-world conditions, WTR solutions should be able to\nhandle signals from various resources with different sampling rates, capturing\ndevices, and frequency bands. However, traditional WTR methods, which rely on\nenergy detection, Convolutional Neural Network (CNN) models, or Deep Learning\n(DL), lack the robustness and adaptability required to generalize across unseen\nenvironments, different sampling devices, and previously unencountered signal\nclasses. In this work, we introduce a Transformer-based foundation model for\nWTR, trained in an unsupervised manner on large-scale, unlabeled wireless\nsignal datasets. Foundation models are designed to learn general-purpose\nrepresentations that transfer effectively across tasks and domains, allowing\ngeneralization towards new technologies and WTR sampling devices. Our approach\nleverages input patching for computational efficiency and incorporates a\ntwo-stage training pipeline: unsupervised pre-training followed by lightweight\nfine-tuning. This enables the model to generalize to new wireless technologies\nand environments using only a small number of labeled samples. Experimental\nresults demonstrate that our model achieves superior accuracy across varying\nsampling rates and frequency bands while maintaining low computational\ncomplexity, supporting the vision of a reusable wireless foundation model\nadaptable to new technologies with minimal retraining.", "authors": ["Mohammad Cheraghinia", "Eli De Poorter", "Jaron Fontaine", "Merouane Debbah", "Adnan Shahid"], "published_date": "2025-05-26", "title_zh": "基於IQ時序的無線技術識別基礎模型", "summary_zh": "無線技術識別(WTR)對現代通訊系統至關重要，能實現高效頻譜管理與異質技術共存。現有WTR方法，如能量檢測、CNN或深度學習，在處理不同取樣率、捕獲設備及頻段訊號時缺乏足夠的穩健性和適應性。本研究提出一種基於Transformer的WTR基礎模型，該模型透過大規模未標記無線訊號資料集進行無監督訓練。基礎模型旨在學習通用表徵，有效遷移至不同任務和領域，進而推廣至新技術和取樣設備。此方法利用輸入修補提高計算效率，並採用兩階段訓練流程：無監督預訓練與輕量微調。實驗結果表明，該模型在不同取樣率和頻段下均能實現卓越準確度，且保持低計算複雜度，支援可重複使用的無線基礎模型願景，能以最少量的重新訓練適應新技術。", "audio": "audios/2505.19390v1.mp3", "timestamp": "2025-05-27T17:17:04.918991"}
{"query": "Diffusion Model", "id": "2505.19717v1", "url": "http://arxiv.org/abs/2505.19717v1", "title": "Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning", "summary": "Imitation learning is a promising approach for enabling generalist\ncapabilities in humanoid robots, but its scaling is fundamentally constrained\nby the scarcity of high-quality expert demonstrations. This limitation can be\nmitigated by leveraging suboptimal, open-ended play data, often easier to\ncollect and offering greater diversity. This work builds upon recent advances\nin generative modeling, specifically Flow Matching, an alternative to Diffusion\nmodels. We introduce a method for estimating the extremum of the learned\ndistribution by leveraging the unique properties of Flow Matching, namely,\ndeterministic transport and support for arbitrary source distributions. We\napply this method to develop several goal-conditioned imitation and\nreinforcement learning algorithms based on Flow Matching, where policies are\nconditioned on both current and goal observations. We explore and compare\ndifferent architectural configurations by combining core components, such as\ncritic, planner, actor, or world model, in various ways. We evaluated our\nagents on the OGBench benchmark and analyzed how different demonstration\nbehaviors during data collection affect performance in a 2D non-prehensile\npushing task. Furthermore, we validated our approach on real hardware by\ndeploying it on the Talos humanoid robot to perform complex manipulation tasks\nbased on high-dimensional image observations, featuring a sequence of\npick-and-place and articulated object manipulation in a realistic kitchen\nenvironment. Experimental videos and code are available at:\nhttps://hucebot.github.io/extremum_flow_matching_website/", "authors": ["Quentin Rouxel", "Clemente Donoso", "Fei Chen", "Serena Ivaldi", "Jean-Baptiste Mouret"], "published_date": "2025-05-26", "title_zh": "離線目標條件強化學習之極值流匹配", "summary_zh": "模仿學習有望賦予人形機器人通用能力，但其擴展受限於高品質專家示範的稀缺性。利用次優、開放式探索數據可緩解此限制，因其更易收集且更具多樣性。本研究基於生成模型Flow Matching的最新進展，提出一種利用Flow Matching獨有特性（確定性傳輸及對任意源分布的支持）來估計學習分布極值的方法。我們將此方法應用於開發基於Flow Matching的目標條件模仿學習和強化學習演算法，策略受當前和目標觀察條件約束。我們探索並比較了不同架構配置，透過各種方式組合評論家、規劃器、執行者或世界模型等核心組件。我們在OGBench基準上評估了智能體，並分析了數據收集期間不同示範行為如何影響2D非抓取推動任務的性能。此外，我們在真實硬體Talos人形機器人上驗證了該方法，使其能夠在高維度圖像觀察基礎上執行複雜的操作任務，包括在真實廚房環境中的一系列拾取放置和關節物體操作。", "audio": "audios/2505.19717v1.mp3", "timestamp": "2025-05-27T17:17:12.366613"}
{"query": "AI", "id": "2505.20075v1", "url": "http://arxiv.org/abs/2505.20075v1", "title": "Curriculum-RLAIF: Curriculum Alignment with Reinforcement Learning from AI Feedback", "summary": "Reward models trained with conventional Reinforcement Learning from AI\nFeedback (RLAIF) methods suffer from limited generalizability, which hinders\nthe alignment performance of the policy model during reinforcement learning\n(RL). This challenge stems from various issues, including distribution shift,\npreference label noise, and mismatches between overly challenging samples and\nmodel capacity. In this paper, we attempt to enhance the generalizability of\nreward models through a data-centric approach, driven by the insight that these\nissues are inherently intertwined from the perspective of data difficulty. To\naddress this, we propose a novel framework, $\\textit{Curriculum-RLAIF}$, which\nconstructs preference pairs with varying difficulty levels and produces a\ncurriculum that progressively incorporates preference pairs of increasing\ndifficulty for reward model training. Our experimental results suggest that\nreward models trained with Curriculum-RLAIF achieve improved generalizability,\nsignificantly increasing the alignment performance of the policy model by a\nlarge margin without incurring additional inference costs compared to various\nnon-curriculum baselines. Detailed analysis and comparisons with alternative\napproaches, including data selection via external pretrained reward models or\ninternal self-selection mechanisms, as well as other curriculum strategies,\nfurther demonstrate the superiority of our approach in terms of simplicity,\nefficiency, and effectiveness.", "authors": ["Mengdi Li", "Jiaye Lin", "Xufeng Zhao", "Wenhao Lu", "Peilin Zhao", "Stefan Wermter", "Di Wang"], "published_date": "2025-05-26", "title_zh": "課程-RLAIF：基於AI回饋的強化學習之課程對齊", "summary_zh": "傳統AI回饋強化學習（RLAIF）訓練的獎勵模型泛化能力有限，影響策略模型於強化學習中的對齊效果。此挑戰源於分佈偏移、偏好標籤雜訊以及過於困難的樣本與模型能力不匹配等問題。本研究從資料難度的角度出發，透過資料中心方法提升獎勵模型的泛化能力。我們提出$\\textit{Curriculum-RLAIF}$框架，構建不同難度的偏好對，並生成課程表，逐步將難度遞增的偏好對納入獎勵模型訓練。實驗結果表明，相較於非課程基準模型，採用Curriculum-RLAIF訓練的獎勵模型具有更佳的泛化能力，顯著提升策略模型的對齊效果，且不增加額外推論成本。與其他方法（包括基於外部預訓練獎勵模型的資料選擇、內部自選擇機制及其他課程策略）的詳細分析和比較，進一步驗證了我們方法的簡潔性、效率和有效性。", "audio": "audios/2505.20075v1.mp3", "timestamp": "2025-05-27T18:25:32.990950"}
{"query": "Foundation Model", "id": "2505.19306v1", "url": "http://arxiv.org/abs/2505.19306v1", "title": "From Single Images to Motion Policies via Video-Generation Environment Representations", "summary": "Autonomous robots typically need to construct representations of their\nsurroundings and adapt their motions to the geometry of their environment.\nHere, we tackle the problem of constructing a policy model for collision-free\nmotion generation, consistent with the environment, from a single input RGB\nimage. Extracting 3D structures from a single image often involves monocular\ndepth estimation. Developments in depth estimation have given rise to large\npre-trained models such as DepthAnything. However, using outputs of these\nmodels for downstream motion generation is challenging due to frustum-shaped\nerrors that arise. Instead, we propose a framework known as Video-Generation\nEnvironment Representation (VGER), which leverages the advances of large-scale\nvideo generation models to generate a moving camera video conditioned on the\ninput image. Frames of this video, which form a multiview dataset, are then\ninput into a pre-trained 3D foundation model to produce a dense point cloud. We\nthen introduce a multi-scale noise approach to train an implicit representation\nof the environment structure and build a motion generation model that complies\nwith the geometry of the representation. We extensively evaluate VGER over a\ndiverse set of indoor and outdoor environments. We demonstrate its ability to\nproduce smooth motions that account for the captured geometry of a scene, all\nfrom a single RGB input image.", "authors": ["Weiming Zhi", "Ziyong Ma", "Tianyi Zhang", "Matthew Johnson-Roberson"], "published_date": "2025-05-25", "title_zh": "基於影片生成環境表徵，從單張圖像到運動策略", "summary_zh": "自主機器人需構建環境表徵並調整運動適應幾何結構。本研究針對單張RGB圖像，提出構建與環境一致的無碰撞運動生成策略模型。為克服單目深度估計產生的錐形誤差，本研究提出視訊生成環境表徵（VGER）框架，利用大規模視訊生成模型，基於輸入圖像生成移動相機視訊。此視訊幀構成多視圖數據集，輸入至預訓練的3D基礎模型以產生密集點雲。進而，導入多尺度雜訊方法訓練環境結構的隱式表徵，並構建符合幾何結構的運動生成模型。VGER在多樣室內外環境中經過廣泛評估，證實其能從單張RGB圖像生成平滑運動，並考量場景的幾何結構。", "audio": "audios/2505.19306v1.mp3", "timestamp": "2025-05-27T18:25:38.978453"}
{"query": "Diffusion Model", "id": "2505.19694v1", "url": "http://arxiv.org/abs/2505.19694v1", "title": "Knowledge-Aligned Counterfactual-Enhancement Diffusion Perception for Unsupervised Cross-Domain Visual Emotion Recognition", "summary": "Visual Emotion Recognition (VER) is a critical yet challenging task aimed at\ninferring emotional states of individuals based on visual cues. However,\nexisting works focus on single domains, e.g., realistic images or stickers,\nlimiting VER models' cross-domain generalizability. To fill this gap, we\nintroduce an Unsupervised Cross-Domain Visual Emotion Recognition (UCDVER)\ntask, which aims to generalize visual emotion recognition from the source\ndomain (e.g., realistic images) to the low-resource target domain (e.g.,\nstickers) in an unsupervised manner. Compared to the conventional unsupervised\ndomain adaptation problems, UCDVER presents two key challenges: a significant\nemotional expression variability and an affective distribution shift. To\nmitigate these issues, we propose the Knowledge-aligned\nCounterfactual-enhancement Diffusion Perception (KCDP) framework. Specifically,\nKCDP leverages a VLM to align emotional representations in a shared knowledge\nspace and guides diffusion models for improved visual affective perception.\nFurthermore, a Counterfactual-Enhanced Language-image Emotional Alignment\n(CLIEA) method generates high-quality pseudo-labels for the target domain.\nExtensive experiments demonstrate that our model surpasses SOTA models in both\nperceptibility and generalization, e.g., gaining 12% improvements over the SOTA\nVER model TGCA-PVT. The project page is at https://yinwen2019.github.io/ucdver.", "authors": ["Wen Yin", "Yong Wang", "Guiduo Duan", "Dongyang Zhang", "Xin Hu", "Yuan-Fang Li", "Tao He"], "published_date": "2025-05-26", "title_zh": "知識對齊反事實增強擴散感知用於無監督跨域視覺情感識別", "summary_zh": "視覺情緒辨識是根據視覺線索推斷個體情緒狀態的重要任務，但現有研究多集中於單一領域，限制了模型的跨域泛化能力。為此，本文提出非監督跨域視覺情緒辨識任務，旨在將視覺情緒辨識從源域（如真實圖像）泛化到低資源目標域（如貼圖）。相較於傳統非監督域適應問題，此任務面臨情緒表達變異性和情感分布偏移兩大挑戰。本文提出知識對齊反事實增強擴散感知框架，利用視覺語言模型在共享知識空間中對齊情緒表徵，並引導擴散模型以提升視覺情感感知。此外，反事實增強的語言圖像情感對齊方法可為目標域生成高品質偽標籤。實驗結果表明，本文模型在感知力和泛化能力上均優於現有最佳模型，例如，相較於最佳視覺情緒辨識模型 TGCA-PVT，效能提升達 12%。項目網頁：https://yinwen2019.github.io/ucdver。", "audio": "audios/2505.19694v1.mp3", "timestamp": "2025-05-27T18:25:46.563092"}
{"query": "AI", "id": "2505.20068v1", "url": "http://arxiv.org/abs/2505.20068v1", "title": "On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction", "summary": "Shared understanding plays a key role in the effective communication in and\nperformance of human-human interactions. With the increasingly common\nintegration of AI into human contexts, the future of personal and workplace\ninteractions will likely see human-AI interaction (HAII) in which the\nperception of shared understanding is important. Existing literature has\naddressed the processes and effects of PSU in human-human interactions, but the\nconstrual remains underexplored in HAII. To better understand PSU in HAII, we\nconducted an online survey to collect user reflections on interactions with a\nlarge language model when it sunderstanding of a situation was thought to be\nsimilar to or different from the participant's. Through inductive thematic\nanalysis, we identified eight dimensions comprising PSU in human-AI\ninteractions: Fluency, aligned operation, fluidity, outcome satisfaction,\ncontextual awareness, lack of humanlike abilities, computational limits, and\nsuspicion.", "authors": ["Qingyu Liang", "Jaime Banks"], "published_date": "2025-05-26", "title_zh": "同頁共識：人機互動中感知共享理解之維度", "summary_zh": "共享理解在人際互動的有效溝通和表現中至關重要。隨著人工智慧日益融入人類環境，人機互動中對共享理解的感知也變得重要。現有文獻已探討人際互動中共享理解的過程和影響，但在人機互動中的建構仍未被充分研究。為更好地理解人機互動中的共享理解，我們進行了一項線上調查，收集使用者在使用大型語言模型時的互動反思，當時該模型對情況的理解被認為與參與者相似或不同。透過歸納主題分析，我們識別了人機互動中共享理解的八個維度：流暢性、一致操作、流動性、結果滿意度、情境感知、缺乏類人能力、計算限制和懷疑。", "audio": "audios/2505.20068v1.mp3", "timestamp": "2025-05-27T19:14:32.996372"}
{"query": "Foundation Model", "id": "2505.19218v1", "url": "http://arxiv.org/abs/2505.19218v1", "title": "Advancing Video Self-Supervised Learning via Image Foundation Models", "summary": "In the past decade, image foundation models (IFMs) have achieved\nunprecedented progress. However, the potential of directly using IFMs for video\nself-supervised representation learning has largely been overlooked. In this\nstudy, we propose an advancing video self-supervised learning (AdViSe)\napproach, aimed at significantly reducing the training overhead of video\nrepresentation models using pre-trained IFMs. Specifically, we first introduce\ntemporal modeling modules (ResNet3D) to IFMs, constructing a video\nrepresentation model. We then employ a video self-supervised learning approach,\nplayback rate perception, to train temporal modules while freezing the IFM\ncomponents. Experiments on UCF101 demonstrate that AdViSe achieves performance\ncomparable to state-of-the-art methods while reducing training time by\n$3.4\\times$ and GPU memory usage by $8.2\\times$. This study offers fresh\ninsights into low-cost video self-supervised learning based on pre-trained\nIFMs. Code is available at https://github.com/JingwWu/advise-video-ssl.", "authors": ["Jingwei Wu", "Zhewei Huang", "Chang Liu"], "published_date": "2025-05-25", "title_zh": "基於圖像基礎模型的影片自監督學習進展", "summary_zh": "近年圖像基礎模型發展迅速，但其在影片自監督表徵學習的應用潛力卻被忽略。本研究提出一種名為AdViSe的影片自監督學習方法，旨在利用預訓練圖像基礎模型大幅降低影片表徵模型的訓練成本。具體而言，我們首先將時間建模模組（ResNet3D）導入圖像基礎模型，構建影片表徵模型。隨後，我們採用回放速率感知的影片自監督學習方法，在凍結圖像基礎模型組件的同時，訓練時間模組。在UCF101上的實驗表明，AdViSe在達到與最先進方法相當的性能時，訓練時間減少3.4倍，GPU記憶體使用量減少8.2倍。本研究為基於預訓練圖像基礎模型的低成本影片自監督學習提供了新思路。程式碼可在https://github.com/JingwWu/advise-video-ssl取得。", "audio": "audios/2505.19218v1.mp3", "timestamp": "2025-05-27T19:14:40.056089"}
{"query": "Diffusion Model", "id": "2505.19685v1", "url": "http://arxiv.org/abs/2505.19685v1", "title": "Graph Guided Diffusion: Unified Guidance for Conditional Graph Generation", "summary": "Diffusion models have emerged as powerful generative models for graph\ngeneration, yet their use for conditional graph generation remains a\nfundamental challenge. In particular, guiding diffusion models on graphs under\narbitrary reward signals is difficult: gradient-based methods, while powerful,\nare often unsuitable due to the discrete and combinatorial nature of graphs,\nand non-differentiable rewards further complicate gradient-based guidance. We\npropose Graph Guided Diffusion (GGDiff), a novel guidance framework that\ninterprets conditional diffusion on graphs as a stochastic control problem to\naddress this challenge. GGDiff unifies multiple guidance strategies, including\ngradient-based guidance (for differentiable rewards), control-based guidance\n(using control signals from forward reward evaluations), and zero-order\napproximations (bridging gradient-based and gradient-free optimization). This\ncomprehensive, plug-and-play framework enables zero-shot guidance of\npre-trained diffusion models under both differentiable and non-differentiable\nreward functions, adapting well-established guidance techniques to graph\ngeneration--a direction largely unexplored. Our formulation balances\ncomputational efficiency, reward alignment, and sample quality, enabling\npractical conditional generation across diverse reward types. We demonstrate\nthe efficacy of GGDiff in various tasks, including constraints on graph motifs,\nfairness, and link prediction, achieving superior alignment with target rewards\nwhile maintaining diversity and fidelity.", "authors": ["Victor M. Tenorio", "Nicolas Zilberstein", "Santiago Segarra", "Antonio G. Marques"], "published_date": "2025-05-26", "title_zh": "圖導向擴散：條件圖生成之統一導引", "summary_zh": "擴散模型已成為強大的圖形生成模型，但條件圖形生成仍是挑戰。在任意獎勵訊號下引導圖形擴散模型困難重重：梯度方法常因圖形的離散性和組合特性而不適用，且不可微獎勵使基於梯度的引導更加複雜。本文提出圖形引導擴散（GGDiff），將圖形的條件擴散視為隨機控制問題，以解決此挑戰。GGDiff統整多種引導策略，包含基於梯度的引導（針對可微獎勵）、基於控制的引導（使用前向獎勵評估的控制訊號）以及零階近似（連接基於梯度和無梯度優化）。此全面、隨插即用框架能針對可微及不可微獎勵函數，對預訓練擴散模型進行零樣本引導，將成熟的引導技術應用於圖形生成，此方向尚未被廣泛探索。我們的公式在計算效率、獎勵對齊和樣本品質之間取得平衡，實現跨多樣獎勵類型的實用條件生成。在圖形motif約束、公平性和連結預測等多項任務中，GGDiff表現出優異的獎勵對齊，同時保持多樣性和保真度。", "audio": "audios/2505.19685v1.mp3", "timestamp": "2025-05-27T19:14:47.996204"}
{"query": "AI", "id": "2505.20066v1", "url": "http://arxiv.org/abs/2505.20066v1", "title": "Automated data curation for self-supervised learning in underwater acoustic analysis", "summary": "The sustainability of the ocean ecosystem is threatened by increased levels\nof sound pollution, making monitoring crucial to understand its variability and\nimpact. Passive acoustic monitoring (PAM) systems collect a large amount of\nunderwater sound recordings, but the large volume of data makes manual analysis\nimpossible, creating the need for automation. Although machine learning offers\na potential solution, most underwater acoustic recordings are unlabeled.\nSelf-supervised learning models have demonstrated success in learning from\nlarge-scale unlabeled data in various domains like computer vision, Natural\nLanguage Processing, and audio. However, these models require large, diverse,\nand balanced datasets for training in order to generalize well. To address\nthis, a fully automated self-supervised data curation pipeline is proposed to\ncreate a diverse and balanced dataset from raw PAM data. It integrates\nAutomatic Identification System (AIS) data with recordings from various\nhydrophones in the U.S. waters. Using hierarchical k-means clustering, the raw\naudio data is sampled and then combined with AIS samples to create a balanced\nand diverse dataset. The resulting curated dataset enables the development of\nself-supervised learning models, facilitating various tasks such as monitoring\nmarine mammals and assessing sound pollution.", "authors": ["Hilde I Hummel", "Sandjai Bhulai", "Burooj Ghani", "Rob van der Mei"], "published_date": "2025-05-26", "title_zh": "水下聲學分析中自監督學習的自動化數據整理", "summary_zh": "海洋生態系統永續性受日益嚴重的噪音汙染威脅，監測對於理解其變異及影響至關重要。被動聲學監測系統收集大量水下錄音，惟數據量龐大，人工分析難以負荷，亟需自動化。機器學習雖提供潛在方案，但多數水下聲學錄音未經標記。自監督學習模型已在電腦視覺、自然語言處理和音訊等領域，展現從大規模未標記數據中學習的成功。然而，這些模型需要大型、多樣且平衡的數據集進行訓練，以確保良好的泛化能力。為此，本文提出一種全自動自監督數據管理流程，旨在從原始被動聲學監測數據中創建多樣且平衡的數據集。該流程整合自動識別系統數據與美國水域多個水聽器的錄音。透過層級式k-means聚類，對原始音訊數據進行採樣，並與自動識別系統樣本結合，以創建平衡且多樣化的數據集。由此產生的數據集有助於開發自監督學習模型，促進監測海洋哺乳動物和評估噪音汙染等任務。", "audio": "audios/2505.20066v1.mp3", "timestamp": "2025-05-27T20:20:32.145780"}
{"query": "Foundation Model", "id": "2505.19203v1", "url": "http://arxiv.org/abs/2505.19203v1", "title": "EnvSDD: Benchmarking Environmental Sound Deepfake Detection", "summary": "Audio generation systems now create very realistic soundscapes that can\nenhance media production, but also pose potential risks. Several studies have\nexamined deepfakes in speech or singing voice. However, environmental sounds\nhave different characteristics, which may make methods for detecting speech and\nsinging deepfakes less effective for real-world sounds. In addition, existing\ndatasets for environmental sound deepfake detection are limited in scale and\naudio types. To address this gap, we introduce EnvSDD, the first large-scale\ncurated dataset designed for this task, consisting of 45.25 hours of real and\n316.74 hours of fake audio. The test set includes diverse conditions to\nevaluate the generalizability, such as unseen generation models and unseen\ndatasets. We also propose an audio deepfake detection system, based on a\npre-trained audio foundation model. Results on EnvSDD show that our proposed\nsystem outperforms the state-of-the-art systems from speech and singing\ndomains.", "authors": ["Han Yin", "Yang Xiao", "Rohan Kumar Das", "Jisheng Bai", "Haohe Liu", "Wenwu Wang", "Mark D Plumbley"], "published_date": "2025-05-25", "title_zh": "EnvSDD：環境聲音深度偽造偵測基準測試", "summary_zh": "音訊生成系統雖能創造逼真音景以提升媒體製作，但也潛藏風險。現有研究多關注語音或歌聲深度偽造，然環境音特性不同，使針對語音及歌聲之偵測方法未必適用。為此，我們建立首個大規模環境音深度偽造資料集EnvSDD，包含45.25小時真實音訊與316.74小時偽造音訊。測試集包含多樣情境，評估其泛化能力，如未見過的生成模型與資料集。同時，我們提出基於預訓練音訊基礎模型的偵測系統，實驗結果顯示其優於現有語音及歌聲領域的最佳系統。", "audio": "audios/2505.19203v1.mp3", "timestamp": "2025-05-27T20:20:38.973757"}
{"query": "Diffusion Model", "id": "2505.19675v1", "url": "http://arxiv.org/abs/2505.19675v1", "title": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement", "summary": "The traditional process of creating labeled datasets is labor-intensive and\nexpensive. Recent breakthroughs in open-source large language models (LLMs)\nhave opened up a new avenue in generating labeled datasets automatically for\nvarious natural language processing (NLP) tasks, providing an alternative to\nsuch an expensive annotation process. However, the reliability of such\nauto-generated labels remains a significant concern due to inherent\ninaccuracies. When learning from noisy labels, the model's generalization is\nlikely to be harmed as it is prone to overfit to those label noises. While\nprevious studies in learning from noisy labels mainly focus on synthetic noise\nand real-world noise, LLM-generated label noise receives less attention. In\nthis paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to\ncalibrate the classifier's prediction, thus enhancing its robustness towards\nLLM-generated noisy labels. SiDyP retrieves potential true label candidates by\nneighborhood label distribution in text embedding space and iteratively refines\nnoisy candidates using a simplex diffusion model. Our framework can increase\nthe performance of the BERT classifier fine-tuned on both zero-shot and\nfew-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%\nrespectively. We demonstrate the effectiveness of SiDyP by conducting extensive\nbenchmarking for different LLMs over a variety of NLP tasks. Our code is\navailable on Github.", "authors": ["Liqin Ye", "Agam Shah", "Chao Zhang", "Sudheer Chava"], "published_date": "2025-05-26", "title_zh": "基於迭代精煉校準預訓練語言分類器於大型語言模型生成之雜訊標籤", "summary_zh": "傳統標註資料集耗時費力。大型語言模型於自動生成標註資料集方面取得突破，為自然語言處理任務提供經濟替代方案。然而，自動生成標籤的準確性仍是主要問題，模型易過擬合噪音標籤，影響泛化能力。針對此問題，本文提出SiDyP框架，利用動態先驗的單純形標籤擴散校準分類器預測，增強其對抗大型語言模型生成噪音標籤的穩健性。SiDyP透過文本嵌入空間的鄰域標籤分佈檢索潛在的真實標籤候選，並使用單純形擴散模型迭代精煉。實驗結果顯示，SiDyP平均提升BERT分類器在零樣本和少樣本噪音標籤資料集上的效能7.21%和7.30%。針對不同大型語言模型和多種自然語言處理任務的大量評測驗證了SiDyP的有效性。代碼已開源。", "audio": "audios/2505.19675v1.mp3", "timestamp": "2025-05-27T20:20:46.293409"}
{"query": "AI", "id": "2505.20059v1", "url": "http://arxiv.org/abs/2505.20059v1", "title": "LPCM: Learning-based Predictive Coding for LiDAR Point Cloud Compression", "summary": "Since the data volume of LiDAR point clouds is very huge, efficient\ncompression is necessary to reduce their storage and transmission costs.\nHowever, existing learning-based compression methods do not exploit the\ninherent angular resolution of LiDAR and ignore the significant differences in\nthe correlation of geometry information at different bitrates. The predictive\ngeometry coding method in the geometry-based point cloud compression (G-PCC)\nstandard uses the inherent angular resolution to predict the azimuth angles.\nHowever, it only models a simple linear relationship between the azimuth angles\nof neighboring points. Moreover, it does not optimize the quantization\nparameters for residuals on each coordinate axis in the spherical coordinate\nsystem. We propose a learning-based predictive coding method (LPCM) with both\nhigh-bitrate and low-bitrate coding modes. LPCM converts point clouds into\npredictive trees using the spherical coordinate system. In high-bitrate coding\nmode, we use a lightweight Long-Short-Term Memory-based predictive (LSTM-P)\nmodule that captures long-term geometry correlations between different\ncoordinates to efficiently predict and compress the elevation angles. In\nlow-bitrate coding mode, where geometry correlation degrades, we introduce a\nvariational radius compression (VRC) module to directly compress the point\nradii. Then, we analyze why the quantization of spherical coordinates differs\nfrom that of Cartesian coordinates and propose a differential evolution\n(DE)-based quantization parameter selection method, which improves\nrate-distortion performance without increasing coding time. Experimental\nresults on the LiDAR benchmark \\textit{SemanticKITTI} and the MPEG-specified\n\\textit{Ford} datasets show that LPCM outperforms G-PCC and other\nlearning-based methods.", "authors": ["Chang Sun", "Hui Yuan", "Shiqi Jiang", "Da Ai", "Wei Zhang", "Raouf Hamzaoui"], "published_date": "2025-05-26", "title_zh": "LPCM：基於學習的用於激光雷達點雲壓縮的預測編碼", "summary_zh": "光達點雲數據量龐大，高效壓縮至關重要。現有基於學習的壓縮方法未利用光達的固有角分辨率，且忽略不同碼率下幾何資訊關聯性的差異。幾何點雲壓縮（G-PCC）標準中的預測幾何編碼雖利用角分辨率預測方位角，但僅建模鄰近點方位角的簡單線性關係，且未針對球坐標系中各坐標軸的殘差優化量化參數。本文提出一種具高、低碼率編碼模式的基於學習的預測編碼方法（LPCM）。LPCM使用球坐標系將點雲轉換為預測樹。在高碼率模式下，採用輕量級長短期記憶預測（LSTM-P）模組捕捉不同坐標間的長期幾何關聯性，有效預測和壓縮俯仰角。在幾何關聯性降低的低碼率模式下，引入變分半徑壓縮（VRC）模組直接壓縮點半徑。接著，分析球坐標與笛卡爾坐標量化差異的原因，並提出一種基於差分演算法（DE）的量化參數選擇方法，在不增加編碼時間的情況下提升率失真效能。在光達基準數據集SemanticKITTI和MPEG指定數據集Ford上的實驗結果表明，LPCM優於G-PCC和其他基於學習的方法。", "audio": "audios/2505.20059v1.mp3", "timestamp": "2025-05-27T21:17:59.660192"}
{"query": "Foundation Model", "id": "2505.18969v1", "url": "http://arxiv.org/abs/2505.18969v1", "title": "Machine Psychophysics: Cognitive Control in Vision-Language Models", "summary": "Cognitive control refers to the ability to flexibly coordinate thought and\naction in pursuit of internal goals. A standard method for assessing cognitive\ncontrol involves conflict tasks that contrast congruent and incongruent trials,\nmeasuring the ability to prioritize relevant information while suppressing\ninterference. We evaluate 108 vision-language models on three classic conflict\ntasks and their more demanding \"squared\" variants across 2,220 trials. Model\nperformance corresponds closely to human behavior under resource constraints\nand reveals individual differences. These results indicate that some form of\nhuman-like executive function have emerged in current multi-modal foundational\nmodels.", "authors": ["Dezhi Luo", "Maijunxian Wang", "Bingyang Wang", "Tianwei Zhao", "Yijiang Li", "Hokin Deng"], "published_date": "2025-05-25", "title_zh": "機器心理物理學：視覺-語言模型中的認知控制", "summary_zh": "認知控制指涉為追求內在目標，彈性協調思維與行動的能力。評估認知控制的常用方法包含衝突任務，其對比一致與不一致試驗，衡量在抑制干擾時優先處理相關訊息的能力。本研究於三項經典衝突任務及其更具挑戰性的平方變體中，評估108個視覺語言模型，共計2,220次試驗。模型表現與資源限制下的人類行為高度一致，並揭示個體差異。結果顯示，現有多模態基礎模型已展現某種形式的類人類執行功能。", "audio": "audios/2505.18969v1.mp3", "timestamp": "2025-05-27T21:18:08.122894"}
{"query": "Diffusion Model", "id": "2505.19656v1", "url": "http://arxiv.org/abs/2505.19656v1", "title": "ReDDiT: Rehashing Noise for Discrete Visual Generation", "summary": "Discrete diffusion models are gaining traction in the visual generative area\nfor their efficiency and compatibility. However, the pioneered attempts still\nfall behind the continuous counterparts, which we attribute to the noise\n(absorbing state) design and sampling heuristics. In this study, we propose the\nrehashing noise framework for discrete diffusion transformer, termed ReDDiT, to\nextend absorbing states and improve expressive capacity of discrete diffusion\nmodels. ReDDiT enriches the potential paths that latent variables can traverse\nduring training with randomized multi-index corruption. The derived rehash\nsampler, which reverses the randomized absorbing paths, guarantees the\ndiversity and low discrepancy of the generation process. These reformulations\nlead to more consistent and competitive generation quality, mitigating the need\nfor heavily tuned randomness. Experiments show that ReDDiT significantly\noutperforms the baseline (reducing gFID from 6.18 to 1.61) and is on par with\nthe continuous counterparts with higher efficiency.", "authors": ["Tianren Ma", "Xiaosong Zhang", "Boyu Yang", "Junlan Feng", "Qixiang Ye"], "published_date": "2025-05-26", "title_zh": "ReDDiT：離散視覺生成之雜訊再散列", "summary_zh": "離散擴散模型因其效率和相容性在視覺生成領域備受關注，但早期嘗試仍落後於連續模型，原因在於雜訊（吸收態）設計和採樣啟發法。本研究提出用於離散擴散轉換器的重雜湊雜訊框架（ReDDiT），以擴展吸收態並提升離散擴散模型的表達能力。ReDDiT透過隨機多索引腐敗豐富潛在變數在訓練期間可能遍歷的路徑。導出的重雜湊採樣器反轉隨機吸收路徑，保證生成過程的多樣性和低差異性。這些重新表述帶來更一致且具競爭力的生成品質，減少了對大量調整隨機性的需求。實驗表明，ReDDiT顯著優於基準線（將gFID從6.18降低至1.61），並在效率更高的前提下，與連續模型相當。", "audio": "audios/2505.19656v1.mp3", "timestamp": "2025-05-27T21:18:18.143036"}
{"query": "AI", "id": "2505.20033v1", "url": "http://arxiv.org/abs/2505.20033v1", "title": "EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion Recognition", "summary": "Effective human-AI interaction relies on AI's ability to accurately perceive\nand interpret human emotions. Current benchmarks for vision and vision-language\nmodels are severely limited, offering a narrow emotional spectrum that\noverlooks nuanced states (e.g., bitterness, intoxication) and fails to\ndistinguish subtle differences between related feelings (e.g., shame vs.\nembarrassment). Existing datasets also often use uncontrolled imagery with\noccluded faces and lack demographic diversity, risking significant bias. To\naddress these critical gaps, we introduce EmoNet Face, a comprehensive\nbenchmark suite. EmoNet Face features: (1) A novel 40-category emotion\ntaxonomy, meticulously derived from foundational research to capture finer\ndetails of human emotional experiences. (2) Three large-scale, AI-generated\ndatasets (EmoNet HQ, Binary, and Big) with explicit, full-face expressions and\ncontrolled demographic balance across ethnicity, age, and gender. (3) Rigorous,\nmulti-expert annotations for training and high-fidelity evaluation. (4) We\nbuild Empathic Insight Face, a model achieving human-expert-level performance\non our benchmark. The publicly released EmoNet Face suite - taxonomy, datasets,\nand model - provides a robust foundation for developing and evaluating AI\nsystems with a deeper understanding of human emotions.", "authors": ["Christoph Schuhmann", "Robert Kaczmarczyk", "Gollam Rabby", "Maurice Kraus", "Felix Friedrich", "Huu Nguyen", "Krishna Kalyan", "Kourosh Nadi", "Kristian Kersting", "Sören Auer"], "published_date": "2025-05-26", "title_zh": "EmoNet-Face：用於合成情感辨識的專家標註基準", "summary_zh": "有效的人機互動仰賴人工智慧準確感知及解讀人類情緒的能力。現有視覺與視覺語言模型的評估基準存在侷限，情感範圍狹窄，忽略了細微的情緒狀態，且無法區分相關情感的細微差異。現有資料集常使用未經控制的圖像，面部遮擋且缺乏人口多樣性，存在偏差風險。為了解決這些問題，我們推出EmoNet Face，一個全面的評估基準。EmoNet Face包含：(1) 一個新穎的40類情感分類法，精確捕捉人類情感體驗的細節。(2) 三個大規模、AI生成的資料集，具有明確的完整面部表情，並控制了族裔、年齡和性別的人口平衡。(3) 嚴格的多專家標注，用於訓練和高保真評估。(4) 我們構建了Empathic Insight Face模型，在我們的基準上實現了人類專家級別的性能。公開發布的EmoNet Face套件，包含分類法、資料集和模型，為開發和評估能更深入理解人類情感的人工智慧系統提供了堅實的基礎。", "audio": "audios/2505.20033v1.mp3", "timestamp": "2025-05-27T22:17:37.358301"}
{"query": "Foundation Model", "id": "2505.18930v1", "url": "http://arxiv.org/abs/2505.18930v1", "title": "WeedNet: A Foundation Model-Based Global-to-Local AI Approach for Real-Time Weed Species Identification and Classification", "summary": "Early identification of weeds is essential for effective management and\ncontrol, and there is growing interest in automating the process using computer\nvision techniques coupled with AI methods. However, challenges associated with\ntraining AI-based weed identification models, such as limited expert-verified\ndata and complexity and variability in morphological features, have hindered\nprogress. To address these issues, we present WeedNet, the first global-scale\nweed identification model capable of recognizing an extensive set of weed\nspecies, including noxious and invasive plant species. WeedNet is an end-to-end\nreal-time weed identification pipeline and uses self-supervised learning,\nfine-tuning, and enhanced trustworthiness strategies. WeedNet achieved 91.02%\naccuracy across 1,593 weed species, with 41% species achieving 100% accuracy.\nUsing a fine-tuning strategy and a Global-to-Local approach, the local Iowa\nWeedNet model achieved an overall accuracy of 97.38% for 85 Iowa weeds, most\nclasses exceeded a 90% mean accuracy per class. Testing across intra-species\ndissimilarity (developmental stages) and inter-species similarity (look-alike\nspecies) suggests that diversity in the images collected, spanning all the\ngrowth stages and distinguishable plant characteristics, is crucial in driving\nmodel performance. The generalizability and adaptability of the Global WeedNet\nmodel enable it to function as a foundational model, with the Global-to-Local\nstrategy allowing fine-tuning for region-specific weed communities. Additional\nvalidation of drone- and ground-rover-based images highlights the potential of\nWeedNet for integration into robotic platforms. Furthermore, integration with\nAI for conversational use provides intelligent agricultural and ecological\nconservation consulting tools for farmers, agronomists, researchers, land\nmanagers, and government agencies across diverse landscapes.", "authors": ["Yanben Shen", "Timilehin T. Ayanlade", "Venkata Naresh Boddepalli", "Mojdeh Saadati", "Ashlyn Rairdin", "Zi K. Deng", "Muhammad Arbab Arshad", "Aditya Balu", "Daren Mueller", "Asheesh K Singh", "Wesley Everman", "Nirav Merchant", "Baskar Ganapathysubramanian", "Meaghan Anderson", "Soumik Sarkar", "Arti Singh"], "published_date": "2025-05-25", "title_zh": "WeedNet：基於基礎模型的雜草物種即時辨識與分類之由全域至區域人工智慧方法", "summary_zh": "及早識別雜草對於有效管理至關重要。利用電腦視覺結合人工智慧自動化此流程的興趣日益濃厚。然而，專家驗證數據有限以及形態特徵複雜多變等因素，阻礙了基於人工智慧的雜草識別模型的發展。為了解決這些問題，本研究提出了 WeedNet，首個全球規模的雜草識別模型，能夠辨識多種雜草，包含有害和入侵植物。WeedNet 是一個端到端即時雜草識別流程，採用自我監督學習、微調和強化可信度策略。WeedNet 在 1593 種雜草上的準確率達到 91.02%，其中 41% 的物種達到 100% 的準確率。透過微調策略和「全球到本地」方法，本地愛荷華 WeedNet 模型在 85 種愛荷華雜草上的整體準確率達到 97.38%，大多數類別的平均準確率超過 90%。跨物種內差異（發育階段）和物種間相似性（外觀相似物種）的測試表明，涵蓋所有生長階段和可區分植物特徵的多樣化圖像對於提升模型性能至關重要。全球 WeedNet 模型的泛化性和適應性使其能夠作為基礎模型發揮作用，「全球到本地」策略使其能夠針對特定區域的雜草群落進行微調。基於無人機和地面巡檢車圖像的額外驗證突顯了 WeedNet 整合到機器人平台的潛力。此外，與會話式人工智慧的整合為不同景觀的農民、農藝師、研究人員、土地管理者和政府機構提供了智慧農業和生態保護諮詢工具。", "audio": "audios/2505.18930v1.mp3", "timestamp": "2025-05-27T22:17:56.721800"}
{"query": "Diffusion Model", "id": "2505.19595v1", "url": "http://arxiv.org/abs/2505.19595v1", "title": "Accelerating Diffusion-based Text-to-Speech Model Training with Dual Modality Alignment", "summary": "The goal of this paper is to optimize the training process of diffusion-based\ntext-to-speech models. While recent studies have achieved remarkable\nadvancements, their training demands substantial time and computational costs,\nlargely due to the implicit guidance of diffusion models in learning complex\nintermediate representations. To address this, we propose A-DMA, an effective\nstrategy for Accelerating training with Dual Modality Alignment. Our method\nintroduces a novel alignment pipeline leveraging both text and speech\nmodalities: text-guided alignment, which incorporates contextual\nrepresentations, and speech-guided alignment, which refines semantic\nrepresentations. By aligning hidden states with discriminative features, our\ntraining scheme reduces the reliance on diffusion models for learning complex\nrepresentations. Extensive experiments demonstrate that A-DMA doubles the\nconvergence speed while achieving superior performance over baselines. Code and\ndemo samples are available at: https://github.com/ZhikangNiu/A-DMA", "authors": ["Jeongsoo Choi", "Zhikang Niu", "Ji-Hoon Kim", "Chunhui Wang", "Joon Son Chung", "Chen Xie"], "published_date": "2025-05-26", "title_zh": "基於雙模態對齊加速擴散文本轉語音模型訓練", "summary_zh": "本研究旨在優化基於擴散模型的文字轉語音訓練過程。 鑑於現有方法訓練耗時且計算成本高昂，主要源於擴散模型在學習複雜中間表示時的隱式引導，我們提出A-DMA，一種利用雙模態對齊加速訓練的有效策略。 A-DMA引入新的對齊流程，結合文本和語音模態：文本引導對齊，融入上下文表示；語音引導對齊，精煉語義表示。 透過將隱藏狀態與判別性特徵對齊，該訓練方案降低了對擴散模型學習複雜表示的依賴。 實驗結果表明，A-DMA能使收斂速度加倍，並取得優於基準模型的性能。 代碼和演示樣本可在https://github.com/ZhikangNiu/A-DMA取得。", "audio": "audios/2505.19595v1.mp3", "timestamp": "2025-05-27T22:18:06.392286"}
{"query": "AI", "id": "2505.20013v1", "url": "http://arxiv.org/abs/2505.20013v1", "title": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback", "summary": "Web agents powered by Large Language Models (LLMs) show promise for\nnext-generation AI, but their limited reasoning in uncertain, dynamic web\nenvironments hinders robust deployment. In this paper, we identify key\nreasoning skills essential for effective web agents, i.e., reflection &\nlookahead, branching, and rollback, and curate trajectory data that exemplifies\nthese abilities by reconstructing the agent's (inference-time) reasoning\nalgorithms into chain-of-thought rationales. We conduct experiments in the\nagent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling\nsalient reasoning patterns into the backbone LLM via simple fine-tuning can\nsubstantially enhance its performance. Our approach yields significant\nimprovements across multiple benchmarks, including WebVoyager, Mind2web-live,\nand SimpleQA (web search), highlighting the potential of targeted reasoning\nskill enhancement for web agents.", "authors": ["Minda Hu", "Tianqing Fang", "Jianshu Zhang", "Junyu Ma", "Zhisong Zhang", "Jingyan Zhou", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Irwin King"], "published_date": "2025-05-26", "title_zh": "WebCoT：藉由在反思、分支與回滾中重構思維鏈來強化Web代理的推理能力", "summary_zh": "基於大型語言模型(LLM)的網路代理展現了下一代人工智慧的潛力，但其在不確定且動態的網路環境中有限的推理能力阻礙了其穩健部署。本文識別了有效網路代理所需的關鍵推理技能，包括反思與前瞻、分支和回滾，並透過將代理的(推理時)推理演算法重構為思維鏈式原理，來整理體現這些能力的軌跡數據。在代理自我改進基準OpenWebVoyager中進行的實驗表明，透過簡單的微調將顯著的推理模式提煉到主幹LLM中，可以大幅提升其性能。該方法在多個基準測試中產生了顯著的改進，包括WebVoyager、Mind2web-live和SimpleQA(網路搜尋)，突顯了針對網路代理的定向推理技能提升的潛力。", "audio": "audios/2505.20013v1.mp3", "timestamp": "2025-05-27T23:17:05.068752"}
{"query": "Foundation Model", "id": "2505.18881v1", "url": "http://arxiv.org/abs/2505.18881v1", "title": "SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes", "summary": "We present the Semantics-aware Dataset and Benchmark Generation Pipeline for\nOpen-vocabulary Object Navigation in Dynamic Scenes (SD-OVON). It utilizes\npretraining multimodal foundation models to generate infinite unique\nphoto-realistic scene variants that adhere to real-world semantics and daily\ncommonsense for the training and the evaluation of navigation agents,\naccompanied with a plugin for generating object navigation task episodes\ncompatible to the Habitat simulator. In addition, we offer two pre-generated\nobject navigation task datasets, SD-OVON-3k and SD-OVON-10k, comprising\nrespectively about 3k and 10k episodes of the open-vocabulary object navigation\ntask, derived from the SD-OVON-Scenes dataset with 2.5k photo-realistic scans\nof real-world environments and the SD-OVON-Objects dataset with 0.9k manually\ninspected scanned and artist-created manipulatable object models. Unlike prior\ndatasets limited to static environments, SD-OVON covers dynamic scenes and\nmanipulatable objects, facilitating both real-to-sim and sim-to-real robotic\napplications. This approach enhances the realism of navigation tasks, the\ntraining and the evaluation of open-vocabulary object navigation agents in\ncomplex settings. To demonstrate the effectiveness of our pipeline and\ndatasets, we propose two baselines and evaluate them along with\nstate-of-the-art baselines on SD-OVON-3k. The datasets, benchmark and source\ncode are publicly available.", "authors": ["Dicong Qiu", "Jiadi You", "Zeying Gong", "Ronghe Qiu", "Hui Xiong", "Junwei Liang"], "published_date": "2025-05-24", "title_zh": "SD-OVON：動態場景下開放詞彙物件導航的語義感知資料集與基準生成流程", "summary_zh": "本研究提出語義感知資料集與基準生成流程SD-OVON，用於動態場景中的開放詞彙物件導航。它利用預訓練多模態基礎模型，生成符合現實世界語義及常識的無限獨特且擬真的場景變體，以訓練和評估導航代理，並提供生成物件導航任務的插件，與Habitat模擬器相容。此外，我們提供兩個預先生成的物件導航任務資料集SD-OVON-3k和SD-OVON-10k，分別包含約3千和1萬個開放詞彙物件導航任務情節，源自SD-OVON-Scenes資料集（包含2.5千個現實環境擬真掃描）以及SD-OVON-Objects資料集（包含0.9千個經人工檢查的掃描和藝術家創建的可操作物件模型）。與僅限於靜態環境的先前資料集不同，SD-OVON涵蓋動態場景和可操作物件，促進真實到模擬及模擬到真實的機器人應用，增強導航任務的真實感，以及在複雜環境中開放詞彙物件導航代理的訓練和評估。為展示流程和資料集的有效性，我們提出兩個基準線，並與最先進的基準線在SD-OVON-3k上進行評估。資料集、基準和原始碼均公開可用。", "audio": "audios/2505.18881v1.mp3", "timestamp": "2025-05-27T23:17:13.030091"}
{"query": "Diffusion Model", "id": "2505.19552v1", "url": "http://arxiv.org/abs/2505.19552v1", "title": "On scalable and efficient training of diffusion samplers", "summary": "We address the challenge of training diffusion models to sample from\nunnormalized energy distributions in the absence of data, the so-called\ndiffusion samplers. Although these approaches have shown promise, they struggle\nto scale in more demanding scenarios where energy evaluations are expensive and\nthe sampling space is high-dimensional. To address this limitation, we propose\na scalable and sample-efficient framework that properly harmonizes the powerful\nclassical sampling method and the diffusion sampler. Specifically, we utilize\nMonte Carlo Markov chain (MCMC) samplers with a novelty-based auxiliary energy\nas a Searcher to collect off-policy samples, using an auxiliary energy function\nto compensate for exploring modes the diffusion sampler rarely visits. These\noff-policy samples are then combined with on-policy data to train the diffusion\nsampler, thereby expanding its coverage of the energy landscape. Furthermore,\nwe identify primacy bias, i.e., the preference of samplers for early experience\nduring training, as the main cause of mode collapse during training, and\nintroduce a periodic re-initialization trick to resolve this issue. Our method\nsignificantly improves sample efficiency on standard benchmarks for diffusion\nsamplers and also excels at higher-dimensional problems and real-world\nmolecular conformer generation.", "authors": ["Minkyu Kim", "Kiyoung Seong", "Dongyeop Woo", "Sungsoo Ahn", "Minsu Kim"], "published_date": "2025-05-26", "title_zh": "關於擴展且高效的擴散採樣器訓練", "summary_zh": "本研究旨在解決無資料情況下，訓練擴散模型以從非標準化能量分佈中抽樣的挑戰。為了解決能量評估成本高昂及抽樣空間高維度問題，我們提出一種可擴展且具樣本效率的框架，協調傳統抽樣方法與擴散抽樣器。具體而言，利用基於新穎性的輔助能量的蒙地卡羅馬可夫鏈抽樣器作為搜索器，收集離策略樣本，並使用輔助能量函數補償擴散抽樣器罕見的模式探索。這些離策略樣本與在策略資料結合，用於訓練擴散抽樣器，從而擴大其對能量景觀的覆蓋範圍。此外，我們發現首因效應是訓練期間模式崩潰的主因，並引入週期性重新初始化技巧來解決此問題。該方法顯著提升擴散抽樣器的樣本效率，並在高維度問題和真實世界分子構象生成中表現出色。", "audio": "audios/2505.19552v1.mp3", "timestamp": "2025-05-27T23:17:18.919116"}
{"query": "AI", "id": "2505.20148v2", "url": "http://arxiv.org/abs/2505.20148v2", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "published_date": "2025-05-26", "title_zh": "礦建任意：開放世界AI代理空間規劃基準測試", "summary_zh": "空間規劃是空間智能的關鍵，需理解並規劃物體在空間中的排列。具備空間規劃能力的人工智慧體能更好地適應機器人操作、自動組裝、城市規劃等現實應用。近期研究試圖構建基準來評估多模態大型語言模型（MLLM）的空間智能，但這些基準主要側重於基於典型視覺問答（VQA）形式的空間推理，抽象空間理解與具體任務執行間存在差距。本研究進一步構建名為MineAnyBuild的綜合基準，旨在評估開放世界人工智慧體在Minecraft遊戲中的空間規劃能力。MineAnyBuild要求人工智慧體根據多模態人類指令生成可執行的建築規劃，包含4000個精心設計的空間規劃任務，並提供利用豐富玩家生成內容無限擴展數據收集的範例。MineAnyBuild透過空間理解、空間推理、創造力及空間常識四個核心維度評估空間規劃能力。基於MineAnyBuild，我們對現有基於MLLM的人工智慧體進行全面評估，揭示其在空間規劃能力方面的嚴重局限性與巨大潛力。MineAnyBuild將為空間智能的評估開闢新途徑，並促進具備空間規劃能力的開放世界人工智慧體的進一步發展。", "audio": "audios/2505.20148v2.mp3", "timestamp": "2025-05-28T01:27:09.959867"}
{"query": "Foundation Model", "id": "2505.19888v2", "url": "http://arxiv.org/abs/2505.19888v2", "title": "Generalized and Personalized Federated Learning with Foundation Models via Orthogonal Transformations", "summary": "Federated Learning (FL) aims to train models across decentralized clients or\ndevices holding local data without the need for centralized data collection,\nthus enhancing data privacy and security. However, achieving both\ngeneralization and personalization in heterogeneous settings remains a\nsignificant challenge. To address this, we introduce FedOT, a novel approach\nthat leverages black-box foundation models. FedOT shares only a global\ntask-dependent classifier across clients while locally adapting features\nthrough orthogonal transformations. By enforcing orthogonality, FedOT mitigates\ngradient conflicts across diverse clients, preserves semantic integrity, and\nachieves robust performance even in the presence of substantial data\nheterogeneity. The strategy of combining global and local parameters enables a\nmore balanced approach for both generalization and personalization,\noutperforming baseline FL methods across multiple benchmarks. Furthermore, our\nextensive analysis confirms that joint optimization of global classifiers and\nlocal orthogonal transformations yields superior performance and suggests\nbroader applicability.", "authors": ["Eun Gyung Kong", "Je Won Yeom", "Yonghoon Jeon", "Taesup Kim"], "published_date": "2025-05-26", "title_zh": "基於正交變換的基礎模型廣義與個性化聯邦學習", "summary_zh": "聯邦學習旨在分散式客戶端或設備上訓練模型，無需集中數據收集，從而增強數據隱私和安全。在異構環境中實現泛化和個性化仍是一大挑戰。本研究提出FedOT，一種利用黑盒基礎模型的新方法。FedOT僅在客戶端之間共享全局任務相關分類器，並通過正交變換局部調整特徵。正交性降低了不同客戶端之間的梯度衝突，保持了語義完整性，並在數據高度異構的情況下實現了穩健的性能。全局和局部參數的結合，在泛化和個性化之間取得更平衡的方法，優於多個基準測試中的基準聯邦學習方法。對全局分類器和局部正交變換的聯合優化，可獲得卓越性能，並具有廣泛的適用性。", "audio": "audios/2505.19888v2.mp3", "timestamp": "2025-05-28T01:27:18.417490"}
{"query": "Diffusion Model", "id": "2505.19751v2", "url": "http://arxiv.org/abs/2505.19751v2", "title": "SAIL: Self-supervised Albedo Estimation from Real Images with a Latent Diffusion Model", "summary": "Intrinsic image decomposition aims at separating an image into its underlying\nalbedo and shading components, isolating the base color from lighting effects\nto enable downstream applications such as virtual relighting and scene editing.\nDespite the rise and success of learning-based approaches, intrinsic image\ndecomposition from real-world images remains a significant challenging task due\nto the scarcity of labeled ground-truth data. Most existing solutions rely on\nsynthetic data as supervised setups, limiting their ability to generalize to\nreal-world scenes. Self-supervised methods, on the other hand, often produce\nalbedo maps that contain reflections and lack consistency under different\nlighting conditions. To address this, we propose SAIL, an approach designed to\nestimate albedo-like representations from single-view real-world images. We\nrepurpose the prior knowledge of a latent diffusion model for unconditioned\nscene relighting as a surrogate objective for albedo estimation. To extract the\nalbedo, we introduce a novel intrinsic image decomposition fully formulated in\nthe latent space. To guide the training of our latent diffusion model, we\nintroduce regularization terms that constrain both the lighting-dependent and\nindependent components of our latent image decomposition. SAIL predicts stable\nalbedo under varying lighting conditions and generalizes to multiple scenes,\nusing only unlabeled multi-illumination data available online.", "authors": ["Hala Djeghim", "Nathan Piasco", "Luis Roldão", "Moussab Bennehar", "Dzmitry Tsishkou", "Céline Loscos", "Désiré Sidibé"], "published_date": "2025-05-26", "title_zh": "SAIL：基於潛在擴散模型的真實圖像自監督反照率估計", "summary_zh": "本研究旨在解決真實世界圖像內在影像分解的難題，即將影像分離為反照率和陰影成分。由於缺乏標記數據，現有方法難以泛化。我們提出一種名為SAIL的方法，從單視角真實圖像估計類似反照率的表示。我們利用潛在擴散模型在無條件場景重照明方面的先驗知識，作為反照率估計的替代目標。通過在潛在空間中構建內在影像分解，並引入正則化項約束光照相關和不相關成分，SAIL能夠預測在不同光照條件下穩定的反照率，並泛化到多個場景，僅使用網路上未標記的多重光照數據。", "audio": "audios/2505.19751v2.mp3", "timestamp": "2025-05-28T01:27:23.691088"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：視覺語言模型中多視角空間定位評估", "summary_zh": "視覺語言模型在理解和推理視覺內容方面表現出色，但在跨視點理解和空間推理方面仍面臨挑戰。目前模型擅長以自我為中心的空間推理，但難以推廣至以他人為中心的視點。我們提出了 ViewSpatial-Bench，首個專為多視點空間定位識別評估而設計的綜合基準，包含五種任務類型，並透過自動化 3D 標註流程生成精確的方向標籤。對多種視覺語言模型在 ViewSpatial-Bench 上的評估顯示，模型在相機視角任務中表現尚可，但在以人為視角的推理中準確度降低。透過在多視角空間資料集上微調模型，整體性能提升了 46.24%，驗證了我們方法的有效性。此研究為具身人工智慧系統的空間智慧建立了關鍵基準，並提供經驗證據表明建模 3D 空間關係可增強視覺語言模型的空間理解能力。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T03:12:48.504476"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "休謨：在視覺-語言-行動模型中引入系統二思維", "summary_zh": "人類在執行複雜實體任務前會進行深思熟慮。此思維模式近期顯著提升大型語言模型在數位領域解決複雜問題的能力。然而，機器人基礎模型與實體世界互動時，深思熟慮的潛力尚未充分開發。本研究提出Hume，一種雙系統視覺-語言-動作(VLA)模型，具備價值導向的系統二思維和級聯動作去噪，探索VLA模型在靈巧機器人控制中類似人類的思維能力。Hume的系統二透過新型價值查詢頭擴展VLA模型骨幹，估算預測動作的狀態-動作價值，進而執行價值導向思維，反覆抽樣多個動作候選，並根據狀態-動作價值選擇其一。Hume的系統一是輕量級反應式視覺運動策略，接收系統二選擇的動作，並執行級聯動作去噪以實現靈巧機器人控制。部署時，系統二以低頻率進行價值導向思維，而系統一非同步接收系統二選擇的動作候選，並即時預測流暢動作。實驗證明，Hume在多個模擬基準和真實機器人部署中，優於現有的最先進VLA模型。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T03:12:56.184034"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：雜訊誘導式佈局用於多主體生成", "summary_zh": "現有文生圖擴散模型難以生成多個不同主體。複雜提示詞常導致主體洩漏，造成數量、屬性和視覺特徵不準確。避免主體洩漏需要掌握各主體空間位置。近期方法透過外部佈局控制提供這些位置，但強制執行此類佈局常與初始噪聲決定的固有佈局衝突，導致與模型先驗不符。本文提出一種新方法，預測與提示詞對齊且源自初始噪聲的空間佈局，並在去噪過程中精煉它。藉由依賴此噪聲誘導佈局，避免與外部強制佈局衝突，更好保留模型先驗。本方法採用小型神經網路預測和精煉每個去噪步驟中演化的噪聲誘導佈局，確保主體間邊界清晰，同時維持一致性。實驗結果表明，相較於現有佈局引導技術，此噪聲對齊策略實現了更佳的文本圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T03:13:02.597159"}
{"query": "AI", "id": "2505.21486v1", "url": "http://arxiv.org/abs/2505.21486v1", "title": "Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming", "summary": "Automating robust hypothesis generation in open environments is pivotal for\nAI cognition. We introduce a novel framework integrating a multi-agent system,\npowered by Large Language Models (LLMs), with Inductive Logic Programming\n(ILP). Our system's LLM agents autonomously define a structured symbolic\nvocabulary (predicates) and relational templates , i.e., \\emph{language bias}\ndirectly from raw textual data. This automated symbolic grounding (the\nconstruction of the language bias), traditionally an expert-driven bottleneck\nfor ILP, then guides the transformation of text into facts for an ILP solver,\nwhich inductively learns interpretable rules. This approach overcomes\ntraditional ILP's reliance on predefined symbolic structures and the\nnoise-sensitivity of pure LLM methods. Extensive experiments in diverse,\nchallenging scenarios validate superior performance, paving a new path for\nautomated, explainable, and verifiable hypothesis generation.", "authors": ["Yang Yang", "Jiemin Wu", "Yutao Yue"], "published_date": "2025-05-27", "title_zh": "穩健假說生成：基於大型語言模型的歸納邏輯程式設計語言偏置自動化", "summary_zh": "本研究提出一種新型框架，藉由大型語言模型驅動的多代理人系統與歸納邏輯程式設計相結合，實現開放環境中穩健的假設自動生成。該系統的語言模型代理人能自主地從原始文本數據中定義結構化符號詞彙（謂詞）與關係模板，即語言偏置。這種自動化的符號基礎（語言偏置的構建）傳統上是歸納邏輯程式設計中由專家驅動的瓶頸，現在可引導文本轉換為歸納邏輯程式求解器的素材，進而歸納學習可解釋的規則。此方法克服了傳統歸納邏輯程式設計對預定義符號結構的依賴，以及純粹語言模型方法對雜訊的敏感性。在多樣且具挑戰性的場景下進行的大量實驗驗證了其卓越性能，為自動、可解釋和可驗證的假設生成開闢了新途徑。", "audio": "audios/2505.21486v1.mp3", "timestamp": "2025-05-28T04:23:32.065216"}
{"query": "Foundation Model", "id": "2505.21382v1", "url": "http://arxiv.org/abs/2505.21382v1", "title": "DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models", "summary": "Low-Rank Adaptation (LoRA) has emerged as one of the most effective,\ncomputationally tractable fine-tuning approaches for training Vision-Language\nModels (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by\nfreezing the pre-trained model weights and injecting trainable low-rank\nmatrices, allowing for efficient learning of these foundation models even on\nedge devices. However, LoRA in decentralized settings still remains under\nexplored, particularly for the theoretical underpinnings due to the lack of\nsmoothness guarantee and model consensus interference (defined formally below).\nThis work improves the convergence rate of decentralized LoRA (DLoRA) to match\nthe rate of decentralized SGD by ensuring gradient smoothness. We also\nintroduce DeCAF, a novel algorithm integrating DLoRA with truncated singular\nvalue decomposition (TSVD)-based matrix factorization to resolve consensus\ninterference. Theoretical analysis shows TSVD's approximation error is bounded\nand consensus differences between DLoRA and DeCAF vanish as rank increases,\nyielding DeCAF's matching convergence rate. Extensive experiments across\nvision/language tasks demonstrate our algorithms outperform local training and\nrivals federated learning under both IID and non-IID data distributions.", "authors": ["Nastaran Saadati", "Zhanhong Jiang", "Joshua R. Waite", "Shreyan Ganguly", "Aditya Balu", "Chinmay Hegde", "Soumik Sarkar"], "published_date": "2025-05-27", "title_zh": "DeCAF：用於基礎模型低秩適應的分散式共識與分解", "summary_zh": "低秩適應(LoRA)已成為訓練視覺語言模型(VLM)和大型語言模型(LLM)最有效且計算可行的微調方法之一。LoRA凍結預訓練模型權重並注入可訓練的低秩矩陣，從而在邊緣設備上實現基礎模型的高效學習。然而，分散式環境中的LoRA仍未充分探索，尤其在理論基礎方面，原因在於缺乏平滑性保證和模型共識干擾。本研究透過確保梯度平滑性，改進分散式LoRA(DLoRA)的收斂速度，使其與分散式SGD的速度相匹配。此外，我們提出DeCAF，一種將DLoRA與基於截斷奇異值分解(TSVD)的矩陣分解相結合的新演算法，以解決共識干擾。理論分析表明，TSVD的近似誤差有界，且隨著秩的增加，DLoRA和DeCAF之間的共識差異消失，從而使DeCAF的收斂速度與之匹配。跨視覺/語言任務的大量實驗表明，我們的演算法優於本地訓練，且在獨立同分布(IID)和非獨立同分布(non-IID)資料分佈下均可與聯邦學習相媲美。", "audio": "audios/2505.21382v1.mp3", "timestamp": "2025-05-28T04:23:40.857751"}
{"query": "Diffusion Model", "id": "2505.21469v1", "url": "http://arxiv.org/abs/2505.21469v1", "title": "PropMolFlow: Property-guided Molecule Generation with Geometry-Complete Flow Matching", "summary": "Molecule generation is advancing rapidly in chemical discovery and drug\ndesign. Flow matching methods have recently set the state of the art (SOTA) in\nunconditional molecule generation, surpassing score-based diffusion models.\nHowever, diffusion models still lead in property-guided generation. In this\nwork, we introduce PropMolFlow, a novel approach for property-guided molecule\ngeneration based on geometry-complete SE(3)-equivariant flow matching.\nIntegrating five different property embedding methods with a Gaussian expansion\nof scalar properties, PropMolFlow outperforms previous SOTA diffusion models in\nconditional molecule generation across various properties while preserving the\nstability and validity of the generated molecules, consistent with its\nunconditional counterpart. Additionally, it enables faster inference with\nsignificantly fewer time steps compared to baseline models. We highlight the\nimportance of validating the properties of generated molecules through DFT\ncalculations performed at the same level of theory as the training data.\nSpecifically, our analysis identifies properties that require DFT validation\nand others where a pretrained SE(3) geometric vector perceptron regressors\nprovide sufficiently accurate predictions on generated molecules. Furthermore,\nwe introduce a new property metric designed to assess the model's ability to\npropose molecules with underrepresented property values, assessing its capacity\nfor out-of-distribution generalization. Our findings reveal shortcomings in\nexisting structural metrics, which mistakenly validate open-shell molecules or\nmolecules with invalid valence-charge configurations, underscoring the need for\nimproved evaluation frameworks. Overall, this work paves the way for developing\ntargeted property-guided generation methods, enhancing the design of molecular\ngenerative models for diverse applications.", "authors": ["Cheng Zeng", "Jirui Jin", "George Karypis", "Mark Transtrum", "Ellad B. Tadmor", "Richard G. Hennig", "Adrian Roitberg", "Stefano Martiniani", "Mingjie Liu"], "published_date": "2025-05-27", "title_zh": "PropMolFlow：基於幾何完整流匹配的性質導向分子生成", "summary_zh": "分子生成技術在化學發現和藥物設計領域快速發展。Flow matching方法在無條件分子生成方面已超越基於分數的擴散模型，達到最先進水平。然而，擴散模型在屬性導向生成方面仍具領先地位。本研究提出PropMolFlow，一種基於幾何完整SE(3)等變Flow matching的屬性導向分子生成新方法。PropMolFlow整合五種不同的屬性嵌入方法和標量屬性的高斯擴展，在多種屬性條件下，其條件分子生成效能超越先前的最先進擴散模型，同時保持生成分子的穩定性和有效性，與其無條件生成能力一致。此外，相較於基準模型，PropMolFlow能以更少的時間步長實現更快的推論。我們強調，驗證生成分子的屬性至關重要，應透過與訓練資料相同理論水平的DFT計算進行驗證。我們的分析特別指出哪些屬性需要DFT驗證，以及哪些屬性可透過預訓練的SE(3)幾何向量感知器迴歸器提供足夠精確的預測。我們還提出一種新的屬性指標，旨在評估模型提出具有代表性不足的屬性值分子的能力，從而評估其分佈外泛化能力。研究結果揭示了現有結構指標的缺點，它們會錯誤地驗證開殼層分子或具有無效價電荷配置的分子，突顯了改進評估框架的必要性。總體而言，這項工作為開發有針對性的屬性導向生成方法鋪平了道路，從而加強了分子生成模型在多樣化應用中的設計。", "audio": "audios/2505.21469v1.mp3", "timestamp": "2025-05-28T04:23:50.297453"}
{"query": "AI", "id": "2505.21482v1", "url": "http://arxiv.org/abs/2505.21482v1", "title": "Tissue-specific predictive performance: A unified estimation and inference framework for multi-category screening tests", "summary": "Multi-Cancer Early Detection (MCED) testing with tissue localization aims to\ndetect and identify multiple cancer types from a single blood sample. Such\ntests have the potential to aid clinical decisions and significantly improve\nhealth outcomes. Despite this promise, MCED testing has not yet achieved\nregulatory approval, reimbursement or broad clinical adoption. One major reason\nfor this shortcoming is uncertainty about test performance resulting from the\nreporting of clinically obtuse metrics. Traditionally, MCED tests report\naggregate measures of test performance, disregarding cancer type, that obscure\nbiological variability and underlying differences in the test's behavior,\nlimiting insight into true effectiveness. Clinically informative evaluation of\nan MCED test's performance requires metrics that are specific to cancer types.\nIn the context of a case-control sampling design, this paper derives analytical\nmethods that estimate cancer-specific intrinsic accuracy, tissue localization\nreadout-specific predictive value and the marginal test classification\ndistribution, each with corresponding confidence interval formulae. A\nsimulation study is presented that evaluates performance of the proposed\nmethodology and provides guidance for implementation. An application to a\npublished MCED test dataset is given. These statistical approaches allow for\nestimation and inference for the pointed metric of an MCED test that allow its\nevaluation to support a potential role in early cancer detection. This\nframework enables more precise clinical decision-making, supports optimized\ntrial designs across classical, digital, AI-driven, and hybrid stratified\ndiagnostic screening platforms, and facilitates informed healthcare decisions\nby clinicians, policymakers, regulators, scientists, and patients.", "authors": ["A. Gregory DiRienzo", "Elie Massaad", "Hutan Ashrafian"], "published_date": "2025-05-27", "title_zh": "組織特異性預測效能：多類別篩檢試驗之統一估計與推論框架", "summary_zh": "多癌早期檢測(MCED)旨在透過單一血液樣本檢測並定位多種癌症。此類檢測具潛力輔助臨床決策並顯著改善健康結果，但因臨床指標不明確導致的效能不確定性，尚未獲得監管批准、報銷或廣泛應用。傳統上，MCED檢測報告忽略癌症類型、彙總的效能指標，掩蓋了生物變異性和檢測行為的潛在差異，限制了對真實效用的了解。本研究針對病例對照抽樣設計，推導出估計癌症特異性固有準確度、組織定位讀數特異性預測值及邊緣檢測分類分佈的分析方法，並提供相應的置信區間公式。研究透過模擬評估所提方法的效能，並給予實施指導，同時應用於已發表的MCED檢測數據集。此統計方法可估計和推論MCED檢測的關鍵指標，支持其在早期癌症檢測中的潛在作用。此框架可實現更精確的臨床決策，支持跨傳統、數位、AI驅動和混合分層診斷篩查平台的優化試驗設計，並促進臨床醫生、政策制定者、監管機構、科學家和患者知情的醫療保健決策。", "audio": "audios/2505.21482v1.mp3", "timestamp": "2025-05-28T05:18:38.826250"}
{"query": "Foundation Model", "id": "2505.21375v1", "url": "http://arxiv.org/abs/2505.21375v1", "title": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution", "summary": "Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data\nfor Earth observation but pose challenges for existing multimodal foundation\nmodels due to two key bottlenecks: (1) limited availability of UHR training\ndata, and (2) token explosion caused by the large image size. To address data\nscarcity, we introduce SuperRS-VQA (avg. 8,376$\\times$8,376) and HighRS-VQA\n(avg. 2,000$\\times$1,912), the highest-resolution vision-language datasets in\nRS to date, covering 22 real-world dialogue tasks. To mitigate token explosion,\nour pilot studies reveal significant redundancy in RS images: crucial\ninformation is concentrated in a small subset of object-centric tokens, while\npruning background tokens (e.g., ocean or forest) can even improve performance.\nMotivated by these findings, we propose two strategies: Background Token\nPruning and Anchored Token Selection, to reduce the memory footprint while\npreserving key semantics.Integrating these techniques, we introduce\nGeoLLaVA-8K, the first RS-focused multimodal large language model capable of\nhandling inputs up to 8K$\\times$8K resolution, built on the LLaVA framework.\nTrained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art\non the XLRS-Bench.", "authors": ["Fengxiang Wang", "Mingshuo Chen", "Yueying Li", "Di Wang", "Haotian Wang", "Zonghao Guo", "Zefan Wang", "Boqi Shan", "Long Lan", "Yulin Wang", "Hongzhen Wang", "Wenjing Yang", "Bo Du", "Jing Zhang"], "published_date": "2025-05-27", "title_zh": "GeoLLaVA-8K：遙感多模態大型語言模型擴展至8K解析度", "summary_zh": "超高解析度遙感影像為地球觀測提供寶貴資料，但因訓練資料有限及影像尺寸過大導致的token爆炸，對現有多模態基礎模型構成挑戰。為解決資料稀缺問題，本研究推出SuperRS-VQA和HighRS-VQA，目前解析度最高的遙感視覺語言數據集，涵蓋22個真實對話任務。為緩解token爆炸，研究發現遙感影像存在顯著冗餘，關鍵訊息集中在少數以物件為中心的token中，刪除背景token甚至能提升效能。基於此，提出背景token剪枝和錨定token選擇策略，以減少記憶體佔用並保留關鍵語義。整合這些技術，本研究推出GeoLLaVA-8K，首個專注於遙感且能處理高達8K解析度輸入的多模態大型語言模型，基於LLaVA框架構建。GeoLLaVA-8K在SuperRS-VQA和HighRS-VQA上訓練，於XLRS-Bench上達到最先進水準。", "audio": "audios/2505.21375v1.mp3", "timestamp": "2025-05-28T05:18:48.157765"}
{"query": "Diffusion Model", "id": "2505.21467v1", "url": "http://arxiv.org/abs/2505.21467v1", "title": "Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion", "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver up to a 34x\nend-to-end speedup without compromising accuracy. For the first time, diffusion\nlanguage models achieve a comparable and even faster latency as the widely\nadopted autoregressive models. Our work successfully paved the way for scaling\nup the diffusion language model to a broader scope of applications across\ndifferent domains.", "authors": ["Zhanqiu Hu", "Jian Meng", "Yash Akhauri", "Mohamed S. Abdelfattah", "Jae-sun Seo", "Zhiru Zhang", "Udit Gupta"], "published_date": "2025-05-27", "title_zh": "透過高效鍵值快取與導引擴散加速擴散語言模型推論", "summary_zh": "擴散語言模型具平行生成詞元與內建雙向性之優勢，可望提升序列建模效率。然現有模型推論速度慢，需多次完整序列正向傳播，導致高運算成本與延遲，長文本情境尤甚。平行生成亦引發詞元不連貫問題，降低降噪步數更導致品質下降。本文提出兩種免訓練技術：FreeCache重複利用穩定之鍵值投射，有效降低運算成本；Guided Diffusion則利用輕量級自迴歸模型指導詞元解遮蔽，大幅減少降噪迭代次數而不犧牲品質。實驗顯示，結合此二方法可實現高達34倍之端對端加速，且不損害準確性。擴散語言模型首次達到與自迴歸模型相當甚至更快的延遲，為其於各領域之廣泛應用奠定基礎。", "audio": "audios/2505.21467v1.mp3", "timestamp": "2025-05-28T05:18:55.177356"}
{"query": "AI", "id": "2505.21479v1", "url": "http://arxiv.org/abs/2505.21479v1", "title": "Are Language Models Consequentialist or Deontological Moral Reasoners?", "summary": "As AI systems increasingly navigate applications in healthcare, law, and\ngovernance, understanding how they handle ethically complex scenarios becomes\ncritical. Previous work has mainly examined the moral judgments in large\nlanguage models (LLMs), rather than their underlying moral reasoning process.\nIn contrast, we focus on a large-scale analysis of the moral reasoning traces\nprovided by LLMs. Furthermore, unlike prior work that attempted to draw\ninferences from only a handful of moral dilemmas, our study leverages over 600\ndistinct trolley problems as probes for revealing the reasoning patterns that\nemerge within different LLMs. We introduce and test a taxonomy of moral\nrationales to systematically classify reasoning traces according to two main\nnormative ethical theories: consequentialism and deontology. Our analysis\nreveals that LLM chains-of-thought tend to favor deontological principles based\non moral obligations, while post-hoc explanations shift notably toward\nconsequentialist rationales that emphasize utility. Our framework provides a\nfoundation for understanding how LLMs process and articulate ethical\nconsiderations, an important step toward safe and interpretable deployment of\nLLMs in high-stakes decision-making environments. Our code is available at\nhttps://github.com/keenansamway/moral-lens .", "authors": ["Keenan Samway", "Max Kleiman-Weiner", "David Guzman Piedrahita", "Rada Mihalcea", "Bernhard Schölkopf", "Zhijing Jin"], "published_date": "2025-05-27", "title_zh": "語言模型是結果論抑或義務論的道德推理者？", "summary_zh": "人工智慧系統於醫療、法律及治理等領域應用日增，其倫理複雜情境之處理至關重要。既有研究多探討大型語言模型之道德判斷，而非其深層道德推理過程。本研究著重於大規模分析語言模型所提供之道德推理軌跡，並利用逾六百個電車難題，揭示不同語言模型內之推理模式。我們導入並測試一套道德理由分類法，依據結果主義與義務論兩大規範倫理理論，系統性地分類推理軌跡。分析顯示，語言模型之思維鏈傾向基於道德義務之義務論原則，事後解釋則顯著轉向強調效用之結果主義理由。本框架為理解語言模型如何處理及闡述倫理考量奠定基礎，有助於在高度風險決策環境中安全且可解釋地部署語言模型。程式碼已公開於https://github.com/keenansamway/moral-lens。", "audio": "audios/2505.21479v1.mp3", "timestamp": "2025-05-28T06:27:20.603558"}
{"query": "Foundation Model", "id": "2505.21357v1", "url": "http://arxiv.org/abs/2505.21357v1", "title": "AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping", "summary": "Accurate crop mapping fundamentally relies on modeling multi-scale\nspatiotemporal patterns, where spatial scales range from individual field\ntextures to landscape-level context, and temporal scales capture both\nshort-term phenological transitions and full growing-season dynamics.\nTransformer-based remote sensing foundation models (RSFMs) offer promising\npotential for crop mapping due to their innate ability for unified\nspatiotemporal processing. However, current RSFMs remain suboptimal for crop\nmapping: they either employ fixed spatiotemporal windows that ignore the\nmulti-scale nature of crop systems or completely disregard temporal information\nby focusing solely on spatial patterns. To bridge these gaps, we present\nAgriFM, a multi-source remote sensing foundation model specifically designed\nfor agricultural crop mapping. Our approach begins by establishing the\nnecessity of simultaneous hierarchical spatiotemporal feature extraction,\nleading to the development of a modified Video Swin Transformer architecture\nwhere temporal down-sampling is synchronized with spatial scaling operations.\nThis modified backbone enables efficient unified processing of long time-series\nsatellite inputs. AgriFM leverages temporally rich data streams from three\nsatellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is\npre-trained on a global representative dataset comprising over 25 million image\nsamples supervised by land cover products. The resulting framework incorporates\na versatile decoder architecture that dynamically fuses these learned\nspatiotemporal representations, supporting diverse downstream tasks.\nComprehensive evaluations demonstrate AgriFM's superior performance over\nconventional deep learning approaches and state-of-the-art general-purpose\nRSFMs across all downstream tasks. Codes will be available at\nurlhttps://github.com/flyakon/AgriFM.", "authors": ["Wenyuan Li", "Shunlin Liang", "Keyan Chen", "Yongzhe Chen", "Han Ma", "Jianglei Xu", "Yichuan Ma", "Shikang Guan", "Husheng Fang", "Zhenwei Shi"], "published_date": "2025-05-27", "title_zh": "AgriFM：用於作物mapping的多源時序遙感基礎模型", "summary_zh": "精準的農作物地圖繪製仰賴多尺度時空模式的建立，其中空間尺度涵蓋田地紋理至景觀層級，時間尺度則包含短期物候變化及完整生長季動態。基於Transformer的遙感基礎模型(RSFM)具備統一時空處理的潛力，然現有模型對農作物地圖繪製仍有不足：或採用固定時空窗而忽略多尺度特性，或僅關注空間模式而忽略時間資訊。為此，我們提出AgriFM，一款專為農業地圖繪製設計的多源遙感基礎模型。本研究首先確立同步階層式時空特徵提取的必要性，並研發改良版的Video Swin Transformer架構，使時間降採樣與空間縮放操作同步。此架構能有效處理長時間序列衛星輸入。AgriFM利用MODIS、Landsat-8/9和Sentinel-2等衛星資料，並以包含超過兩千五百萬影像樣本的全球代表性資料集進行預訓練，該資料集受土地覆蓋產品監督。此框架整合了能動態融合學習到的時空表徵之多功能解碼器架構，支援多樣的下游任務。綜合評估顯示，AgriFM在所有下游任務中均優於傳統深度學習方法及最先進的通用RSFM。", "audio": "audios/2505.21357v1.mp3", "timestamp": "2025-05-28T06:27:28.068773"}
{"query": "Diffusion Model", "id": "2505.21437v1", "url": "http://arxiv.org/abs/2505.21437v1", "title": "CoDA: Coordinated Diffusion Noise Optimization for Whole-Body Manipulation of Articulated Objects", "summary": "Synthesizing whole-body manipulation of articulated objects, including body\nmotion, hand motion, and object motion, is a critical yet challenging task with\nbroad applications in virtual humans and robotics. The core challenges are\ntwofold. First, achieving realistic whole-body motion requires tight\ncoordination between the hands and the rest of the body, as their movements are\ninterdependent during manipulation. Second, articulated object manipulation\ntypically involves high degrees of freedom and demands higher precision, often\nrequiring the fingers to be placed at specific regions to actuate movable\nparts. To address these challenges, we propose a novel coordinated diffusion\nnoise optimization framework. Specifically, we perform noise-space optimization\nover three specialized diffusion models for the body, left hand, and right\nhand, each trained on its own motion dataset to improve generalization.\nCoordination naturally emerges through gradient flow along the human kinematic\nchain, allowing the global body posture to adapt in response to hand motion\nobjectives with high fidelity. To further enhance precision in hand-object\ninteraction, we adopt a unified representation based on basis point sets (BPS),\nwhere end-effector positions are encoded as distances to the same BPS used for\nobject geometry. This unified representation captures fine-grained spatial\nrelationships between the hand and articulated object parts, and the resulting\ntrajectories serve as targets to guide the optimization of diffusion noise,\nproducing highly accurate interaction motion. We conduct extensive experiments\ndemonstrating that our method outperforms existing approaches in motion quality\nand physical plausibility, and enables various capabilities such as object pose\ncontrol, simultaneous walking and manipulation, and whole-body generation from\nhand-only data.", "authors": ["Huaijin Pi", "Zhi Cen", "Zhiyang Dou", "Taku Komura"], "published_date": "2025-05-27", "title_zh": "CoDA：用於具體物件全身操控的協同擴散雜訊最佳化", "summary_zh": "本文提出一種新型協調擴散雜訊優化框架，用於合成人體對關節物體的全身操控動作，包括身體、手部和物體運動。此框架針對身體、左手和右手分別訓練擴散模型，並在雜訊空間進行優化以提升泛化能力。透過人體運動鏈的梯度流，身體姿勢能自然適應手部運動目標。為提高手部與物體互動的精確度，採用基於基點集（BPS）的統一表示，將末端執行器的位置編碼為與物體幾何相同的BPS距離。實驗結果表明，該方法在動作質量和物理合理性方面優於現有方法，並能實現物體姿態控制、同步行走與操控等功能。", "audio": "audios/2505.21437v1.mp3", "timestamp": "2025-05-28T06:27:33.068444"}
{"query": "AI", "id": "2505.21448v1", "url": "http://arxiv.org/abs/2505.21448v1", "title": "OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers", "summary": "Lip synchronization is the task of aligning a speaker's lip movements in\nvideo with corresponding speech audio, and it is essential for creating\nrealistic, expressive video content. However, existing methods often rely on\nreference frames and masked-frame inpainting, which limit their robustness to\nidentity consistency, pose variations, facial occlusions, and stylized content.\nIn addition, since audio signals provide weaker conditioning than visual cues,\nlip shape leakage from the original video will affect lip sync quality. In this\npaper, we present OmniSync, a universal lip synchronization framework for\ndiverse visual scenarios. Our approach introduces a mask-free training paradigm\nusing Diffusion Transformer models for direct frame editing without explicit\nmasks, enabling unlimited-duration inference while maintaining natural facial\ndynamics and preserving character identity. During inference, we propose a\nflow-matching-based progressive noise initialization to ensure pose and\nidentity consistency, while allowing precise mouth-region editing. To address\nthe weak conditioning signal of audio, we develop a Dynamic Spatiotemporal\nClassifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance\nstrength over time and space. We also establish the AIGC-LipSync Benchmark, the\nfirst evaluation suite for lip synchronization in diverse AI-generated videos.\nExtensive experiments demonstrate that OmniSync significantly outperforms prior\nmethods in both visual quality and lip sync accuracy, achieving superior\nresults in both real-world and AI-generated videos.", "authors": ["Ziqiao Peng", "Jiwen Liu", "Haoxian Zhang", "Xiaoqiang Liu", "Songlin Tang", "Pengfei Wan", "Di Zhang", "Hongyan Liu", "Jun He"], "published_date": "2025-05-27", "title_zh": "OmniSync：基於擴散Transformer的通用唇語同步方法", "summary_zh": "唇語同步旨在使影片中講者的唇部動作與對應的語音音訊對齊，這對創造逼真且具表現力的影片內容至關重要。現有方法常依賴參考幀和遮罩幀修復，限制了其在身份一致性、姿勢變化、面部遮擋和風格化內容方面的穩健性。此外，音訊信號的條件約束弱於視覺線索，原始影片的唇形洩漏會影響唇語同步品質。本文提出OmniSync，一種適用於多樣視覺場景的通用唇語同步框架。該方法採用無遮罩訓練範式，使用Diffusion Transformer模型進行直接幀編輯，無需顯式遮罩，從而實現無限時長的推論，同時保持自然的面部動態並保留角色身份。在推論階段，我們提出基於流匹配的漸進式噪聲初始化，以確保姿勢和身份一致性，並允許精確的口部區域編輯。為了解決音訊的弱條件約束信號問題，我們開發了一種動態時空無分類器引導（DS-CFG）機制，可隨時間和空間自適應地調整引導強度。我們還建立了AIGC-LipSync基準，這是首個針對多樣人工智慧生成影片中唇語同步的評估套件。大量實驗表明，OmniSync在視覺品質和唇語同步準確性方面均顯著優於先前方法，在真實世界和人工智慧生成影片中均取得了卓越成果。", "audio": "audios/2505.21448v1.mp3", "timestamp": "2025-05-28T07:18:32.119650"}
{"query": "Foundation Model", "id": "2505.21356v1", "url": "http://arxiv.org/abs/2505.21356v1", "title": "Towards Robust Automated Perceptual Voice Quality Assessment with Deep Learning", "summary": "Objective: Perceptual voice quality assessment plays a critical role in\ndiagnosing and monitoring voice disorders by providing standardized evaluation\nof vocal function. Traditionally, this process relies on expert raters\nutilizing standard scales, such as the Consensus Auditory-Perceptual Evaluation\nof Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain\n(GRBAS). However, these metrics are inherently subjective and susceptible to\ninter-rater variability, motivating the need for automated and objective\nassessment methods. Methods: We propose Voice Quality Assessment Network\n(VOQANet), a deep learning-based framework with an attention mechanism that\nleverages a Speech Foundation Model (SFM) to capture high-level acoustic and\nprosodic information from raw speech. To enhance robustness and\ninterpretability, we present VOQANet+, which integrates handcrafted acoustic\nfeatures such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM\nembeddings. Results: Sentence-based input yields stronger performance than\nvowel-based input, especially at the patient level. VOQANet consistently\noutperforms baseline methods in RMSE and PCC, while VOQANet+ performs even\nbetter and maintains robustness under noisy conditions. Conclusion: Combining\nSFM embeddings with domain-informed acoustic features improves interpretability\nand resilience. Significance: VOQANet+ shows strong potential for deployment in\nreal-world and telehealth settings, addressing the limitations of subjective\nperceptual assessments with an interpretable and noise-resilient solution.", "authors": ["Whenty Ariyanti", "Kuan-Yu Chen", "Sabato Marco Siniscalchi", "Hsin-Min Wang", "Yu Tsao"], "published_date": "2025-05-27", "title_zh": "基於深度學習的穩健自動感知語音品質評估", "summary_zh": "目標：知覺語音品質評估在診斷和監測語音障礙中至關重要，傳統上依賴專家使用CAPE-V和GRBAS等標準量表，但這些指標具主觀性且易受評估者差異影響，故需自動化客觀評估方法。方法：提出基於深度學習的VOQANet，其具備注意力機制，利用語音基礎模型(SFM)捕捉原始語音中的高階聲學和韻律資訊。為提升穩健性和可解釋性，VOQANet+整合了人工設計的聲學特徵，如抖動、閃爍和諧波雜訊比(HNR)與SFM嵌入。結果：基於句子的輸入優於基於母音的輸入，尤其是在患者層面。VOQANet在RMSE和PCC方面始終優於基準方法，VOQANet+表現更佳，並在嘈雜條件下保持穩健性。結論：結合SFM嵌入和領域知識的聲學特徵可提高可解釋性和彈性。意義：VOQANet+在實際應用和遠程醫療環境中展現出強大的潛力，以可解釋且抗噪的解決方案克服了主觀知覺評估的局限性。", "audio": "audios/2505.21356v1.mp3", "timestamp": "2025-05-28T07:18:41.016156"}
{"query": "Diffusion Model", "id": "2505.21426v1", "url": "http://arxiv.org/abs/2505.21426v1", "title": "Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks", "summary": "Agent-Based Models (ABMs) are powerful tools for studying emergent properties\nin complex systems. In ABMs, agent behaviors are governed by local interactions\nand stochastic rules. However, these rules are, in general, non-differentiable,\nlimiting the use of gradient-based methods for optimization, and thus\nintegration with real-world data. We propose a novel framework to learn a\ndifferentiable surrogate of any ABM by observing its generated data. Our method\ncombines diffusion models to capture behavioral stochasticity and graph neural\nnetworks to model agent interactions. Distinct from prior surrogate approaches,\nour method introduces a fundamental shift: rather than approximating\nsystem-level outputs, it models individual agent behavior directly, preserving\nthe decentralized, bottom-up dynamics that define ABMs. We validate our\napproach on two ABMs (Schelling's segregation model and a Predator-Prey\necosystem) showing that it replicates individual-level patterns and accurately\nforecasts emergent dynamics beyond training. Our results demonstrate the\npotential of combining diffusion models and graph learning for data-driven ABM\nsimulation.", "authors": ["Francesco Cozzi", "Marco Pangallo", "Alan Perotti", "André Panisson", "Corrado Monti"], "published_date": "2025-05-27", "title_zh": "基於圖擴散網絡的個體行為學習於基於代理模型中", "summary_zh": "基於代理人模型（ABMs）擅長研究複雜系統中的湧現特性。ABMs中，代理人的行為受局部互動和隨機規則支配，但這些規則通常不可微，限制了梯度方法的優化和與真實數據的整合。本文提出一種新穎框架，透過觀察ABM生成的數據，學習其可微分的替代模型。該方法結合擴散模型捕捉行為隨機性，並利用圖神經網路模擬代理人互動。不同於以往的替代方法，本文直接模擬個體代理人行為，而非近似系統層級的輸出，保留了定義ABM的去中心化、自下而上的動態特性。在Schelling隔離模型和捕食者-獵物生態系統兩個ABM上的驗證表明，該方法能複製個體層級的模式，並準確預測訓練範圍外的湧現動態。研究結果展示了結合擴散模型和圖學習進行數據驅動ABM模擬的潛力。", "audio": "audios/2505.21426v1.mp3", "timestamp": "2025-05-28T07:18:47.788390"}
{"query": "AI", "id": "2505.21445v1", "url": "http://arxiv.org/abs/2505.21445v1", "title": "VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin", "summary": "The performance of speaker verification systems is adversely affected by\nspeaker aging. However, due to challenges in data collection, particularly the\nlack of sustained and large-scale longitudinal data for individuals, research\non speaker aging remains difficult. In this paper, we present VoxAging, a\nlarge-scale longitudinal dataset collected from 293 speakers (226 English\nspeakers and 67 Mandarin speakers) over several years, with the longest time\nspan reaching 17 years (approximately 900 weeks). For each speaker, the data\nwere recorded at weekly intervals. We studied the phenomenon of speaker aging\nand its effects on advanced speaker verification systems, analyzed individual\nspeaker aging processes, and explored the impact of factors such as age group\nand gender on speaker aging research.", "authors": ["Zhiqi Ai", "Meixuan Bao", "Zhiyong Chen", "Zhi Yang", "Xinnuo Li", "Shugong Xu"], "published_date": "2025-05-27", "title_zh": "VoxAging：基於大型縱向語料庫的英漢雙語持續追蹤說話人聲紋老化", "summary_zh": "語者驗證系統效能受語者老化影響。然因數據蒐集挑戰，特別是缺乏個人長期大規模縱向數據，語者老化研究困難重重。本文提出VoxAging，一大型縱向數據集，包含293位語者(226位英語語者及67位華語語者)歷年數據，最長跨度達17年(約900週)。每位語者數據按週記錄。研究語者老化現象及其對先進語者驗證系統之影響，分析個別語者老化過程，並探討年齡層及性別等因素對語者老化研究之影響。", "audio": "audios/2505.21445v1.mp3", "timestamp": "2025-05-28T08:25:53.699498"}
{"query": "Foundation Model", "id": "2505.21322v1", "url": "http://arxiv.org/abs/2505.21322v1", "title": "Assured Autonomy with Neuro-Symbolic Perception", "summary": "Many state-of-the-art AI models deployed in cyber-physical systems (CPS),\nwhile highly accurate, are simply pattern-matchers.~With limited security\nguarantees, there are concerns for their reliability in safety-critical and\ncontested domains. To advance assured AI, we advocate for a paradigm shift that\nimbues data-driven perception models with symbolic structure, inspired by a\nhuman's ability to reason over low-level features and high-level context. We\npropose a neuro-symbolic paradigm for perception (NeuSPaPer) and illustrate how\njoint object detection and scene graph generation (SGG) yields deep scene\nunderstanding.~Powered by foundation models for offline knowledge extraction\nand specialized SGG algorithms for real-time deployment, we design a framework\nleveraging structured relational graphs that ensures the integrity of\nsituational awareness in autonomy. Using physics-based simulators and\nreal-world datasets, we demonstrate how SGG bridges the gap between low-level\nsensor perception and high-level reasoning, establishing a foundation for\nresilient, context-aware AI and advancing trusted autonomy in CPS.", "authors": ["R. Spencer Hallyburton", "Miroslav Pajic"], "published_date": "2025-05-27", "title_zh": "基於神經符號感知的可靠自主性", "summary_zh": "當前網路實體系統中的先進AI模型雖精準，但僅為模式匹配，缺乏安全保障，難以在安全攸關領域應用。為提升AI可靠性，我們提倡將符號結構融入數據驅動的感知模型，模擬人類基於低階特徵和高階語境的推理能力。我們提出神經符號感知範例NeuSPaPer，藉由物件偵測和場景圖生成(SGG)實現深度場景理解。此框架利用基礎模型離線提取知識，並以專業SGG演算法即時部署，構建結構化關係圖，確保自主系統情境感知的完整性。透過物理模擬器和真實數據集，我們證明SGG能彌合低階感測器感知和高階推理之間的差距，為具韌性且感知語境的AI奠定基礎，進而推進網路實體系統中可信賴的自主性。", "audio": "audios/2505.21322v1.mp3", "timestamp": "2025-05-28T08:25:59.720613"}
{"query": "Diffusion Model", "id": "2505.21400v1", "url": "http://arxiv.org/abs/2505.21400v1", "title": "A Convergence Theory for Diffusion Language Models: An Information-Theoretic Perspective", "summary": "Diffusion models have emerged as a powerful paradigm for modern generative\nmodeling, demonstrating strong potential for large language models (LLMs).\nUnlike conventional autoregressive (AR) models that generate tokens\nsequentially, diffusion models enable parallel token sampling, leading to\nfaster generation and eliminating left-to-right generation constraints. Despite\ntheir empirical success, the theoretical understanding of diffusion model\napproaches remains underdeveloped. In this work, we develop convergence\nguarantees for diffusion language models from an information-theoretic\nperspective. Our analysis demonstrates that the sampling error, measured by the\nKullback-Leibler (KL) divergence, decays inversely with the number of\niterations $T$ and scales linearly with the mutual information between tokens\nin the target text sequence. In particular, we establish matching upper and\nlower bounds, up to some constant factor, to demonstrate the tightness of our\nconvergence analysis. These results offer novel theoretical insights into the\npractical effectiveness of diffusion language models.", "authors": ["Gen Li", "Changxiao Cai"], "published_date": "2025-05-27", "title_zh": "擴散語言模型收斂理論：資訊理論視角", "summary_zh": "擴散模型已成為強大的生成模型範例，對大型語言模型展現巨大潛力。相較於依序生成詞元的傳統自迴歸模型，擴散模型能平行取樣詞元，加速生成並消除由左至右的生成限制。儘管實證上獲得成功，對擴散模型方法的理論理解仍不充分。本研究從資訊理論角度發展擴散語言模型的收斂性保證。分析表明，以Kullback-Leibler散度衡量的取樣誤差，與迭代次數T成反比衰減，並與目標文本序列中詞元間的互信息成線性比例。具體而言，我們建立了匹配的上下界（僅相差常數因子），以證明收斂性分析的嚴謹性。這些結果為擴散語言模型的實際有效性提供了新的理論見解。", "audio": "audios/2505.21400v1.mp3", "timestamp": "2025-05-28T08:26:05.057172"}
{"query": "AI", "id": "2505.21419v1", "url": "http://arxiv.org/abs/2505.21419v1", "title": "Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs", "summary": "Today's cloud-hosted applications and services are complex systems, and a\nperformance or functional instability can have dozens or hundreds of potential\nroot causes. Our hypothesis is that by combining the pattern matching\ncapabilities of modern AI tools with a natural multi-modal RAG LLM interface,\nproblem identification and resolution can be simplified. ARCA is a new\nmulti-modal RAG LLM system that targets this domain. Step-wise evaluations show\nthat ARCA outperforms state-of-the-art alternatives.", "authors": ["Yifan Wang", "Kenneth P. Birman"], "published_date": "2025-05-27", "title_zh": "利用多模態RAG LLM診斷與解決雲平台不穩定性", "summary_zh": "現今雲端應用程式及服務複雜，效能或功能不穩定可能有多種潛在根源。本研究假設，結合現代AI工具的模式匹配能力與自然多模態RAG LLM介面，可簡化問題識別與解決。ARCA為一種針對此領域的新型多模態RAG LLM系統。逐步評估顯示，ARCA優於現有技術。", "audio": "audios/2505.21419v1.mp3", "timestamp": "2025-05-28T09:20:24.090597"}
{"query": "Foundation Model", "id": "2505.21317v1", "url": "http://arxiv.org/abs/2505.21317v1", "title": "A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features", "summary": "Understanding cellular responses to stimuli is crucial for biological\ndiscovery and drug development. Transcriptomics provides interpretable,\ngene-level insights, while microscopy imaging offers rich predictive features\nbut is harder to interpret. Weakly paired datasets, where samples share\nbiological states, enable multimodal learning but are scarce, limiting their\nutility for training and multimodal inference. We propose a framework to\nenhance transcriptomics by distilling knowledge from microscopy images. Using\nweakly paired data, our method aligns and binds modalities, enriching gene\nexpression representations with morphological information. To address data\nscarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal\ndistillation using pretrained foundation models, achieving state-of-the-art\nresults, and (2) PEA (Perturbation Embedding Augmentation), a novel\naugmentation technique that enhances transcriptomics data while preserving\ninherent biological information. These strategies improve the predictive power\nand retain the interpretability of transcriptomics, enabling rich unimodal\nrepresentations for complex biological tasks.", "authors": ["Ihab Bendidi", "Yassir El Mesbahi", "Alisandra K. Denton", "Karush Suri", "Kian Kenyon-Dean", "Auguste Genovesio", "Emmanuel Noutahi"], "published_date": "2025-05-27", "title_zh": "一種透過形態特徵提升轉錄組學表徵的跨模態知識蒸餾與數據增強方法", "summary_zh": "理解細胞對刺激的反應對於生物學發現和藥物開發至關重要。轉錄體學提供可解釋的基因層面洞察，而顯微鏡成像提供豐富的預測特徵，但難以解釋。弱配對數據集（樣本共享生物學狀態）支持多模態學習，但稀缺性限制了其訓練和多模態推斷的效用。我們提出一個框架，通過從顯微鏡圖像中提取知識來增強轉錄體學。利用弱配對數據，該方法對齊並結合模態，利用形態學資訊豐富基因表達表示。為了解決數據稀缺問題，我們引入了(1) Semi-Clipped，一種改編自CLIP的跨模態蒸餾方法，利用預訓練基礎模型實現最先進的成果，以及(2) PEA（擾動嵌入增強），一種新型增強技術，可在保留固有生物學資訊的同時增強轉錄體學數據。這些策略提高了轉錄體學的預測能力並保留其可解釋性，從而為複雜的生物學任務提供豐富的單模態表示。", "audio": "audios/2505.21317v1.mp3", "timestamp": "2025-05-28T09:20:31.050157"}
{"query": "Diffusion Model", "id": "2505.21325v1", "url": "http://arxiv.org/abs/2505.21325v1", "title": "MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on", "summary": "Video Virtual Try-On (VVT) aims to simulate the natural appearance of\ngarments across consecutive video frames, capturing their dynamic variations\nand interactions with human body motion. However, current VVT methods still\nface challenges in terms of spatiotemporal consistency and garment content\npreservation. First, they use diffusion models based on the U-Net, which are\nlimited in their expressive capability and struggle to reconstruct complex\ndetails. Second, they adopt a separative modeling approach for spatial and\ntemporal attention, which hinders the effective capture of structural\nrelationships and dynamic consistency across frames. Third, their expression of\ngarment details remains insufficient, affecting the realism and stability of\nthe overall synthesized results, especially during human motion. To address the\nabove challenges, we propose MagicTryOn, a video virtual try-on framework built\nupon the large-scale video diffusion Transformer.We replace the U-Net\narchitecture with a diffusion Transformer and combine full self-attention to\njointly model the spatiotemporal consistency of videos. We design a\ncoarse-to-fine garment preservation strategy. The coarse strategy integrates\ngarment tokens during the embedding stage, while the fine strategy incorporates\nmultiple garment-based conditions, such as semantics, textures, and contour\nlines during the denoising stage. Moreover, we introduce a mask-aware loss to\nfurther optimize garment region fidelity. Extensive experiments on both image\nand video try-on datasets demonstrate that our method outperforms existing SOTA\nmethods in comprehensive evaluations and generalizes to in-the-wild scenarios.", "authors": ["Guangyuan Li", "Siming Zheng", "Hao Zhang", "Jinwei Chen", "Junsheng Luan", "Binkai Ou", "Lei Zhao", "Bo Li", "Peng-Tao Jiang"], "published_date": "2025-05-27", "title_zh": "MagicTryOn：基於擴散Transformer的服裝保留式影片虛擬試穿", "summary_zh": "視訊虛擬試穿旨在模擬服裝在連續影格中的自然外觀，捕捉動態變化與人體動作互動。現有方法在時空一致性和服裝內容保留方面仍面臨挑戰。其一，基於U-Net的擴散模型表達能力有限，難以重建複雜細節。其二，空間與時間注意力分離建模阻礙跨影格結構關係與動態一致性的有效捕捉。其三，服裝細節表達不足，影響合成結果的真實性與穩定性，尤其在人體運動時。為解決上述問題，我們提出MagicTryOn，一個基於大規模視訊擴散Transformer的視訊虛擬試穿框架。採用擴散Transformer取代U-Net，並結合全自注意力聯合建模視訊的時空一致性。設計由粗到細的服裝保留策略，粗略策略在嵌入階段整合服裝標記，精細策略在去噪階段納入多種基於服裝的條件，如語義、紋理和輪廓線。此外，引入掩碼感知損失以優化服裝區域逼真度。在圖像和視訊試穿數據集上的大量實驗表明，該方法在綜合評估中優於現有技術，並能推廣至實際場景。", "audio": "audios/2505.21325v1.mp3", "timestamp": "2025-05-28T09:20:38.959911"}
{"query": "AI", "id": "2505.21418v1", "url": "http://arxiv.org/abs/2505.21418v1", "title": "Autonomous Multi-Modal LLM Agents for Treatment Planning in Focused Ultrasound Ablation Surgery", "summary": "Focused Ultrasound Ablation Surgery (FUAS) has emerged as a promising\nnon-invasive therapeutic modality, valued for its safety and precision.\nNevertheless, its clinical implementation entails intricate tasks such as\nmultimodal image interpretation, personalized dose planning, and real-time\nintraoperative decision-making processes that demand intelligent assistance to\nimprove efficiency and reliability. We introduce FUAS-Agents, an autonomous\nagent system that leverages the multimodal understanding and tool-using\ncapabilities of large language models (LLMs). By integrating patient profiles\nand MRI data, FUAS-Agents orchestrates a suite of specialized medical AI tools,\nincluding segmentation, treatment dose prediction, and clinical guideline\nretrieval, to generate personalized treatment plans comprising MRI image, dose\nparameters, and therapeutic strategies. We evaluate the system in a uterine\nfibroid treatment scenario. Human assessment by four senior FUAS experts\nindicates that 82.5%, 82.5%, 87.5%, and 97.5% of the generated plans were rated\n4 or above (on a 5-point scale) in terms of completeness, accuracy, fluency,\nand clinical compliance, respectively. These results demonstrate the potential\nof LLM-driven agents in enhancing decision-making across complex clinical\nworkflows, and exemplify a translational paradigm that combines general-purpose\nmodels with specialized expert systems to solve practical challenges in\nvertical healthcare domains.", "authors": ["Lina Zhao", "Jiaxing Bai", "Zihao Bian", "Qingyue Chen", "Yafang Li", "Guangbo Li", "Min He", "Huaiyuan Yao", "Zongjiu Zhang"], "published_date": "2025-05-27", "title_zh": "聚焦超音波消融手術治療計畫之自主多模態大型語言模型代理", "summary_zh": "聚焦超音波消融手術(FUAS)是一種具前景的非侵入性療法，但其臨床應用涉及複雜的多模態影像判讀、個人化劑量計畫和即時術中決策。為提升效率和可靠性，我們提出FUAS-Agents，一種利用大型語言模型(LLM)多模態理解和工具使用能力的自主代理系統。該系統整合患者資料和MRI影像，協調分割、治療劑量預測和臨床指南檢索等專業醫療AI工具，生成包含MRI影像、劑量參數和治療策略的個人化治療計畫。在子宮肌瘤治療案例中，四位資深FUAS專家評估結果顯示，生成計畫在完整性、準確性、流暢性和臨床合規性方面，分別有82.5%、82.5%、87.5%和97.5%獲得4分以上（5分制）。結果表明，LLM驅動的代理系統有潛力加強複雜臨床流程中的決策，並展示了一種將通用模型與專業知識系統結合，以解決垂直醫療領域實際挑戰的轉化範例。", "audio": "audios/2505.21418v1.mp3", "timestamp": "2025-05-28T10:20:55.376053"}
{"query": "Foundation Model", "id": "2505.21237v1", "url": "http://arxiv.org/abs/2505.21237v1", "title": "Unfolding A Few Structures for The Many: Memory-Efficient Compression of Conformer and Speech Foundation Models", "summary": "This paper presents a novel memory-efficient model compression approach for\nConformer ASR and speech foundation systems. Our approach features a unique\n\"small-to-large\" design. A compact \"seed\" model containing a few Conformer or\nTransformer blocks is trained and unfolded many times to emulate the\nperformance of larger uncompressed models with different logical depths. The\nseed model and many unfolded paths are jointly trained within a single\nunfolding cycle. The KL-divergence between the largest unfolded and smallest\nseed models is used in a self-distillation process to minimize their\nperformance disparity. Experimental results show that our foldable model\nproduces ASR performance comparable to individually constructed Conformer and\nwav2vec2/HuBERT speech foundation models under various depth configurations,\nwhile requiring only minimal memory and storage. Conformer and wav2vec2 models\nwith a reduction of 35% and 30% parameters are obtained without loss of\nperformance, respectively.", "authors": ["Zhaoqing Li", "Haoning Xu", "Xurong Xie", "Zengrui Jin", "Tianzi Wang", "Xunying Liu"], "published_date": "2025-05-27", "title_zh": "為眾多應用解構少數架構：Conformer與語音基礎模型之記憶體效率壓縮", "summary_zh": "本文提出一種針對Conformer語音辨識和語音基礎系統的新型記憶體效率模型壓縮方法。該方法採用獨特的「由小到大」設計，訓練一個包含少量Conformer或Transformer模塊的精簡「種子」模型，並多次展開以模擬具有不同邏輯深度的大型未壓縮模型的性能。種子模型和多個展開路徑在單個展開週期內聯合訓練。最大展開模型和最小種子模型之間的KL散度用於自我蒸餾過程，以最小化它們的性能差異。實驗結果表明，我們的可折疊模型在各種深度配置下產生與單獨構建的Conformer和wav2vec2/HuBERT語音基礎模型相當的語音辨識性能，同時僅需最小的記憶體和儲存空間。在不損失性能的情況下，分別獲得參數減少35%和30%的Conformer和wav2vec2模型。", "audio": "audios/2505.21237v1.mp3", "timestamp": "2025-05-28T10:21:00.674808"}
{"query": "Diffusion Model", "id": "2505.21223v1", "url": "http://arxiv.org/abs/2505.21223v1", "title": "Simulations of the churning mode: toroidally symmetric plasma convection and turbulence around the X-points in a snowflake divertor", "summary": "Using a reduced MHD model, extended to include field-aligned thermal\nconduction, we present numerical simulations of the churning mode (CM): a\ntoroidally symmetric, non-linear plasma vortex in the vicinity of the null\npoints in a snowflake (SF) divertor (Ryutov et al., Phys. Scr. 89 088002,\n2014). Simulations are carried out across a range of inter-null separations,\n$d_{xx}$, and inter-null orientations, $\\theta$, primarily in conditions\nrelevant to the MAST-U tokamak. We find that, when $d_{xx}$ is small, the CM\ninduces additional transport across the X-points when $\\beta_{pm} \\gtrsim 8$ %,\nwhere $\\beta_{pm}$ is the ratio of the plasma pressure in the null region to\npoloidal magnetic pressure at the midplane. This transport also increases\napproximately linearly as $d_{xx}$ is reduced. A diffusive model of this\ntransport is shown to predict the total transport across the null points, where\ndiffusion coefficients of up to $\\sim 10^2$ m$^2$s$^{-1}$ centred on a small\nregion around the X-points are used. However, the CM also results in\nsignificant changes to the flux surfaces in the null region which is not\ncaptured by this diffusive model. The changes in magnetic geometry mean the\nfractional exhaust power delivered to each divertor leg is highly sensitive to\n$\\beta_{pm}$, $d_{xx}$ and $\\theta$. For small values of $\\theta$, the CM can\ninduce a change in topology, redirecting exhaust power from a secondary\ndivertor leg on the high field side to one on the low field side. Similar\nbehaviour is found in the fraction of exhaust power going to the inner and\nouter divertor. Such changes in the flux surfaces may not be captured by\nGrad-Shafranov solvers and so may be a source of error in the magnetic\nreconstruction of SF experiments. We consistently find that the fractional\nexhaust power going to a secondary divertor leg on the high field side is\nsmall, consistent with SF experiments.", "authors": ["D Power", "M V Umansky", "V A Soukhanovskii"], "published_date": "2025-05-27", "title_zh": "攪盪模式模擬：雪花散熱器中環向對稱電漿對流與X點周圍的湍流", "summary_zh": "本研究利用包含場向熱傳導的簡化磁流體動力學模型，數值模擬了雪花型偏濾器零點附近的環向對稱非線性電漿渦旋——攪動模。模擬涵蓋不同零點間距$d_{xx}$與零點方向$\\theta$，主要針對MAST-U都卜勒實驗裝置的條件。研究發現，當$d_{xx}$較小時，若$\\beta_{pm}$（零點區域電漿壓力與中平面極向磁壓之比）超過8%，攪動模會在X點間產生額外傳輸，且此傳輸量隨著$d_{xx}$減小而近似線性增加。擴散模型能預測零點間的總傳輸，擴散係數高達$\\sim 10^2$ m$^2$s$^{-1}$，集中於X點周圍的小區域。然而，攪動模也顯著改變了零點區域的磁通面結構，此點擴散模型無法捕捉。磁場幾何結構的變化導致各偏濾器支腳的排氣功率分配比例對$\\beta_{pm}$、$d_{xx}$和$\\theta$高度敏感。對於較小的$\\theta$值，攪動模能誘發拓撲結構變化，將排氣功率從高場側的次級偏濾器支腳重新導向低場側。內外偏濾器的排氣功率分配比例也呈現類似行為。Grad-Shafranov解算器可能無法捕捉磁通面的這些變化，進而造成雪花型實驗磁場重構的誤差。研究一致發現，高場側次級偏濾器支腳的排氣功率比例較小，與雪花型實驗結果一致。", "audio": "audios/2505.21223v1.mp3", "timestamp": "2025-05-28T10:21:10.477577"}
{"query": "AI", "id": "2505.21398v1", "url": "http://arxiv.org/abs/2505.21398v1", "title": "A Structured Unplugged Approach for Foundational AI Literacy in Primary Education", "summary": "Younger generations are growing up in a world increasingly shaped by\nintelligent technologies, making early AI literacy crucial for developing the\nskills to critically understand and navigate them. However, education in this\nfield often emphasizes tool-based learning, prioritizing usage over\nunderstanding the underlying concepts. This lack of knowledge leaves\nnon-experts, especially children, prone to misconceptions, unrealistic\nexpectations, and difficulties in recognizing biases and stereotypes. In this\npaper, we propose a structured and replicable teaching approach that fosters\nfoundational AI literacy in primary students, by building upon core\nmathematical elements closely connected to and of interest in primary\ncurricula, to strengthen conceptualization, data representation, classification\nreasoning, and evaluation of AI. To assess the effectiveness of our approach,\nwe conducted an empirical study with thirty-one fifth-grade students across two\nclasses, evaluating their progress through a post-test and a satisfaction\nsurvey. Our results indicate improvements in terminology understanding and\nusage, features description, logical reasoning, and evaluative skills, with\nstudents showing a deeper comprehension of decision-making processes and their\nlimitations. Moreover, the approach proved engaging, with students particularly\nenjoying activities that linked AI concepts to real-world reasoning. Materials:\nhttps://github.com/tail-unica/ai-literacy-primary-ed.", "authors": ["Maria Cristina Carrisi", "Mirko Marras", "Sara Vergallo"], "published_date": "2025-05-27", "title_zh": "小學基礎人工智慧素養結構化離線教學法", "summary_zh": "年輕世代在智慧科技日益普及的環境中成長，早期的人工智慧素養至關重要，能培養批判性理解和應對能力。現有教育多著重工具使用，忽略底層概念理解，導致非專業人士，特別是兒童，容易產生誤解、不切實際的期望，以及難以辨識偏見和刻板印象。本文提出一套結構化且可複製的教學方法，透過與小學課程密切相關的核心數學元素，培養小學生的基礎人工智慧素養，強化概念化、資料呈現、分類推理和人工智慧評估能力。我們對兩班共31名五年級學生進行實證研究，通過後測和滿意度調查評估學習成效。結果顯示，學生在術語理解與使用、特徵描述、邏輯推理和評估技能方面均有提升，並對決策過程及其局限性有更深入的理解。此方法亦具吸引力，學生尤其喜愛將人工智慧概念與現實世界推理連結的活動。材料：https://github.com/tail-unica/ai-literacy-primary-ed", "audio": "audios/2505.21398v1.mp3", "timestamp": "2025-05-28T11:16:28.698552"}
{"query": "Foundation Model", "id": "2505.21137v1", "url": "http://arxiv.org/abs/2505.21137v1", "title": "Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction", "summary": "Spoken Grammatical Error Correction (SGEC) and Feedback (SGECF) are crucial\nfor second language learners, teachers and test takers. Traditional SGEC\nsystems rely on a cascaded pipeline consisting of an ASR, a module for\ndisfluency detection (DD) and removal and one for GEC. With the rise of\nend-to-end (E2E) speech foundation models, we investigate their effectiveness\nin SGEC and feedback generation. This work introduces a pseudo-labelling\nprocess to address the challenge of limited labelled data, expanding the\ntraining data size from 77 hours to approximately 2500 hours, leading to\nimproved performance. Additionally, we prompt an E2E Whisper-based SGEC model\nwith fluent transcriptions, showing a slight improvement in SGEC performance,\nwith more significant gains in feedback generation. Finally, we assess the\nimpact of increasing model size, revealing that while pseudo-labelled data does\nnot yield performance gain for a larger Whisper model, training with prompts\nproves beneficial.", "authors": ["Mengjie Qian", "Rao Ma", "Stefano Bannò", "Kate M. Knill", "Mark J. F. Gales"], "published_date": "2025-05-27", "title_zh": "規模化與提示工程：提升端到端語音語法錯誤校正", "summary_zh": "口語語法錯誤更正(SGEC)及回饋(SGECF)對二語學習者至關重要。傳統SGEC系統仰賴ASR、語暢性檢測與移除、及GEC模組串聯。本研究探討端到端(E2E)語音基礎模型在SGEC和回饋生成中的效用。為了解決標註數據有限的問題，引入偽標籤流程，將訓練數據從77小時擴展到約2500小時，提升性能。此外，使用流暢的文本提示E2E Whisper模型，略微改善SGEC性能，顯著提升回饋生成效果。評估模型尺寸影響，發現偽標籤數據對較大Whisper模型無助益，但提示訓練有效。", "audio": "audios/2505.21137v1.mp3", "timestamp": "2025-05-28T11:16:35.330447"}
{"query": "Diffusion Model", "id": "2505.21205v1", "url": "http://arxiv.org/abs/2505.21205v1", "title": "Sci-Fi: Symmetric Constraint for Frame Inbetweening", "summary": "Frame inbetweening aims to synthesize intermediate video sequences\nconditioned on the given start and end frames. Current state-of-the-art methods\nmainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs)\nby incorporating end-frame constraints via directly fine-tuning or omitting\ntraining. We identify a critical limitation in their design: Their injections\nof the end-frame constraint usually utilize the same mechanism that originally\nimposed the start-frame (single image) constraint. However, since the original\nI2V-DMs are adequately trained for the start-frame condition in advance,\nnaively introducing the end-frame constraint by the same mechanism with much\nless (even zero) specialized training probably can't make the end frame have a\nstrong enough impact on the intermediate content like the start frame. This\nasymmetric control strength of the two frames over the intermediate content\nlikely leads to inconsistent motion or appearance collapse in generated frames.\nTo efficiently achieve symmetric constraints of start and end frames, we\npropose a novel framework, termed Sci-Fi, which applies a stronger injection\nfor the constraint of a smaller training scale. Specifically, it deals with the\nstart-frame constraint as before, while introducing the end-frame constraint by\nan improved mechanism. The new mechanism is based on a well-designed\nlightweight module, named EF-Net, which encodes only the end frame and expands\nit into temporally adaptive frame-wise features injected into the I2V-DM. This\nmakes the end-frame constraint as strong as the start-frame constraint,\nenabling our Sci-Fi to produce more harmonious transitions in various\nscenarios. Extensive experiments prove the superiority of our Sci-Fi compared\nwith other baselines.", "authors": ["Liuhan Chen", "Xiaodong Cun", "Xiaoyu Li", "Xianyi He", "Shenghai Yuan", "Jie Chen", "Ying Shan", "Li Yuan"], "published_date": "2025-05-27", "title_zh": "科幻：用於幀間預測的對稱約束", "summary_zh": "影格補間旨在根據給定的起始和結束影格合成中間影片序列。現有方法主要擴展預訓練的圖像到影片擴散模型，透過微調或省略訓練來加入結束影格約束。然而，這些方法在設計上存在缺陷：它們通常使用與施加起始影格約束相同的機制來注入結束影格約束。由於原始圖像到影片擴散模型已充分訓練以適應起始影格，因此以相同機制引入訓練量不足的結束影格約束，可能無法像起始影格一樣對中間內容產生足夠強的影響。這種兩影格間控制強度的不對稱性可能導致生成影格中的運動不一致或外觀崩潰。為有效實現起始和結束影格的對稱約束，我們提出名為Sci-Fi的新框架，以更小的訓練規模實現更強的約束注入。具體而言，Sci-Fi沿用既有方式處理起始影格約束，同時透過改良機制引入結束影格約束。此機制基於精心設計的輕量級模組EF-Net，僅編碼結束影格並將其擴展為時序自適應的逐影格特徵，注入到圖像到影片擴散模型中。這使結束影格約束與起始影格約束同樣強大，使Sci-Fi能夠在各種場景中產生更協調的轉變。廣泛實驗證明Sci-Fi優於其他基準方法。", "audio": "audios/2505.21205v1.mp3", "timestamp": "2025-05-28T11:16:43.758405"}
{"query": "AI", "id": "2505.21355v1", "url": "http://arxiv.org/abs/2505.21355v1", "title": "Prostate Cancer Screening with Artificial Intelligence-Enhanced Micro-Ultrasound: A Comparative Study with Traditional Methods", "summary": "Background and objective: Micro-ultrasound (micro-US) is a novel imaging\nmodality with diagnostic accuracy comparable to MRI for detecting clinically\nsignificant prostate cancer (csPCa). We investigated whether artificial\nintelligence (AI) interpretation of micro-US can outperform clinical screening\nmethods using PSA and digital rectal examination (DRE). Methods: We\nretrospectively studied 145 men who underwent micro-US guided biopsy (79 with\ncsPCa, 66 without). A self-supervised convolutional autoencoder was used to\nextract deep image features from 2D micro-US slices. Random forest classifiers\nwere trained using five-fold cross-validation to predict csPCa at the slice\nlevel. Patients were classified as csPCa-positive if 88 or more consecutive\nslices were predicted positive. Model performance was compared with a\nclassifier using PSA, DRE, prostate volume, and age. Key findings and\nlimitations: The AI-based micro-US model and clinical screening model achieved\nAUROCs of 0.871 and 0.753, respectively. At a fixed threshold, the micro-US\nmodel achieved 92.5% sensitivity and 68.1% specificity, while the clinical\nmodel showed 96.2% sensitivity but only 27.3% specificity. Limitations include\na retrospective single-center design and lack of external validation.\nConclusions and clinical implications: AI-interpreted micro-US improves\nspecificity while maintaining high sensitivity for csPCa detection. This method\nmay reduce unnecessary biopsies and serve as a low-cost alternative to\nPSA-based screening. Patient summary: We developed an AI system to analyze\nprostate micro-ultrasound images. It outperformed PSA and DRE in detecting\naggressive cancer and may help avoid unnecessary biopsies.", "authors": ["Muhammad Imran", "Wayne G. Brisbane", "Li-Ming Su", "Jason P. Joseph", "Wei Shao"], "published_date": "2025-05-27", "title_zh": "人工智慧增強型微超音波前列腺癌篩檢：與傳統方法的比較研究", "summary_zh": "背景與目標：微型超聲(micro-US)是一種新的影像技術，診斷臨床顯著前列腺癌(csPCa)的準確性與MRI相當。本研究探討使用AI解讀微型超聲，能否優於PSA和肛門指檢(DRE)等臨床篩檢方法。方法：回顧性研究145名接受微型超聲引導活檢的男性（79名患有csPCa，66名未患病）。使用自監督卷積自編碼器從2D微型超聲切片中提取深度影像特徵。使用五重交叉驗證訓練隨機森林分類器，以預測切片層面的csPCa。若連續88個或更多切片預測為陽性，則將患者分類為csPCa陽性。模型效能與使用PSA、DRE、前列腺體積和年齡的分類器進行比較。主要發現與局限性：基於AI的微型超聲模型和臨床篩檢模型，其曲線下面積(AUROC)分別為0.871和0.753。在固定閾值下，微型超聲模型達到92.5%的敏感性和68.1%的特異性，而臨床模型顯示96.2%的敏感性，但特異性僅為27.3%。局限性包括回顧性單中心設計和缺乏外部驗證。結論與臨床意義：AI解讀的微型超聲提高了csPCa檢測的特異性，同時保持了高敏感性。此方法可能減少不必要的活檢，並可作為基於PSA篩檢的低成本替代方案。患者總結：我們開發了一種AI系統來分析前列腺微型超聲影像。它在檢測侵襲性癌症方面優於PSA和DRE，可能有助於避免不必要的活檢。", "audio": "audios/2505.21355v1.mp3", "timestamp": "2025-05-28T12:38:42.967318"}
{"query": "Foundation Model", "id": "2505.21050v1", "url": "http://arxiv.org/abs/2505.21050v1", "title": "Advancing high-fidelity 3D and Texture Generation with 2.5D latents", "summary": "Despite the availability of large-scale 3D datasets and advancements in 3D\ngenerative models, the complexity and uneven quality of 3D geometry and texture\ndata continue to hinder the performance of 3D generation techniques. In most\nexisting approaches, 3D geometry and texture are generated in separate stages\nusing different models and non-unified representations, frequently leading to\nunsatisfactory coherence between geometry and texture. To address these\nchallenges, we propose a novel framework for joint generation of 3D geometry\nand texture. Specifically, we focus in generate a versatile 2.5D\nrepresentations that can be seamlessly transformed between 2D and 3D. Our\napproach begins by integrating multiview RGB, normal, and coordinate images\ninto a unified representation, termed as 2.5D latents. Next, we adapt\npre-trained 2D foundation models for high-fidelity 2.5D generation, utilizing\nboth text and image conditions. Finally, we introduce a lightweight 2.5D-to-3D\nrefiner-decoder framework that efficiently generates detailed 3D\nrepresentations from 2.5D images. Extensive experiments demonstrate that our\nmodel not only excels in generating high-quality 3D objects with coherent\nstructure and color from text and image inputs but also significantly\noutperforms existing methods in geometry-conditioned texture generation.", "authors": ["Xin Yang", "Jiantao Lin", "Yingjie Xu", "Haodong Li", "Yingcong Chen"], "published_date": "2025-05-27", "title_zh": "以2.5D潛變量推進高保真3D與紋理生成", "summary_zh": "儘管大型3D數據集和3D生成模型有所進展，但3D幾何和紋理數據的複雜性及品質不均仍阻礙了3D生成技術的效能。現有方法多半分階段生成3D幾何和紋理，採用不同模型和非統一表示，導致幾何與紋理間的連貫性不佳。為了解決這些問題，本文提出一個聯合生成3D幾何和紋理的新框架，著重於生成可在2D和3D間無縫轉換的多功能2.5D表示。該方法首先整合多視角RGB、法線和座標圖像到一個統一的2.5D潛在表示中。接著，調整預訓練的2D基礎模型，用於高保真2.5D生成，並利用文本和圖像條件。最後，引入一個輕量級的2.5D到3D精煉器-解碼器框架，有效率地從2.5D圖像生成細緻的3D表示。實驗結果表明，該模型不僅擅長從文本和圖像輸入生成具有連貫結構和色彩的高品質3D物件，且在幾何條件紋理生成方面顯著優於現有方法。", "audio": "audios/2505.21050v1.mp3", "timestamp": "2025-05-28T12:38:49.885974"}
{"query": "Diffusion Model", "id": "2505.21179v1", "url": "http://arxiv.org/abs/2505.21179v1", "title": "Normalized Attention Guidance: Universal Negative Guidance for Diffusion Model", "summary": "Negative guidance -- explicitly suppressing unwanted attributes -- remains a\nfundamental challenge in diffusion models, particularly in few-step sampling\nregimes. While Classifier-Free Guidance (CFG) works well in standard settings,\nit fails under aggressive sampling step compression due to divergent\npredictions between positive and negative branches. We present Normalized\nAttention Guidance (NAG), an efficient, training-free mechanism that applies\nextrapolation in attention space with L1-based normalization and refinement.\nNAG restores effective negative guidance where CFG collapses while maintaining\nfidelity. Unlike existing approaches, NAG generalizes across architectures\n(UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image,\nvideo), functioning as a \\textit{universal} plug-in with minimal computational\noverhead. Through extensive experimentation, we demonstrate consistent\nimprovements in text alignment (CLIP Score), fidelity (FID, PFID), and\nhuman-perceived quality (ImageReward). Our ablation studies validate each\ndesign component, while user studies confirm significant preference for\nNAG-guided outputs. As a model-agnostic inference-time approach requiring no\nretraining, NAG provides effortless negative guidance for all modern diffusion\nframeworks -- pseudocode in the Appendix!", "authors": ["Dar-Yen Chen", "Hmrishav Bandyopadhyay", "Kai Zou", "Yi-Zhe Song"], "published_date": "2025-05-27", "title_zh": "正規化注意力引導：擴散模型的通用負引導", "summary_zh": "負向引導，即抑制不必要屬性，是擴散模型中的基本挑戰，尤其是在少步採樣中。無分類器引導(CFG)在標準設定下表現良好，但在激進的採樣步壓縮下會失效，因正向和負向分支的預測發散。我們提出正規化注意力引導(NAG)，一種高效、無需訓練的機制，在注意力空間中應用基於L1正規化和精煉的推斷。NAG恢復了CFG崩潰時的有效負向引導，同時保持了保真度。與現有方法不同，NAG可泛化於各種架構(UNet, DiT)、採樣方案(少步, 多步)和模態(圖像, 影片)，作為一個通用外掛程式，計算開銷極小。大量實驗證明，NAG在文字對齊(CLIP Score)、保真度(FID, PFID)和人類感知品質(ImageReward)方面均有持續提升。消融研究驗證了每個設計組件，使用者研究證實了對NAG引導輸出的顯著偏好。作為一種無需重新訓練的模型無關推論方法，NAG為所有現代擴散框架提供毫不費力的負向引導。", "audio": "audios/2505.21179v1.mp3", "timestamp": "2025-05-28T12:38:59.904530"}
{"query": "AI", "id": "2505.21349v1", "url": "http://arxiv.org/abs/2505.21349v1", "title": "Out of the Past: An AI-Enabled Pipeline for Traffic Simulation from Noisy, Multimodal Detector Data and Stakeholder Feedback", "summary": "How can a traffic simulation be designed to faithfully reflect real-world\ntraffic conditions? Past data-driven approaches to traffic simulation in the\nliterature have relied on unrealistic or suboptimal heuristics. They also fail\nto adequately account for the effects of uncertainty and multimodality in the\ndata on simulation outcomes. In this work, we integrate advances in AI to\nconstruct a three-step, end-to-end pipeline for generating a traffic simulation\nfrom detector data: computer vision for vehicle counting from camera footage,\ncombinatorial optimization for vehicle route generation from multimodal data,\nand large language models for iterative simulation refinement from natural\nlanguage feedback. Using a road network from Strongsville, Ohio as a testbed,\nwe demonstrate that our pipeline can accurately capture the city's traffic\npatterns in a granular simulation. Beyond Strongsville, our traffic simulation\nframework can be generalized to other municipalities with different levels of\ndata and infrastructure availability.", "authors": ["Rex Chen", "Karen Wu", "John McCartney", "Norman Sadeh", "Fei Fang"], "published_date": "2025-05-27", "title_zh": "溯古知今：基於人工智慧管線，利用雜訊多模態偵測器資料與利害關係人回饋進行交通模擬", "summary_zh": "如何設計交通模擬以真實反映現實交通狀況？過往文獻中基於資料驅動的交通模擬方法依賴不切實際或次佳的啟發法，且未能充分考量資料中的不確定性和多模態性對模擬結果的影響。本研究整合人工智慧進展，構建一個三步驟端到端流程，從偵測器資料生成交通模擬：利用電腦視覺從攝影鏡頭影像中進行車輛計數、利用組合優化從多模態資料中生成車輛路線，並利用大型語言模型從自然語言回饋中迭代改進模擬。以俄亥俄州斯特朗斯維爾的道路網路為試驗平台，證明該流程能精確捕捉該市的細緻交通模式。此外，此交通模擬框架可推廣至其他數據和基礎設施可用性不同的城市。", "audio": "audios/2505.21349v1.mp3", "timestamp": "2025-05-28T13:31:49.589215"}
{"query": "Foundation Model", "id": "2505.20973v1", "url": "http://arxiv.org/abs/2505.20973v1", "title": "Towards Conversational Development Environments: Using Theory-of-Mind and Multi-Agent Architectures for Requirements Refinement", "summary": "Foundation Models (FMs) have shown remarkable capabilities in various natural\nlanguage tasks. However, their ability to accurately capture stakeholder\nrequirements remains a significant challenge for using FMs for software\ndevelopment. This paper introduces a novel approach that leverages an\nFM-powered multi-agent system called AlignMind to address this issue. By having\na cognitive architecture that enhances FMs with Theory-of-Mind capabilities,\nour approach considers the mental states and perspectives of software makers.\nThis allows our solution to iteratively clarify the beliefs, desires, and\nintentions of stakeholders, translating these into a set of refined\nrequirements and a corresponding actionable natural language workflow in the\noften-overlooked requirements refinement phase of software engineering, which\nis crucial after initial elicitation. Through a multifaceted evaluation\ncovering 150 diverse use cases, we demonstrate that our approach can accurately\ncapture the intents and requirements of stakeholders, articulating them as both\nspecifications and a step-by-step plan of action. Our findings suggest that the\npotential for significant improvements in the software development process\njustifies these investments. Our work lays the groundwork for future innovation\nin building intent-first development environments, where software makers can\nseamlessly collaborate with AIs to create software that truly meets their\nneeds.", "authors": ["Keheliya Gallaba", "Ali Arabat", "Dayi Lin", "Mohammed Sayagh", "Ahmed E. Hassan"], "published_date": "2025-05-27", "title_zh": "邁向對話式開發環境：運用心智理論與多代理人架構進行需求精煉", "summary_zh": "基於大型語言模型（LLM）的基礎模型（FM）在自然語言任務中表現卓越，但其準確掌握利害關係人需求的能力仍是軟體開發應用的主要挑戰。本文提出一種新穎方法，利用名為AlignMind的FM驅動多代理系統解決此問題。透過具備心智理論認知架構以強化FM，本方法考量軟體開發者的心理狀態與觀點，進而迭代釐清利害關係人的信念、慾望與意圖，將之轉化為精煉的需求集合與可執行的自然語言工作流程。針對軟體工程中常被忽略、但在初步引導後至關重要的需求精煉階段，我們透過涵蓋150個不同用例的多面向評估證明，此方法可準確捕捉利害關係人的意圖與需求，並將其闡述為規格與逐步行動計畫。研究結果顯示，此投資能顯著改善軟體開發流程。本研究為未來構建意圖優先的開發環境奠定基礎，使軟體開發者得以與AI無縫協作，創造真正滿足需求的軟體。", "audio": "audios/2505.20973v1.mp3", "timestamp": "2025-05-28T13:32:00.526682"}
{"query": "Diffusion Model", "id": "2505.21146v1", "url": "http://arxiv.org/abs/2505.21146v1", "title": "IKMo: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned Motion Diffusion Model", "summary": "Existing human motion generation methods with trajectory and pose inputs\noperate global processing on both modalities, leading to suboptimal outputs. In\nthis paper, we propose IKMo, an image-keyframed motion generation method based\non the diffusion model with trajectory and pose being decoupled. The trajectory\nand pose inputs go through a two-stage conditioning framework. In the first\nstage, the dedicated optimization module is applied to refine inputs. In the\nsecond stage, trajectory and pose are encoded via a Trajectory Encoder and a\nPose Encoder in parallel. Then, motion with high spatial and semantic fidelity\nis guided by a motion ControlNet, which processes the fused trajectory and pose\ndata. Experiment results based on HumanML3D and KIT-ML datasets demonstrate\nthat the proposed method outperforms state-of-the-art on all metrics under\ntrajectory-keyframe constraints. In addition, MLLM-based agents are implemented\nto pre-process model inputs. Given texts and keyframe images from users, the\nagents extract motion descriptions, keyframe poses, and trajectories as the\noptimized inputs into the motion generation model. We conducts a user study\nwith 10 participants. The experiment results prove that the MLLM-based agents\npre-processing makes generated motion more in line with users' expectation. We\nbelieve that the proposed method improves both the fidelity and controllability\nof motion generation by the diffusion model.", "authors": ["Yang Zhao", "Yan Zhang", "Xubo Yang"], "published_date": "2025-05-27", "title_zh": "IKMo：基於軌跡-姿態條件運動擴散模型的圖像關鍵幀運動生成", "summary_zh": "現有人體動作生成方法全局處理軌跡與姿態輸入，導致結果欠佳。本研究提出IKMo，一種基於擴散模型的圖像關鍵幀動作生成方法，解耦軌跡與姿態。採用兩階段調節框架：首先，專用優化模組精煉輸入；其次，軌跡與姿態分別經軌跡編碼器和姿態編碼器平行編碼。融合後的軌跡與姿態數據由動作ControlNet處理，生成具備高空間和語義保真度的動作。在HumanML3D和KIT-ML數據集上的實驗結果表明，本方法在軌跡-關鍵幀約束下優於現有技術。此外，導入基於MLLM的代理預處理模型輸入。使用者提供文本與關鍵幀圖像，代理提取動作描述、關鍵幀姿態及軌跡，作為優化輸入。包含十名參與者的使用者研究證明，MLLM代理預處理使生成的動作更符合使用者期望。本研究藉由擴散模型，提升動作生成的保真度與可控性。", "audio": "audios/2505.21146v1.mp3", "timestamp": "2025-05-28T13:32:11.447177"}
{"query": "AI", "id": "2505.21344v1", "url": "http://arxiv.org/abs/2505.21344v1", "title": "The Multilingual Divide and Its Impact on Global AI Safety", "summary": "Despite advances in large language model capabilities in recent years, a\nlarge gap remains in their capabilities and safety performance for many\nlanguages beyond a relatively small handful of globally dominant languages.\nThis paper provides researchers, policymakers and governance experts with an\noverview of key challenges to bridging the \"language gap\" in AI and minimizing\nsafety risks across languages. We provide an analysis of why the language gap\nin AI exists and grows, and how it creates disparities in global AI safety. We\nidentify barriers to address these challenges, and recommend how those working\nin policy and governance can help address safety concerns associated with the\nlanguage gap by supporting multilingual dataset creation, transparency, and\nresearch.", "authors": ["Aidan Peppin", "Julia Kreutzer", "Alice Schoenauer Sebag", "Kelly Marchisio", "Beyza Ermis", "John Dang", "Samuel Cahyawijaya", "Shivalika Singh", "Seraphina Goldfarb-Tarrant", "Viraat Aryabumi", "Aakanksha", "Wei-Yin Ko", "Ahmet Üstün", "Matthias Gallé", "Marzieh Fadaee", "Sara Hooker"], "published_date": "2025-05-27", "title_zh": "多語隔閡及其對全球人工智慧安全的影響", "summary_zh": "近年來大型語言模型能力雖有進展，但在多數非全球主導語言上，其能力與安全性仍存在巨大差距。本文旨在為研究者、政策制定者和治理專家概述彌合人工智慧「語言差距」及降低跨語言安全風險之關鍵挑戰。我們分析了人工智慧語言差距存在與擴大的原因，及其如何造成全球人工智慧安全之不平等。我們進一步指出應對挑戰之阻礙，並建議政策與治理領域工作者如何透過支持多語資料集創建、透明化及研究，協助解決與語言差距相關之安全疑慮。", "audio": "audios/2505.21344v1.mp3", "timestamp": "2025-05-28T14:19:50.789192"}
{"query": "Foundation Model", "id": "2505.20783v1", "url": "http://arxiv.org/abs/2505.20783v1", "title": "FM-Planner: Foundation Model Guided Path Planning for Autonomous Drone Navigation", "summary": "Path planning is a critical component in autonomous drone operations,\nenabling safe and efficient navigation through complex environments. Recent\nadvances in foundation models, particularly large language models (LLMs) and\nvision-language models (VLMs), have opened new opportunities for enhanced\nperception and intelligent decision-making in robotics. However, their\npractical applicability and effectiveness in global path planning remain\nrelatively unexplored. This paper proposes foundation model-guided path\nplanners (FM-Planner) and presents a comprehensive benchmarking study and\npractical validation for drone path planning. Specifically, we first\nsystematically evaluate eight representative LLM and VLM approaches using\nstandardized simulation scenarios. To enable effective real-time navigation, we\nthen design an integrated LLM-Vision planner that combines semantic reasoning\nwith visual perception. Furthermore, we deploy and validate the proposed path\nplanner through real-world experiments under multiple configurations. Our\nfindings provide valuable insights into the strengths, limitations, and\nfeasibility of deploying foundation models in real-world drone applications and\nproviding practical implementations in autonomous flight. Project site:\nhttps://github.com/NTU-ICG/FM-Planner.", "authors": ["Jiaping Xiao", "Cheng Wen Tsao", "Yuhang Zhang", "Mir Feroskhan"], "published_date": "2025-05-27", "title_zh": "FM-Planner：基於基礎模型的自主無人機導航路徑規劃", "summary_zh": "路徑規劃是無人機自主運作的關鍵，能在複雜環境中實現安全高效的導航。大型語言模型和視覺語言模型等基礎模型的最新進展，為機器人領域增強感知和智慧決策帶來了新機遇。然而，它們在全局路徑規劃中的實際應用和有效性仍有待探索。本文提出基礎模型引導的路徑規劃器（FM-Planner），並對無人機路徑規劃進行了全面的基準測試和實際驗證。具體而言，我們首先使用標準化模擬場景，系統地評估了八種具有代表性的語言模型和視覺語言模型方法。為了實現有效的即時導航，我們設計了一種整合語言模型-視覺的規劃器，將語義推理與視覺感知相結合。此外，我們在多種配置下通過真實世界的實驗部署和驗證了所提出的路徑規劃器。研究結果為在真實世界無人機應用中部署基礎模型的可行性、優缺點以及自主飛行的實際應用提供了寶貴的見解。項目網站：https://github.com/NTU-ICG/FM-Planner。", "audio": "audios/2505.20783v1.mp3", "timestamp": "2025-05-28T14:19:58.628134"}
{"query": "Diffusion Model", "id": "2505.21144v1", "url": "http://arxiv.org/abs/2505.21144v1", "title": "FastFace: Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention", "summary": "In latest years plethora of identity-preserving adapters for a personalized\ngeneration with diffusion models have been released. Their main disadvantage is\nthat they are dominantly trained jointly with base diffusion models, which\nsuffer from slow multi-step inference. This work aims to tackle the challenge\nof training-free adaptation of pretrained ID-adapters to diffusion models\naccelerated via distillation - through careful re-design of classifier-free\nguidance for few-step stylistic generation and attention manipulation\nmechanisms in decoupled blocks to improve identity similarity and fidelity, we\npropose universal FastFace framework. Additionally, we develop a disentangled\npublic evaluation protocol for id-preserving adapters.", "authors": ["Sergey Karpukhin", "Vadim Titov", "Andrey Kuznetsov", "Aibek Alanov"], "published_date": "2025-05-27", "title_zh": "FastFace：藉由引導與注意力機制於蒸餾擴散中調整身份保持", "summary_zh": "近年湧現大量用於個人化生成且具備身分保持能力的擴散模型適配器。其主要缺點在於多與基礎擴散模型聯合訓練，導致多步推論緩慢。本研究旨在解決預訓練身分適配器免訓練適應加速擴散模型的問題。透過重新設計用於少量步驟風格生成的無分類器引導，並在解耦區塊中採用注意力操作機制來改善身分相似度和保真度，我們提出通用FastFace框架。此外，我們亦開發了一種解耦的公共評估協定，用於評估身分保持適配器。", "audio": "audios/2505.21144v1.mp3", "timestamp": "2025-05-28T14:20:03.607690"}
{"query": "AI", "id": "2505.21342v1", "url": "http://arxiv.org/abs/2505.21342v1", "title": "PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims", "summary": "Patent claims define the scope of protection for an invention. If there are\nambiguities in a claim, it is rejected by the patent office. In the US, this is\nreferred to as indefiniteness (35 U.S.C {\\S} 112(b)) and is among the most\nfrequent reasons for patent application rejection. The development of automatic\nmethods for patent definiteness examination has the potential to make patent\ndrafting and examination more efficient, but no annotated dataset has been\npublished to date.\n  We introduce PEDANTIC (\\underline{P}at\\underline{e}nt\n\\underline{D}efiniteness Ex\\underline{a}mi\\underline{n}a\\underline{ti}on\n\\underline{C}orpus), a novel dataset of 14k US patent claims from patent\napplications relating to Natural Language Processing (NLP), annotated with\nreasons for indefiniteness. We construct PEDANTIC using a fully automatic\npipeline that retrieves office action documents from the USPTO and uses Large\nLanguage Models (LLMs) to extract the reasons for indefiniteness. A human\nvalidation study confirms the pipeline's accuracy in generating high-quality\nannotations. To gain insight beyond binary classification metrics, we implement\nan LLM-as-Judge evaluation that compares the free-form reasoning of every\nmodel-cited reason with every examiner-cited reason. We show that LLM agents\nbased on Qwen 2.5 32B and 72B struggle to outperform logistic regression\nbaselines on definiteness prediction, even though they often correctly identify\nthe underlying reasons. PEDANTIC provides a valuable resource for patent AI\nresearchers, enabling the development of advanced examination models. We will\npublicly release the dataset and code.", "authors": ["Valentin Knappich", "Annemarie Friedrich", "Anna Hätty", "Simon Razniewski"], "published_date": "2025-05-27", "title_zh": "PEDANTIC：用於專利請求項中定指性自動檢測的資料集", "summary_zh": "專利請求項界定了發明的保護範圍，含糊不清的請求項會被專利局駁回。美國稱此為不明確性（35 U.S.C § 112(b)），是專利申請被駁回的常見原因。自動專利明確性審查方法有潛力提高專利撰寫和審查效率，但目前尚無已發佈的標註資料集。\n\n本研究推出PEDANTIC（專利明確性審查語料庫），一個包含1.4萬條美國專利請求項的新資料集，這些請求項來自自然語言處理（NLP）相關的專利申請，並標註了不明確性的原因。PEDANTIC採用全自動流程構建，該流程從美國專利商標局檢索官方意見書，並使用大型語言模型（LLM）提取不明確性的原因。人為驗證研究證實了該流程在生成高品質標註方面的準確性。\n\n為了獲得超越二元分類指標的見解，我們實施了LLM即判官評估，將每個模型引用的理由與每個審查員引用的理由進行自由形式的推理比較。結果表明，即使基於Qwen 2.5 32B和72B的LLM代理能夠正確識別潛在原因，它們在明確性預測方面也難以超越邏輯回歸基準。PEDANTIC為專利AI研究人員提供了一個寶貴的資源，有助於開發先進的審查模型。我們將公開發佈資料集和程式碼。", "audio": "audios/2505.21342v1.mp3", "timestamp": "2025-05-28T15:19:50.602583"}
{"query": "Foundation Model", "id": "2505.20745v1", "url": "http://arxiv.org/abs/2505.20745v1", "title": "Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation", "summary": "Auscultation, particularly heart sound, is a non-invasive technique that\nprovides essential vital sign information. Recently, self-supervised acoustic\nrepresentation foundation models (FMs) have been proposed to offer insights\ninto acoustics-based vital signs. However, there has been little exploration of\nthe extent to which auscultation is encoded in these pre-trained FM\nrepresentations. In this work, using a publicly available phonocardiogram (PCG)\ndataset and a heart rate (HR) estimation model, we conduct a layer-wise\ninvestigation of six acoustic representation FMs: HuBERT, wav2vec2, wavLM,\nWhisper, Contrastive Language-Audio Pretraining (CLAP), and an in-house CLAP\nmodel. Additionally, we implement the baseline method from Nie et al., 2024\n(which relies on acoustic features) and show that overall, representation\nvectors from pre-trained foundation models (FMs) offer comparable performance\nto the baseline. Notably, HR estimation using the representations from the\naudio encoder of the in-house CLAP model outperforms the results obtained from\nthe baseline, achieving a lower mean absolute error (MAE) across various\ntrain/validation/test splits despite the domain mismatch.", "authors": ["Jingping Nie", "Dung T. Tran", "Karan Thakkar", "Vasudha Kowtha", "John Huang", "Carlos Avendano", "Erdrin Azemi", "Vikramjit Mitra"], "published_date": "2025-05-27", "title_zh": "基石模型隱藏表徵於心音聽診心率估計之應用", "summary_zh": "聽診，特別是心音，是一種重要的非侵入性生命徵象技術。近年來，自監督聲學表徵基礎模型(FM)被提出以深入了解基於聲學的生命徵象。然而，對這些預訓練FM表徵中聽診編碼程度的探索仍然不足。本研究利用公開的心音圖(PCG)數據集和心率(HR)估計模型，分層研究了六個聲學表徵FM：HuBERT、wav2vec2、wavLM、Whisper、Contrastive Language-Audio Pretraining (CLAP)和一個內部CLAP模型。結果表明，預訓練FM的表徵向量整體上提供了與基線方法相當的性能。值得注意的是，使用內部CLAP模型音訊編碼器的表徵進行HR估計，儘管存在領域不匹配，但在各種訓練/驗證/測試集上，其平均絕對誤差(MAE)均低於基線方法。", "audio": "audios/2505.20745v1.mp3", "timestamp": "2025-05-28T15:19:58.981010"}
{"query": "Diffusion Model", "id": "2505.21135v1", "url": "http://arxiv.org/abs/2505.21135v1", "title": "Learning Single Index Models with Diffusion Priors", "summary": "Diffusion models (DMs) have demonstrated remarkable ability to generate\ndiverse and high-quality images by efficiently modeling complex data\ndistributions. They have also been explored as powerful generative priors for\nsignal recovery, resulting in a substantial improvement in the quality of\nreconstructed signals. However, existing research on signal recovery with\ndiffusion models either focuses on specific reconstruction problems or is\nunable to handle nonlinear measurement models with discontinuous or unknown\nlink functions. In this work, we focus on using DMs to achieve accurate\nrecovery from semi-parametric single index models, which encompass a variety of\npopular nonlinear models that may have {\\em discontinuous} and {\\em unknown}\nlink functions. We propose an efficient reconstruction method that only\nrequires one round of unconditional sampling and (partial) inversion of DMs.\nTheoretical analysis on the effectiveness of the proposed methods has been\nestablished under appropriate conditions. We perform numerical experiments on\nimage datasets for different nonlinear measurement models. We observe that\ncompared to competing methods, our approach can yield more accurate\nreconstructions while utilizing significantly fewer neural function\nevaluations.", "authors": ["Anqi Tang", "Youming Chen", "Shuchen Xue", "Zhaoqiang Liu"], "published_date": "2025-05-27", "title_zh": "基於擴散先驗的單指標模型學習", "summary_zh": "擴散模型擅長生成多樣且高品質的圖像，有效模擬複雜數據分佈。其作為生成先驗應用於信號恢復，顯著提升重建信號品質。然而，現有研究多側重特定重建問題，或無法處理具不連續或未知連接函數的非線性測量模型。本研究著重利用擴散模型，自半參數單指標模型中精確重建，涵蓋多種具不連續及未知連接函數的常見非線性模型。我們提出一種高效重建方法，僅需一輪無條件採樣及擴散模型的（部分）反演。理論分析驗證了該方法在特定條件下的有效性。針對不同非線性測量模型的圖像數據集，數值實驗顯示，相較於其他方法，本方法能以顯著更少的神經函數評估次數，實現更精確的重建。", "audio": "audios/2505.21135v1.mp3", "timestamp": "2025-05-28T15:20:06.520588"}
{"query": "AI", "id": "2505.21318v1", "url": "http://arxiv.org/abs/2505.21318v1", "title": "Beyond Chemical QA: Evaluating LLM's Chemical Reasoning with Modular Chemical Operations", "summary": "While large language models (LLMs) with Chain-of-Thought (CoT) reasoning\nexcel in mathematics and coding, their potential for systematic reasoning in\nchemistry, a domain demanding rigorous structural analysis for real-world tasks\nlike drug design and reaction engineering, remains untapped. Current benchmarks\nfocus on simple knowledge retrieval, neglecting step-by-step reasoning required\nfor complex tasks such as molecular optimization and reaction prediction. To\naddress this, we introduce ChemCoTBench, a reasoning framework that bridges\nmolecular structure understanding with arithmetic-inspired operations,\nincluding addition, deletion, and substitution, to formalize chemical\nproblem-solving into transparent, step-by-step workflows. By treating molecular\ntransformations as modular \"chemical operations\", the framework enables\nslow-thinking reasoning, mirroring the logic of mathematical proofs while\ngrounding solutions in real-world chemical constraints. We evaluate models on\ntwo high-impact tasks: Molecular Property Optimization and Chemical Reaction\nPrediction. These tasks mirror real-world challenges while providing structured\nevaluability. By providing annotated datasets, a reasoning taxonomy, and\nbaseline evaluations, ChemCoTBench bridges the gap between abstract reasoning\nmethods and practical chemical discovery, establishing a foundation for\nadvancing LLMs as tools for AI-driven scientific innovation.", "authors": ["Hao Li", "He Cao", "Bin Feng", "Yanjun Shao", "Xiangru Tang", "Zhiyuan Yan", "Li Yuan", "Yonghong Tian", "Yu Li"], "published_date": "2025-05-27", "title_zh": "超越化學QA：以模組化學操作評估大型語言模型的化學推理能力", "summary_zh": "大型語言模型雖擅長基於思維鏈的數學和程式碼推理，但其在化學領域的系統性推理潛力尚未開發。現有基準測試側重於簡單知識檢索，忽略了分子優化和反應預測等複雜任務所需的逐步推理。為此，我們提出了ChemCoTBench，一種將分子結構理解與加法、刪除和替換等運算相結合的推理框架，將化學問題求解形式化為透明的逐步工作流程。透過將分子轉化視為模組化的化學操作，該框架實現了緩慢思考推理，體現了數學證明的邏輯，並將解決方案紮根於真實化學約束中。我們在分子性質優化和化學反應預測這兩個高影響任務上評估模型。ChemCoTBench透過提供帶註釋的數據集、推理分類和基準評估，彌合了抽象推理方法與實際化學發現之間的差距，為推進大型語言模型作為人工智慧驅動科學創新工具奠定基礎。", "audio": "audios/2505.21318v1.mp3", "timestamp": "2025-05-28T16:22:24.014238"}
{"query": "Foundation Model", "id": "2505.20729v1", "url": "http://arxiv.org/abs/2505.20729v1", "title": "Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting", "summary": "Sparse-view scene reconstruction often faces significant challenges due to\nthe constraints imposed by limited observational data. These limitations result\nin incomplete information, leading to suboptimal reconstructions using existing\nmethodologies. To address this, we present Intern-GS, a novel approach that\neffectively leverages rich prior knowledge from vision foundation models to\nenhance the process of sparse-view Gaussian Splatting, thereby enabling\nhigh-quality scene reconstruction. Specifically, Intern-GS utilizes vision\nfoundation models to guide both the initialization and the optimization process\nof 3D Gaussian splatting, effectively addressing the limitations of sparse\ninputs. In the initialization process, our method employs DUSt3R to generate a\ndense and non-redundant gaussian point cloud. This approach significantly\nalleviates the limitations encountered by traditional structure-from-motion\n(SfM) methods, which often struggle under sparse-view constraints. During the\noptimization process, vision foundation models predict depth and appearance for\nunobserved views, refining the 3D Gaussians to compensate for missing\ninformation in unseen regions. Extensive experiments demonstrate that Intern-GS\nachieves state-of-the-art rendering quality across diverse datasets, including\nboth forward-facing and large-scale scenes, such as LLFF, DTU, and Tanks and\nTemples.", "authors": ["Xiangyu Sun", "Runnan Chen", "Mingming Gong", "Dong Xu", "Tongliang Liu"], "published_date": "2025-05-27", "title_zh": "Intern-GS：視覺模型引導的稀疏視角三維高斯潑濺", "summary_zh": "稀疏視角場景重建因觀測數據有限而面臨挑戰，導致資訊不完整，重建效果欠佳。為此，我們提出 Intern-GS，一種利用視覺基礎模型先驗知識增強稀疏視角高斯濺射的新方法，實現高品質場景重建。Intern-GS 利用視覺基礎模型引導 3D 高斯濺射的初始化和優化，有效解決稀疏輸入限制。在初始化階段，使用 DUSt3R 生成密集且非冗餘的高斯點雲，緩解傳統運動結構法在稀疏視角下的局限。在優化階段，視覺基礎模型預測未觀察視角的深度和外觀，優化 3D 高斯以彌補未見區域的資訊缺失。實驗表明，Intern-GS 在 LLFF、DTU 和 Tanks and Temples 等多個數據集上，針對前向和大型場景均實現了最先進的渲染品質。", "audio": "audios/2505.20729v1.mp3", "timestamp": "2025-05-28T16:22:30.933871"}
{"query": "Diffusion Model", "id": "2505.21114v1", "url": "http://arxiv.org/abs/2505.21114v1", "title": "Differentiable Solver Search for Fast Diffusion Sampling", "summary": "Diffusion models have demonstrated remarkable generation quality but at the\ncost of numerous function evaluations. Recently, advanced ODE-based solvers\nhave been developed to mitigate the substantial computational demands of\nreverse-diffusion solving under limited sampling steps. However, these solvers,\nheavily inspired by Adams-like multistep methods, rely solely on t-related\nLagrange interpolation. We show that t-related Lagrange interpolation is\nsuboptimal for diffusion model and reveal a compact search space comprised of\ntime steps and solver coefficients. Building on our analysis, we propose a\nnovel differentiable solver search algorithm to identify more optimal solver.\nEquipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and\nFlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256\nwith only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of\n2.33 with only 10 steps. Notably, our searched solver outperforms traditional\nsolvers by a significant margin. Moreover, our searched solver demonstrates\ngenerality across various model architectures, resolutions, and model sizes.", "authors": ["Shuai Wang", "Zexian Li", "Qipeng zhang", "Tianhui Song", "Xubin Li", "Tiezheng Ge", "Bo Zheng", "Limin Wang"], "published_date": "2025-05-27", "title_zh": "快速擴散採樣之可微分求解器搜尋", "summary_zh": "擴散模型展現卓越生成品質，但運算量龐大。近期基於常微分方程的求解器旨在減少反向擴散過程的運算需求，但這些求解器仰賴與時間相關的拉格朗日插值法，我們指出此方法並非最佳解。我們揭示了一個由時間步長和求解器係數組成的精簡搜尋空間，並提出一種可微分的求解器搜尋演算法，以尋找更優解。使用此求解器，修正流模型SiT-XL/2和FlowDCN-XL/2在ImageNet256上僅需10步即可達到2.40和2.35的FID分數，DDPM模型DiT-XL/2僅需10步即可達到2.33的FID分數。我們搜尋出的求解器明顯優於傳統求解器，並展現了跨模型架構、解析度和模型大小的通用性。", "audio": "audios/2505.21114v1.mp3", "timestamp": "2025-05-28T16:22:37.321135"}
{"query": "AI", "id": "2505.21312v1", "url": "http://arxiv.org/abs/2505.21312v1", "title": "Generalised Time-Series Analysis of Fault Mechanics Using Explainable AI", "summary": "Understanding how faults nucleate and grow is a critical problem in\nearthquake science and hazard assessment. This study examines fault development\nin Alzo granite under triaxial pressures ranging from 5 to 40 MPa by applying a\nTime Delay Neural Network (TDNN) to multi-parameter acoustic emission (AE)\ndata. The TDNN integrates waveform-derived attributes, including peak delay and\nscattering attenuation, with occurrence-based metrics such as time\ndistributions, Gutenberg-Richter b-values, and spatial fractal dimensions, to\ncharacterize the transition from distributed microcracking to localised\nfaulting. Optimised via genetic algorithms, the TDNN dynamically weights these\nparameters, enabling accurate characterisation of fault growth stages. Our\nresults delineate three distinct phases of fault evolution: nucleation of\nrandom microcracks indicated by changes in elastic wave scattering, initiation\nof fault growth reflected in evolving AE spatial and magnitude distributions,\nand fault coalescence marked by exponential increases in peak delay and b-value\nshifts. The model predicts the timing and magnitude of stress drops across\nvarying pressures and failure mechanisms, from axial splitting to shear\nlocalisation, providing deeper insights into fault mechanics through\nexplainable AI models.", "authors": ["Thomas King", "Sergio Vinciguerra"], "published_date": "2025-05-27", "title_zh": "基於可解釋人工智慧的斷層力學廣義時間序列分析", "summary_zh": "本研究運用時間延遲神經網路（TDNN）分析多參數聲發射（AE）數據，探討Alzo花崗岩在5至40 MPa三軸壓力下的斷層發展過程。TDNN整合波形屬性（如峰值延遲、散射衰減）與基於發生率的指標（時間分布、Gutenberg-Richter b值、空間碎形維數），藉由基因演算法優化，動態權衡這些參數，精準刻劃斷層生長階段。研究結果界定斷層演化的三個階段：彈性波散射變化顯示隨機微裂紋的成核、AE空間與震級分布演變反映斷層生長的啟動，以及峰值延遲指數增長和b值偏移標誌斷層的聚合。該模型能預測不同壓力和破壞機制（從軸向分裂到剪切局部化）下的應力降時間和幅度，透過可解釋的人工智慧模型，深入了解斷層力學。", "audio": "audios/2505.21312v1.mp3", "timestamp": "2025-05-28T17:16:47.742349"}
{"query": "Foundation Model", "id": "2505.20685v1", "url": "http://arxiv.org/abs/2505.20685v1", "title": "GIT-BO: High-Dimensional Bayesian Optimization with Tabular Foundation Models", "summary": "Bayesian optimization (BO) effectively optimizes expensive black-box\nfunctions but faces significant challenges in high-dimensional spaces\n(dimensions exceeding 100) due to the curse of dimensionality. Existing\nhigh-dimensional BO methods typically leverage low-dimensional embeddings or\nstructural assumptions to mitigate this challenge, yet these approaches\nfrequently incur considerable computational overhead and rigidity due to\niterative surrogate retraining and fixed assumptions. To address these\nlimitations, we propose Gradient-Informed Bayesian Optimization using Tabular\nFoundation Models (GIT-BO), an approach that utilizes a pre-trained tabular\nfoundation model (TFM) as a surrogate, leveraging its gradient information to\nadaptively identify low-dimensional subspaces for optimization. We propose a\nway to exploit internal gradient computations from the TFM's forward pass by\ncreating a gradient-informed diagnostic matrix that reveals the most sensitive\ndirections of the TFM's predictions, enabling optimization in a continuously\nre-estimated active subspace without the need for repeated model retraining.\nExtensive empirical evaluation across 23 synthetic and real-world benchmarks\ndemonstrates that GIT-BO consistently outperforms four state-of-the-art\nGaussian process-based high-dimensional BO methods, showing superior\nscalability and optimization performances, especially as dimensionality\nincreases up to 500 dimensions. This work establishes foundation models,\naugmented with gradient-informed adaptive subspace identification, as highly\ncompetitive alternatives to traditional Gaussian process-based approaches for\nhigh-dimensional Bayesian optimization tasks.", "authors": ["Rosen Ting-Ying Yu", "Cyril Picard", "Faez Ahmed"], "published_date": "2025-05-27", "title_zh": "GIT-BO：基於表格基礎模型的高維貝葉斯優化", "summary_zh": "貝氏優化在高維空間中面臨維度災難。現有方法常因重複訓練代理模型和固定假設而產生額外計算負擔和限制。為了解決這些問題，我們提出基於表格基礎模型且具梯度資訊的貝氏優化（GIT-BO），利用預訓練的表格基礎模型作為代理，並透過其梯度資訊自適應地識別低維子空間進行優化。我們透過表格基礎模型前向傳播的內部梯度計算，建立梯度資訊診斷矩陣，揭示模型預測中最敏感的方向，從而在持續重新估算的有效子空間中進行優化，無需重複模型訓練。在23個合成和真實基準測試中，GIT-BO始終優於四種最先進的基於高斯過程的高維貝氏優化方法，展現出卓越的可擴展性和優化性能，尤其是在維度增加到500維時。這項研究確立了基礎模型，透過梯度資訊自適應子空間識別進行增強，成為高維貝氏優化任務中傳統基於高斯過程方法的極具競爭力的替代方案。", "audio": "audios/2505.20685v1.mp3", "timestamp": "2025-05-28T17:16:59.288129"}
{"query": "Diffusion Model", "id": "2505.21101v1", "url": "http://arxiv.org/abs/2505.21101v1", "title": "Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance", "summary": "Classifier-Free Guidance (CFG) is a widely used technique for improving\nconditional diffusion models by linearly combining the outputs of conditional\nand unconditional denoisers. While CFG enhances visual quality and improves\nalignment with prompts, it often reduces sample diversity, leading to a\nchallenging trade-off between quality and diversity. To address this issue, we\nmake two key contributions. First, CFG generally does not correspond to a\nwell-defined denoising diffusion model (DDM). In particular, contrary to common\nintuition, CFG does not yield samples from the target distribution associated\nwith the limiting CFG score as the noise level approaches zero -- where the\ndata distribution is tilted by a power $w \\gt 1$ of the conditional\ndistribution. We identify the missing component: a R\\'enyi divergence term that\nacts as a repulsive force and is required to correct CFG and render it\nconsistent with a proper DDM. Our analysis shows that this correction term\nvanishes in the low-noise limit. Second, motivated by this insight, we propose\na Gibbs-like sampling procedure to draw samples from the desired tilted\ndistribution. This method starts with an initial sample from the conditional\ndiffusion model without CFG and iteratively refines it, preserving diversity\nwhile progressively enhancing sample quality. We evaluate our approach on both\nimage and text-to-audio generation tasks, demonstrating substantial\nimprovements over CFG across all considered metrics. The code is available at\nhttps://github.com/yazidjanati/cfgig", "authors": ["Badr Moufad", "Yazid Janati", "Alain Durmus", "Ahmed Ghorbel", "Eric Moulines", "Jimmy Olsson"], "published_date": "2025-05-27", "title_zh": "具備無分類器吉布斯式引導的條件擴散模型", "summary_zh": "無分類器引導(CFG)是一種廣泛使用的技術，透過線性組合條件和非條件降噪器的輸出，以改善條件擴散模型。儘管CFG能提升視覺品質並改善與提示的對齊，但往往會降低樣本多樣性，導致品質與多樣性之間難以取捨。為了解決此問題，我們提出了兩個主要貢獻。首先，CFG通常不對應於一個定義完善的降噪擴散模型(DDM)。特別是，與常見的直覺相反，當雜訊水平接近於零時，CFG不會產生來自目標分佈的樣本，該目標分佈與極限CFG分數相關聯，其中數據分佈由條件分佈的$w \\gt 1$次冪傾斜。我們確定了缺失的組成部分：一個R\\'enyi散度項，它作為一種排斥力，是校正CFG並使其與適當的DDM一致所必需的。我們的分析表明，這種校正項在低雜訊限制下消失。其次，受此啟發，我們提出了一種類似吉布斯抽樣的程序，用於從所需的傾斜分佈中抽取樣本。此方法從沒有CFG的條件擴散模型的初始樣本開始，並迭代地細化它，在逐步提高樣本品質的同時，保留多樣性。我們在圖像和文本到音訊生成任務中評估了我們的方法，證明在所有考慮的指標上，相較於CFG都有顯著的改進。", "audio": "audios/2505.21101v1.mp3", "timestamp": "2025-05-28T17:17:15.495231"}
{"query": "AI", "id": "2505.21301v1", "url": "http://arxiv.org/abs/2505.21301v1", "title": "How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian", "summary": "People can categorize the same entity at multiple taxonomic levels, such as\nbasic (bear), superordinate (animal), and subordinate (grizzly bear). While\nprior research has focused on basic-level categories, this study is the first\nattempt to examine the organization of categories by analyzing exemplars\nproduced at the subordinate level. We present a new Italian psycholinguistic\ndataset of human-generated exemplars for 187 concrete words. We then use these\ndata to evaluate whether textual and vision LLMs produce meaningful exemplars\nthat align with human category organization across three key tasks: exemplar\ngeneration, category induction, and typicality judgment. Our findings show a\nlow alignment between humans and LLMs, consistent with previous studies.\nHowever, their performance varies notably across different semantic domains.\nUltimately, this study highlights both the promises and the constraints of\nusing AI-generated exemplars to support psychological and linguistic research.", "authors": ["Andrea Pedrotti", "Giulia Rambelli", "Caterina Villani", "Marianna Bolognesi"], "published_date": "2025-05-27", "title_zh": "人類與大型語言模型如何組織概念知識：探索義大利語中的下位範疇", "summary_zh": "人們能以多種分類層級組織同一實體，如基本層級（熊）、上層級（動物）和下層級（灰熊）。過往研究多集中於基本層級，本研究首度分析下層級範例，探討分類組織。我們提出一個義大利語心理語言學資料集，包含187個具體詞彙的人工生成範例。利用此資料集，我們評估文本和視覺大型語言模型（LLM）能否生成符合人類分類組織的有意義範例，涵蓋範例生成、類別歸納和典型性判斷三項任務。結果顯示，人類與LLM的一致性偏低，但不同語義領域的表現差異顯著。本研究突顯了利用AI生成範例輔助心理學和語言學研究的潛力與限制。", "audio": "audios/2505.21301v1.mp3", "timestamp": "2025-05-28T18:25:09.724311"}
{"query": "Foundation Model", "id": "2505.20629v1", "url": "http://arxiv.org/abs/2505.20629v1", "title": "Incorporating Flexible Image Conditioning into Text-to-Video Diffusion Models without Training", "summary": "Text-image-to-video (TI2V) generation is a critical problem for controllable\nvideo generation using both semantic and visual conditions. Most existing\nmethods typically add visual conditions to text-to-video (T2V) foundation\nmodels by finetuning, which is costly in resources and only limited to a few\npredefined conditioning settings. To tackle this issue, we introduce a unified\nformulation for TI2V generation with flexible visual conditioning. Furthermore,\nwe propose an innovative training-free approach, dubbed FlexTI2V, that can\ncondition T2V foundation models on an arbitrary amount of images at arbitrary\npositions. Specifically, we firstly invert the condition images to noisy\nrepresentation in a latent space. Then, in the denoising process of T2V models,\nour method uses a novel random patch swapping strategy to incorporate visual\nfeatures into video representations through local image patches. To balance\ncreativity and fidelity, we use a dynamic control mechanism to adjust the\nstrength of visual conditioning to each video frame. Extensive experiments\nvalidate that our method surpasses previous training-free image conditioning\nmethods by a notable margin. We also show more insights of our method by\ndetailed ablation study and analysis.", "authors": ["Bolin Lai", "Sangmin Lee", "Xu Cao", "Xiang Li", "James M. Rehg"], "published_date": "2025-05-27", "title_zh": "無需訓練將彈性圖像條件融入文本到視頻擴散模型", "summary_zh": "文本圖像轉影片生成是可控影片生成的關鍵問題，它利用語義和視覺條件。現有方法通常透過微調將視覺條件加入文本轉影片基礎模型，這耗費資源且僅限於少數預定義設置。為了解決此問題，我們提出一個統一的文本圖像轉影片生成公式，具有靈活的視覺條件。此外，我們提出一種創新的免訓練方法FlexTI2V，可在任意位置以任意數量的圖像為文本轉影片基礎模型提供條件。具體而言，我們首先將條件圖像反轉為潛在空間中的噪聲表示。然後，在文本轉影片模型的去噪過程中，我們的方法使用一種新穎的隨機補丁交換策略，透過局部圖像補丁將視覺特徵整合到影片表示中。為了平衡創造力和保真度，我們使用動態控制機制來調整每個影片幀的視覺條件強度。大量實驗驗證了我們的方法明顯優於先前的免訓練圖像條件方法。我們還透過詳細的消融研究和分析展示了我們方法的更多見解。", "audio": "audios/2505.20629v1.mp3", "timestamp": "2025-05-28T18:25:16.521309"}
{"query": "Diffusion Model", "id": "2505.21070v1", "url": "http://arxiv.org/abs/2505.21070v1", "title": "Minute-Long Videos with Dual Parallelisms", "summary": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs.", "authors": ["Zeqing Wang", "Bowen Zheng", "Xingyi Yang", "Yuecong Xu", "Xinchao Wang"], "published_date": "2025-05-27", "title_zh": "具雙重並行性的一分鐘短片", "summary_zh": "基於擴散轉換器(DiT)的影片擴散模型雖能大規模生成高品質影片，但處理長影片時延遲高、記憶體成本大。為此，我們提出一種名為DualParal的新型分散式推論策略，將時間幀和模型層在多個GPU上並行處理。為了解決因擴散模型需同步噪音水平導致原始並行性序列化的問題，我們採用分塊去噪方案，以遞減的噪音水平處理幀序列。每個GPU處理特定的塊和層子集，並將結果傳遞給下一個GPU，實現異步計算和通訊。為進一步優化效能，我們採用特徵快取來儲存和重用先前塊的特徵作為上下文，以減少GPU間的通訊和冗餘計算；並採用協調噪音初始化策略，透過共享初始噪音模式確保全局一致的時間動態，且無需額外資源成本。此方法可快速生成無瑕疵的無限長影片。應用於最新的擴散轉換器影片生成器，在8個RTX 4090 GPU上，我們的技術能有效生成1025幀影片，延遲降低高達6.54倍，記憶體成本降低1.48倍。", "audio": "audios/2505.21070v1.mp3", "timestamp": "2025-05-28T18:25:24.123505"}
{"query": "AI", "id": "2505.21286v1", "url": "http://arxiv.org/abs/2505.21286v1", "title": "PACT: A Contract-Theoretic Framework for Pricing Agentic AI Services Powered by Large Language Models", "summary": "Agentic AI, often powered by large language models (LLMs), is becoming\nincreasingly popular and adopted to support autonomous reasoning,\ndecision-making, and task execution across various domains. While agentic AI\nholds great promise, its deployment as services for easy access raises critical\nchallenges in pricing, due to high infrastructure and computation costs,\nmulti-dimensional and task-dependent Quality of Service (QoS), and growing\nconcerns around liability in high-stakes applications. In this work, we propose\nPACT, a Pricing framework for cloud-based Agentic AI services through a\nContract-Theoretic approach, which models QoS along both objective (e.g.,\nresponse time) and subjective (e.g., user satisfaction) dimensions. PACT\naccounts for computational, infrastructure, and potential liability costs for\nthe service provider, while ensuring incentive compatibility and individual\nrationality for the user under information asymmetry. Through contract-based\nselection, users receive tailored service offerings aligned with their needs.\nNumerical evaluations demonstrate that PACT improves QoS alignment between\nusers and providers and offers a scalable, liable approach to pricing agentic\nAI services in the future.", "authors": ["Ya-Ting Yang", "Quanyan Zhu"], "published_date": "2025-05-27", "title_zh": "PACT：基於合約理論的大型語言模型驅動自主智能體服務定價框架", "summary_zh": "基於大型語言模型的自主AI日益普及，應用於各領域的自主推理、決策與任務執行。然而，作為雲端服務部署，其定價面臨基礎設施和算力成本高昂、多維度且任務相關的服務品質(QoS)，以及高風險應用中責任歸屬等挑戰。本研究提出PACT框架，透過契約理論為雲端自主AI服務定價，模型涵蓋客觀（如響應時間）和主觀（如用戶滿意度）QoS維度。PACT考慮服務提供商的算力、基礎設施和潛在責任成本，同時確保資訊不對稱下用戶的激勵相容性和個體理性。透過契約選擇，用戶獲得符合需求的客製化服務。數值評估表明，PACT改善了用戶與提供商之間的QoS匹配，為未來自主AI服務的定價提供可擴展且可歸責的方法。", "audio": "audios/2505.21286v1.mp3", "timestamp": "2025-05-28T19:15:28.510040"}
{"query": "Foundation Model", "id": "2505.20617v1", "url": "http://arxiv.org/abs/2505.20617v1", "title": "OccLE: Label-Efficient 3D Semantic Occupancy Prediction", "summary": "3D semantic occupancy prediction offers an intuitive and efficient scene\nunderstanding and has attracted significant interest in autonomous driving\nperception. Existing approaches either rely on full supervision, which demands\ncostly voxel-level annotations, or on self-supervision, which provides limited\nguidance and yields suboptimal performance. To address these challenges, we\npropose OccLE, a Label-Efficient 3D Semantic Occupancy Prediction that takes\nimages and LiDAR as inputs and maintains high performance with limited voxel\nannotations. Our intuition is to decouple the semantic and geometric learning\ntasks and then fuse the learned feature grids from both tasks for the final\nsemantic occupancy prediction. Therefore, the semantic branch distills 2D\nfoundation model to provide aligned pseudo labels for 2D and 3D semantic\nlearning. The geometric branch integrates image and LiDAR inputs in cross-plane\nsynergy based on their inherency, employing semi-supervision to enhance\ngeometry learning. We fuse semantic-geometric feature grids through Dual Mamba\nand incorporate a scatter-accumulated projection to supervise unannotated\nprediction with aligned pseudo labels. Experiments show that OccLE achieves\ncompetitive performance with only 10% of voxel annotations, reaching a mIoU of\n16.59% on the SemanticKITTI validation set.", "authors": ["Naiyu Fang", "Zheyuan Zhou", "Fayao Liu", "Xulei Yang", "Jiacheng Wei", "Lemiao Qiu", "Guosheng Lin"], "published_date": "2025-05-27", "title_zh": "OccLE：標籤高效的三維語義佔據預測", "summary_zh": "三維語義佔據預測在自動駕駛感知中備受關注，但現有方法仰賴高成本的完整監督或效果有限的自我監督。為解決此問題，我們提出OccLE，一種標籤高效的三維語義佔據預測方法，以圖像和光達為輸入，僅需少量體素標註即可維持高性能。OccLE將語義和幾何學習解耦，融合兩者學習到的特徵網格以進行最終預測。語義分支藉由2D基礎模型提煉對齊的偽標籤，用於2D和3D語義學習。幾何分支基於圖像和光達的內在特性，採用半監督方式在跨平面協同中整合輸入，以增強幾何學習。我們透過Dual Mamba融合語義幾何特徵網格，並整合散射累積投影，以對齊的偽標籤監督未標註的預測。實驗表明，OccLE僅使用10%的體素標註即可達到具競爭力的性能，在SemanticKITTI驗證集上達到16.59%的mIoU。", "audio": "audios/2505.20617v1.mp3", "timestamp": "2025-05-28T19:15:35.013771"}
{"query": "Diffusion Model", "id": "2505.21036v1", "url": "http://arxiv.org/abs/2505.21036v1", "title": "RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy", "summary": "Video generation using diffusion models is highly computationally intensive,\nwith 3D attention in Diffusion Transformer (DiT) models accounting for over\n80\\% of the total computational resources. In this work, we introduce {\\bf\nRainFusion}, a novel training-free sparse attention method that exploits\ninherent sparsity nature in visual data to accelerate attention computation\nwhile preserving video quality. Specifically, we identify three unique sparse\npatterns in video generation attention calculations--Spatial Pattern, Temporal\nPattern and Textural Pattern. The sparse pattern for each attention head is\ndetermined online with negligible overhead (\\textasciitilde\\,0.2\\%) with our\nproposed {\\bf ARM} (Adaptive Recognition Module) during inference. Our proposed\n{\\bf RainFusion} is a plug-and-play method, that can be seamlessly integrated\ninto state-of-the-art 3D-attention video generation models without additional\ntraining or calibration. We evaluate our method on leading open-sourced models\nincluding HunyuanVideo, OpenSoraPlan-1.2 and CogVideoX-5B, demonstrating its\nbroad applicability and effectiveness. Experimental results show that\nRainFusion achieves over {\\bf 2\\(\\times\\)} speedup in attention computation\nwhile maintaining video quality, with only a minimal impact on VBench scores\n(-0.2\\%).", "authors": ["Aiyue Chen", "Bin Dong", "Jingru Li", "Jing Lin", "Yiwu Yao", "Gongyi Wang"], "published_date": "2025-05-27", "title_zh": "RainFusion：基於多維視覺冗餘的自適應影片生成加速", "summary_zh": "基於擴散模型的影片生成計算量龐大，其中擴散轉換器(DiT)模型的3D注意力佔據超過80%的計算資源。本研究提出RainFusion，一種免訓練的稀疏注意力方法，利用視覺資料的內在稀疏性加速注意力計算，同時維持影片品質。RainFusion辨識出影片生成注意力計算中的三種獨特稀疏模式：空間模式、時間模式和紋理模式。在推論階段，透過提出的自適應辨識模組(ARM)，線上決定每個注意力頭的稀疏模式，開銷可忽略不計(約0.2%)。RainFusion是一種即插即用方法，可無縫整合到最先進的3D注意力影片生成模型中，無需額外訓練或校準。在HunyuanVideo、OpenSoraPlan-1.2和CogVideoX-5B等模型上的評估表明，RainFusion具有廣泛的適用性和有效性。實驗結果顯示，RainFusion在保持影片品質的同時，將注意力計算速度提升超過2倍，且對VBench分數的影響極小(-0.2%)。", "audio": "audios/2505.21036v1.mp3", "timestamp": "2025-05-28T19:15:42.594575"}
{"query": "AI", "id": "2505.21281v1", "url": "http://arxiv.org/abs/2505.21281v1", "title": "RLJP: Legal Judgment Prediction via First-Order Logic Rule-enhanced with Large Language Models", "summary": "Legal Judgment Prediction (LJP) is a pivotal task in legal AI. Existing\nsemantic-enhanced LJP models integrate judicial precedents and legal knowledge\nfor high performance. But they neglect legal reasoning logic, a critical\ncomponent of legal judgments requiring rigorous logical analysis. Although some\napproaches utilize legal reasoning logic for high-quality predictions, their\nlogic rigidity hinders adaptation to case-specific logical frameworks,\nparticularly in complex cases that are lengthy and detailed. This paper\nproposes a rule-enhanced legal judgment prediction framework based on\nfirst-order logic (FOL) formalism and comparative learning (CL) to develop an\nadaptive adjustment mechanism for legal judgment logic and further enhance\nperformance in LJP. Inspired by the process of human exam preparation, our\nmethod follows a three-stage approach: first, we initialize judgment rules\nusing the FOL formalism to capture complex reasoning logic accurately; next, we\npropose a Confusion-aware Contrastive Learning (CACL) to dynamically optimize\nthe judgment rules through a quiz consisting of confusable cases; finally, we\nutilize the optimized judgment rules to predict legal judgments. Experimental\nresults on two public datasets show superior performance across all metrics.\nThe code is publicly available{https://anonymous.4open.science/r/RLJP-FDF1}.", "authors": ["Yue Zhang", "Zhiliang Tian", "Shicheng Zhou", "Haiyang Wang", "Wenqing Hou", "Yuying Liu", "Xuechen Zhao", "Minlie Huang", "Ye Wang", "Bin Zhou"], "published_date": "2025-05-27", "title_zh": "RLJP：基於一階邏輯規則增強大型語言模型的法律判決預測", "summary_zh": "法律判決預測是法律人工智慧的關鍵任務。現有語義增強模型整合司法判例與法律知識以提升效能，但忽略了法律推理邏輯。雖然部分方法利用法律推理邏輯來提高預測品質，但其邏輯僵化限制了對特定案件邏輯框架的適應性，尤其是在冗長且複雜的案件中。本文提出基於一階邏輯形式主義和比較學習的規則增強法律判決預測框架，以開發法律判決邏輯的自適應調整機制，並進一步提高法律判決預測的效能。受人類備考過程啟發，本方法分為三個階段：首先，使用一階邏輯形式主義初始化判決規則，以準確捕捉複雜的推理邏輯；其次，提出一種混淆感知對比學習，透過由易混淆案例組成的測驗動態優化判決規則；最後，利用優化後的判決規則預測法律判決。在兩個公共資料集上的實驗結果表明，該方法在所有指標上均表現出色。", "audio": "audios/2505.21281v1.mp3", "timestamp": "2025-05-28T20:21:03.171040"}
{"query": "Foundation Model", "id": "2505.20606v1", "url": "http://arxiv.org/abs/2505.20606v1", "title": "Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation", "summary": "Whisper's robust performance in automatic speech recognition (ASR) is often\nattributed to its massive 680k-hour training set, an impractical scale for most\nresearchers. In this work, we examine how linguistic and acoustic diversity in\ntraining data affect the robustness of the ASR model and reveal that\ntranscription generalization is primarily driven by acoustic variation rather\nthan linguistic richness. We find that targeted acoustic augmentation methods\ncould significantly improve the generalization ability of ASR models, reducing\nword-error rates by up to 19.24 percent on unseen datasets when training on the\n960-hour Librispeech dataset. These findings highlight strategic acoustically\nfocused data augmentation as a promising alternative to massive datasets for\nbuilding robust ASR models, offering a potential solution to future foundation\nASR models when massive human speech data is lacking.", "authors": ["Dancheng Liu", "Amir Nassereldine", "Chenhui Xu", "Jinjun Xiong"], "published_date": "2025-05-27", "title_zh": "基於聲學感知數據增強的穩健ASR基礎模型預訓練研究", "summary_zh": "Whisper在語音辨識上的優異表現歸功於其龐大的訓練資料集，但此規模對多數研究者而言不切實際。本研究探討訓練資料中語言和聲音多樣性如何影響語音辨識模型的穩健性，並揭示轉錄泛化主要由聲音變異驅動，而非語言豐富性。研究發現，有針對性的聲音增強方法可顯著提升語音辨識模型的泛化能力，在960小時Librispeech資料集上訓練時，於未見資料集上降低高達19.24%的詞錯誤率。此結果表明，策略性的聲音導向資料增強是建立穩健語音辨識模型的可行替代方案，為未來缺乏大量人類語音資料的基礎語音辨識模型提供潛在解決方案。", "audio": "audios/2505.20606v1.mp3", "timestamp": "2025-05-28T20:21:47.506632"}
{"query": "Diffusion Model", "id": "2505.21032v1", "url": "http://arxiv.org/abs/2505.21032v1", "title": "FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models", "summary": "Internal representations are crucial for understanding deep neural networks,\nsuch as their properties and reasoning patterns, but remain difficult to\ninterpret. While mapping from feature space to input space aids in interpreting\nthe former, existing approaches often rely on crude approximations. We propose\nusing a conditional diffusion model - a pretrained high-fidelity diffusion\nmodel conditioned on spatially resolved feature maps - to learn such a mapping\nin a probabilistic manner. We demonstrate the feasibility of this approach\nacross various pretrained image classifiers from CNNs to ViTs, showing\nexcellent reconstruction capabilities. Through qualitative comparisons and\nrobustness analysis, we validate our method and showcase possible applications,\nsuch as the visualization of concept steering in input space or investigations\nof the composite nature of the feature space. This approach has broad potential\nfor improving feature space understanding in computer vision models.", "authors": ["Nils Neukirch", "Johanna Vielhaben", "Nils Strodthoff"], "published_date": "2025-05-27", "title_zh": "FeatInv：使用條件擴散模型從特徵空間到輸入空間的空間解析映射", "summary_zh": "深度神經網路的內部表徵對於理解其特性和推理模式至關重要，但難以詮釋。將特徵空間映射到輸入空間有助於理解前者，但現有方法通常依賴粗略近似。本文提出使用條件擴散模型，即基於空間解析特徵圖預訓練的高保真度擴散模型，以機率方式學習這種映射。我們驗證了此方法在各種預訓練圖像分類器（從 CNN 到 ViT）中的可行性，展現了出色的重建能力。透過定性比較和穩健性分析，我們驗證了該方法，並展示了可能的應用，例如輸入空間中的概念引導可視化或特徵空間複合特性的研究。此方法在提升電腦視覺模型中特徵空間理解方面具有廣泛潛力。", "audio": "audios/2505.21032v1.mp3", "timestamp": "2025-05-28T20:21:57.883108"}
{"query": "AI", "id": "2505.21239v1", "url": "http://arxiv.org/abs/2505.21239v1", "title": "LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners", "summary": "Cognitive Diagnosis (CD) has become a critical task in AI-empowered\neducation, supporting personalized learning by accurately assessing students'\ncognitive states. However, traditional CD models often struggle in cold-start\nscenarios due to the lack of student-exercise interaction data. Recent\nNLP-based approaches leveraging pre-trained language models (PLMs) have shown\npromise by utilizing textual features but fail to fully bridge the gap between\nsemantic understanding and cognitive profiling. In this work, we propose\nLanguage Models as Zeroshot Cognitive Diagnosis Learners (LMCD), a novel\nframework designed to handle cold-start challenges by harnessing large language\nmodels (LLMs). LMCD operates via two primary phases: (1) Knowledge Diffusion,\nwhere LLMs generate enriched contents of exercises and knowledge concepts\n(KCs), establishing stronger semantic links; and (2) Semantic-Cognitive Fusion,\nwhere LLMs employ causal attention mechanisms to integrate textual information\nand student cognitive states, creating comprehensive profiles for both students\nand exercises. These representations are efficiently trained with off-the-shelf\nCD models. Experiments on two real-world datasets demonstrate that LMCD\nsignificantly outperforms state-of-the-art methods in both exercise-cold and\ndomain-cold settings. The code is publicly available at\nhttps://github.com/TAL-auroraX/LMCD", "authors": ["Yu He", "Zihan Yao", "Chentao Song", "Tianyu Qi", "Jun Liu", "Ming Li", "Qing Huang"], "published_date": "2025-05-27", "title_zh": "LMCD：語言模型是零樣本認知診斷學習器", "summary_zh": "認知診斷在人工智慧教育中至關重要，透過準確評估學生認知狀態來支援個人化學習。然而，傳統模型常因缺乏互動數據而在冷啟動情境中受限。近期基於自然語言處理的方法雖利用預訓練語言模型展現潛力，但未能完全彌合語義理解與認知剖析間的差距。本研究提出語言模型即零樣本認知診斷學習器(LMCD)，透過大型語言模型應對冷啟動挑戰。LMCD包含知識擴散與語義認知融合兩階段：前者利用大型語言模型生成更豐富的習題與知識概念內容，建立更強的語義連結；後者利用因果注意力機制整合文本資訊與學生認知狀態，創建全面的學生與習題檔案。此表徵能以現成認知診斷模型有效訓練。實驗結果顯示，LMCD在習題冷啟動和領域冷啟動情境中均顯著優於現有方法。程式碼已公開。", "audio": "audios/2505.21239v1.mp3", "timestamp": "2025-05-28T21:17:31.313460"}
{"query": "Foundation Model", "id": "2505.20574v1", "url": "http://arxiv.org/abs/2505.20574v1", "title": "xChemAgents: Agentic AI for Explainable Quantum Chemistry", "summary": "Recent progress in multimodal graph neural networks has demonstrated that\naugmenting atomic XYZ geometries with textual chemical descriptors can enhance\npredictive accuracy across a range of electronic and thermodynamic properties.\nHowever, naively appending large sets of heterogeneous descriptors often\ndegrades performance on tasks sensitive to molecular shape or symmetry, and\nundermines interpretability. xChemAgents proposes a cooperative agent framework\nthat injects physics-aware reasoning into multimodal property prediction.\nxChemAgents comprises two language-model-based agents: a Selector, which\nadaptively identifies a sparse, weighted subset of descriptors relevant to each\ntarget, and provides a natural language rationale; and a Validator, which\nenforces physical constraints such as unit consistency and scaling laws through\niterative dialogue. On standard benchmark datasets, xChemAgents achieves up to\na 22\\% reduction in mean absolute error over strong baselines, while producing\nfaithful, human-interpretable explanations. Experiment results highlight the\npotential of cooperative, self-verifying agents to enhance both accuracy and\ntransparency in foundation-model-driven materials science. The implementation\nand accompanying dataset are available anonymously at\nhttps://github.com/KurbanIntelligenceLab/xChemAgents.", "authors": ["Can Polat", "Mehmet Tuncel", "Hasan Kurban", "Erchin Serpedin", "Mustafa Kurban"], "published_date": "2025-05-26", "title_zh": "xChemAgents：用於可解釋量子化學的主體型人工智慧", "summary_zh": "多模態圖神經網絡的進展表明，結合原子XYZ幾何結構與文字化學描述符能提升電子和熱力學性質預測的準確性。然而，直接附加大量異質描述符常會降低對分子形狀或對稱性敏感任務的性能，並損害可解釋性。xChemAgents提出一個協作代理框架，將物理感知推理注入多模態性質預測。xChemAgents包含兩個基於語言模型的代理：選擇器，自適應地識別與每個目標相關的稀疏加權描述符子集，並提供自然語言理由；驗證器，透過迭代對話執行物理約束，如單位一致性和比例定律。在標準基準數據集上，xChemAgents相較於強基線，平均絕對誤差最多降低22％，同時產生忠實、易於人類理解的解釋。實驗結果突顯了協作、自我驗證代理在提升基礎模型驅動的材料科學準確性和透明度方面的潛力。實現程式碼和相關數據集可在匿名連結取得。", "audio": "audios/2505.20574v1.mp3", "timestamp": "2025-05-28T21:17:56.600169"}
{"query": "Diffusion Model", "id": "2505.21005v1", "url": "http://arxiv.org/abs/2505.21005v1", "title": "Efficient and Unbiased Sampling from Boltzmann Distributions via Variance-Tuned Diffusion Models", "summary": "Score-based diffusion models (SBDMs) are powerful amortized samplers for\nBoltzmann distributions; however, imperfect score estimates bias downstream\nMonte Carlo estimates. Classical importance sampling (IS) can correct this\nbias, but computing exact likelihoods requires solving the probability-flow\nordinary differential equation (PF-ODE), a procedure that is prohibitively\ncostly and scales poorly with dimensionality. We introduce Variance-Tuned\nDiffusion Importance Sampling (VT-DIS), a lightweight post-training method that\nadapts the per-step noise covariance of a pretrained SBDM by minimizing the\n$\\alpha$-divergence ($\\alpha=2$) between its forward diffusion and reverse\ndenoising trajectories. VT-DIS assigns a single trajectory-wise importance\nweight to the joint forward-reverse process, yielding unbiased expectation\nestimates at test time with negligible overhead compared to standard sampling.\nOn the DW-4, LJ-13, and alanine-dipeptide benchmarks, VT-DIS achieves effective\nsample sizes of approximately 80 %, 35 %, and 3.5 %, respectively, while using\nonly a fraction of the computational budget required by vanilla diffusion + IS\nor PF-ODE-based IS.", "authors": ["Fengzhe Zhang", "Laurence I. Midgley", "José Miguel Hernández-Lobato"], "published_date": "2025-05-27", "title_zh": "基於方差調整擴散模型的玻爾茲曼分佈之高效無偏採樣", "summary_zh": "基於分數的擴散模型是玻爾茲曼分佈的有效攤銷採樣器，但有偏差的分數估計會影響蒙地卡羅估計。重要性抽樣可修正此偏差，然精確似然函數計算需解概率流常微分方程，成本高昂且維度擴展性差。本文提出方差調整擴散重要性抽樣，一種輕量級後訓練方法，透過最小化正向擴散和逆向去噪軌跡間的$\\alpha$散度（$\\alpha=2$），調整預訓練擴散模型的每步雜訊共變異數。此方法為聯合正反向過程分配單一軌跡權重，於測試時產生無偏差期望估計，相較標準採樣額外開銷可忽略。在DW-4、LJ-13和丙氨酸二肽基準測試中，有效樣本量分別約為80%、35%和3.5%，僅需傳統擴散模型加重要性抽樣或基於概率流常微分方程重要性抽樣計算預算的一小部分。", "audio": "audios/2505.21005v1.mp3", "timestamp": "2025-05-28T21:18:26.342234"}
{"query": "AI", "id": "2505.21187v1", "url": "http://arxiv.org/abs/2505.21187v1", "title": "Making Every Event Count: Balancing Data Efficiency and Accuracy in Event Camera Subsampling", "summary": "Event cameras offer high temporal resolution and power efficiency, making\nthem well-suited for edge AI applications. However, their high event rates\npresent challenges for data transmission and processing. Subsampling methods\nprovide a practical solution, but their effect on downstream visual tasks\nremains underexplored. In this work, we systematically evaluate six\nhardware-friendly subsampling methods using convolutional neural networks for\nevent video classification on various benchmark datasets. We hypothesize that\nevents from high-density regions carry more task-relevant information and are\ntherefore better suited for subsampling. To test this, we introduce a simple\ncausal density-based subsampling method, demonstrating improved classification\naccuracy in sparse regimes. Our analysis further highlights key factors\naffecting subsampling performance, including sensitivity to hyperparameters and\nfailure cases in scenarios with large event count variance. These findings\nprovide insights for utilization of hardware-efficient subsampling strategies\nthat balance data efficiency and task accuracy. The code for this paper will be\nreleased at: https://github.com/hesamaraghi/event-camera-subsampling-methods.", "authors": ["Hesam Araghi", "Jan van Gemert", "Nergis Tomen"], "published_date": "2025-05-27", "title_zh": "讓每次事件都重要：事件相機次採樣中資料效率與準確性的平衡", "summary_zh": "事件相機具備高時間解析度和能源效率，適用於邊緣人工智慧應用。然而，其高事件率對資料傳輸和處理構成挑戰。子採樣方法提供可行的解決方案，但其對下游視覺任務的影響尚待研究。本研究系統性評估六種硬體友善的子採樣方法，使用卷積神經網路對多個基準資料集進行事件影片分類。我們假設來自高密度區域的事件包含更多任務相關資訊，因此更適合子採樣。為驗證此假設，我們提出一種基於因果密度的簡化子採樣方法，證實在稀疏狀態下能提升分類準確度。分析進一步強調影響子採樣效能的關鍵因素，包含對超參數的敏感度以及在大事件計數變異情境下的失效情況。這些發現為運用兼顧資料效率和任務準確度的硬體高效子採樣策略提供參考。", "audio": "audios/2505.21187v1.mp3", "timestamp": "2025-05-28T22:18:02.540689"}
{"query": "Foundation Model", "id": "2505.20518v1", "url": "http://arxiv.org/abs/2505.20518v1", "title": "SeisCoDE: 3D Seismic Interpretation Foundation Model with Contrastive Self-Distillation Learning", "summary": "Seismic interpretation is vital for understanding subsurface structures but\nremains labor-intensive, subjective, and computationally demanding. While deep\nlearning (DL) offers promise, its success hinges on large, high-quality\ndatasets, often scarce in geophysics. Foundation Models (FMs), which have shown\nsignificant success in fields like natural language processing and computer\nvision, offer a transformative opportunity for seismic interpretation by\nenabling knowledge transfer and generalization across interpretation tasks.\nHowever, the application of FMs in this domain remains limited, especially at\nthe 3D scale, due to the absence of a domain-specific pretraining workflow.\nHere, our study sought to develop a pretraining strategy for 3D seismic\ninterpretation by introducing a vision transformer-based Seismic Contrastive\nSelf-Distillation Encoder (SeisCoDE), a novel self-supervised learning (SSL)\nframework that leverages seismic signal processing and attribute analysis,\npreserving seismic structural integrity during pretraining. By leveraging\ncontrastive learning and self-distillation, SeisCoDE learns meaningful latent\nrepresentations without the need for labeled data (zero-shot approach). Results\nindicate that SeisCoDE effectively captures critical seismic features and\ncharacteristics, producing robust latent feature representations that drive\ndownstream seismic interpretation. It demonstrates enhanced generalization\nabilities across different seismic interpretation tasks, outperforming the\nconventional supervised learning UNet method. Overall, this research emphasizes\nthe potential of FMs informed by seismic image processing and attribute\nanalysis principles, paving the way for continued innovation integrating FMs\nfor seismic interpretation, with the potential to revolutionize subsurface\ncharacterization and geophysical seismic exploration.", "authors": ["Goodluck Archibong", "Ardiansyah Koeshidayatullah", "Umair Waheed", "Weichang Li", "Dicky Harishidayat", "Motaz Alfarraj"], "published_date": "2025-05-26", "title_zh": "SeisCoDE：基於對比自蒸餾學習的三維地震解釋基礎模型", "summary_zh": "地震解釋對理解地下構造至關重要，但仍耗時費力且具主觀性。深度學習雖具潛力，卻受限於地球物理學中常見的大型高品質數據集稀缺。基礎模型在自然語言處理和計算機視覺等領域展現優勢，透過知識轉移和泛化，為地震解釋提供變革性機會。然而，由於缺乏領域特定的預訓練流程，基礎模型在3D地震解釋中的應用仍然有限。本研究旨在開發一種3D地震解釋的預訓練策略，引入基於視覺轉換器的Seismic Contrastive Self-Distillation Encoder (SeisCoDE)，這是一種新型自監督學習框架，利用地震訊號處理和屬性分析，在預訓練期間保持地震結構的完整性。透過對比學習和自我蒸餾，SeisCoDE無需標記數據即可學習有意義的潛在表示（零樣本方法）。結果表明，SeisCoDE有效捕捉關鍵的地震特徵，產生驅動下游地震解釋的穩健潛在特徵表示。它展現了跨不同地震解釋任務的更強泛化能力，優於傳統的監督學習UNet方法。總體而言，這項研究強調了以地震圖像處理和屬性分析原則為基礎的基礎模型的潛力，為將基礎模型整合到地震解釋中以進行持續創新鋪平了道路，並有可能徹底改變地下特徵描述和地球物理地震勘探。", "audio": "audios/2505.20518v1.mp3", "timestamp": "2025-05-28T22:19:24.065370"}
{"query": "Diffusion Model", "id": "2505.21002v1", "url": "http://arxiv.org/abs/2505.21002v1", "title": "Facial Attribute Based Text Guided Face Anonymization", "summary": "The increasing prevalence of computer vision applications necessitates\nhandling vast amounts of visual data, often containing personal information.\nWhile this technology offers significant benefits, it should not compromise\nprivacy. Data privacy regulations emphasize the need for individual consent for\nprocessing personal data, hindering researchers' ability to collect\nhigh-quality datasets containing the faces of the individuals. This paper\npresents a deep learning-based face anonymization pipeline to overcome this\nchallenge. Unlike most of the existing methods, our method leverages recent\nadvancements in diffusion-based inpainting models, eliminating the need for\ntraining Generative Adversarial Networks. The pipeline employs a three-stage\napproach: face detection with RetinaNet, feature extraction with VGG-Face, and\nrealistic face generation using the state-of-the-art BrushNet diffusion model.\nBrushNet utilizes the entire image, face masks, and text prompts specifying\ndesired facial attributes like age, ethnicity, gender, and expression. This\nenables the generation of natural-looking images with unrecognizable\nindividuals, facilitating the creation of privacy-compliant datasets for\ncomputer vision research.", "authors": ["Mustafa İzzet Muştu", "Hazım Kemal Ekenel"], "published_date": "2025-05-27", "title_zh": "基於面部屬性的文本導向人臉匿名化", "summary_zh": "電腦視覺應用日趨普及，處理大量視覺數據成必然，其中常含個人資訊。此技術雖益處良多，然不應損害隱私。數據隱私條例強調處理個人數據需個體同意，阻礙研究人員收集含人臉之高品質數據集。本文提出基於深度學習之人臉匿名化流程以應對此挑戰。不同於既有方法，本方法採用基於擴散之圖像修復模型，無需訓練生成對抗網路。該流程分三階段：RetinaNet人臉偵測、VGG-Face特徵提取，及BrushNet擴散模型生成逼真人臉。BrushNet利用完整圖像、人臉遮罩及文字提示（指定年齡、種族、性別、表情等臉部特徵），生成外觀自然且無法辨識個體之圖像，有助於創建符合隱私規範之電腦視覺研究數據集。", "audio": "audios/2505.21002v1.mp3", "timestamp": "2025-05-28T22:20:02.169050"}
{"query": "AI", "id": "2505.21184v1", "url": "http://arxiv.org/abs/2505.21184v1", "title": "PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing", "summary": "To construct responsible and secure AI applications, harmful information data\nis widely utilized for adversarial testing and the development of safeguards.\nExisting studies mainly leverage Large Language Models (LLMs) to synthesize\ndata to obtain high-quality task datasets at scale, thereby avoiding costly\nhuman annotation. However, limited by the safety alignment mechanisms of LLMs,\nthe synthesis of harmful data still faces challenges in generation reliability\nand content diversity. In this study, we propose a novel harmful information\nsynthesis framework, PoisonSwarm, which applies the model crowdsourcing\nstrategy to generate diverse harmful data while maintaining a high success\nrate. Specifically, we generate abundant benign data as the based templates in\na counterfactual manner. Subsequently, we decompose each based template into\nmultiple semantic units and perform unit-by-unit toxification and final\nrefinement through dynamic model switching, thus ensuring the success of\nsynthesis. Experimental results demonstrate that PoisonSwarm achieves\nstate-of-the-art performance in synthesizing different categories of harmful\ndata with high scalability and diversity.", "authors": ["Yu Yan", "Sheng Sun", "Zhifei Zheng", "Ziji Hao", "Teli Liu", "Min Liu"], "published_date": "2025-05-27", "title_zh": "毒群：基於模型眾包的通用有害資訊合成", "summary_zh": "為構建負責任且安全的AI應用，有害資訊數據廣泛用於對抗性測試與安全措施開發。現有研究主要利用大型語言模型合成數據，以規模化獲取高品質任務數據集，避免高昂的人工標註成本。然而，受限於大型語言模型的安全對齊機制，有害數據的合成在生成可靠性與內容多樣性方面仍面臨挑戰。本研究提出名為PoisonSwarm的新型有害資訊合成框架，應用模型眾包策略生成多樣化有害數據，同時維持高成功率。具體而言，以反事實方式生成大量良性數據作為基礎模板，隨後將每個模板分解為多個語義單元，透過動態模型切換逐單元進行毒化與最終精煉，確保合成成功。實驗結果表明，PoisonSwarm在合成不同類別的有害數據方面實現了最先進的性能，並具有高度可擴展性與多樣性。", "audio": "audios/2505.21184v1.mp3", "timestamp": "2025-05-28T23:17:46.242440"}
{"query": "Foundation Model", "id": "2505.20510v1", "url": "http://arxiv.org/abs/2505.20510v1", "title": "CPathAgent: An Agent-based Foundation Model for Interpretable High-Resolution Pathology Image Analysis Mimicking Pathologists' Diagnostic Logic", "summary": "Recent advances in computational pathology have led to the emergence of\nnumerous foundation models. However, these approaches fail to replicate the\ndiagnostic process of pathologists, as they either simply rely on\ngeneral-purpose encoders with multi-instance learning for classification or\ndirectly apply multimodal models to generate reports from images. A significant\nlimitation is their inability to emulate the diagnostic logic employed by\npathologists, who systematically examine slides at low magnification for\noverview before progressively zooming in on suspicious regions to formulate\ncomprehensive diagnoses. To address this gap, we introduce CPathAgent, an\ninnovative agent-based model that mimics pathologists' reasoning processes by\nautonomously executing zoom-in/out and navigation operations across pathology\nimages based on observed visual features. To achieve this, we develop a\nmulti-stage training strategy unifying patch-level, region-level, and\nwhole-slide capabilities within a single model, which is essential for\nmimicking pathologists, who require understanding and reasoning capabilities\nacross all three scales. This approach generates substantially more detailed\nand interpretable diagnostic reports compared to existing methods, particularly\nfor huge region understanding. Additionally, we construct an expert-validated\nPathMMU-HR$^{2}$, the first benchmark for huge region analysis, a critical\nintermediate scale between patches and whole slides, as diagnosticians\ntypically examine several key regions rather than entire slides at once.\nExtensive experiments demonstrate that CPathAgent consistently outperforms\nexisting approaches across three scales of benchmarks, validating the\neffectiveness of our agent-based diagnostic approach and highlighting a\npromising direction for the future development of computational pathology.", "authors": ["Yuxuan Sun", "Yixuan Si", "Chenglu Zhu", "Kai Zhang", "Zhongyi Shui", "Bowen Ding", "Tao Lin", "Lin Yang"], "published_date": "2025-05-26", "title_zh": "CPathAgent：模擬病理醫師診斷邏輯的可解釋高解析度病理影像分析之基於Agent的基礎模型", "summary_zh": "計算病理學的發展催生了眾多基礎模型，但它們未能重現病理學家的診斷流程，通常僅依賴通用編碼器或直接使用多模態模型生成報告。這些方法缺乏模擬病理學家診斷邏輯的能力，後者會先低倍放大觀察切片全貌，再逐步放大可疑區域，最終形成完整診斷。為了解決此問題，我們提出 CPathAgent，一種模仿病理學家推理過程的基於代理的模型，能根據視覺特徵自主執行病理圖像的縮放和導航操作。我們開發了多階段訓練策略，將切片、區域和全切片能力整合到單一模型中，以模擬病理學家在三個尺度上的理解和推理能力。相較於現有方法，該方法能產生更詳細且可解釋的診斷報告，尤其是在理解大區域方面。此外，我們建立了經專家驗證的 PathMMU-HR$^{2}$，這是首個針對大區域分析的基準測試，該區域是切片和全切片之間的一個關鍵中間尺度，因為診斷人員通常會檢查幾個關鍵區域，而不是一次檢查整個切片。大量實驗表明，CPathAgent 在三個尺度的基準測試中均優於現有方法，驗證了我們基於代理的診斷方法的有效性，並突顯了計算病理學未來發展的一個有希望的方向。", "audio": "audios/2505.20510v1.mp3", "timestamp": "2025-05-28T23:17:58.849342"}
{"query": "Diffusion Model", "id": "2505.20984v1", "url": "http://arxiv.org/abs/2505.20984v1", "title": "Generative Image Compression by Estimating Gradients of the Rate-variable Feature Distribution", "summary": "While learned image compression (LIC) focuses on efficient data transmission,\ngenerative image compression (GIC) extends this framework by integrating\ngenerative modeling to produce photo-realistic reconstructed images. In this\npaper, we propose a novel diffusion-based generative modeling framework\ntailored for generative image compression. Unlike prior diffusion-based\napproaches that indirectly exploit diffusion modeling, we reinterpret the\ncompression process itself as a forward diffusion path governed by stochastic\ndifferential equations (SDEs). A reverse neural network is trained to\nreconstruct images by reversing the compression process directly, without\nrequiring Gaussian noise initialization. This approach achieves smooth rate\nadjustment and photo-realistic reconstructions with only a minimal number of\nsampling steps. Extensive experiments on benchmark datasets demonstrate that\nour method outperforms existing generative image compression approaches across\na range of metrics, including perceptual distortion, statistical fidelity, and\nno-reference quality assessments.", "authors": ["Minghao Han", "Weiyi You", "Jinhua Zhang", "Leheng Zhang", "Ce Zhu", "Shuhang Gu"], "published_date": "2025-05-27", "title_zh": "基於速率可變特徵分佈梯度估計的生成式圖像壓縮", "summary_zh": "本研究提出一種新型基於擴散模型的生成式影像壓縮框架，不同於以往間接利用擴散模型的方法，本研究將壓縮過程重新詮釋為由隨機微分方程控制的前向擴散路徑。透過訓練逆向神經網路直接逆轉壓縮過程以重建影像，無需高斯雜訊初始化。此方法僅需少量取樣步驟即可實現平滑速率調整和逼真影像重建。在基準數據集上的實驗結果表明，本方法在感知失真、統計保真度和無參考品質評估等多項指標上均優於現有的生成式影像壓縮方法。", "audio": "audios/2505.20984v1.mp3", "timestamp": "2025-05-28T23:18:08.064104"}
{"query": "AI", "id": "2505.21419v2", "url": "http://arxiv.org/abs/2505.21419v2", "title": "Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs", "summary": "Today's cloud-hosted applications and services are complex systems, and a\nperformance or functional instability can have dozens or hundreds of potential\nroot causes. Our hypothesis is that by combining the pattern matching\ncapabilities of modern AI tools with a natural multi-modal RAG LLM interface,\nproblem identification and resolution can be simplified. ARCA is a new\nmulti-modal RAG LLM system that targets this domain. Step-wise evaluations show\nthat ARCA outperforms state-of-the-art alternatives.", "authors": ["Yifan Wang", "Kenneth P. Birman"], "published_date": "2025-05-27", "title_zh": "使用多模態RAG LLM診斷與解決雲平台不穩定性", "summary_zh": "現今雲端應用程式及服務複雜，效能或功能不穩可能有多種根本原因。我們假設結合現代AI工具的模式匹配能力與自然多模態RAG LLM介面，可簡化問題識別與解決。ARCA是一個針對此領域的新多模態RAG LLM系統。逐步評估顯示，ARCA優於現有技術。", "audio": "audios/2505.21419v2.mp3", "timestamp": "2025-05-29T01:26:28.311806"}
{"query": "Foundation Model", "id": "2505.21357v2", "url": "http://arxiv.org/abs/2505.21357v2", "title": "AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping", "summary": "Accurate crop mapping fundamentally relies on modeling multi-scale\nspatiotemporal patterns, where spatial scales range from individual field\ntextures to landscape-level context, and temporal scales capture both\nshort-term phenological transitions and full growing-season dynamics.\nTransformer-based remote sensing foundation models (RSFMs) offer promising\npotential for crop mapping due to their innate ability for unified\nspatiotemporal processing. However, current RSFMs remain suboptimal for crop\nmapping: they either employ fixed spatiotemporal windows that ignore the\nmulti-scale nature of crop systems or completely disregard temporal information\nby focusing solely on spatial patterns. To bridge these gaps, we present\nAgriFM, a multi-source remote sensing foundation model specifically designed\nfor agricultural crop mapping. Our approach begins by establishing the\nnecessity of simultaneous hierarchical spatiotemporal feature extraction,\nleading to the development of a modified Video Swin Transformer architecture\nwhere temporal down-sampling is synchronized with spatial scaling operations.\nThis modified backbone enables efficient unified processing of long time-series\nsatellite inputs. AgriFM leverages temporally rich data streams from three\nsatellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is\npre-trained on a global representative dataset comprising over 25 million image\nsamples supervised by land cover products. The resulting framework incorporates\na versatile decoder architecture that dynamically fuses these learned\nspatiotemporal representations, supporting diverse downstream tasks.\nComprehensive evaluations demonstrate AgriFM's superior performance over\nconventional deep learning approaches and state-of-the-art general-purpose\nRSFMs across all downstream tasks. Codes will be available at\nhttps://github.com/flyakon/AgriFM.", "authors": ["Wenyuan Li", "Shunlin Liang", "Keyan Chen", "Yongzhe Chen", "Han Ma", "Jianglei Xu", "Yichuan Ma", "Shikang Guan", "Husheng Fang", "Zhenwei Shi"], "published_date": "2025-05-27", "title_zh": "AgriFM：用於農作物製圖的多源時序遙感基礎模型", "summary_zh": "精準的作物繪圖仰賴多尺度時空模式，涵蓋田地紋理至地景尺度的空間資訊，以及短期物候變化至完整生長季的時序動態。基於Transformer的遙感基礎模型（RSFM）具備統一時空處理能力，在作物繪圖上潛力無窮。然而，現有RSFM忽略作物系統的多尺度特性或完全忽略時序資訊，因而表現不佳。為此，我們提出AgriFM，一款專為農業作物繪圖設計的多源遙感基礎模型。我們首先確立同步分層時空特徵提取的必要性，進而開發改良的Video Swin Transformer架構，將時間降採樣與空間縮放操作同步化，實現長時間序列衛星資料的高效統一處理。AgriFM利用來自MODIS、Landsat-8/9和Sentinel-2等三種衛星來源的豐富時序資料流，並在全球代表性資料集上進行預訓練，該資料集包含超過2500萬個由土地覆蓋產品監督的圖像樣本。最終框架整合了多功能的解碼器架構，可動態融合學習到的時空表示，支援多樣化的下游任務。綜合評估表明，在所有下游任務中，AgriFM的效能均優於傳統深度學習方法和最先進的通用RSFM。代碼將於https://github.com/flyakon/AgriFM提供。", "audio": "audios/2505.21357v2.mp3", "timestamp": "2025-05-29T01:26:36.445453"}
{"query": "Diffusion Model", "id": "2505.21325v2", "url": "http://arxiv.org/abs/2505.21325v2", "title": "MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on", "summary": "Video Virtual Try-On (VVT) aims to simulate the natural appearance of\ngarments across consecutive video frames, capturing their dynamic variations\nand interactions with human body motion. However, current VVT methods still\nface challenges in terms of spatiotemporal consistency and garment content\npreservation. First, they use diffusion models based on the U-Net, which are\nlimited in their expressive capability and struggle to reconstruct complex\ndetails. Second, they adopt a separative modeling approach for spatial and\ntemporal attention, which hinders the effective capture of structural\nrelationships and dynamic consistency across frames. Third, their expression of\ngarment details remains insufficient, affecting the realism and stability of\nthe overall synthesized results, especially during human motion. To address the\nabove challenges, we propose MagicTryOn, a video virtual try-on framework built\nupon the large-scale video diffusion Transformer. We replace the U-Net\narchitecture with a diffusion Transformer and combine full self-attention to\njointly model the spatiotemporal consistency of videos. We design a\ncoarse-to-fine garment preservation strategy. The coarse strategy integrates\ngarment tokens during the embedding stage, while the fine strategy incorporates\nmultiple garment-based conditions, such as semantics, textures, and contour\nlines during the denoising stage. Moreover, we introduce a mask-aware loss to\nfurther optimize garment region fidelity. Extensive experiments on both image\nand video try-on datasets demonstrate that our method outperforms existing SOTA\nmethods in comprehensive evaluations and generalizes to in-the-wild scenarios.", "authors": ["Guangyuan Li", "Siming Zheng", "Hao Zhang", "Jinwei Chen", "Junsheng Luan", "Binkai Ou", "Lei Zhao", "Bo Li", "Peng-Tao Jiang"], "published_date": "2025-05-27", "title_zh": "MagicTryOn：利用擴散轉換器實現服裝保留的影片虛擬試穿", "summary_zh": "視訊虛擬試穿旨在模擬服裝在連續視訊幀中的自然呈現，捕捉動態變化及與人體互動。現有方法在時空一致性及服裝細節保留方面仍有不足。針對U-Net架構表達能力有限、空間時間注意力分離建模、服裝細節表現不足等問題，我們提出MagicTryOn，一個基於大型視訊擴散Transformer的視訊虛擬試穿框架。該框架以擴散Transformer取代U-Net，結合完整自注意力聯合建模視訊時空一致性，並設計由粗到精的服裝細節保留策略，在嵌入階段整合服裝標記，在去噪階段納入語義、紋理和輪廓等多重服裝條件。此外，引入遮罩感知損失以優化服裝區域逼真度。大量實驗表明，該方法在圖像和視訊試穿數據集上優於現有技術，並能推廣至真實場景。", "audio": "audios/2505.21325v2.mp3", "timestamp": "2025-05-29T01:26:43.367940"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "聽其言：音訊驅動的多人對話影片生成", "summary_zh": "音訊驅動的人體動畫技術在產生同步面部動作和具吸引力的視覺影片方面取得了顯著進展。然而，現有方法主要關注單人動畫，難以處理多音訊流輸入，面臨音訊與人物間的錯誤綁定問題，且在指令遵循能力方面存在局限。為了解決此問題，本文提出一個新任務：多人對話影片生成，並引入新框架MultiTalk以應對多人生成過程中的挑戰。具體而言，針對音訊注入，我們研究了多種方案，並提出標籤旋轉位置嵌入（L-RoPE）方法以解決音訊和人物的綁定問題。此外，在訓練過程中，我們觀察到部分參數訓練和多任務訓練對於保留基礎模型的指令遵循能力至關重要。MultiTalk在多個數據集上，包括口語頭像、口語全身和多人數據集，相較於其他方法展現了卓越的性能，證明了我們方法的強大生成能力。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T03:13:06.989410"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "語言模型中轉向向量的（不）可靠性理解", "summary_zh": "操縱向量是一種輕量方法，透過在推論時對激活添加學習偏差來控制語言模型行為。儘管操縱表現出有希望的性能，但近期研究顯示其在某些情況下可能不可靠甚至適得其反。本研究探討提示類型和激活差異幾何對操縱可靠性的影響。研究發現，所有七種提示類型皆產生淨正向操縱效果，但在樣本間表現出高變異性，且效果經常與預期相反。沒有任何提示類型明顯優於其他類型，但不同提示類型產生的操縱向量在方向上經常不同（以餘弦相似度衡量）。訓練集激活差異間較高的餘弦相似度預測更有效的操縱。正向和負向激活分離度更高的數據集更易於操縱。研究結果表明，當目標行為未被連貫方向表示時，向量操縱是不可靠的。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T03:13:17.559652"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL：語義感知漸進式光達場景生成", "summary_zh": "基於擴散模型，LiDAR大規模3D場景生成取得進展。現有基於體素的方法雖能生成幾何結構及語義標籤，但既有距離視圖方法僅限於產生未標記LiDAR場景，仰賴預訓練分割模型預測語義圖常導致次優跨模態一致性。為解決此限制，同時保留距離視圖表徵的優勢，如計算效率及簡化網路設計，我們提出Spiral，一種新型距離視圖LiDAR擴散模型，可同步生成深度、反射圖像及語義圖。此外，我們引入語義感知指標以評估生成標記距離視圖資料的品質。在SemanticKITTI和nuScenes數據集上的實驗表明，Spiral以最小的參數尺寸實現了最先進的性能，優於結合生成和分割模型的兩步方法。我們也驗證了Spiral生成的距離圖像可有效用於下游分割訓練中的合成數據增強，顯著減少LiDAR數據的標記工作。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T03:13:28.939281"}
{"query": "AI", "id": "2505.22639v1", "url": "http://arxiv.org/abs/2505.22639v1", "title": "Navigating the AI-Energy Nexus with Geopolitical Insight", "summary": "This working paper examines how geopolitical strategies and energy resource\nmanagement intersect with Artificial Intelligence (AI) development, delineating\nthe AI-energy nexus as critical to sustaining U.S. AI leadership. By analyzing\nthe centralized approaches of authoritarian regimes like China and Gulf\nnations, alongside market-driven approaches in the U.S., the paper explores\ndivergent strategies to allocate resources for AI energy needs. It underscores\nthe role of energy infrastructure, market dynamics, and state-led initiatives\nin shaping global AI competition. Recommendations include adopting\ngeopolitically informed analyses and leveraging both market and non-market\nstrengths to enhance U.S. competitiveness. This research aims to inform\npolicymakers, technologists, and researchers about the strategic implications\nof the AI-energy nexus and offers insights into advancing U.S. global\nleadership in AI amidst evolving technological paradigms.", "authors": ["Nidhi Kalra", "Robin Wang", "Ismael Arciniegas Rueda"], "published_date": "2025-05-28", "title_zh": "以地緣政治視角探討人工智慧與能源的關聯", "summary_zh": "本研究探討地緣政治策略與能源資源管理如何影響人工智慧發展，指出人工智慧-能源連結對於維持美國人工智慧領導地位至關重要。透過分析中國與波斯灣國家等集權政體的集中式方法，以及美國的市場驅動方法，研究探討分配資源以滿足人工智慧能源需求的策略差異。強調能源基礎設施、市場動態與國家主導措施在塑造全球人工智慧競爭中的作用。建議採納具地緣政治意識的分析，並利用市場與非市場優勢來提升美國的競爭力。本研究旨在向政策制定者、技術專家與研究人員闡明人工智慧-能源連結的戰略意涵，並為在不斷演變的技術範式中推進美國在全球人工智慧領域的領導地位提供見解。", "audio": "audios/2505.22639v1.mp3", "timestamp": "2025-05-29T04:24:16.855499"}
{"query": "Foundation Model", "id": "2505.22622v1", "url": "http://arxiv.org/abs/2505.22622v1", "title": "Principled Out-of-Distribution Generalization via Simplicity", "summary": "Modern foundation models exhibit remarkable out-of-distribution (OOD)\ngeneralization, solving tasks far beyond the support of their training data.\nHowever, the theoretical principles underpinning this phenomenon remain\nelusive. This paper investigates this problem by examining the compositional\ngeneralization abilities of diffusion models in image generation. Our analysis\nreveals that while neural network architectures are expressive enough to\nrepresent a wide range of models -- including many with undesirable behavior on\nOOD inputs -- the true, generalizable model that aligns with human expectations\ntypically corresponds to the simplest among those consistent with the training\ndata.\n  Motivated by this observation, we develop a theoretical framework for OOD\ngeneralization via simplicity, quantified using a predefined simplicity metric.\nWe analyze two key regimes: (1) the constant-gap setting, where the true model\nis strictly simpler than all spurious alternatives by a fixed gap, and (2) the\nvanishing-gap setting, where the fixed gap is replaced by a smoothness\ncondition ensuring that models close in simplicity to the true model yield\nsimilar predictions. For both regimes, we study the regularized maximum\nlikelihood estimator and establish the first sharp sample complexity guarantees\nfor learning the true, generalizable, simple model.", "authors": ["Jiawei Ge", "Amanda Wang", "Shange Tang", "Chi Jin"], "published_date": "2025-05-28", "title_zh": "基於簡潔性的原則性異分布泛化", "summary_zh": "現代基石模型展現卓越的分布外泛化能力，能解決超出訓練數據範圍的任務，但其理論基礎仍不明朗。本文研究擴散模型在圖像生成中的組合泛化能力，揭示神經網路架構雖足以表示多種模型（包括分布外表現不佳者），但符合人類預期的可泛化模型通常是與訓練數據一致的最簡模型。\n\n基於此，本文建立基於簡約性的分布外泛化理論框架，並以預定義的簡約性指標量化。分析兩種主要情況：（1）恆定差距：真實模型比所有虛假替代方案簡單固定差距；（2）消失差距：固定差距被平滑性條件取代，確保簡約性接近真實模型的模型產生相似預測。針對兩種情況，本文研究正規化最大似然估計器，並為學習真實、可泛化的簡約模型建立首個精確的樣本複雜度保證。", "audio": "audios/2505.22622v1.mp3", "timestamp": "2025-05-29T04:24:22.602398"}
{"query": "Diffusion Model", "id": "2505.22618v1", "url": "http://arxiv.org/abs/2505.22618v1", "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding", "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.", "authors": ["Chengyue Wu", "Hao Zhang", "Shuchen Xue", "Zhijian Liu", "Shizhe Diao", "Ligeng Zhu", "Ping Luo", "Song Han", "Enze Xie"], "published_date": "2025-05-28", "title_zh": "Fast-dLLM：透過啟用 KV 快取與平行解碼加速 Diffusion LLM，無需訓練", "summary_zh": "基於擴散的大型語言模型展現了非自迴歸文本生成的潛力，但實際推論速度受限於缺乏鍵值快取及並行解碼時的品質下降。為了解決此問題，本文提出了一種針對雙向擴散模型的區塊式近似鍵值快取機制，實現快取重用且性能損失可忽略不計。此外，研究指出並行解碼中生成品質下降的根本原因是條件獨立性假設下符記依賴關係的破壞。為此，本文提出了一種感知信心的並行解碼策略，選擇性地解碼超過信心閾值的符記，從而減輕依賴關係違規並保持生成品質。實驗結果表明，在多個大型語言模型基準測試中，LLaDA和Dream模型可實現高達27.6倍的吞吐量提升，且準確性損失極小，縮小了與自迴歸模型的性能差距，為擴散大型語言模型的實際部署鋪平了道路。", "audio": "audios/2505.22618v1.mp3", "timestamp": "2025-05-29T04:24:28.630020"}
{"query": "AI", "id": "2505.22627v1", "url": "http://arxiv.org/abs/2505.22627v1", "title": "Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions", "summary": "While densely annotated image captions significantly facilitate the learning\nof robust vision-language alignment, methodologies for systematically\noptimizing human annotation efforts remain underexplored. We introduce\nChain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize\nthe number of annotated samples and improve their comprehensiveness under fixed\nbudget constraints (e.g., total human annotation time). The framework is built\nupon two key insights. First, sequential annotation reduces redundant workload\ncompared to conventional parallel annotation, as subsequent annotators only\nneed to annotate the ``residual'' -- the missing visual information that\nprevious annotations have not covered. Second, humans process textual input\nfaster by reading while outputting annotations with much higher throughput via\ntalking; thus a multimodal interface enables optimized efficiency. We evaluate\nour framework from two aspects: intrinsic evaluations that assess the\ncomprehensiveness of semantic units, obtained by parsing detailed captions into\nobject-attribute trees and analyzing their effective connections; extrinsic\nevaluation measures the practical usage of the annotated captions in\nfacilitating vision-language alignment. Experiments with eight participants\nshow our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30\nunits/sec) and retrieval performance (41.13\\% vs. 40.52\\%) over the parallel\nmethod.", "authors": ["Yijun Shen", "Delong Chen", "Fan Liu", "Xingyu Wang", "Chuanyi Zhang", "Liang Yao", "Yuhui Zheng"], "published_date": "2025-05-28", "title_zh": "鏈式對話者 (CoTalk)：快速人工標註密集圖像描述", "summary_zh": "現有密集標註的圖像描述有助於提升視覺語言對齊的學習，但系統性優化人工標註的方法仍待探索。我們提出一種AI迴路方法Chain-of-Talkers (CoTalk)，旨在固定預算下最大化標註樣本數量並提高其完整性。CoTalk基於兩個關鍵洞見：一，序列標註減少冗餘工作量，後續標註者僅需標註先前未涵蓋的剩餘視覺資訊；二，多模態介面透過語音輸出提高效率，讓人們能更快地處理文本輸入。我們從本質評估和外在評估兩方面評估CoTalk，前者評估語義單元的完整性，後者評估標註描述在促進視覺語言對齊的實際應用。實驗結果顯示，相較於平行標註方法，CoTalk提高了標註速度（0.42 vs. 0.30 單位/秒）和檢索效能（41.13% vs. 40.52%）。", "audio": "audios/2505.22627v1.mp3", "timestamp": "2025-05-29T05:19:14.891881"}
{"query": "Foundation Model", "id": "2505.22608v1", "url": "http://arxiv.org/abs/2505.22608v1", "title": "Effective and Efficient One-pass Compression of Speech Foundation Models Using Sparsity-aware Self-pinching Gates", "summary": "This paper presents a novel approach for speech foundation models compression\nthat tightly integrates model pruning and parameter update into a single stage.\nHighly compact layer-level tied self-pinching gates each containing only a\nsingle learnable threshold are jointly trained with uncompressed models and\nused in fine-grained neuron level pruning. Experiments conducted on the\nLibriSpeech-100hr corpus suggest that our approach reduces the number of\nparameters of wav2vec2.0-base and HuBERT-large models by 65% and 60%\nrespectively, while incurring no statistically significant word error rate\n(WER) increase on the test-clean dataset. Compared to previously published\nmethods on the same task, our approach not only achieves the lowest WER of\n7.05% on the test-clean dataset under a comparable model compression ratio of\n4.26x, but also operates with at least 25% less model compression time.", "authors": ["Haoning Xu", "Zhaoqing Li", "Youjun Chen", "Huimeng Wang", "Guinan Li", "Mengzhe Geng", "Chengxi Deng", "Xunying Liu"], "published_date": "2025-05-28", "title_zh": "基於稀疏感知自擠壓閘的語音基礎模型高效單遍壓縮", "summary_zh": "本研究提出一種新型語音基礎模型壓縮方法，將模型剪枝與參數更新緊密整合於單一階段。採用高壓縮比的層級連結自緊縮閘，每個閘僅包含單一可學習閾值，與未壓縮模型聯合訓練，並用於細粒度神經元級別剪枝。在 LibriSpeech-100hr 語料庫上的實驗表明，該方法可將 wav2vec2.0-base 和 HuBERT-large 模型的參數數量分別減少 65% 和 60%，同時在 test-clean 數據集上未造成顯著的詞錯誤率 (WER) 增加。與先前發表的同類方法相比，本方法不僅在 4.26 倍模型壓縮率下，於 test-clean 數據集上實現了最低的 WER（7.05%），且壓縮時間至少減少 25%。", "audio": "audios/2505.22608v1.mp3", "timestamp": "2025-05-29T05:19:20.444798"}
{"query": "Diffusion Model", "id": "2505.22569v1", "url": "http://arxiv.org/abs/2505.22569v1", "title": "ImageReFL: Balancing Quality and Diversity in Human-Aligned Diffusion Models", "summary": "Recent advances in diffusion models have led to impressive image generation\ncapabilities, but aligning these models with human preferences remains\nchallenging. Reward-based fine-tuning using models trained on human feedback\nimproves alignment but often harms diversity, producing less varied outputs. In\nthis work, we address this trade-off with two contributions. First, we\nintroduce \\textit{combined generation}, a novel sampling strategy that applies\na reward-tuned diffusion model only in the later stages of the generation\nprocess, while preserving the base model for earlier steps. This approach\nmitigates early-stage overfitting and helps retain global structure and\ndiversity. Second, we propose \\textit{ImageReFL}, a fine-tuning method that\nimproves image diversity with minimal loss in quality by training on real\nimages and incorporating multiple regularizers, including diffusion and ReFL\nlosses. Our approach outperforms conventional reward tuning methods on standard\nquality and diversity metrics. A user study further confirms that our method\nbetter balances human preference alignment and visual diversity. The source\ncode can be found at https://github.com/ControlGenAI/ImageReFL .", "authors": ["Dmitrii Sorokin", "Maksim Nakhodnov", "Andrey Kuznetsov", "Aibek Alanov"], "published_date": "2025-05-28", "title_zh": "ImageReFL：人類對齊擴散模型中品質與多樣性的平衡", "summary_zh": "擴散模型在圖像生成方面取得顯著進展，但與人類偏好對齊仍具挑戰。基於人類回饋訓練的模型雖能提升對齊度，卻常損害多樣性。本文提出兩項貢獻以解決此權衡。首先，我們引入結合生成策略，僅在生成後期應用獎勵調整過的擴散模型，前期則保留基礎模型，以減輕早期過擬合，並保留整體結構與多樣性。其次，我們提出ImageReFL微調方法，透過真實圖像訓練及多重正則化（包含擴散和ReFL損失）來提升圖像多樣性，且品質損失極小。實驗證明，本方法在品質與多樣性指標上優於傳統獎勵調整方法。使用者研究進一步證實，本方法更能平衡人類偏好對齊與視覺多樣性。原始碼位於https://github.com/ControlGenAI/ImageReFL 。", "audio": "audios/2505.22569v1.mp3", "timestamp": "2025-05-29T05:19:26.851832"}
{"query": "AI", "id": "2505.22605v1", "url": "http://arxiv.org/abs/2505.22605v1", "title": "Transformers for Secure Hardware Systems: Applications, Challenges, and Outlook", "summary": "The rise of hardware-level security threats, such as side-channel attacks,\nhardware Trojans, and firmware vulnerabilities, demands advanced detection\nmechanisms that are more intelligent and adaptive. Traditional methods often\nfall short in addressing the complexity and evasiveness of modern attacks,\ndriving increased interest in machine learning-based solutions. Among these,\nTransformer models, widely recognized for their success in natural language\nprocessing and computer vision, have gained traction in the security domain due\nto their ability to model complex dependencies, offering enhanced capabilities\nin identifying vulnerabilities, detecting anomalies, and reinforcing system\nintegrity. This survey provides a comprehensive review of recent advancements\non the use of Transformers in hardware security, examining their application\nacross key areas such as side-channel analysis, hardware Trojan detection,\nvulnerability classification, device fingerprinting, and firmware security.\nFurthermore, we discuss the practical challenges of applying Transformers to\nsecure hardware systems, and highlight opportunities and future research\ndirections that position them as a foundation for next-generation\nhardware-assisted security. These insights pave the way for deeper integration\nof AI-driven techniques into hardware security frameworks, enabling more\nresilient and intelligent defenses.", "authors": ["Banafsheh Saber Latibari", "Najmeh Nazari", "Avesta Sasan", "Houman Homayoun", "Pratik Satam", "Soheil Salehi", "Hossein Sayadi"], "published_date": "2025-05-28", "title_zh": "用於安全硬體系統的變換器：應用、挑戰與展望", "summary_zh": "硬體層級安全威脅日益增加，傳統檢測方法難以應對。Transformer模型在自然語言處理和計算機視覺領域表現出色，其建模複雜依賴關係的能力使其在安全領域備受關注。本文全面回顧了Transformer模型在硬體安全領域的最新進展，包括旁路分析、硬體木馬檢測、漏洞分類、設備指紋識別和韌體安全等關鍵應用。同時，探討了將Transformer模型應用於硬體安全系統的實際挑戰，並強調了其作為下一代硬體輔助安全基礎的機會和未來研究方向，旨在推動AI驅動技術更深入地整合到硬體安全框架中，實現更具彈性和智慧的防禦。", "audio": "audios/2505.22605v1.mp3", "timestamp": "2025-05-29T06:27:52.403090"}
{"query": "Foundation Model", "id": "2505.22549v1", "url": "http://arxiv.org/abs/2505.22549v1", "title": "DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation Models", "summary": "Scaling foundation model training with Distributed Data Parallel (DDP)\nmethods is bandwidth-limited. Existing infrequent communication methods like\nLocal SGD were designed to synchronize only model parameters and cannot be\ntrivially applied to adaptive optimizers due to additional optimizer states.\nCurrent approaches extending Local SGD either lack convergence guarantees or\nrequire synchronizing all optimizer states, tripling communication costs. We\npropose Desynced Low Communication Adaptive Optimizers (DES-LOC), a family of\noptimizers assigning independent synchronization periods to parameters and\nmomenta, enabling lower communication costs while preserving convergence.\nThrough extensive experiments on language models of up to 1.7B, we show that\nDES-LOC can communicate 170x less than DDP and 2x less than the previous\nstate-of-the-art Local ADAM. Furthermore, unlike previous heuristic approaches,\nDES-LOC is suited for practical training scenarios prone to system failures.\nDES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for\nfoundation model training.", "authors": ["Alex Iacob", "Lorenzo Sani", "Mher Safaryan", "Paris Giampouras", "Samuel Horváth", "Andrej Jovanovic", "Meghdad Kurmanji", "Preslav Aleksandrov", "William F. Shen", "Xinchi Qiu", "Nicholas D. Lane"], "published_date": "2025-05-28", "title_zh": "DES-LOC：用於訓練基礎模型的非同步低通訊自適應最佳化器", "summary_zh": "採用分散式資料並行(DDP)方法擴展基礎模型訓練受限於頻寬。現有的不頻繁通訊方法如Local SGD僅同步模型參數，無法直接應用於具適應性的優化器，因其涉及額外的優化器狀態。現有Local SGD擴展方法或缺乏收斂保證，或需同步所有優化器狀態，使通訊成本增加三倍。本文提出非同步低通訊自適應優化器(DES-LOC)，此優化器家族為參數和動量分配獨立的同步週期，降低通訊成本並保持收斂性。在多達17億參數的語言模型上進行的大量實驗表明，DES-LOC的通訊量比DDP少170倍，比先前的Local ADAM少2倍。此外，與先前的啟發式方法不同，DES-LOC適用於易發生系統故障的實際訓練場景。DES-LOC為基礎模型訓練提供可擴展、頻寬效率高且具容錯能力的解決方案。", "audio": "audios/2505.22549v1.mp3", "timestamp": "2025-05-29T06:27:58.991821"}
{"query": "Diffusion Model", "id": "2505.22524v1", "url": "http://arxiv.org/abs/2505.22524v1", "title": "Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo", "summary": "Discrete diffusion models have become highly effective across various\ndomains. However, real-world applications often require the generative process\nto adhere to certain constraints but without task-specific fine-tuning. To this\nend, we propose a training-free method based on Sequential Monte Carlo (SMC) to\nsample from the reward-aligned target distribution at the test time. Our\napproach leverages twisted SMC with an approximate locally optimal proposal,\nobtained via a first-order Taylor expansion of the reward function. To address\nthe challenge of ill-defined gradients in discrete spaces, we incorporate a\nGumbel-Softmax relaxation, enabling efficient gradient-based approximation\nwithin the discrete generative framework. Empirical results on both synthetic\ndatasets and image modelling validate the effectiveness of our approach.", "authors": ["Chinmay Pani", "Zijing Ou", "Yingzhen Li"], "published_date": "2025-05-28", "title_zh": "離散擴散模型於測試時基於序貫蒙地卡羅的對齊", "summary_zh": "離散擴散模型在多個領域表現出色。然而，實際應用常需生成過程遵守特定約束，且無需針對任務進行微調。本文提出一種基於序列蒙地卡羅(SMC)的免訓練方法，在測試時從獎勵對齊的目標分佈中採樣。此方法利用扭曲SMC，搭配透過獎勵函數一階泰勒展開獲得的近似局部最佳提議。為了解決離散空間中梯度不明確的挑戰，我們採用Gumbel-Softmax鬆弛，在離散生成框架內實現有效的基於梯度的近似。合成數據集和圖像建模的實驗結果驗證了該方法的有效性。", "audio": "audios/2505.22524v1.mp3", "timestamp": "2025-05-29T06:28:04.459617"}
{"query": "AI", "id": "2505.22604v1", "url": "http://arxiv.org/abs/2505.22604v1", "title": "Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective", "summary": "Rapid advances in Artificial Intelligence Generated Images (AIGI) have\nfacilitated malicious use, such as forgery and misinformation. Therefore,\nnumerous methods have been proposed to detect fake images. Although such\ndetectors have been proven to be universally vulnerable to adversarial attacks,\ndefenses in this field are scarce. In this paper, we first identify that\nadversarial training (AT), widely regarded as the most effective defense,\nsuffers from performance collapse in AIGI detection. Through an\ninformation-theoretic lens, we further attribute the cause of collapse to\nfeature entanglement, which disrupts the preservation of feature-label mutual\ninformation. Instead, standard detectors show clear feature separation.\nMotivated by this difference, we propose Training-free Robust Detection via\nInformation-theoretic Measures (TRIM), the first training-free adversarial\ndefense for AIGI detection. TRIM builds on standard detectors and quantifies\nfeature shifts using prediction entropy and KL divergence. Extensive\nexperiments across multiple datasets and attacks validate the superiority of\nour TRIM, e.g., outperforming the state-of-the-art defense by 33.88% (28.91%)\non ProGAN (GenImage), while well maintaining original accuracy.", "authors": ["Ruixuan Zhang", "He Wang", "Zhengyu Zhao", "Zhiqing Guo", "Xun Yang", "Yunfeng Diao", "Meng Wang"], "published_date": "2025-05-28", "title_zh": "基於資訊理論視角之無代價對抗魯棒AI生成圖像檢測", "summary_zh": "人工智慧生成圖像(AIGI)快速發展，導致偽造和不實資訊等惡意應用。因此，出現許多檢測假圖像的方法。儘管這些檢測器普遍容易受到對抗性攻擊，但相關防禦措施仍然稀缺。本文首先指出，被廣泛認為最有效的防禦手段—對抗性訓練(AT)，在AIGI檢測中會發生效能崩潰。透過資訊理論分析，我們將崩潰原因歸因於特徵糾纏，這會擾亂特徵與標籤之間互資訊的保留。相反地，標準檢測器顯示出清晰的特徵分離。基於此，我們提出透過資訊理論測量進行無訓練的穩健檢測(TRIM)，這是第一個針對AIGI檢測的無訓練對抗性防禦方法。TRIM建立在標準檢測器的基礎上，並使用預測熵和KL散度來量化特徵偏移。大量實驗驗證了TRIM的優越性，例如，在ProGAN (GenImage)上，TRIM的效能分別超越了最先進的防禦方法33.88% (28.91%)，同時維持了原有的準確性。", "audio": "audios/2505.22604v1.mp3", "timestamp": "2025-05-29T07:18:54.392850"}
{"query": "Foundation Model", "id": "2505.22287v1", "url": "http://arxiv.org/abs/2505.22287v1", "title": "New Tools are Needed for Tracking Adherence to AI Model Behavioral Use Clauses", "summary": "Foundation models have had a transformative impact on AI. A combination of\nlarge investments in research and development, growing sources of digital data\nfor training, and architectures that scale with data and compute has led to\nmodels with powerful capabilities. Releasing assets is fundamental to\nscientific advancement and commercial enterprise. However, concerns over\nnegligent or malicious uses of AI have led to the design of mechanisms to limit\nthe risks of the technology. The result has been a proliferation of licenses\nwith behavioral-use clauses and acceptable-use-policies that are increasingly\nbeing adopted by commonly used families of models (Llama, Gemma, Deepseek) and\na myriad of smaller projects. We created and deployed a custom AI licenses\ngenerator to facilitate license creation and have quantitatively and\nqualitatively analyzed over 300 customized licenses created with this tool.\nAlongside this we analyzed 1.7 million models licenses on the HuggingFace model\nhub. Our results show increasing adoption of these licenses, interest in tools\nthat support their creation and a convergence on common clause configurations.\nIn this paper we take the position that tools for tracking adoption of, and\nadherence to, these licenses is the natural next step and urgently needed in\norder to ensure they have the desired impact of ensuring responsible use.", "authors": ["Daniel McDuff", "Tim Korjakow", "Kevin Klyman", "Danish Contractor"], "published_date": "2025-05-28", "title_zh": "追蹤人工智慧模型行為使用條款遵循情況所需之新工具", "summary_zh": "基於大型投資、數據增長及可擴展架構，基石模型已對人工智慧產生變革性影響。為降低風險，行為使用條款及可接受使用政策被廣泛應用於常見模型及小型項目。我們開發了客製化許可證生成器，分析了超過300個由此工具產生的許可證，並檢視了HuggingFace上的170萬個模型許可證。結果顯示，此類許可證採用率不斷提高，且對相關工具有需求，並趨向於常見條款配置。本文認為，追蹤許可證採用及遵守情況的工具是確保責任使用之必要下一步，且具迫切性。", "audio": "audios/2505.22287v1.mp3", "timestamp": "2025-05-29T07:18:59.088060"}
{"query": "Diffusion Model", "id": "2505.22523v1", "url": "http://arxiv.org/abs/2505.22523v1", "title": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image Generative Models", "summary": "Generating high-quality, multi-layer transparent images from text prompts can\nunlock a new level of creative control, allowing users to edit each layer as\neffortlessly as editing text outputs from LLMs. However, the development of\nmulti-layer generative models lags behind that of conventional text-to-image\nmodels due to the absence of a large, high-quality corpus of multi-layer\ntransparent data. In this paper, we address this fundamental challenge by: (i)\nreleasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)\ndataset of 200K (20K) multilayer transparent images with accurate alpha mattes,\n(ii) introducing a trainingfree synthesis pipeline that generates such data on\ndemand using off-the-shelf diffusion models, and (iii) delivering a strong,\nopen-source multi-layer generation model, ART+, which matches the aesthetics of\nmodern text-to-image generation models. The key technical contributions\ninclude: LayerFLUX, which excels at generating high-quality single transparent\nlayers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple\nLayerFLUX outputs into complete images, guided by human-annotated semantic\nlayout. To ensure higher quality, we apply a rigorous filtering stage to remove\nartifacts and semantic mismatches, followed by human selection. Fine-tuning the\nstate-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which\noutperforms the original ART in 60% of head-to-head user study comparisons and\neven matches the visual quality of images generated by the FLUX.1-[dev] model.\nWe anticipate that our work will establish a solid dataset foundation for the\nmulti-layer transparent image generation task, enabling research and\napplications that require precise, editable, and visually compelling layered\nimagery.", "authors": ["Junwen Chen", "Heyang Jiang", "Yanbin Wang", "Keming Wu", "Ji Li", "Chao Zhang", "Keiji Yanai", "Dong Chen", "Yuhui Yuan"], "published_date": "2025-05-28", "title_zh": "棱鏡層：用於高品質多層透明圖像生成模型的開放數據", "summary_zh": "從文字提示生成高品質多層透明圖像，能賦予使用者更細緻的創作控制，如同編輯大型語言模型產生的文字一般。然而，由於缺乏大型且高品質的多層透明數據集，多層生成模型發展落後於傳統文字轉圖像模型。本研究旨在解決此挑戰：(i) 公開首個超高保真度多層透明圖像數據集PrismLayersPro，包含20萬（2萬）張具精確Alpha遮罩的圖像；(ii) 提出一種無需訓練的合成管線，利用現成擴散模型按需生成此類數據；(iii) 推出強大的開源多層生成模型ART+，其美學風格與現代文字轉圖像模型相符。主要技術貢獻包括：擅長生成具精確Alpha遮罩的高品質單層透明圖像的LayerFLUX，以及在人工標注語義佈局引導下，將多個LayerFLUX輸出組合成完整圖像的MultiLayerFLUX。為確保更高品質，我們採用嚴格的過濾階段，去除偽影和語義不匹配，並進行人工選擇。在合成數據集PrismLayersPro上微調最先進的ART模型，得到ART+，其在使用者研究中勝過原版ART的比例達60%，甚至達到FLUX.1-[dev]模型的視覺品質。預期本研究將為多層透明圖像生成任務奠定堅實的數據集基礎，促進需要精確、可編輯且具視覺吸引力的分層圖像的研究和應用。", "audio": "audios/2505.22523v1.mp3", "timestamp": "2025-05-29T07:19:08.933047"}
{"query": "AI", "id": "2505.22602v1", "url": "http://arxiv.org/abs/2505.22602v1", "title": "One Rank at a Time: Cascading Error Dynamics in Sequential Learning", "summary": "Sequential learning -- where complex tasks are broken down into simpler,\nhierarchical components -- has emerged as a paradigm in AI. This paper views\nsequential learning through the lens of low-rank linear regression, focusing\nspecifically on how errors propagate when learning rank-1 subspaces\nsequentially. We present an analysis framework that decomposes the learning\nprocess into a series of rank-1 estimation problems, where each subsequent\nestimation depends on the accuracy of previous steps. Our contribution is a\ncharacterization of the error propagation in this sequential process,\nestablishing bounds on how errors -- e.g., due to limited computational budgets\nand finite precision -- affect the overall model accuracy. We prove that these\nerrors compound in predictable ways, with implications for both algorithmic\ndesign and stability guarantees.", "authors": ["Mahtab Alizadeh Vandchali", "Fangshuo", "Liao", "Anastasios Kyrillidis"], "published_date": "2025-05-28", "title_zh": "一次一秩：序列學習中級聯誤差動態", "summary_zh": "序列學習將複雜任務分解為簡化的層級組件，已成為人工智慧的一種範例。本文以低秩線性迴歸的角度審視序列學習，著重於學習秩一子空間時的誤差傳播。我們提出一種分析框架，將學習過程分解為一系列秩一估計問題，後續估計取決於先前步驟的準確性。本文的主要貢獻在於描述此序列過程中的誤差傳播，確立誤差（例如，由於有限的計算預算和有限的精度）如何影響整體模型準確性的界限。我們證明這些誤差以可預測的方式累積，對演算法設計和穩定性保證均有影響。", "audio": "audios/2505.22602v1.mp3", "timestamp": "2025-05-29T08:25:31.698746"}
{"query": "Foundation Model", "id": "2505.22209v1", "url": "http://arxiv.org/abs/2505.22209v1", "title": "A Survey on Training-free Open-Vocabulary Semantic Segmentation", "summary": "Semantic segmentation is one of the most fundamental tasks in image\nunderstanding with a long history of research, and subsequently a myriad of\ndifferent approaches. Traditional methods strive to train models up from\nscratch, requiring vast amounts of computational resources and training data.\nIn the advent of moving to open-vocabulary semantic segmentation, which asks\nmodels to classify beyond learned categories, large quantities of finely\nannotated data would be prohibitively expensive. Researchers have instead\nturned to training-free methods where they leverage existing models made for\ntasks where data is more easily acquired. Specifically, this survey will cover\nthe history, nuance, idea development and the state-of-the-art in training-free\nopen-vocabulary semantic segmentation that leverages existing multi-modal\nclassification models. We will first give a preliminary on the task definition\nfollowed by an overview of popular model archetypes and then spotlight over 30\napproaches split into broader research branches: purely CLIP-based, those\nleveraging auxiliary visual foundation models and ones relying on generative\nmethods. Subsequently, we will discuss the limitations and potential problems\nof current research, as well as provide some underexplored ideas for future\nstudy. We believe this survey will serve as a good onboarding read to new\nresearchers and spark increased interest in the area.", "authors": ["Naomi Kombol", "Ivan Martinović", "Siniša Šegvić"], "published_date": "2025-05-28", "title_zh": "無需訓練的開放詞彙語義分割綜述", "summary_zh": "語義分割是圖像理解的基礎任務之一，擁有悠久的研究歷史和眾多方法。傳統方法需從頭訓練模型，耗費大量計算資源和數據。為實現開放詞彙語義分割，即模型能分類未學習類別，精細標註數據成本高昂。研究者轉向免訓練方法，利用易於獲取數據的任務模型。本文綜述了免訓練開放詞彙語義分割的歷史、細節、思路發展和最新技術，重點介紹了利用現有多模態分類模型的方法。首先概述任務定義和流行模型，然後重點介紹30多種方法，分為三類：純粹基於CLIP、利用輔助視覺基礎模型以及依賴生成方法。隨後，討論了當前研究的局限性和潛在問題，並為未來研究提供了一些未充分探索的想法。本文旨在為新研究人員提供入門指導，並激發對該領域的興趣。", "audio": "audios/2505.22209v1.mp3", "timestamp": "2025-05-29T08:25:38.626648"}
{"query": "Diffusion Model", "id": "2505.22489v1", "url": "http://arxiv.org/abs/2505.22489v1", "title": "Cascaded 3D Diffusion Models for Whole-body 3D 18-F FDG PET/CT synthesis from Demographics", "summary": "We propose a cascaded 3D diffusion model framework to synthesize\nhigh-fidelity 3D PET/CT volumes directly from demographic variables, addressing\nthe growing need for realistic digital twins in oncologic imaging, virtual\ntrials, and AI-driven data augmentation. Unlike deterministic phantoms, which\nrely on predefined anatomical and metabolic templates, our method employs a\ntwo-stage generative process. An initial score-based diffusion model\nsynthesizes low-resolution PET/CT volumes from demographic variables alone,\nproviding global anatomical structures and approximate metabolic activity. This\nis followed by a super-resolution residual diffusion model that refines spatial\nresolution. Our framework was trained on 18-F FDG PET/CT scans from the AutoPET\ndataset and evaluated using organ-wise volume and standardized uptake value\n(SUV) distributions, comparing synthetic and real data between demographic\nsubgroups. The organ-wise comparison demonstrated strong concordance between\nsynthetic and real images. In particular, most deviations in metabolic uptake\nvalues remained within 3-5% of the ground truth in subgroup analysis. These\nfindings highlight the potential of cascaded 3D diffusion models to generate\nanatomically and metabolically accurate PET/CT images, offering a robust\nalternative to traditional phantoms and enabling scalable, population-informed\nsynthetic imaging for clinical and research applications.", "authors": ["Siyeop Yoon", "Sifan Song", "Pengfei Jin", "Matthew Tivnan", "Yujin Oh", "Sekeun Kim", "Dufan Wu", "Xiang Li", "Quanzheng Li"], "published_date": "2025-05-28", "title_zh": "基於人口統計資料之全身3D 18-F FDG PET/CT合成的級聯3D擴散模型", "summary_zh": "本研究提出一種級聯3D擴散模型框架，直接從人口統計變數合成高保真3D PET/CT體積，以應對腫瘤影像、虛擬試驗和AI驅動資料增強對真實數位分身日益增長的需求。不同於依賴預定義模板的確定性模型，本方法採用兩階段生成過程：首先，基於評分的擴散模型僅從人口統計變數合成低解析度PET/CT體積，提供整體結構和近似代謝活動；然後，超解析度殘差擴散模型細化空間解析度。該框架使用AutoPET數據集的18-F FDG PET/CT掃描進行訓練，並透過器官體積和標準攝取值（SUV）分佈評估合成數據和真實數據在不同人口統計亞群之間的差異。器官比較顯示合成影像和真實影像高度一致，且亞群分析中代謝攝取值偏差大多維持在真實值的3-5%以內。研究結果表明，級聯3D擴散模型具備生成解剖學和代謝精確PET/CT影像的潛力，為傳統模型提供了一種穩健的替代方案，並實現了可擴展的、基於人群資訊的合成影像，適用於臨床和研究應用。", "audio": "audios/2505.22489v1.mp3", "timestamp": "2025-05-29T08:25:46.271935"}
{"query": "AI", "id": "2505.22583v1", "url": "http://arxiv.org/abs/2505.22583v1", "title": "GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On Git", "summary": "Benchmarks for Software Engineering (SE) AI agents, most notably SWE-bench,\nhave catalyzed progress in programming capabilities of AI agents. However, they\noverlook critical developer workflows such as Version Control System (VCS)\noperations. To address this issue, we present GitGoodBench, a novel benchmark\nfor evaluating AI agent performance on VCS tasks. GitGoodBench covers three\ncore Git scenarios extracted from permissive open-source Python, Java, and\nKotlin repositories. Our benchmark provides three datasets: a comprehensive\nevaluation suite (900 samples), a rapid prototyping version (120 samples), and\na training corpus (17,469 samples). We establish baseline performance on the\nprototyping version of our benchmark using GPT-4o equipped with custom tools,\nachieving a 21.11% solve rate overall. We expect GitGoodBench to serve as a\ncrucial stepping stone toward truly comprehensive SE agents that go beyond mere\nprogramming.", "authors": ["Tobias Lindenbauer", "Egor Bogomolov", "Yaroslav Zharov"], "published_date": "2025-05-28", "title_zh": "GitGoodBench：一種評估代理在Git上表現的新型基準", "summary_zh": "軟體工程人工智慧代理基準測試，如SWE-bench，促進了人工智慧代理程式的程式設計能力。然而，它們忽略了版本控制系統（VCS）操作等關鍵開發者工作流程。為解決此問題，我們提出GitGoodBench，一個評估人工智慧代理程式在VCS任務上表現的新基準。GitGoodBench涵蓋了從開放原始碼Python、Java和Kotlin儲存庫中提取的三個核心Git場景。我們的基準測試提供三個資料集：一個綜合評估套件（900個樣本），一個快速原型版本（120個樣本）和一個訓練語料庫（17,469個樣本）。我們使用配備自定義工具的GPT-4o在原型版本上建立了基準性能，總體解決率為21.11%。我們預期GitGoodBench將成為邁向超越單純程式設計的真正全面性軟體工程代理程式的關鍵墊腳石。", "audio": "audios/2505.22583v1.mp3", "timestamp": "2025-05-29T09:20:03.927277"}
{"query": "Foundation Model", "id": "2505.22133v1", "url": "http://arxiv.org/abs/2505.22133v1", "title": "Developing a Top-tier Framework in Naturalistic Conditions Challenge for Categorized Emotion Prediction: From Speech Foundation Models and Learning Objective to Data Augmentation and Engineering Choices", "summary": "Speech emotion recognition (SER), particularly for naturally expressed\nemotions, remains a challenging computational task. Key challenges include the\ninherent subjectivity in emotion annotation and the imbalanced distribution of\nemotion labels in datasets. This paper introduces the \\texttt{SAILER} system\ndeveloped for participation in the INTERSPEECH 2025 Emotion Recognition\nChallenge (Task 1). The challenge dataset, which contains natural emotional\nspeech from podcasts, serves as a valuable resource for studying imbalanced and\nsubjective emotion annotations. Our system is designed to be simple,\nreproducible, and effective, highlighting critical choices in modeling,\nlearning objectives, data augmentation, and engineering choices. Results show\nthat even a single system (without ensembling) can outperform more than 95\\% of\nthe submissions, with a Macro-F1 score exceeding 0.4. Moreover, an ensemble of\nthree systems further improves performance, achieving a competitively ranked\nscore (top-3 performing team). Our model is at:\nhttps://github.com/tiantiaf0627/vox-profile-release.", "authors": ["Tiantian Feng", "Thanathai Lertpetchpun", "Dani Byrd", "Shrikanth Narayanan"], "published_date": "2025-05-28", "title_zh": "在自然情境下開發頂尖框架以應對分類情緒預測之挑戰：從語音基礎模型與學習目標到數據增強與工程選擇", "summary_zh": "語音情感辨識，尤其是在自然表達的情感方面，仍然是一項具有挑戰性的計算任務。情感標註的內在主觀性以及數據集中情感標籤的不平衡分佈是主要挑戰。\\texttt{SAILER}系統為參與INTERSPEECH 2025情感辨識挑戰賽（任務1）而開發，該挑戰賽數據集包含來自Podcast的自然情感語音，是研究不平衡和主觀情感標註的寶貴資源。本系統設計簡潔、可重現且有效，著重於建模、學習目標、數據擴增和工程選擇方面的關鍵考量。結果表明，即使是單個系統（不進行集成）也能夠超越95%以上的提交結果，其Macro-F1分數超過0.4。此外，三個系統的集成進一步提高了性能，取得了具有競爭力的排名分數（排名前三的團隊）。模型位於：https://github.com/tiantiaf0627/vox-profile-release。", "audio": "audios/2505.22133v1.mp3", "timestamp": "2025-05-29T09:20:11.469147"}
{"query": "Diffusion Model", "id": "2505.22407v1", "url": "http://arxiv.org/abs/2505.22407v1", "title": "Self-Reflective Reinforcement Learning for Diffusion-based Image Reasoning Generation", "summary": "Diffusion models have recently demonstrated exceptional performance in image\ngeneration task. However, existing image generation methods still significantly\nsuffer from the dilemma of image reasoning, especially in logic-centered image\ngeneration tasks. Inspired by the success of Chain of Thought (CoT) and\nReinforcement Learning (RL) in LLMs, we propose SRRL, a self-reflective RL\nalgorithm for diffusion models to achieve reasoning generation of logical\nimages by performing reflection and iteration across generation trajectories.\nThe intermediate samples in the denoising process carry noise, making accurate\nreward evaluation difficult. To address this challenge, SRRL treats the entire\ndenoising trajectory as a CoT step with multi-round reflective denoising\nprocess and introduces condition guided forward process, which allows for\nreflective iteration between CoT steps. Through SRRL-based iterative diffusion\ntraining, we introduce image reasoning through CoT into generation tasks\nadhering to physical laws and unconventional physical phenomena for the first\ntime. Notably, experimental results of case study exhibit that the superior\nperformance of our SRRL algorithm even compared with GPT-4o. The project page\nis https://jadenpan0.github.io/srrl.github.io/.", "authors": ["Jiadong Pan", "Zhiyuan Ma", "Kaiyan Zhang", "Ning Ding", "Bowen Zhou"], "published_date": "2025-05-28", "title_zh": "用於擴散模型的自反思增強學習，以生成圖像推理", "summary_zh": "擴散模型在圖像生成方面表現出色，但現有方法在圖像推理上仍有不足，尤其是在邏輯圖像生成中。受大型語言模型中思維鏈和強化學習成功的啟發，我們提出自反思強化學習(SRRL)，透過在生成軌跡中進行反思和迭代，使擴散模型能夠推理生成邏輯圖像。由於去噪過程中的中間樣本帶有雜訊，難以準確評估獎勵，SRRL將整個去噪軌跡視為多輪反思去噪的思維鏈步驟，並引入條件引導的前向過程，實現思維鏈步驟間的反思迭代。透過基於SRRL的迭代擴散訓練，我們首次將透過思維鏈的圖像推理引入到遵循物理定律和非常規物理現象的生成任務中。案例研究實驗結果表明，我們的SRRL算法性能優於GPT-4o。專案頁面為https://jadenpan0.github.io/srrl.github.io/。", "audio": "audios/2505.22407v1.mp3", "timestamp": "2025-05-29T09:20:18.392542"}
{"query": "AI", "id": "2505.22563v1", "url": "http://arxiv.org/abs/2505.22563v1", "title": "Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings", "summary": "Understanding whether large language models (LLMs) and the human brain\nconverge on similar computational principles remains a fundamental and\nimportant question in cognitive neuroscience and AI. Do the brain-like patterns\nobserved in LLMs emerge simply from scaling, or do they reflect deeper\nalignment with the architecture of human language processing? This study\nfocuses on the sentence-level neural mechanisms of language models,\nsystematically investigating how hierarchical representations in LLMs align\nwith the dynamic neural responses during human sentence comprehension. By\ncomparing hierarchical embeddings from 14 publicly available LLMs with fMRI\ndata collected from participants, who were exposed to a naturalistic narrative\nstory, we constructed sentence-level neural prediction models to precisely\nidentify the model layers most significantly correlated with brain region\nactivations. Results show that improvements in model performance drive the\nevolution of representational architectures toward brain-like hierarchies,\nparticularly achieving stronger functional and anatomical correspondence at\nhigher semantic abstraction levels.", "authors": ["Yu Lei", "Xingyang Ge", "Yi Zhang", "Yiming Yang", "Bolei Ma"], "published_date": "2025-05-28", "title_zh": "大型語言模型是否如人腦般思考？來自fMRI與階層式嵌入的句子層級證據", "summary_zh": "理解大型語言模型（LLMs）與人腦是否在相似計算原則上趨同，是認知神經科學和人工智慧領域的重要課題。LLMs中觀察到的類腦模式，僅僅源於規模擴展，還是反映了與人類語言處理架構更深層次的對齊？本研究關注LLMs的句子級神經機制，系統性地探討LLMs中的層級表徵如何與人類句子理解過程中的動態神經反應對應。通過比較14個公開LLMs的層級嵌入與參與者在自然敘事故事中的fMRI數據，我們構建了句子級神經預測模型，以精確識別與大腦區域激活最顯著相關的模型層。結果表明，模型性能的提升驅動了表徵架構向類腦層級結構的演進，尤其是在較高語義抽象層面實現了更強的功能和解剖學對應關係。", "audio": "audios/2505.22563v1.mp3", "timestamp": "2025-05-29T10:20:47.132926"}
{"query": "Foundation Model", "id": "2505.22072v1", "url": "http://arxiv.org/abs/2505.22072v1", "title": "On-the-fly Routing for Zero-shot MoE Speaker Adaptation of Speech Foundation Models for Dysarthric Speech Recognition", "summary": "This paper proposes a novel MoE-based speaker adaptation framework for\nfoundation models based dysarthric speech recognition. This approach enables\nzero-shot adaptation and real-time processing while incorporating domain\nknowledge. Speech impairment severity and gender conditioned adapter experts\nare dynamically combined using on-the-fly predicted speaker-dependent routing\nparameters. KL-divergence is used to further enforce diversity among experts\nand their generalization to unseen speakers. Experimental results on the\nUASpeech corpus suggest that on-the-fly MoE-based adaptation produces\nstatistically significant WER reductions of up to 1.34% absolute (6.36%\nrelative) over the unadapted baseline HuBERT/WavLM models. Consistent WER\nreductions of up to 2.55% absolute (11.44% relative) and RTF speedups of up to\n7 times are obtained over batch-mode adaptation across varying speaker-level\ndata quantities. The lowest published WER of 16.35% (46.77% on very low\nintelligibility) is obtained.", "authors": ["Shujie HU", "Xurong Xie", "Mengzhe Geng", "Jiajun Deng", "Huimeng Wang", "Guinan Li", "Chengxi Deng", "Tianzi Wang", "Mingyu Cui", "Helen Meng", "Xunying Liu"], "published_date": "2025-05-28", "title_zh": "針對構音障礙語音辨識，語音基礎模型之零樣本 MoE 揚聲器適應的即時路由", "summary_zh": "本文提出一種基於混合專家模型（MoE）的口語障礙語音辨識基礎模型之新穎說話人適應框架。此方法結合領域知識，實現零樣本適應和即時處理。語音障礙嚴重程度和性別條件化的適配器專家，透過即時預測的說話人相關路由參數進行動態組合。KL散度用於進一步增強專家之間的多樣性及其對未見說話人的泛化能力。在UASpeech語料庫上的實驗結果表明，基於MoE的即時適應相較於未適應的HuBERT/WavLM基準模型，產生具有統計顯著性的WER降低，最高可達1.34%（絕對值）或6.36%（相對值）。與批次模式適應相比，在不同的說話人數據量下，獲得了一致的WER降低，最高可達2.55%（絕對值）或11.44%（相對值），以及高達7倍的RTF速度提升。最終取得已發表的最低WER，為16.35%（極低清晰度下為46.77%）。", "audio": "audios/2505.22072v1.mp3", "timestamp": "2025-05-29T10:20:54.150408"}
{"query": "Diffusion Model", "id": "2505.22391v1", "url": "http://arxiv.org/abs/2505.22391v1", "title": "Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation", "summary": "Modeling physical systems in a generative manner offers several advantages,\nincluding the ability to handle partial observations, generate diverse\nsolutions, and address both forward and inverse problems. Recently, diffusion\nmodels have gained increasing attention in the modeling of physical systems,\nparticularly those governed by partial differential equations (PDEs). However,\ndiffusion models only access noisy data $\\boldsymbol{x}_t$ at intermediate\nsteps, making it infeasible to directly enforce constraints on the clean sample\n$\\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are\ntypically applied to the expectation of clean samples\n$\\mathbb{E}[\\boldsymbol{x}_0|\\boldsymbol{x}_t]$, which is estimated using the\nlearned score network. However, imposing PDE constraints on the expectation\ndoes not strictly represent the one on the true clean data, known as Jensen's\nGap. This gap creates a trade-off: enforcing PDE constraints may come at the\ncost of reduced accuracy in generative modeling. To address this, we propose a\nsimple yet effective post-hoc distillation approach, where PDE constraints are\nnot injected directly into the diffusion process, but instead enforced during a\npost-hoc distillation stage. We term our method as Physics-Informed\nDistillation of Diffusion Models (PIDDM). This distillation not only\nfacilitates single-step generation with improved PDE satisfaction, but also\nsupport both forward and inverse problem solving and reconstruction from\nrandomly partial observation. Extensive experiments across various PDE\nbenchmarks demonstrate that PIDDM significantly improves PDE satisfaction over\nseveral recent and competitive baselines, such as PIDM, DiffusionPDE, and\nECI-sampling, with less computation overhead. Our approach can shed light on\nmore efficient and effective strategies for incorporating physical constraints\ninto diffusion models.", "authors": ["Yi Zhang", "Difan Zou"], "published_date": "2025-05-28", "title_zh": "基於物理資訊提煉的擴散模型用於偏微分方程約束生成", "summary_zh": "生成式物理系統建模具有處理局部觀測、生成多樣解以及解決正向和逆向問題等優勢。擴散模型在偏微分方程式（PDE）建模中備受關注。然而，擴散模型僅能存取中間步驟的雜訊數據，難以直接對乾淨樣本施加約束。通常做法是對乾淨樣本期望值施加約束，但這會產生Jensen不等式間隙，造成PDE約束與生成模型準確性之間的權衡。本文提出一種後處理蒸餾方法PIDDM，不在擴散過程中直接注入PDE約束，而是在蒸餾階段強制執行。此方法不僅能以更佳的PDE滿足度實現單步生成，還支援正向和逆向問題求解以及隨機局部觀測下的重建。實驗表明，PIDDM在多個PDE基準測試中顯著提高了PDE滿足度，且計算開銷較小，為將物理約束納入擴散模型提供了更有效率的策略。", "audio": "audios/2505.22391v1.mp3", "timestamp": "2025-05-29T10:21:01.367959"}
{"query": "AI", "id": "2505.22551v1", "url": "http://arxiv.org/abs/2505.22551v1", "title": "Deep Learning-Based BMD Estimation from Radiographs with Conformal Uncertainty Quantification", "summary": "Limited DXA access hinders osteoporosis screening. This proof-of-concept\nstudy proposes using widely available knee X-rays for opportunistic Bone\nMineral Density (BMD) estimation via deep learning, emphasizing robust\nuncertainty quantification essential for clinical use. An EfficientNet model\nwas trained on the OAI dataset to predict BMD from bilateral knee radiographs.\nTwo Test-Time Augmentation (TTA) methods were compared: traditional averaging\nand a multi-sample approach. Crucially, Split Conformal Prediction was\nimplemented to provide statistically rigorous, patient-specific prediction\nintervals with guaranteed coverage. Results showed a Pearson correlation of\n0.68 (traditional TTA). While traditional TTA yielded better point predictions,\nthe multi-sample approach produced slightly tighter confidence intervals (90%,\n95%, 99%) while maintaining coverage. The framework appropriately expressed\nhigher uncertainty for challenging cases. Although anatomical mismatch between\nknee X-rays and standard DXA limits immediate clinical use, this method\nestablishes a foundation for trustworthy AI-assisted BMD screening using\nroutine radiographs, potentially improving early osteoporosis detection.", "authors": ["Long Hui", "Wai Lok Yeung"], "published_date": "2025-05-28", "title_zh": "基於深度學習的X光片骨密度估計與共形不確定性量化", "summary_zh": "骨質密度儀普及率低阻礙骨質疏鬆篩檢。本研究提出利用深度學習，從廣泛可用的膝蓋X光片中估算骨質密度，並著重於臨床應用所需的穩健不確定性量化。利用OAI資料集訓練EfficientNet模型，從雙側膝關節X光片預測骨質密度。比較了兩種測試時增強方法：傳統平均法和多樣本法。採用分流一致性預測，提供具有保證覆蓋率、且針對個別患者的統計嚴謹預測區間。結果顯示，傳統平均法的皮爾森相關係數為0.68。雖然傳統平均法產生更好的點預測，但多樣本法在保持覆蓋率的同時，產生了稍微更窄的信賴區間（90%、95%、99%）。該框架能適當表達高難度案例的不確定性。儘管膝蓋X光片與標準骨質密度儀在解剖結構上存在差異，限制了直接臨床應用，但此方法為使用常規X光片的可靠AI輔助骨質密度篩檢奠定了基礎，有潛力改善骨質疏鬆症的早期檢測。", "audio": "audios/2505.22551v1.mp3", "timestamp": "2025-05-29T11:16:00.410904"}
{"query": "Foundation Model", "id": "2505.21928v1", "url": "http://arxiv.org/abs/2505.21928v1", "title": "Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology", "summary": "Gastrointestinal (GI) diseases represent a clinically significant burden,\nnecessitating precise diagnostic approaches to optimize patient outcomes.\nConventional histopathological diagnosis, heavily reliant on the subjective\ninterpretation of pathologists, suffers from limited reproducibility and\ndiagnostic variability. To overcome these limitations and address the lack of\npathology-specific foundation models for GI diseases, we develop Digepath, a\nspecialized foundation model for GI pathology. Our framework introduces a\ndual-phase iterative optimization strategy combining pretraining with\nfine-screening, specifically designed to address the detection of sparsely\ndistributed lesion areas in whole-slide images. Digepath is pretrained on more\nthan 353 million image patches from over 200,000 hematoxylin and eosin-stained\nslides of GI diseases. It attains state-of-the-art performance on 33 out of 34\ntasks related to GI pathology, including pathological diagnosis, molecular\nprediction, gene mutation prediction, and prognosis evaluation, particularly in\ndiagnostically ambiguous cases and resolution-agnostic tissue classification.We\nfurther translate the intelligent screening module for early GI cancer and\nachieve near-perfect 99.6% sensitivity across 9 independent medical\ninstitutions nationwide. The outstanding performance of Digepath highlights its\npotential to bridge critical gaps in histopathological practice. This work not\nonly advances AI-driven precision pathology for GI diseases but also\nestablishes a transferable paradigm for other pathology subspecialties.", "authors": ["Lianghui Zhu", "Xitong Ling", "Minxi Ouyang", "Xiaoping Liu", "Mingxi Fu", "Tian Guan", "Fanglei Fu", "Xuanyu Wang", "Maomao Zeng", "Mingxi Zhu", "Yibo Jin", "Liming Liu", "Song Duan", "Qiming He", "Yizhi Wang", "Luxi Xie", "Houqiang Li", "Yonghong He", "Sufang Tian"], "published_date": "2025-05-28", "title_zh": "智能胃腸病理學之次專科特定基礎模型", "summary_zh": "腸胃道疾病造成顯著的臨床負擔，精確診斷至關重要。傳統組織病理學診斷仰賴病理學家主觀判讀，重現性低且診斷差異大。為克服這些限制，我們開發了專為腸胃道病理設計的Digepath基礎模型，解決缺乏特定病理基礎模型的問題。該框架採用雙階段迭代優化策略，結合預訓練與精細篩選，專門用於檢測全玻片影像中稀疏分佈的病灶區域。Digepath使用超過20萬張腸胃道疾病蘇木精伊紅染色玻片，共計3.53億張影像切片進行預訓練。在34項腸胃道病理相關任務中的33項達到最佳效能，包括病理診斷、分子預測、基因突變預測和預後評估，尤其在診斷不明確的案例和分辨率無關的組織分類方面表現出色。我們進一步將智慧篩選模組應用於早期腸胃道癌症，在全國9家獨立醫療機構中達到接近完美的99.6%敏感度。Digepath的卓越表現突顯了其在組織病理學實踐中填補關鍵缺口的潛力。這項工作不僅推進了人工智慧驅動的腸胃道疾病精準病理學，也為其他病理亞專科建立了可轉移的範例。", "audio": "audios/2505.21928v1.mp3", "timestamp": "2025-05-29T11:16:08.178858"}
{"query": "Diffusion Model", "id": "2505.22322v1", "url": "http://arxiv.org/abs/2505.22322v1", "title": "A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric Perspective", "summary": "Diffusion models have shown strong performance in generating high-quality\ntabular data, but they carry privacy risks by reproducing exact training\nsamples. While prior work focuses on dataset-level augmentation to reduce\nmemorization, little is known about which individual samples contribute most.\nWe present the first data-centric study of memorization dynamics in tabular\ndiffusion models. We quantify memorization for each real sample based on how\nmany generated samples are flagged as replicas, using a relative distance\nratio. Our empirical analysis reveals a heavy-tailed distribution of\nmemorization counts: a small subset of samples contributes disproportionately\nto leakage, confirmed via sample-removal experiments. To understand this, we\ndivide real samples into top- and non-top-memorized groups and analyze their\ntraining-time behaviors. We track when each sample is first memorized and\nmonitor per-epoch memorization intensity (AUC). Memorized samples are memorized\nslightly earlier and show stronger signals in early training. Based on these\ninsights, we propose DynamicCut, a two-stage, model-agnostic mitigation method:\n(a) rank samples by epoch-wise intensity, (b) prune a tunable top fraction, and\n(c) retrain on the filtered dataset. Across multiple tabular datasets and\nmodels, DynamicCut reduces memorization with minimal impact on data diversity\nand downstream performance. It also complements augmentation-based defenses.\nFurthermore, DynamicCut enables cross-model transferability: high-ranked\nsamples identified from one model (e.g., a diffusion model) are also effective\nfor reducing memorization when removed from others, such as GANs and VAEs.", "authors": ["Zhengyu Fang", "Zhimeng Jiang", "Huiyuan Chen", "Xiaoge Zhang", "Kaiyu Tang", "Xiao Li", "Jing Li"], "published_date": "2025-05-28", "title_zh": "表格擴散模型記憶現象之深入探討：以數據為中心的視角", "summary_zh": "擴散模型在生成高品質表格數據方面表現出色，但也存在洩漏訓練樣本的隱私風險。雖然先前研究側重於數據集層面的增強來減少記憶，但對於哪些個體樣本貢獻最大知之甚少。本研究首次針對表格擴散模型中的記憶動態進行數據中心分析，利用相對距離比量化每個真實樣本的記憶程度，基於生成的樣本中被標記為副本的數量。實證分析揭示了記憶計數的重尾分佈：一小部分樣本不成比例地導致洩漏，樣本移除實驗證實了這一點。為理解此現象，將真實樣本分為高記憶組和非高記憶組，分析其訓練時的行為，追蹤每個樣本首次被記憶的時間，並監測每週期記憶強度 (AUC)。高記憶樣本被記憶的時間略早，並在早期訓練中顯示出更強的信號。基於這些發現，提出DynamicCut，一種與模型無關的兩階段緩解方法：(a) 按週期強度對樣本進行排序，(b) 修剪可調整的頂部比例，以及 (c) 在過濾後的數據集上重新訓練。在多個表格數據集和模型中，DynamicCut 減少了記憶，同時對數據多樣性和下游性能的影響最小，並且可與基於增強的防禦方法互補。此外，DynamicCut 實現了跨模型的可遷移性：從一個模型（例如，擴散模型）識別出的高排名樣本在從其他模型（例如，GAN 和 VAE）中移除時，也能有效地減少記憶。", "audio": "audios/2505.22322v1.mp3", "timestamp": "2025-05-29T11:16:22.820693"}
{"query": "AI", "id": "2505.22541v1", "url": "http://arxiv.org/abs/2505.22541v1", "title": "A Human-Centric Approach to Explainable AI for Personalized Education", "summary": "Deep neural networks form the backbone of artificial intelligence research,\nwith potential to transform the human experience in areas ranging from\nautonomous driving to personal assistants, healthcare to education. However,\ntheir integration into the daily routines of real-world classrooms remains\nlimited. It is not yet common for a teacher to assign students individualized\nhomework targeting their specific weaknesses, provide students with instant\nfeedback, or simulate student responses to a new exam question. While these\nmodels excel in predictive performance, this lack of adoption can be attributed\nto a significant weakness: the lack of explainability of model decisions,\nleading to a lack of trust from students, parents, and teachers. This thesis\naims to bring human needs to the forefront of eXplainable AI (XAI) research,\ngrounded in the concrete use case of personalized learning and teaching. We\nframe the contributions along two verticals: technical advances in XAI and\ntheir aligned human studies. We investigate explainability in AI for education,\nrevealing systematic disagreements between post-hoc explainers and identifying\na need for inherently interpretable model architectures. We propose four novel\ntechnical contributions in interpretability with a multimodal modular\narchitecture (MultiModN), an interpretable mixture-of-experts model\n(InterpretCC), adversarial training for explainer stability, and a\ntheory-driven LLM-XAI framework to present explanations to students\n(iLLuMinaTE), which we evaluate in diverse settings with professors, teachers,\nlearning scientists, and university students. By combining empirical\nevaluations of existing explainers with novel architectural designs and human\nstudies, our work lays a foundation for human-centric AI systems that balance\nstate-of-the-art performance with built-in transparency and trust.", "authors": ["Vinitra Swamy"], "published_date": "2025-05-28", "title_zh": "以人為本的可解釋人工智慧於個人化教育之應用", "summary_zh": "深度神經網路是人工智慧研究的基石，具改變人類經驗的潛力，但其在實際課堂中的應用仍受限。模型卓越的預測性能與實際採用間存在落差，主因是模型決策缺乏可解釋性，導致學生、家長和教師缺乏信任。本論文以個人化學習和教學為例，將人類需求置於可解釋人工智慧（XAI）研究的前沿，貢獻體現在XAI技術進步及其相關的人類研究。研究探討教育人工智慧的可解釋性，揭示事後解釋器間的系統性分歧，並強調內在可解釋模型架構的需求。論文提出四項可解釋性技術貢獻：多模態模組化架構(MultiModN)、可解釋的專家混合模型(InterpretCC)、用於解釋器穩定性的對抗式訓練，以及一個理論驅動的LLM-XAI框架(iLLuMinaTE)，用於向學生呈現解釋。透過結合現有解釋器的實證評估、新穎的架構設計和人類研究，本研究為以人為本的AI系統奠定基礎，在先進性能與內建透明度和信任之間取得平衡。", "audio": "audios/2505.22541v1.mp3", "timestamp": "2025-05-29T12:37:41.342312"}
{"query": "Foundation Model", "id": "2505.21926v1", "url": "http://arxiv.org/abs/2505.21926v1", "title": "Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning", "summary": "In natural language processing (NLP) and computer vision (CV), the successful\napplication of foundation models across diverse tasks has demonstrated their\nremarkable potential. However, despite the rich structural and textual\ninformation embedded in knowledge graphs (KGs), existing research of foundation\nmodel for KG has primarily focused on their structural aspects, with most\nefforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This\nlimitation has hindered progress in addressing more challenging out-of-KG\ntasks. In this paper, we introduce MERRY, a foundation model for general\nknowledge graph reasoning, and investigate its performance across two task\ncategories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG\nquestion answering, KGQA). We not only utilize the structural information, but\nalso the textual information in KGs. Specifically, we propose a\nmulti-perspective Conditional Message Passing (CMP) encoding architecture to\nbridge the gap between textual and structural modalities, enabling their\nseamless integration. Additionally, we introduce a dynamic residual fusion\nmodule to selectively retain relevant textual information and a flexible edge\nscoring mechanism to adapt to diverse downstream tasks. Comprehensive\nevaluations on 28 datasets demonstrate that MERRY outperforms existing\nbaselines in most scenarios, showcasing strong reasoning capabilities within\nKGs and excellent generalization to out-of-KG tasks such as KGQA.", "authors": ["Yin Hua", "Zhiqiang Liu", "Mingyang Chen", "Zheng Fang", "Chi Man Wong", "Lingxiao Li", "Chi Man Vong", "Huajun Chen", "Wen Zhang"], "published_date": "2025-05-28", "title_zh": "超越補全：通用知識圖譜推理的基礎模型", "summary_zh": "在自然語言處理與電腦視覺領域，基石模型於多樣任務的成功應用展現其卓越潛力。然而，儘管知識圖譜蘊含豐富的結構與文本資訊，現有知識圖譜基石模型研究主要聚焦於結構面向，且多侷限於圖譜內任務（如知識圖譜補全），阻礙了更具挑戰性的圖譜外任務之進展。本研究提出 MERRY，一個通用知識圖譜推理基石模型，並評估其於圖譜內推理（如知識圖譜補全）和圖譜外任務（如知識圖譜問答）兩大類別的效能。MERRY 不僅運用結構資訊，更利用知識圖譜中的文本資訊。具體而言，本研究提出多視角條件訊息傳遞編碼架構，以彌合文本與結構模態間之鴻溝，實現無縫整合。此外，引入動態殘差融合模組以選擇性保留相關文本資訊，並採用靈活的邊緣評分機制以適應多樣的下游任務。在 28 個資料集上的全面評估顯示，MERRY 在多數情境下優於現有基準模型，展現了強大的圖譜內推理能力以及對圖譜外任務（如知識圖譜問答）的優異泛化能力。", "audio": "audios/2505.21926v1.mp3", "timestamp": "2025-05-29T12:38:02.654719"}
{"query": "Diffusion Model", "id": "2505.22246v1", "url": "http://arxiv.org/abs/2505.22246v1", "title": "StateSpaceDiffuser: Bringing Long Context to Diffusion World Models", "summary": "World models have recently become promising tools for predicting realistic\nvisuals based on actions in complex environments. However, their reliance on a\nshort sequence of observations causes them to quickly lose track of context. As\na result, visual consistency breaks down after just a few steps, and generated\nscenes no longer reflect information seen earlier. This limitation of the\nstate-of-the-art diffusion-based world models comes from their lack of a\nlasting environment state. To address this problem, we introduce\nStateSpaceDiffuser, where a diffusion model is enabled to perform on\nlong-context tasks by integrating a sequence representation from a state-space\nmodel (Mamba), representing the entire interaction history. This design\nrestores long-term memory without sacrificing the high-fidelity synthesis of\ndiffusion models. To rigorously measure temporal consistency, we develop an\nevaluation protocol that probes a model's ability to reinstantiate seen content\nin extended rollouts. Comprehensive experiments show that StateSpaceDiffuser\nsignificantly outperforms a strong diffusion-only baseline, maintaining a\ncoherent visual context for an order of magnitude more steps. It delivers\nconsistent views in both a 2D maze navigation and a complex 3D environment.\nThese results establish that bringing state-space representations into\ndiffusion models is highly effective in demonstrating both visual details and\nlong-term memory.", "authors": ["Nedko Savov", "Naser Kazemi", "Deheng Zhang", "Danda Pani Paudel", "Xi Wang", "Luc Van Gool"], "published_date": "2025-05-28", "title_zh": "狀態空間擴散器：將長程上下文引入擴散世界模型", "summary_zh": "世界模型擅長根據複雜環境中的動作預測逼真視覺效果，但其對短序列觀察的依賴導致快速丟失上下文。因此，視覺一致性在幾步後崩潰，生成場景不再反映先前資訊。為了解決此問題，我們引入 StateSpaceDiffuser，它整合來自狀態空間模型（Mamba）的序列表示，代表完整的互動歷史，使擴散模型能夠執行長上下文任務。此設計恢復長期記憶，且不犧牲擴散模型的高保真合成能力。我們開發了一項評估協議，嚴格衡量時間一致性，探測模型在擴展rollout中重新實例化先前內容的能力。實驗表明，StateSpaceDiffuser顯著優於僅使用擴散模型的基準，保持視覺上下文連貫性長達數個數量級。在2D迷宮導航和複雜3D環境中，它均提供一致的視圖。這些結果表明，將狀態空間表示引入擴散模型能有效展現視覺細節和長期記憶。", "audio": "audios/2505.22246v1.mp3", "timestamp": "2025-05-29T12:38:14.294646"}
{"query": "AI", "id": "2505.22531v1", "url": "http://arxiv.org/abs/2505.22531v1", "title": "Training RL Agents for Multi-Objective Network Defense Tasks", "summary": "Open-ended learning (OEL) -- which emphasizes training agents that achieve\nbroad capability over narrow competency -- is emerging as a paradigm to develop\nartificial intelligence (AI) agents to achieve robustness and generalization.\nHowever, despite promising results that demonstrate the benefits of OEL,\napplying OEL to develop autonomous agents for real-world cybersecurity\napplications remains a challenge.\n  We propose a training approach, inspired by OEL, to develop autonomous\nnetwork defenders. Our results demonstrate that like in other domains, OEL\nprinciples can translate into more robust and generalizable agents for cyber\ndefense. To apply OEL to network defense, it is necessary to address several\ntechnical challenges. Most importantly, it is critical to provide a task\nrepresentation approach over a broad universe of tasks that maintains a\nconsistent interface over goals, rewards and action spaces. This way, the\nlearning agent can train with varying network conditions, attacker behaviors,\nand defender goals while being able to build on previously gained knowledge.\n  With our tools and results, we aim to fundamentally impact research that\napplies AI to solve cybersecurity problems. Specifically, as researchers\ndevelop gyms and benchmarks for cyber defense, it is paramount that they\nconsider diverse tasks with consistent representations, such as those we\npropose in our work.", "authors": ["Andres Molina-Markham", "Luis Robaina", "Sean Steinle", "Akash Trivedi", "Derek Tsui", "Nicholas Potteiger", "Lauren Brandt", "Ransom Winder", "Ahmed Ridley"], "published_date": "2025-05-28", "title_zh": "多目標網絡防禦任務中強化學習代理的訓練", "summary_zh": "開放式學習(OEL)著重培養具備廣泛能力而非狹隘專精的智能體，正逐漸成為開發具備穩健性和泛化能力人工智慧(AI)智能體的新典範。儘管OEL已展現出其優勢，但將其應用於開發現實世界網絡安全應用的自主智能體仍然是一項挑戰。\n\n本文提出一種受OEL啟發的訓練方法，以開發自主網絡防禦者。研究結果表明，與其他領域類似，OEL原則可轉化為更穩健和泛化的網絡防禦智能體。將OEL應用於網絡防禦需解決若干技術挑戰，最重要的是，需提供一種能在廣泛任務範圍內保持目標、獎勵和行動空間一致性的任務表示方法。如此，學習智能體便能在不同的網絡環境、攻擊者行為和防禦者目標下進行訓練，同時能建立在先前獲得的知識之上。\n\n透過我們的工具和研究成果，旨在對應用AI解決網絡安全問題的研究產生根本性的影響。特別是，當研究人員為網絡防禦開發模擬環境和基準時，至關重要的是要考慮具有一致性表示的多樣化任務，例如我們在工作中提出的那些。", "audio": "audios/2505.22531v1.mp3", "timestamp": "2025-05-29T13:29:49.583127"}
{"query": "Foundation Model", "id": "2505.21923v1", "url": "http://arxiv.org/abs/2505.21923v1", "title": "FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design", "summary": "Designing analog circuits from performance specifications is a complex,\nmulti-stage process encompassing topology selection, parameter inference, and\nlayout feasibility. We introduce FALCON, a unified machine learning framework\nthat enables fully automated, specification-driven analog circuit synthesis\nthrough topology selection and layout-constrained optimization. Given a target\nperformance, FALCON first selects an appropriate circuit topology using a\nperformance-driven classifier guided by human design heuristics. Next, it\nemploys a custom, edge-centric graph neural network trained to map circuit\ntopology and parameters to performance, enabling gradient-based parameter\ninference through the learned forward model. This inference is guided by a\ndifferentiable layout cost, derived from analytical equations capturing\nparasitic and frequency-dependent effects, and constrained by design rules. We\ntrain and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave\ncircuits, generated and simulated using Cadence Spectre across 20\nexpert-designed topologies. Through this evaluation, FALCON demonstrates >99\\%\naccuracy in topology inference, <10\\% relative error in performance prediction,\nand efficient layout-aware design that completes in under 1 second per\ninstance. Together, these results position FALCON as a practical and extensible\nfoundation model for end-to-end analog circuit design automation.", "authors": ["Asal Mehradfar", "Xuzhe Zhao", "Yilun Huang", "Emir Ceyani", "Yankai Yang", "Shihao Han", "Hamidreza Aghasi", "Salman Avestimehr"], "published_date": "2025-05-28", "title_zh": "FALCON：全自動佈局約束類比電路設計的機器學習框架", "summary_zh": "FALCON是一個整合機器學習框架，用於全自動、規格驅動的類比電路合成。它結合拓撲選擇與佈局約束最佳化，首先根據效能需求和設計經驗，利用效能驅動的分類器選擇合適的電路拓撲。接著，採用客製化的邊緣中心圖神經網路，將電路拓撲和參數映射到效能，透過學習的前向模型進行基於梯度的參數推斷。此推斷受可微分的佈局成本引導，該成本源於分析方程式，捕捉寄生效應和頻率相關效應，並受設計規則約束。FALCON 在包含百萬級模擬毫米波電路的客製化數據集上進行訓練和評估，數據集基於 Cadence Spectre 模擬的 20 種專家設計拓撲。評估結果顯示，FALCON 在拓撲推斷方面具有超過 99% 的準確度，效能預測方面相對誤差小於 10%，並且能實現高效的佈局感知設計，每個實例完成時間少於 1 秒。這些結果表明 FALCON 是一個實用且可擴展的基礎模型，適用於端到端類比電路設計自動化。", "audio": "audios/2505.21923v1.mp3", "timestamp": "2025-05-29T13:29:57.934687"}
{"query": "Diffusion Model", "id": "2505.22193v1", "url": "http://arxiv.org/abs/2505.22193v1", "title": "Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion", "summary": "Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI\nthat aims to use quantum properties to improve the performances of their\nclassical counterparts. However, existing algorithms are not easily scalable\ndue to the limitations of near-term quantum devices. Following our previous\nwork on QDMs, here we propose and implement two physics-inspired protocols. In\nthe first, we use the formalism of quantum stochastic walks, showing that a\nspecific interplay of quantum and classical dynamics in the forward process\nproduces statistically more robust models generating sets of MNIST images with\nlower Fr\\'echet Inception Distance (FID) than using totally classical dynamics.\nIn the second approach, we realize an algorithm to generate images by\nexploiting the intrinsic noise of real IBM quantum hardware with only four\nqubits. Our work could be a starting point to pave the way for new scenarios\nfor large-scale algorithms in quantum Generative AI, where quantum noise is\nneither mitigated nor corrected, but instead exploited as a useful resource.", "authors": ["Marco Parigi", "Stefano Martina", "Francesco Aldo Venturelli", "Filippo Caruso"], "published_date": "2025-05-28", "title_zh": "基於真實硬體雜訊量子擴散的物理啟發生成式人工智慧模型", "summary_zh": "量子擴散模型是生成式人工智慧的新興範例，旨在利用量子特性來提升效能。然而，由於近期量子裝置的限制，現有演算法難以擴展。繼我們先前的量子擴散模型研究，本文提出並實作兩種受物理啟發的協定。第一種方法運用量子隨機行走的形式，展示正向過程中量子和古典動力學的特定相互作用，能產生統計上更穩健的模型，其生成MNIST圖像集的Fréchet啟始距離(FID)低於純古典動力學。第二種方法則利用IBM真實量子硬體的固有雜訊，僅用四個量子位元實現圖像生成演算法。此研究可為量子生成式人工智慧大規模演算法開創新局，將量子雜訊視為有用資源而非減輕或校正的對象。", "audio": "audios/2505.22193v1.mp3", "timestamp": "2025-05-29T13:30:05.020995"}
{"query": "AI", "id": "2505.22526v1", "url": "http://arxiv.org/abs/2505.22526v1", "title": "AI instructional agent improves student's perceived learner control and learning outcome: empirical evidence from a randomized controlled trial", "summary": "This study examines the impact of an AI instructional agent on students'\nperceived learner control and academic performance in a medium demanding course\nwith lecturing as the main teaching strategy. Based on a randomized controlled\ntrial, three instructional conditions were compared: a traditional human\nteacher, a self-paced MOOC with chatbot support, and an AI instructional agent\ncapable of delivering lectures and responding to questions in real time.\nStudents in the AI instructional agent group reported significantly higher\nlevels of perceived learner control compared to the other groups. They also\ncompleted the learning task more efficiently and engaged in more frequent\ninteractions with the instructional system. Regression analyzes showed that\nperceived learner control positively predicted post-test performance, with\nbehavioral indicators such as reduced learning time and higher interaction\nfrequency supporting this relationship. These findings suggest that AI\ninstructional agents, when designed to support personalized pace and responsive\ninteraction, can enhance both students' learning experience and learning\noutcomes.", "authors": ["Fei Qin", "Zhanxin Hao", "Jifan Yu", "Zhiyuan Liu", "Yu Zhang"], "published_date": "2025-05-28", "title_zh": "人工智慧教學代理提升學生感知學習者控制感與學習成果：來自隨機對照試驗的實證證據", "summary_zh": "本研究探討AI教學媒介對學生在以講授為主的課程中，其學習者控制感及學業表現之影響。透過隨機對照試驗，比較傳統教師、具聊天機器人支援的自學MOOC及能即時授課並回應提問的AI教學媒介三種教學情境。結果顯示，AI教學媒介組的學生回報顯著更高的學習者控制感，且更有效率地完成學習任務，與教學系統的互動也更頻繁。迴歸分析顯示，學習者控制感正向預測後測表現，學習時間縮短及互動頻率提高等行為指標亦支持此關係。研究表明，若AI教學媒介經設計能支援個人化步調及回應式互動，可提升學生的學習體驗與成效。", "audio": "audios/2505.22526v1.mp3", "timestamp": "2025-05-29T14:18:17.435783"}
{"query": "Foundation Model", "id": "2505.21920v1", "url": "http://arxiv.org/abs/2505.21920v1", "title": "InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective", "summary": "The Segment Anything Model (SAM), a vision foundation model, exhibits\nimpressive zero-shot capabilities in general tasks but struggles in specialized\ndomains. Parameter-efficient fine-tuning (PEFT) is a promising approach to\nunleash the potential of SAM in novel scenarios. However, existing PEFT methods\nfor SAM neglect the domain-invariant relations encoded in the pre-trained\nmodel. To bridge this gap, we propose InfoSAM, an information-theoretic\napproach that enhances SAM fine-tuning by distilling and preserving its\npre-trained segmentation knowledge. Specifically, we formulate the knowledge\ntransfer process as two novel mutual information-based objectives: (i) to\ncompress the domain-invariant relation extracted from pre-trained SAM,\nexcluding pseudo-invariant information as possible, and (ii) to maximize mutual\ninformation between the relational knowledge learned by the teacher\n(pre-trained SAM) and the student (fine-tuned model). The proposed InfoSAM\nestablishes a robust distillation framework for PEFT of SAM. Extensive\nexperiments across diverse benchmarks validate InfoSAM's effectiveness in\nimproving SAM family's performance on real-world tasks, demonstrating its\nadaptability and superiority in handling specialized scenarios.", "authors": ["Yuanhong Zhang", "Muyao Yuan", "Weizhan Zhang", "Tieliang Gong", "Wen Wen", "Jiangyong Ying", "Weijie Shi"], "published_date": "2025-05-28", "title_zh": "InfoSAM：基於信息論視角的 Segment Anything 模型微調", "summary_zh": "分割一切模型(SAM)雖在通用任務中展現卓越的零樣本能力，但在特定領域表現不佳。參數高效微調(PEFT)有望釋放SAM在新情境中的潛力。現有SAM的PEFT方法忽略了預訓練模型中編碼的領域不變關係。本研究提出InfoSAM，一種基於資訊理論的方法，透過提煉和保留預訓練的分割知識來強化SAM微調。具體而言，知識轉移過程被公式化為兩個基於互資訊的新穎目標：(i)壓縮從預訓練SAM提取的領域不變關係，盡可能排除偽不變資訊；(ii)最大化教師(預訓練SAM)和學生(微調模型)學習到的關係知識之間的互資訊。InfoSAM為SAM的PEFT建立了一個穩健的蒸餾框架。廣泛實驗驗證了InfoSAM在改善SAM系列於真實世界任務中的表現，展現了其在處理特定情境中的適應性和優越性。", "audio": "audios/2505.21920v1.mp3", "timestamp": "2025-05-29T14:18:29.920781"}
{"query": "Diffusion Model", "id": "2505.22165v1", "url": "http://arxiv.org/abs/2505.22165v1", "title": "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes", "summary": "Diffusion models have emerged as a promising approach for text generation,\nwith recent works falling into two main categories: discrete and continuous\ndiffusion models. Discrete diffusion models apply token corruption\nindependently using categorical distributions, allowing for different diffusion\nprogress across tokens but lacking fine-grained control. Continuous diffusion\nmodels map tokens to continuous spaces and apply fine-grained noise, but the\ndiffusion progress is uniform across tokens, limiting their ability to capture\nsemantic nuances. To address these limitations, we propose\n\\textbf{\\underline{N}}on-simultan\\textbf{\\underline{e}}ous\nC\\textbf{\\underline{o}}ntinuous \\textbf{\\underline{Diff}}usion Models\n(NeoDiff), a novel diffusion model that integrates the strengths of both\ndiscrete and continuous approaches. NeoDiff introduces a Poisson diffusion\nprocess for the forward process, enabling a flexible and fine-grained noising\nparadigm, and employs a time predictor for the reverse process to adaptively\nmodulate the denoising progress based on token semantics. Furthermore, NeoDiff\nutilizes an optimized schedule for inference to ensure more precise noise\ncontrol and improved performance. Our approach unifies the theories of discrete\nand continuous diffusion models, offering a more principled and effective\nframework for text generation. Experimental results on several text generation\ntasks demonstrate NeoDiff's superior performance compared to baselines of\nnon-autoregressive continuous and discrete diffusion models, iterative-based\nmethods and autoregressive diffusion-based methods. These results highlight\nNeoDiff's potential as a powerful tool for generating high-quality text and\nadvancing the field of diffusion-based text generation.", "authors": ["Bocheng Li", "Zhujin Gao", "Linli Xu"], "published_date": "2025-05-28", "title_zh": "非同步擴散過程下連續與離散文本擴散之統一", "summary_zh": "擴散模型於文本生成領域嶄露頭角，現有研究分為離散與連續兩類。離散模型雖可針對不同token進行獨立擾動，缺乏細緻控制；連續模型雖可施加細粒度雜訊，擾動過程卻缺乏彈性。為克服上述局限，我們提出非同步連續擴散模型(NeoDiff)，融合離散與連續模型的優點。NeoDiff在前向過程中引入Poisson擴散，實現靈活且細緻的雜訊添加；在反向過程中採用時間預測器，根據token語義自適應調整降噪進度。此外，NeoDiff採用優化推論策略，以實現更精確的雜訊控制與效能提升。本研究統一了離散與連續擴散模型的理論基礎，為文本生成提供更具原則性且有效之框架。實驗結果顯示，在多項文本生成任務中，相較於非自迴歸連續與離散擴散模型、迭代式方法以及自迴歸擴散模型，NeoDiff展現出卓越效能。以上結果突顯NeoDiff作為生成高品質文本之強大工具，以及推進基於擴散之文本生成領域的潛力。", "audio": "audios/2505.22165v1.mp3", "timestamp": "2025-05-29T14:18:38.989274"}
{"query": "AI", "id": "2505.22525v1", "url": "http://arxiv.org/abs/2505.22525v1", "title": "Thinking with Generated Images", "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images.", "authors": ["Ethan Chern", "Zhulin Hu", "Steffi Chern", "Siqi Kou", "Jiadi Su", "Yan Ma", "Zhijie Deng", "Pengfei Liu"], "published_date": "2025-05-28", "title_zh": "利用生成圖像進行思考", "summary_zh": "本研究提出「以生成圖像思考」的新範例，藉由自發性生成中間視覺思考步驟，從根本上改變大型多模態模型(LMM)進行視覺推理的方式，使其能在文字與視覺模態間自然思考。目前LMM的視覺推理受限於處理固定圖像或僅以文字鏈式思考(CoT)推理。「以生成圖像思考」開啟認知能力的新維度，模型能主動建構中間視覺想法、評估自身視覺假設，並在推理過程中完善這些假設。我們展示了此方法在兩方面之有效性：(1)以中間視覺子目標生成圖像，將複雜視覺任務分解為可管理的組成部分，並逐步生成與整合；(2)以自我批判生成圖像，模型產生初始視覺假設，透過文字推理分析其缺點，並根據自身批判產生精確輸出。在視覺生成基準測試中，我們的模型相較於基準方法有顯著改進，在處理複雜的多物件場景時，效能提升高達50%(從38%到57%)。無論是生物化學家探索新型蛋白質結構、建築師迭代空間設計、法醫分析師重建犯罪現場，或是籃球運動員設想戰略，此方法使AI模型能進行人類創造性、分析性和戰略性思維的視覺想像與迭代精進。我們在https://github.com/GAIR-NLP/thinking-with-generated-images釋出開源套件。", "audio": "audios/2505.22525v1.mp3", "timestamp": "2025-05-29T15:19:36.585470"}
{"query": "Foundation Model", "id": "2505.21906v1", "url": "http://arxiv.org/abs/2505.21906v1", "title": "Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge", "summary": "Vision-language-action (VLA) models have emerged as the next generation of\nmodels in robotics. However, despite leveraging powerful pre-trained\nVision-Language Models (VLMs), existing end-to-end VLA systems often lose key\ncapabilities during fine-tuning as the model adapts to specific robotic tasks.\nWe argue that a generalizable VLA model should retain and expand upon the VLM's\ncore competencies: 1) Open-world embodied reasoning - the VLA should inherit\nthe knowledge from VLM, i.e., recognize anything that the VLM can recognize,\ncapable of solving math problems, possessing visual-spatial intelligence, 2)\nReasoning following - effectively translating the open-world reasoning into\nactionable steps for the robot. In this work, we introduce ChatVLA-2, a novel\nmixture-of-expert VLA model coupled with a specialized three-stage training\npipeline designed to preserve the VLM's original strengths while enabling\nactionable reasoning. To validate our approach, we design a math-matching task\nwherein a robot interprets math problems written on a whiteboard and picks\ncorresponding number cards from a table to solve equations. Remarkably, our\nmethod exhibits exceptional mathematical reasoning and OCR capabilities,\ndespite these abilities not being explicitly trained within the VLA.\nFurthermore, we demonstrate that the VLA possesses strong spatial reasoning\nskills, enabling it to interpret novel directional instructions involving\npreviously unseen objects. Overall, our method showcases reasoning and\ncomprehension abilities that significantly surpass state-of-the-art imitation\nlearning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a\nsubstantial advancement toward developing truly generalizable robotic\nfoundation models endowed with robust reasoning capacities.", "authors": ["Zhongyi Zhou", "Yichen Zhu", "Junjie Wen", "Chaomin Shen", "Yi Xu"], "published_date": "2025-05-28", "title_zh": "基於預訓練知識的開放世界具身推理視覺-語言-動作模型", "summary_zh": "視覺語言動作模型(VLA)正崛起為機器人領域的新一代模型。儘管利用了強大的預訓練視覺語言模型(VLM)，現有的端到端VLA系統在微調過程中，往往會因適應特定任務而喪失關鍵能力。我們認為，通用VLA模型應保留並擴展VLM的核心能力：1)開放世界具體推理，繼承VLM的知識，具備數學解題和視覺空間智能；2)推理遵循，將開放世界推理有效地轉化為機器人的可執行步驟。本研究提出ChatVLA-2，一種新穎的混合專家VLA模型，結合專門的三階段訓練流程，旨在保留VLM的原始優勢，同時實現可執行推理。我們設計了數學匹配任務驗證此方法，機器人解讀白板上的數學問題，並從桌面上選擇相應的數字卡來解方程式。實驗結果顯示，即使VLA未經明確訓練，也能展現出色的數學推理和光學字元辨識(OCR)能力。此外，VLA具備強大的空間推理能力，能夠理解涉及先前未見物體的新穎方向指令。總體而言，ChatVLA-2的推理和理解能力顯著超越了OpenVLA、DexVLA和pi-zero等最先進的模仿學習方法。本研究代表了在開發具有強大推理能力的通用機器人基礎模型方面的重要進展。", "audio": "audios/2505.21906v1.mp3", "timestamp": "2025-05-29T15:19:53.051699"}
{"query": "Diffusion Model", "id": "2505.22129v1", "url": "http://arxiv.org/abs/2505.22129v1", "title": "What Makes for Text to 360-degree Panorama Generation with Stable Diffusion?", "summary": "Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion,\nhas stimulated research to adapt them to 360-degree panorama generation. Prior\nwork has demonstrated the feasibility of using conventional low-rank adaptation\ntechniques on pre-trained diffusion models to generate panoramic images.\nHowever, the substantial domain gap between perspective and panoramic images\nraises questions about the underlying mechanisms enabling this empirical\nsuccess. We hypothesize and examine that the trainable counterparts exhibit\ndistinct behaviors when fine-tuned on panoramic data, and such an adaptation\nconceals some intrinsic mechanism to leverage the prior knowledge within the\npre-trained diffusion models. Our analysis reveals the following: 1) the query\nand key matrices in the attention modules are responsible for common\ninformation that can be shared between the panoramic and perspective domains,\nthus are less relevant to panorama generation; and 2) the value and output\nweight matrices specialize in adapting pre-trained knowledge to the panoramic\ndomain, playing a more critical role during fine-tuning for panorama\ngeneration. We empirically verify these insights by introducing a simple\nframework called UniPano, with the objective of establishing an elegant\nbaseline for future research. UniPano not only outperforms existing methods but\nalso significantly reduces memory usage and training time compared to prior\ndual-branch approaches, making it scalable for end-to-end panorama generation\nwith higher resolution. The code will be released.", "authors": ["Jinhong Ni", "Chang-Bin Zhang", "Qiang Zhang", "Jing Zhang"], "published_date": "2025-05-28", "title_zh": "什麼使得穩定擴散模型能生成優質的文字轉360度全景圖？", "summary_zh": "文本生成圖像擴散模型（如Stable Diffusion）的興起，激發了將其應用於360度全景圖生成的研究。先前的研究表明，在預訓練擴散模型上使用傳統的低秩適配技術生成全景圖像是可行的。然而，透視圖像和全景圖像之間存在顯著的領域差異，引發了關於此經驗成功的潛在機制的疑問。我們假設並檢驗了可訓練的組件在全景數據上進行微調時表現出不同的行為，並且這種適配隱藏了利用預訓練擴散模型中先驗知識的內在機制。我們的分析表明：1)注意力模塊中的查詢和鍵矩陣負責可以在全景和透視領域之間共享的通用信息，因此與全景生成的關聯性較低；2)值和輸出權重矩陣專門用於將預訓練知識適配到全景領域，在全景生成的微調過程中發揮更關鍵的作用。我們通過引入一個名為UniPano的簡單框架來驗證這些見解，旨在為未來的研究建立一個優雅的基準。UniPano不僅優於現有方法，而且與先前的雙分支方法相比，顯著降低了記憶體使用量和訓練時間，使其能夠以更高的分辨率進行端到端全景生成。程式碼將會公開。", "audio": "audios/2505.22129v1.mp3", "timestamp": "2025-05-29T15:20:04.592823"}
{"query": "AI", "id": "2505.22477v1", "url": "http://arxiv.org/abs/2505.22477v1", "title": "Human-Centered Human-AI Collaboration (HCHAC)", "summary": "In the intelligent era, the interaction between humans and intelligent\nsystems fundamentally involves collaboration with autonomous intelligent\nagents. Human-AI Collaboration (HAC) represents a novel type of human-machine\nrelationship facilitated by autonomous intelligent machines equipped with AI\ntechnologies. In this paradigm, AI agents serve not only as auxiliary tools but\nalso as active teammates, partnering with humans to accomplish tasks\ncollaboratively. Human-centered AI (HCAI) emphasizes that humans play critical\nleadership roles in the collaboration. This human-led collaboration imparts new\ndimensions to the human-machine relationship, necessitating innovative research\nperspectives, paradigms, and agenda to address the unique challenges posed by\nHAC. This chapter delves into the essence of HAC from the human-centered\nperspective, outlining its core concepts and distinguishing features. It\nreviews the current research methodologies and research agenda within the HAC\nfield from the HCAI perspective, highlighting advancements and ongoing studies.\nFurthermore, a framework for human-centered HAC (HCHAC) is proposed by\nintegrating these reviews and analyses. A case study of HAC in the context of\nautonomous vehicles is provided, illustrating practical applications and the\nsynergistic interactions between humans and AI agents. Finally, it identifies\npotential future research directions aimed at enhancing the effectiveness,\nreliability, and ethical integration of human-centered HAC systems in diverse\ndomains.", "authors": ["Qi Gao", "Wei Xu", "Hanxi Pan", "Mowei Shen", "Zaifeng Gao"], "published_date": "2025-05-28", "title_zh": "以人為本的人機協作", "summary_zh": "在智慧時代，人與智慧系統的互動本質上是與自主智慧體的協作。人機協作代表一種新型人機關係，由具備人工智慧的自主智慧機器促成。人工智慧不僅是輔助工具，更是積極的隊友，與人類合作完成任務。以人為本的人工智慧強調人類在協作中扮演關鍵領導角色。這種以人為主的協作賦予人機關係新的維度，需要創新的研究視角、範式與議程，以應對人機協作帶來的獨特挑戰。本章從以人為本的角度探討人機協作的本質，概述其核心概念與特徵，回顧以人為本的人工智慧視角下人機協作領域的研究方法與議程，提出人本導向的人機協作框架，並以自駕車為例探討其實際應用與人機協同互動。最後，指出未來研究方向，旨在提升人本導向人機協作系統在各領域的效能、可靠性與倫理整合。", "audio": "audios/2505.22477v1.mp3", "timestamp": "2025-05-29T16:22:57.646790"}
{"query": "Foundation Model", "id": "2505.21904v1", "url": "http://arxiv.org/abs/2505.21904v1", "title": "CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation", "summary": "Instance segmentation demands costly per-pixel annotations and large models.\nWe introduce CAST, a semi-supervised knowledge distillation (SSKD) framework\nthat compresses pretrained vision foundation models (VFM) into compact experts\nusing limited labeled and abundant unlabeled data. CAST unfolds in three\nstages: (1) domain adaptation of the VFM teacher(s) via self-training with\ncontrastive pixel calibration, (2) distillation into a compact student via a\nunified multi-objective loss that couples standard supervision and\npseudo-labels with our instance-aware pixel-wise contrastive term, and (3)\nfine-tuning on labeled data to remove residual pseudo-label bias. Central to\nCAST is an \\emph{instance-aware pixel-wise contrastive loss} that fuses mask\nand class scores to mine informative negatives and enforce clear inter-instance\nmargins. By maintaining this contrastive signal across both adaptation and\ndistillation, we align teacher and student embeddings and fully leverage\nunlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses\nits adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs.\n15.2) and outperforms state-of-the-art semi-supervised approaches.", "authors": ["Pardis Taghavi", "Tian Liu", "Renjie Li", "Reza Langari", "Zhengzhong Tu"], "published_date": "2025-05-28", "title_zh": "CAST：對比適應與提煉於半監督實例分割", "summary_zh": "實例分割需要高昂的像素級標註和大型模型。我們提出CAST，一種半監督知識蒸餾框架，利用有限標註和大量未標註數據，將預訓練視覺基礎模型壓縮為精簡專家模型。CAST分為三個階段：(1)通過對比像素校準自訓練，進行視覺基礎模型教師的領域適應；(2)通過統一的多目標損失，將知識蒸餾到精簡學生模型中，該損失將標準監督和偽標籤與我們的實例感知像素級對比項結合；(3)在標註數據上微調，以消除殘餘的偽標籤偏差。CAST的核心是實例感知像素級對比損失，它融合了掩碼和類別分數，以挖掘信息豐富的負樣本並加強實例間的邊距。通過在適應和蒸餾過程中保持這種對比信號，我們對齊了教師和學生的嵌入，並充分利用了未標註圖像。在Cityscapes和ADE20K上，我們體積小約11倍的學生模型超越了其適應後的視覺基礎模型教師，分別提升了+3.4 AP（33.9 vs. 30.5）和+1.5 AP（16.7 vs. 15.2），並優於最先進的半監督方法。", "audio": "audios/2505.21904v1.mp3", "timestamp": "2025-05-29T16:23:07.944721"}
{"query": "Diffusion Model", "id": "2505.22126v1", "url": "http://arxiv.org/abs/2505.22126v1", "title": "SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model", "summary": "Recent years have seen rapid advances in AI-driven image generation. Early\ndiffusion models emphasized perceptual quality, while newer multimodal models\nlike GPT-4o-image integrate high-level reasoning, improving semantic\nunderstanding and structural composition. Scientific illustration generation\nexemplifies this evolution: unlike general image synthesis, it demands accurate\ninterpretation of technical content and transformation of abstract ideas into\nclear, standardized visuals. This task is significantly more\nknowledge-intensive and laborious, often requiring hours of manual work and\nspecialized tools. Automating it in a controllable, intelligent manner would\nprovide substantial practical value. Yet, no benchmark currently exists to\nevaluate AI on this front. To fill this gap, we introduce SridBench, the first\nbenchmark for scientific figure generation. It comprises 1,120 instances\ncurated from leading scientific papers across 13 natural and computer science\ndisciplines, collected via human experts and MLLMs. Each sample is evaluated\nalong six dimensions, including semantic fidelity and structural accuracy.\nExperimental results reveal that even top-tier models like GPT-4o-image lag\nbehind human performance, with common issues in text/visual clarity and\nscientific correctness. These findings highlight the need for more advanced\nreasoning-driven visual generation capabilities.", "authors": ["Yifan Chang", "Yukang Feng", "Jianwen Sun", "Jiaxin Ai", "Chuanhao Li", "S. Kevin Zhou", "Kaipeng Zhang"], "published_date": "2025-05-28", "title_zh": "SridBench：圖像生成模型科學研究圖例繪製基準測試", "summary_zh": "近年來，人工智慧圖像生成技術快速發展。早期擴散模型側重於感知品質，而新型多模態模型如GPT-4o-image則整合了高階推理，提升了語義理解和結構組成。科學圖例生成是此演變的典範，它不同於一般圖像合成，需要準確地解讀技術內容，並將抽象概念轉化為清晰、標準化的視覺效果。此任務知識密集且費力，通常需要數小時的手動操作和專業工具。以可控、智慧的方式實現自動化具有顯著的實用價值。然而，目前尚無基準來評估人工智慧在此領域的表現。為填補空白，我們推出了SridBench，這是首個科學圖例生成基準。它包含13個自然科學和電腦科學領域的頂尖論文中提取的1120個範例，由人工專家和多語言大型模型共同收集。每個樣本都從語義保真度和結構準確性等六個維度進行評估。實驗結果表明，即使是GPT-4o-image等頂級模型也落後於人類表現，常見問題包括文字/視覺清晰度和科學正確性。這些發現突顯了對更先進的、由推理驅動的視覺生成能力的需求。", "audio": "audios/2505.22126v1.mp3", "timestamp": "2025-05-29T16:23:16.754352"}
{"query": "AI", "id": "2505.22467v1", "url": "http://arxiv.org/abs/2505.22467v1", "title": "Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems", "summary": "Large Language Model-based Multi-Agent Systems (MASs) have emerged as a\npowerful paradigm for tackling complex tasks through collaborative\nintelligence. Nevertheless, the question of how agents should be structurally\norganized for optimal cooperation remains largely unexplored. In this position\npaper, we aim to gently redirect the focus of the MAS research community toward\nthis critical dimension: develop topology-aware MASs for specific tasks.\nSpecifically, the system consists of three core components - agents,\ncommunication links, and communication patterns - that collectively shape its\ncoordination performance and efficiency. To this end, we introduce a\nsystematic, three-stage framework: agent selection, structure profiling, and\ntopology synthesis. Each stage would trigger new research opportunities in\nareas such as language models, reinforcement learning, graph learning, and\ngenerative modeling; together, they could unleash the full potential of MASs in\ncomplicated real-world applications. Then, we discuss the potential challenges\nand opportunities in the evaluation of multiple systems. We hope our\nperspective and framework can offer critical new insights in the era of agentic\nAI.", "authors": ["Jiaxi Yang", "Mengqi Zhang", "Yiqiao Jin", "Hao Chen", "Qingsong Wen", "Lu Lin", "Yi He", "Weijie Xu", "James Evans", "Jindong Wang"], "published_date": "2025-05-28", "title_zh": "基於大型語言模型的多代理系統應優先研究拓撲結構學習", "summary_zh": "基於大型語言模型的多代理系統已成為解決複雜任務的強大範例。然而，代理應如何組織以實現最佳協作仍待探索。本文旨在將研究重點轉向此關鍵面向：開發針對特定任務的拓撲感知多代理系統。該系統包含代理、通訊連結和通訊模式三個核心組件，共同影響協調效能和效率。為此，我們提出一個系統性的三階段框架：代理選擇、結構剖析和拓撲合成。每個階段都將引發語言模型、強化學習、圖學習和生成模型等領域的新研究機會；它們共同釋放多代理系統在複雜現實應用中的潛力。我們進一步討論多系統評估中的潛在挑戰與機會。期望我們的觀點和框架能在代理人工智慧時代提供關鍵的新見解。", "audio": "audios/2505.22467v1.mp3", "timestamp": "2025-05-29T17:17:00.521134"}
{"query": "Foundation Model", "id": "2505.21857v1", "url": "http://arxiv.org/abs/2505.21857v1", "title": "Revisiting Bayesian Model Averaging in the Era of Foundation Models", "summary": "We revisit the classical, full-fledged Bayesian model averaging (BMA)\nparadigm to ensemble pre-trained and/or lightly-finetuned foundation models to\nenhance the classification performance on image and text data. To make BMA\ntractable under foundation models, we introduce trainable linear classifiers\nthat take frozen features from the pre-trained foundation models as inputs. The\nmodel posteriors over the linear classifiers tell us which linear heads and\nfrozen features are better suited for a given dataset, resulting in a\nprincipled model ensembling method. Furthermore, we propose a computationally\ncheaper, optimizable model averaging scheme (OMA). In OMA, we directly optimize\nthe model ensemble weights, just like those weights based on model posterior\ndistributions in BMA, by reducing the amount of surprise (expected entropy of\nthe predictions) we get from predictions of ensembled models. With the rapid\ndevelopment of foundation models, these approaches will enable the\nincorporation of future, possibly significantly better foundation models to\nenhance the performance of challenging classification tasks.", "authors": ["Mijung Park"], "published_date": "2025-05-28", "title_zh": "基礎模型時代下貝氏模型平均法的再探", "summary_zh": "本文重新探討傳統的完整貝氏模型平均法(BMA)，以整合預訓練和/或輕微微調的基礎模型，從而提升圖像和文本數據的分類效能。為了使BMA在基礎模型下易於處理，我們引入可訓練的線性分類器，這些分類器將來自預訓練基礎模型的凍結特徵作為輸入。線性分類器的模型後驗概率可得知哪些線性頭和凍結特徵更適合給定的數據集，從而產生一種有原則的模型集成方法。此外，我們提出一種計算成本更低、可優化的模型平均方案(OMA)。在OMA中，我們透過減少模型集成預測中的不確定性(預測的期望熵)，直接優化模型集成權重，如同BMA中基於模型後驗分佈的權重。隨著基礎模型的快速發展，這些方法將有助於整合未來可能顯著更優越的基礎模型，進而提升具挑戰性分類任務的效能。", "audio": "audios/2505.21857v1.mp3", "timestamp": "2025-05-29T17:17:11.645363"}
{"query": "Diffusion Model", "id": "2505.22111v1", "url": "http://arxiv.org/abs/2505.22111v1", "title": "Autoregression-free video prediction using diffusion model for mitigating error propagation", "summary": "Existing long-term video prediction methods often rely on an autoregressive\nvideo prediction mechanism. However, this approach suffers from error\npropagation, particularly in distant future frames. To address this limitation,\nthis paper proposes the first AutoRegression-Free (ARFree) video prediction\nframework using diffusion models. Different from an autoregressive video\nprediction mechanism, ARFree directly predicts any future frame tuples from the\ncontext frame tuple. The proposed ARFree consists of two key components: 1) a\nmotion prediction module that predicts a future motion using motion feature\nextracted from the context frame tuple; 2) a training method that improves\nmotion continuity and contextual consistency between adjacent future frame\ntuples. Our experiments with two benchmark datasets show that the proposed\nARFree video prediction framework outperforms several state-of-the-art video\nprediction methods.", "authors": ["Woonho Ko", "Jin Bok Park", "Il Yong Chun"], "published_date": "2025-05-28", "title_zh": "利用擴散模型實現無自迴歸的影片預測以緩解誤差傳播", "summary_zh": "現有長時影片預測方法常依賴自迴歸機制，但此方法易產生誤差傳播，尤其在預測遠期影格時。為解決此限制，本研究提出首個基於擴散模型的非自迴歸影片預測框架(ARFree)。ARFree不同於自迴歸機制，可直接從上下文影格組預測任何未來影格組。此框架包含：1)運動預測模組，利用從上下文影格組提取的運動特徵預測未來運動；2)訓練方法，提升相鄰未來影格組之間的運動連續性和上下文一致性。實驗結果顯示，所提出的ARFree影片預測框架優於多種先進方法。", "audio": "audios/2505.22111v1.mp3", "timestamp": "2025-05-29T17:17:19.173071"}
{"query": "AI", "id": "2505.22451v1", "url": "http://arxiv.org/abs/2505.22451v1", "title": "AI Mathematician: Towards Fully Automated Frontier Mathematical Research", "summary": "Large Reasoning Models (LRMs) have made significant progress in mathematical\ncapabilities in recent times. However, these successes have been primarily\nconfined to competition-level problems. In this work, we propose AI\nMathematician (AIM) framework, which harnesses the reasoning strength of LRMs\nto support frontier mathematical research. We have identified two critical\nchallenges of mathematical research compared to competition, {\\it the intrinsic\ncomplexity of research problems} and {\\it the requirement of procedural rigor}.\nTo address these challenges, AIM incorporates two core strategies: an\nexploration mechanism to foster longer solution paths, and the pessimistic\nreasonable verification method to ensure reliability.\n  This early version of AIM already exhibits strong capability in tackling\nresearch-level tasks. We conducted extensive experiments across several\nreal-world mathematical topics and obtained promising results. AIM is able to\nautonomously construct substantial portions of proofs and uncover non-trivial\ninsights within each research area. These findings highlight the potential of\nLRMs in mathematical discovery and suggest that LRM-based agent systems could\nsignificantly accelerate mathematical research in the future.", "authors": ["Yuanhang Liu", "Yanxing Huang", "Yanqiao Wang", "Peng Li", "Yang Liu"], "published_date": "2025-05-28", "title_zh": "人工智慧數學家：邁向全自動前沿數學研究", "summary_zh": "大型推理模型(LRM)近年在數學能力上取得顯著進展，但主要集中於競賽級問題。本文提出AI數學家(AIM)框架，利用LRM的推理能力來支持前沿數學研究。相較於競賽，數學研究存在研究問題的內在複雜性與程序嚴謹性的要求兩個關鍵挑戰。為了解決這些問題，AIM採用兩個核心策略：促進更長解題路徑的探索機制，以及確保可靠性的悲觀合理驗證方法。早期版本的AIM已展現出處理研究級任務的強大能力。我們在多個真實數學領域進行了廣泛實驗，並取得了有希望的成果。AIM能夠自主構建大部分證明，並在每個研究領域中發現重要的見解。這些發現突顯了LRM在數學發現方面的潛力，並表明基於LRM的代理系統可能顯著加速未來的數學研究。", "audio": "audios/2505.22451v1.mp3", "timestamp": "2025-05-29T18:26:00.466912"}
{"query": "Foundation Model", "id": "2505.21835v1", "url": "http://arxiv.org/abs/2505.21835v1", "title": "TuneComp: Joint Fine-tuning and Compression for Large Foundation Models", "summary": "To reduce model size during post-training, compression methods, including\nknowledge distillation, low-rank approximation, and pruning, are often applied\nafter fine-tuning the model. However, sequential fine-tuning and compression\nsacrifices performance, while creating a larger than necessary model as an\nintermediate step. In this work, we aim to reduce this gap, by directly\nconstructing a smaller model while guided by the downstream task. We propose to\njointly fine-tune and compress the model by gradually distilling it to a pruned\nlow-rank structure. Experiments demonstrate that joint fine-tuning and\ncompression significantly outperforms other sequential compression methods.", "authors": ["Xiangyu Chen", "Jing Liu", "Ye Wang", "Matthew Brand", "Pu", "Wang", "Toshiaki Koike-Akino"], "published_date": "2025-05-27", "title_zh": "TuneComp：大型基礎模型的聯合微調與壓縮", "summary_zh": "為縮減模型尺寸，後訓練壓縮方法（如知識蒸餾、低秩近似和剪枝）常於微調後應用。然而，循序微調與壓縮會犧牲效能，並產生不必要的大型中介模型。本研究旨在縮小此差距，於下游任務引導下直接構建較小模型。我們提出聯合微調與壓縮，將模型逐步蒸餾至剪枝後的低秩結構。實驗表明，相較於其他循序壓縮方法，聯合微調與壓縮表現顯著更優。", "audio": "audios/2505.21835v1.mp3", "timestamp": "2025-05-29T18:26:06.905182"}
{"query": "Diffusion Model", "id": "2505.22106v1", "url": "http://arxiv.org/abs/2505.22106v1", "title": "AudioTurbo: Fast Text-to-Audio Generation with Rectified Diffusion", "summary": "Diffusion models have significantly improved the quality and diversity of\naudio generation but are hindered by slow inference speed. Rectified flow\nenhances inference speed by learning straight-line ordinary differential\nequation (ODE) paths. However, this approach requires training a flow-matching\nmodel from scratch and tends to perform suboptimally, or even poorly, at low\nstep counts. To address the limitations of rectified flow while leveraging the\nadvantages of advanced pre-trained diffusion models, this study integrates\npre-trained models with the rectified diffusion method to improve the\nefficiency of text-to-audio (TTA) generation. Specifically, we propose\nAudioTurbo, which learns first-order ODE paths from deterministic noise sample\npairs generated by a pre-trained TTA model. Experiments on the AudioCaps\ndataset demonstrate that our model, with only 10 sampling steps, outperforms\nprior models and reduces inference to 3 steps compared to a flow-matching-based\nacceleration model.", "authors": ["Junqi Zhao", "Jinzheng Zhao", "Haohe Liu", "Yun Chen", "Lu Han", "Xubo Liu", "Mark Plumbley", "Wenwu Wang"], "published_date": "2025-05-28", "title_zh": "音訊渦輪：基於修正擴散的快速文本轉音訊生成", "summary_zh": "擴散模型顯著提升了音訊生成品質和多樣性，但推論速度較慢。修正流通過學習直線常微分方程路徑來加速推論。然而，此方法需從頭訓練流匹配模型，且在低步數下表現欠佳。為了解決修正流的限制並利用預訓練擴散模型的優勢，本研究整合預訓練模型與修正擴散方法，以提高文本到音訊生成的效率。具體而言，我們提出AudioTurbo，它從預訓練文本到音訊模型產生的確定性噪聲樣本對中學習一階常微分方程路徑。在AudioCaps數據集上的實驗表明，我們的模型僅需10個採樣步驟，即優於先前的模型，並且與基於流匹配的加速模型相比，推論可減少至3個步驟。", "audio": "audios/2505.22106v1.mp3", "timestamp": "2025-05-29T18:26:15.915243"}
{"query": "AI", "id": "2505.22450v1", "url": "http://arxiv.org/abs/2505.22450v1", "title": "Position: All Current Generative Fidelity and Diversity Metrics are Flawed", "summary": "Any method's development and practical application is limited by our ability\nto measure its reliability. The popularity of generative modeling emphasizes\nthe importance of good synthetic data metrics. Unfortunately, previous works\nhave found many failure cases in current metrics, for example lack of outlier\nrobustness and unclear lower and upper bounds. We propose a list of desiderata\nfor synthetic data metrics, and a suite of sanity checks: carefully chosen\nsimple experiments that aim to detect specific and known generative modeling\nfailure modes. Based on these desiderata and the results of our checks, we\narrive at our position: all current generative fidelity and diversity metrics\nare flawed. This significantly hinders practical use of synthetic data. Our aim\nis to convince the research community to spend more effort in developing\nmetrics, instead of models. Additionally, through analyzing how current metrics\nfail, we provide practitioners with guidelines on how these metrics should\n(not) be used.", "authors": ["Ossi Räisä", "Boris van Breugel", "Mihaela van der Schaar"], "published_date": "2025-05-28", "title_zh": "立場：所有當前生成模型的保真度和多樣性指標均存在缺陷", "summary_zh": "任何方法的發展與應用受限於其可靠性的衡量能力。生成模型日益普及，更突顯優良合成數據指標的重要性。然而，既有指標存在諸多缺陷，如缺乏離群值穩健性和不明確的上下界。本文提出合成數據指標的理想特性，並設計一系列健全性檢驗，即針對特定生成模型失效模式的精心實驗。基於這些特性與檢驗結果，我們認為當前所有生成保真度和多樣性指標均存在缺陷，嚴重阻礙合成數據的實際應用。我們旨在促使研究社群投入更多精力於指標開發，而非模型本身。此外，透過分析現有指標的不足，我們為實踐者提供使用指導。", "audio": "audios/2505.22450v1.mp3", "timestamp": "2025-05-29T19:13:28.291241"}
{"query": "Foundation Model", "id": "2505.21801v1", "url": "http://arxiv.org/abs/2505.21801v1", "title": "Query, Don't Train: Privacy-Preserving Tabular Prediction from EHR Data via SQL Queries", "summary": "Electronic health records (EHRs) contain richly structured, longitudinal data\nessential for predictive modeling, yet stringent privacy regulations (e.g.,\nHIPAA, GDPR) often restrict access to individual-level records. We introduce\nQuery, Don't Train (QDT): a structured-data foundation-model interface enabling\ntabular inference via LLM-generated SQL over EHRs. Instead of training on or\naccessing individual-level examples, QDT uses a large language model (LLM) as a\nschema-aware query planner to generate privacy-compliant SQL queries from a\nnatural language task description and a test-time input. The model then\nextracts summary-level population statistics through these SQL queries and the\nLLM performs, chain-of-thought reasoning over the results to make predictions.\nThis inference-time-only approach (1) eliminates the need for supervised model\ntraining or direct data access, (2) ensures interpretability through symbolic,\nauditable queries, (3) naturally handles missing features without imputation or\npreprocessing, and (4) effectively manages high-dimensional numerical data to\nenhance analytical capabilities. We validate QDT on the task of 30-day hospital\nreadmission prediction for Type 2 diabetes patients using a MIMIC-style EHR\ncohort, achieving F1 = 0.70, which outperforms TabPFN (F1 = 0.68). To our\nknowledge, this is the first demonstration of LLM-driven, privacy-preserving\nstructured prediction using only schema metadata and aggregate statistics -\noffering a scalable, interpretable, and regulation-compliant alternative to\nconventional foundation-model pipelines.", "authors": ["Josefa Lia Stoisser", "Marc Boubnovski Martell", "Kaspar Märtens", "Lawrence Phillips", "Stephen Michael Town", "Rory Donovan-Maiye", "Julien Fauqueur"], "published_date": "2025-05-27", "title_zh": "查詢，而非訓練：基於SQL查詢的EHR數據隱私保護表格預測", "summary_zh": "電子病歷(EHRs)包含豐富的結構化縱向資料，對於預測建模至關重要，但嚴格的隱私法規（例如HIPAA、GDPR）限制了對個體資料的訪問。我們提出「查詢，而非訓練」(QDT)：一種結構化資料基礎模型介面，透過大型語言模型(LLM)產生的SQL在EHRs上進行表格推論。QDT不基於個體資料進行訓練或訪問，而是使用LLM作為模式感知查詢規劃器，從自然語言任務描述和測試時輸入生成符合隱私規定的SQL查詢。該模型透過這些SQL查詢提取彙總層級的群體統計數據，並利用LLM對結果進行鏈式思考推理以做出預測。此種僅在推論階段進行的方法，(1)無需監督模型訓練或直接資料訪問，(2)透過符號化的可審計查詢確保可解釋性，(3)自然處理缺失特徵而無需插補或預處理，(4)有效管理高維數值資料以增強分析能力。我們在MIMIC風格的EHR數據集上驗證QDT在2型糖尿病患者30天住院再入院預測任務上的表現，達到F1=0.70，優於TabPFN(F1=0.68)。據我們所知，這是首個僅使用模式元數據和彙總統計數據，透過LLM驅動的隱私保護結構化預測示例，為傳統基礎模型流程提供了一種可擴展、可解釋且符合法規的替代方案。", "audio": "audios/2505.21801v1.mp3", "timestamp": "2025-05-29T19:13:38.378752"}
{"query": "Diffusion Model", "id": "2505.22090v1", "url": "http://arxiv.org/abs/2505.22090v1", "title": "High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models", "summary": "Three-dimensional ultrasound enables real-time volumetric visualization of\nanatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces the\nreliance on precise probe orientation, potentially making ultrasound more\naccessible to clinicians with varying levels of experience and improving\nautomated measurements and post-exam analysis. However, achieving both high\nvolume rates and high image quality remains a significant challenge. While 3D\ndiverging waves can provide high volume rates, they suffer from limited tissue\nharmonic generation and increased multipath effects, which degrade image\nquality. One compromise is to retain the focusing in elevation while leveraging\nunfocused diverging waves in the lateral direction to reduce the number of\ntransmissions per elevation plane. Reaching the volume rates achieved by full\n3D diverging waves, however, requires dramatically undersampling the number of\nelevation planes. Subsequently, to render the full volume, simple interpolation\ntechniques are applied. This paper introduces a novel approach to 3D ultrasound\nreconstruction from a reduced set of elevation planes by employing diffusion\nmodels (DMs) to achieve increased spatial and temporal resolution. We compare\nboth traditional and supervised deep learning-based interpolation methods on a\n3D cardiac ultrasound dataset. Our results show that DM-based reconstruction\nconsistently outperforms the baselines in image quality and downstream task\nperformance. Additionally, we accelerate inference by leveraging the temporal\nconsistency inherent to ultrasound sequences. Finally, we explore the\nrobustness of the proposed method by exploiting the probabilistic nature of\ndiffusion posterior sampling to quantify reconstruction uncertainty and\ndemonstrate improved recall on out-of-distribution data with synthetic\nanomalies under strong subsampling.", "authors": ["Tristan S. W. Stevens", "Oisín Nolan", "Oudom Somphone", "Jean-Luc Robert", "Ruud J. G. van Sloun"], "published_date": "2025-05-28", "title_zh": "基於擴散模型的高容量速率三維超聲重建", "summary_zh": "三維超聲能即時呈現解剖結構的體積影像。相較於傳統二維超聲，三維成像降低了對探頭方向的精確要求，有潛力讓不同經驗的臨床醫生更容易使用超聲，並改善自動測量和檢查後分析。然而，兼顧高體積速率和高圖像品質仍是一大挑戰。本文提出一種新的三維超聲重建方法，透過擴散模型 (DMs) 從減少的仰角平面集合中重建，以提高空間和時間解析度。我們比較了傳統和基於監督式深度學習的插值方法，並在三維心臟超聲數據集上進行了驗證。結果表明，基於擴散模型的重建在圖像品質和後續任務效能方面始終優於基準方法。此外，我們利用超聲序列固有的時間一致性來加速推論。最後，我們利用擴散後驗抽樣的概率性質來量化重建不確定性，並展示了在強烈次採樣下，使用合成異常值處理分布外數據時，回召率有所提高，以此探索所提出方法的穩健性。", "audio": "audios/2505.22090v1.mp3", "timestamp": "2025-05-29T19:13:45.645931"}
{"query": "AI", "id": "2505.22443v1", "url": "http://arxiv.org/abs/2505.22443v1", "title": "Frequency Resource Management in 6G User-Centric CFmMIMO: A Hybrid Reinforcement Learning and Metaheuristic Approach", "summary": "As sixth-generation (6G) networks continue to evolve, AI-driven solutions are\nplaying a crucial role in enabling more efficient and adaptive resource\nmanagement in wireless communication. One of the key innovations in 6G is\nuser-centric cell-free massive Multiple-Input Multiple-Output (UC-CFmMIMO), a\nparadigm that eliminates traditional cell boundaries and enhances network\nperformance by dynamically assigning access points (APs) to users. This\napproach is particularly well-suited for vehicular networks, offering seamless,\nhomogeneous, ultra-reliable, and low-latency connectivity. However, in dense\nnetworks, a key challenge lies in efficiently allocating frequency resources\nwithin a limited shared subband spectrum while accounting for frequency\nselectivity and the dependency of signal propagation on bandwidth. These\nfactors make resource allocation increasingly complex, especially in dynamic\nenvironments where maintaining Quality of Service (QoS) is critical. This paper\ntackles these challenges by proposing a hybrid multi-user allocation strategy\nthat integrates reinforcement learning (RL) and metaheuristic optimization to\nenhance spectral efficiency (SE), ensure fairness, and mitigate interference\nwithin shared subbands. To assess its effectiveness, we compare this hybrid\napproach with two other methods: the bio-inspired Aquila Optimizer (AO) and\nDeep Deterministic Policy Gradient (DDPG)-based Actor-Critic Reinforcement\nLearning (AC-RL). Our evaluation is grounded in real-world patterns and channel\ncharacteristics, utilizing the 3GPP-3D channel modeling framework (QuaDRiGa) to\ncapture realistic propagation conditions. The results demonstrate that the\nproposed hybrid strategy achieves a superior balance among competing\nobjectives, underscoring the role of AI-driven resource allocation in advancing\nUC-CFmMIMO systems for next-generation wireless networks.", "authors": ["Selina Cheggour", "Valeria Loscri"], "published_date": "2025-05-28", "title_zh": "6G以用戶為中心CFmMIMO中的頻率資源管理：一種混合強化學習與元啟發式方法", "summary_zh": "隨著第六代(6G)網路發展，AI驅動解決方案在無線通訊資源管理中扮演關鍵角色。使用者中心無蜂巢式大規模MIMO (UC-CFmMIMO)透過動態分配接取點(AP)，消除傳統蜂巢邊界並提升網路效能，尤其適用於車聯網，提供無縫、均質、超可靠且低延遲的連線。然而，在密集網路中，如何在有限共享頻段內有效分配頻率資源，同時考量頻率選擇性和訊號傳播對頻寬的依賴，是重要挑戰。本文提出一種混合式多使用者分配策略，整合強化學習(RL)和元啟發式優化，以提高頻譜效率(SE)、確保公平性並減輕共享頻段內的干擾。透過基於真實場景和通道特性的3GPP-3D通道建模框架(QuaDRiGa)，將此混合方法與Aquila Optimizer (AO)及基於深度確定性策略梯度(DDPG)的Actor-Critic強化學習(AC-RL)進行比較。結果顯示，所提出的混合策略在相互競爭的目標之間取得了更好的平衡，突顯了AI驅動資源分配在推進UC-CFmMIMO系統於下一代無線網路中的作用。", "audio": "audios/2505.22443v1.mp3", "timestamp": "2025-05-29T20:20:54.500754"}
{"query": "Foundation Model", "id": "2505.21732v1", "url": "http://arxiv.org/abs/2505.21732v1", "title": "LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing", "summary": "Training foundation models such as ViTs and LLMs requires tremendous\ncomputing cost. Low-rank matrix or tensor factorization offers a\nparameter-efficient alternative, but often downgrades performance due to the\nrestricted parameter space. In this work, we introduce {\\textbf{Latent Crossing\n(LaX)}} -- a simple yet effective plug-and-play module that enhances the\ncapacity of low-rank models by enabling information flow across low-rank\nsubspaces. We extensively validate the benefits of LaX on pre-training tasks\nwith ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters.\nLaX boosts low-rank model performance to match or exceed the full-rank\nbaselines while using 2-3\\(\\times\\) fewer parameters. When equipped with\nlow-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently\nimproves performance on arithmetic and common sense reasoning tasks with\nnegligible cost.", "authors": ["Ruijie Zhang", "Ziyue Liu", "Zhengyang Wang", "Zheng Zhang"], "published_date": "2025-05-27", "title_zh": "LaX：基於潛在交叉提升基礎模型的低秩訓練", "summary_zh": "訓練如ViT和LLM等基礎模型需耗費龐大算力。低秩矩陣或張量分解提供了一種參數高效的替代方案，但常因受限的參數空間而降低效能。本研究提出潛在交叉(LaX)，一個簡潔有效的隨插即用模組，藉由促進低秩子空間間的資訊流來提升低秩模型的能力。在ViT-Base/Large和60M至1B參數的LLaMA類模型上的預訓練任務中，LaX顯著提升了低秩模型的效能，使其達到或超越完整秩基準線，同時減少了2-3倍的參數。當用於微調LLaMA-7/13B的低秩適配器(LoRA)時，LaX在算術和常識推理任務上持續提升效能，且成本可忽略不計。", "audio": "audios/2505.21732v1.mp3", "timestamp": "2025-05-29T20:21:03.466752"}
{"query": "Diffusion Model", "id": "2505.22008v1", "url": "http://arxiv.org/abs/2505.22008v1", "title": "Align-DA: Align Score-based Atmospheric Data Assimilation with Multiple Preferences", "summary": "Data assimilation (DA) aims to estimate the full state of a dynamical system\nby combining partial and noisy observations with a prior model forecast,\ncommonly referred to as the background. In atmospheric applications, this\nproblem is fundamentally ill-posed due to the sparsity of observations relative\nto the high-dimensional state space. Traditional methods address this challenge\nby simplifying background priors to regularize the solution, which are\nempirical and require continual tuning for application. Inspired by alignment\ntechniques in text-to-image diffusion models, we propose Align-DA, which\nformulates DA as a generative process and uses reward signals to guide\nbackground priors, replacing manual tuning with data-driven alignment.\nSpecifically, we train a score-based model in the latent space to approximate\nthe background-conditioned prior, and align it using three complementary reward\nsignals for DA: (1) assimilation accuracy, (2) forecast skill initialized from\nthe assimilated state, and (3) physical adherence of the analysis fields.\nExperiments with multiple reward signals demonstrate consistent improvements in\nanalysis quality across different evaluation metrics and observation-guidance\nstrategies. These results show that preference alignment, implemented as a soft\nconstraint, can automatically adapt complex background priors tailored to DA,\noffering a promising new direction for advancing the field.", "authors": ["Jing-An Sun", "Hang Fan", "Junchao Gong", "Ben Fei", "Kun Chen", "Fenghua Ling", "Wenlong Zhang", "Wanghan Xu", "Li Yan", "Pierre Gentine", "Lei Bai"], "published_date": "2025-05-28", "title_zh": "Align-DA：基於分數之大氣資料同化與多重偏好對齊", "summary_zh": "資料同化旨在結合局部雜訊觀測與先驗模型預測（背景），估計動態系統的完整狀態。在大氣應用中，由於觀測稀疏，問題本質上是不適定的。傳統方法透過簡化背景先驗以正則化解，但具有經驗性且需持續調整。受文圖擴散模型對齊技術啟發，我們提出Align-DA，將資料同化構建為生成過程，並使用獎勵訊號引導背景先驗，以數據驅動對齊取代手動調整。具體而言，我們在潛在空間訓練基於分數的模型，以近似背景條件先驗，並使用三個互補的資料同化獎勵訊號對齊它：（1）同化準確性；（2）自同化狀態初始化的預報技巧；（3）分析場的物理一致性。多重獎勵訊號的實驗證明，在不同評估指標和觀測引導策略下，分析品質均有持續提升。結果表明，作為軟約束實現的偏好對齊，可以自動調整針對資料同化量身定制的複雜背景先驗，為推動該領域發展提供有希望的新方向。", "audio": "audios/2505.22008v1.mp3", "timestamp": "2025-05-29T20:21:21.039814"}
{"query": "AI", "id": "2505.22440v1", "url": "http://arxiv.org/abs/2505.22440v1", "title": "Data-Driven Antenna Miniaturization: A Knowledge-Based System Integrating Quantum PSO and Predictive Machine Learning Models", "summary": "The rapid evolution of wireless technologies necessitates automated design\nframeworks to address antenna miniaturization and performance optimization\nwithin constrained development cycles. This study demonstrates a machine\nlearning enhanced workflow integrating Quantum-Behaved Dynamic Particle Swarm\nOptimization (QDPSO) with ANSYS HFSS simulations to accelerate antenna design.\nThe QDPSO algorithm autonomously optimized loop dimensions in 11.53 seconds,\nachieving a resonance frequency of 1.4208 GHz a 12.7 percent reduction compared\nto conventional 1.60 GHz designs. Machine learning models (SVM, Random Forest,\nXGBoost, and Stacked ensembles) predicted resonance frequencies in 0.75 seconds\nusing 936 simulation datasets, with stacked models showing superior training\naccuracy (R2=0.9825) and SVM demonstrating optimal validation performance\n(R2=0.7197). The complete design cycle, encompassing optimization, prediction,\nand ANSYS validation, required 12.42 minutes on standard desktop hardware\n(Intel i5-8500, 16GB RAM), contrasting sharply with the 50-hour benchmark of\nPSADEA-based approaches. This 240 times of acceleration eliminates traditional\ntrial-and-error methods that often extend beyond seven expert-led days. The\nsystem enables precise specifications of performance targets with automated\ngeneration of fabrication-ready parameters, particularly benefiting compact\nconsumer devices requiring rapid frequency tuning. By bridging AI-driven\noptimization with CAD validation, this framework reduces engineering workloads\nwhile ensuring production-ready designs, establishing a scalable paradigm for\nnext-generation RF systems in 6G and IoT applications.", "authors": ["Khan Masood Parvez", "Sk Md Abidar Rahaman", "Ali Shiri Sichani"], "published_date": "2025-05-28", "title_zh": "數據驅動的天線小型化：整合量子粒子群優化與預測性機器學習模型的知識庫系統", "summary_zh": "無線技術快速發展，促使自動化設計框架用於天線小型化和性能優化。本研究展示結合量子行為動態粒子群優化(QDPSO)與ANSYS HFSS模擬的機器學習增強工作流程，加速天線設計。QDPSO算法於11.53秒內自主優化環路尺寸，實現1.4208 GHz的諧振頻率，較傳統1.60 GHz設計降低12.7%。機器學習模型在0.75秒內預測諧振頻率，堆疊模型展現優異訓練準確度(R2=0.9825)，SVM模型驗證性能最佳(R2=0.7197)。完整設計週期在標準硬體上耗時12.42分鐘，相較於基於PSADEA方法的50小時基準，加速240倍。此系統實現精確性能目標規範，自動生成可直接用於製造的參數，尤其適用於需要快速頻率調諧的消費性電子產品，並為6G和物聯網應用建立可擴展範例。", "audio": "audios/2505.22440v1.mp3", "timestamp": "2025-05-29T21:17:43.036046"}
{"query": "Foundation Model", "id": "2505.21698v1", "url": "http://arxiv.org/abs/2505.21698v1", "title": "MedBridge: Bridging Foundation Vision-Language Models to Medical Image Diagnosis", "summary": "Recent vision-language foundation models deliver state-of-the-art results on\nnatural image classification but falter on medical images due to pronounced\ndomain shifts. At the same time, training a medical foundation model requires\nsubstantial resources, including extensive annotated data and high\ncomputational capacity. To bridge this gap with minimal overhead, we introduce\nMedBridge, a lightweight multimodal adaptation framework that re-purposes\npretrained VLMs for accurate medical image diagnosis. MedBridge comprises three\nkey components. First, a Focal Sampling module that extracts high-resolution\nlocal regions to capture subtle pathological features and compensate for the\nlimited input resolution of general-purpose VLMs. Second, a Query Encoder\n(QEncoder) injects a small set of learnable queries that attend to the frozen\nfeature maps of VLM, aligning them with medical semantics without retraining\nthe entire backbone. Third, a Mixture of Experts mechanism, driven by learnable\nqueries, harnesses the complementary strength of diverse VLMs to maximize\ndiagnostic performance. We evaluate MedBridge on five medical imaging\nbenchmarks across three key adaptation tasks, demonstrating its superior\nperformance in both cross-domain and in-domain adaptation settings, even under\nvarying levels of training data availability. Notably, MedBridge achieved over\n6-15% improvement in AUC compared to state-of-the-art VLM adaptation methods in\nmulti-label thoracic disease diagnosis, underscoring its effectiveness in\nleveraging foundation models for accurate and data-efficient medical diagnosis.\nOur code is available at https://github.com/ai-med/MedBridge.", "authors": ["Yitong Li", "Morteza Ghahremani", "Christian Wachinger"], "published_date": "2025-05-27", "title_zh": "MedBridge：連接基礎視覺語言模型至醫學影像診斷", "summary_zh": "近年視覺語言基礎模型在自然圖像分類表現出色，但因領域差異在醫學影像上表現不佳。訓練醫學基礎模型需要大量資源。為克服此問題，我們提出 MedBridge，一種輕量級多模態適應框架，將預訓練的 VLMs 重新用於精確的醫學影像診斷。MedBridge 包含三個關鍵組件：焦點採樣模組，提取高解析度局部區域以捕捉病理特徵；查詢編碼器 (QEncoder)，注入可學習查詢以關注 VLMs 的凍結特徵圖，使其與醫學語義對齊；混合專家機制，利用可學習查詢驅動，結合不同 VLMs 的優勢以最大化診斷性能。在五個醫學影像基準上進行評估，MedBridge 在跨領域和領域內適應設置中表現優異，即使在不同訓練數據量下也是如此。在多標籤胸腔疾病診斷中，MedBridge 的 AUC 較其他 VLM 適應方法提高了 6-15%，證明其有效利用基礎模型進行精確且數據高效的醫學診斷。程式碼位於 https://github.com/ai-med/MedBridge。", "audio": "audios/2505.21698v1.mp3", "timestamp": "2025-05-29T21:17:57.025746"}
{"query": "Diffusion Model", "id": "2505.22002v1", "url": "http://arxiv.org/abs/2505.22002v1", "title": "D-Fusion: Direct Preference Optimization for Aligning Diffusion Models with Visually Consistent Samples", "summary": "The practical applications of diffusion models have been limited by the\nmisalignment between generated images and corresponding text prompts. Recent\nstudies have introduced direct preference optimization (DPO) to enhance the\nalignment of these models. However, the effectiveness of DPO is constrained by\nthe issue of visual inconsistency, where the significant visual disparity\nbetween well-aligned and poorly-aligned images prevents diffusion models from\nidentifying which factors contribute positively to alignment during\nfine-tuning. To address this issue, this paper introduces D-Fusion, a method to\nconstruct DPO-trainable visually consistent samples. On one hand, by performing\nmask-guided self-attention fusion, the resulting images are not only\nwell-aligned, but also visually consistent with given poorly-aligned images. On\nthe other hand, D-Fusion can retain the denoising trajectories of the resulting\nimages, which are essential for DPO training. Extensive experiments demonstrate\nthe effectiveness of D-Fusion in improving prompt-image alignment when applied\nto different reinforcement learning algorithms.", "authors": ["Zijing Hu", "Fengda Zhang", "Kun Kuang"], "published_date": "2025-05-28", "title_zh": "D-Fusion：使用直接偏好最佳化對齊擴散模型與視覺一致樣本", "summary_zh": "擴散模型的實際應用受限於生成圖像與文本提示詞的不一致。直接偏好優化(DPO)雖可提升一致性，但視覺不一致問題限制了其效能。此問題源於對齊良好和對齊不良圖像間的顯著視覺差異，阻礙擴散模型在微調過程中識別促進對齊的因素。為了解決此問題，本研究提出D-Fusion，一種構建DPO可訓練且視覺一致樣本的方法。透過遮罩引導的自注意力融合，產生的圖像不僅對齊良好，且與給定的對齊不良圖像在視覺上保持一致。此外，D-Fusion保留了生成圖像的去噪軌跡，這對DPO訓練至關重要。大量實驗證明，D-Fusion在應用於不同強化學習算法時，能有效改善提示詞與圖像的對齊程度。", "audio": "audios/2505.22002v1.mp3", "timestamp": "2025-05-29T21:18:07.699967"}
{"query": "AI", "id": "2505.22418v1", "url": "http://arxiv.org/abs/2505.22418v1", "title": "AI Trust Reshaping Administrative Burdens: Understanding Trust-Burden Dynamics in LLM-Assisted Benefits Systems", "summary": "Supplemental Nutrition Assistance Program (SNAP) is an essential benefit\nsupport system provided by the US administration to 41 million federally\ndetermined low-income applicants. Through interviews with such applicants\nacross a diverse set of experiences with the SNAP system, our findings reveal\nthat new AI technologies like LLMs can alleviate traditional burdens but also\nintroduce new burdens. We introduce new types of learning, compliance, and\npsychological costs that transform the administrative burden on applicants. We\nalso identify how trust in AI across three dimensions--competence, integrity,\nand benevolence--is perceived to reduce administrative burdens, which may stem\nfrom unintended and untoward overt trust in the system. We discuss calibrating\nappropriate levels of user trust in LLM-based administrative systems,\nmitigating newly introduced burdens. In particular, our findings suggest that\nevidence-based information disclosure is necessary in benefits administration\nand propose directions for future research on trust-burden dynamics in\nAI-assisted administration systems.", "authors": ["Jeongwon Jo", "He Zhang", "Jie Cai", "Nitesh Goyal"], "published_date": "2025-05-28", "title_zh": "AI信任重塑行政負擔：理解大語言模型輔助福利系統中的信任-負擔動態關係", "summary_zh": "美國政府向4100萬低收入申請者提供糧食券補助計畫（SNAP），是重要的福利支持系統。透過訪談不同SNAP經驗的申請者，研究發現大型語言模型等AI技術雖能減輕傳統負擔，卻也帶來新的負擔，包含學習、合規和心理成本，進而轉變申請者的行政負擔。對AI在能力、正直和仁慈三個維度的信任，能減少行政負擔，但也可能源於對系統過度的信任。報告討論如何校準使用者對基於大型語言模型的行政系統的信任度，減輕新引入的負擔。研究表明，在福利行政管理中，基於證據的資訊披露至關重要，並為未來研究AI輔助行政系統中的信任-負擔動態關係提出方向。", "audio": "audios/2505.22418v1.mp3", "timestamp": "2025-05-29T22:17:48.211501"}
{"query": "Foundation Model", "id": "2505.21684v1", "url": "http://arxiv.org/abs/2505.21684v1", "title": "Incentivizing Permissionless Distributed Learning of LLMs", "summary": "We describe an incentive system for distributed deep learning of foundational\nmodels where peers are rewarded for contributions. The incentive system,\n\\textit{Gauntlet}, has been deployed on the bittensor blockchain and used to\ntrain a 1.2B LLM with completely permissionless contributions of\npseudo-gradients: no control over the users that can register or their\nhardware. \\textit{Gauntlet} can be applied to any synchronous distributed\ntraining scheme that relies on aggregating updates or pseudo-gradients. We rely\non a two-stage mechanism for fast filtering of peer uptime, reliability, and\nsynchronization, combined with the core component that estimates the loss\nbefore and after individual pseudo-gradient contributions. We utilized an\nOpenSkill rating system to track competitiveness of pseudo-gradient scores\nacross time. Finally, we introduce a novel mechanism to ensure peers on the\nnetwork perform unique computations. Our live 1.2B run, which has paid out\nreal-valued tokens to participants based on the value of their contributions,\nyielded a competitive (on a per-iteration basis) 1.2B model that demonstrates\nthe utility of our incentive system.", "authors": ["Joel Lidin", "Amir Sarfi", "Evangelos Pappas", "Samuel Dare", "Eugene Belilovsky", "Jacob Steeves"], "published_date": "2025-05-27", "title_zh": "激勵無許可分散式大型語言模型學習", "summary_zh": "本文提出名為Gauntlet的激勵系統，用於分散式深度學習基礎模型，獎勵貢獻者。該系統已部署於Bittensor區塊鏈，用於訓練一個12億參數的大型語言模型，允許完全無需許可的偽梯度貢獻。Gauntlet適用於任何同步分散式訓練方案，仰賴更新或偽梯度聚合。其採用兩階段機制快速篩選節點的運行時間、可靠性和同步性，並估計個體偽梯度貢獻前後的損失。利用OpenSkill評級系統追蹤偽梯度分數的競爭力。此外，引入一種新機制確保網路節點執行獨特的計算。實際運行的12億參數模型，根據貢獻價值向參與者支付真實代幣，證明了該激勵系統的效用，產生了一個具競爭力的模型（以每次迭代為基準）。", "audio": "audios/2505.21684v1.mp3", "timestamp": "2025-05-29T22:17:54.425787"}
{"query": "Diffusion Model", "id": "2505.22000v1", "url": "http://arxiv.org/abs/2505.22000v1", "title": "Collaborative Learning for Unsupervised Multimodal Remote Sensing Image Registration: Integrating Self-Supervision and MIM-Guided Diffusion-Based Image Translation", "summary": "The substantial modality-induced variations in radiometric, texture, and\nstructural characteristics pose significant challenges for the accurate\nregistration of multimodal images. While supervised deep learning methods have\ndemonstrated strong performance, they often rely on large-scale annotated\ndatasets, limiting their practical application. Traditional unsupervised\nmethods usually optimize registration by minimizing differences in feature\nrepresentations, yet often fail to robustly capture geometric discrepancies,\nparticularly under substantial spatial and radiometric variations, thus\nhindering convergence stability. To address these challenges, we propose a\nCollaborative Learning framework for Unsupervised Multimodal Image\nRegistration, named CoLReg, which reformulates unsupervised registration\nlearning into a collaborative training paradigm comprising three components:\n(1) a cross-modal image translation network, MIMGCD, which employs a learnable\nMaximum Index Map (MIM) guided conditional diffusion model to synthesize\nmodality-consistent image pairs; (2) a self-supervised intermediate\nregistration network which learns to estimate geometric transformations using\naccurate displacement labels derived from MIMGCD outputs; (3) a distilled\ncross-modal registration network trained with pseudo-label predicted by the\nintermediate network. The three networks are jointly optimized through an\nalternating training strategy wherein each network enhances the performance of\nthe others. This mutual collaboration progressively reduces modality\ndiscrepancies, enhances the quality of pseudo-labels, and improves registration\naccuracy. Extensive experimental results on multiple datasets demonstrate that\nour ColReg achieves competitive or superior performance compared to\nstate-of-the-art unsupervised approaches and even surpasses several supervised\nbaselines.", "authors": ["Xiaochen Wei", "Weiwei Guo", "Wenxian Yu"], "published_date": "2025-05-28", "title_zh": "非監督多模遙感影像配準協同學習：整合自我監督與MIM引導之擴散模型影像轉換", "summary_zh": "多模態影像因輻射、紋理及結構特徵的顯著差異，精確配準極具挑戰。監督式深度學習方法雖表現出色，但仰賴大量標註數據，限制其實用性。傳統非監督式方法透過最小化特徵差異優化配準，卻難以有效捕捉幾何差異，尤其在空間及輻射變化大時，影響收斂穩定性。為此，我們提出名為CoLReg的協作式學習框架，將非監督式配準學習重構為協作訓練模式，包含：(1)跨模態影像轉換網路MIMGCD，採用可學習的最大索引圖(MIM)引導條件擴散模型，合成模態一致的影像對；(2)自監督中間配準網路，學習使用MIMGCD輸出衍生的精確位移標籤估計幾何轉換；(3)經中間網路預測的偽標籤訓練的精煉跨模態配準網路。三網路透過交替訓練策略聯合優化，相互提升效能。此協作逐步減少模態差異，提升偽標籤品質，改善配準精確度。多個數據集的廣泛實驗結果表明，相較於最先進的非監督式方法，CoLReg達到具競爭力甚至更優越的效能，且超越數個監督式基準線。", "audio": "audios/2505.22000v1.mp3", "timestamp": "2025-05-29T22:18:04.793416"}
{"query": "AI", "id": "2505.22343v1", "url": "http://arxiv.org/abs/2505.22343v1", "title": "Empowering Intelligent Low-altitude Economy with Large AI Model Deployment", "summary": "Low-altitude economy (LAE) represents an emerging economic paradigm that\nredefines commercial and social aerial activities. Large artificial\nintelligence models (LAIMs) offer transformative potential to further enhance\nthe intelligence of LAE services. However, deploying LAIMs in LAE poses several\nchallenges, including the significant gap between their computational/storage\ndemands and the limited onboard resources of LAE entities, the mismatch between\nlab-trained LAIMs and dynamic physical environments, and the inefficiencies of\ntraditional decoupled designs for sensing, communication, and computation. To\naddress these issues, we first propose a hierarchical system architecture\ntailored for LAIM deployment and present representative LAE application\nscenarios. Next, we explore key enabling techniques that facilitate the mutual\nco-evolution of LAIMs and low-altitude systems, and introduce a task-oriented\nexecution pipeline for scalable and adaptive service delivery. Then, the\nproposed framework is validated through real-world case studies. Finally, we\noutline open challenges to inspire future research.", "authors": ["Zhonghao Lyu", "Yulan Gao", "Junting Chen", "Hongyang Du", "Jie Xu", "Kaibin Huang", "Dong In Kim"], "published_date": "2025-05-28", "title_zh": "以大型人工智慧模型部署賦能智慧低空經濟", "summary_zh": "低空經濟作為新興經濟模式，重塑商業及社會航空活動。大型人工智慧模型(LAIMs)具備提升低空經濟服務智慧的潛力。然而，LAIMs於低空經濟部署面臨運算/儲存需求與低空實體有限資源間的差距、實驗室訓練LAIMs與動態物理環境的不匹配，以及傳統感測、通訊與計算分離設計的低效等挑戰。為解決這些問題，本文提出針對LAIMs部署的分層系統架構，並展示具代表性的低空經濟應用場景。進一步探討促進LAIMs與低空系統協同演進的關鍵技術，並介紹用於可擴展及自適應服務交付的任務導向執行流程。所提出的框架透過真實案例研究驗證，最後概述開放性挑戰以啟發未來研究。", "audio": "audios/2505.22343v1.mp3", "timestamp": "2025-05-29T23:17:17.737827"}
{"query": "Foundation Model", "id": "2505.21644v1", "url": "http://arxiv.org/abs/2505.21644v1", "title": "Geometric Feature Prompting of Image Segmentation Models", "summary": "Advances in machine learning, especially the introduction of transformer\narchitectures and vision transformers, have led to the development of highly\ncapable computer vision foundation models. The segment anything model (known\ncolloquially as SAM and more recently SAM 2), is a highly capable foundation\nmodel for segmentation of natural images and has been further applied to\nmedical and scientific image segmentation tasks. SAM relies on prompts --\npoints or regions of interest in an image -- to generate associated\nsegmentations.\n  In this manuscript we propose the use of a geometrically motivated prompt\ngenerator to produce prompt points that are colocated with particular features\nof interest. Focused prompting enables the automatic generation of sensitive\nand specific segmentations in a scientific image analysis task using SAM with\nrelatively few point prompts. The image analysis task examined is the\nsegmentation of plant roots in rhizotron or minirhizotron images, which has\nhistorically been a difficult task to automate. Hand annotation of rhizotron\nimages is laborious and often subjective; SAM, initialized with GeomPrompt\nlocal ridge prompts has the potential to dramatically improve rhizotron image\nprocessing.\n  The authors have concurrently released an open source software suite called\ngeomprompt https://pypi.org/project/geomprompt/ that can produce point prompts\nin a format that enables direct integration with the segment-anything package.", "authors": ["Kenneth Ball", "Erin Taylor", "Nirav Patel", "Andrew Bartels", "Gary Koplik", "James Polly", "Jay Hineman"], "published_date": "2025-05-27", "title_zh": "圖像分割模型的幾何特徵提示", "summary_zh": "機器學習，特別是變形金剛架構和視覺變形金剛的進展，促成了高效能電腦視覺基礎模型的發展。分割萬物模型 (SAM) 是一個強大的自然圖像分割基礎模型，並已應用於醫學和科學圖像分割任務。SAM 依賴提示（圖像中的點或感興趣區域）來產生相關的分割。\n\n本文提出使用幾何驅動的提示產生器，以生成與特定感興趣特徵共置的提示點。聚焦提示能夠在使用相對較少的點提示下，利用 SAM 在科學圖像分析任務中自動生成靈敏且具體的分割。所檢視的圖像分析任務是根箱或微根箱圖像中植物根部的分割，這項任務在歷史上難以自動化。人工標注根箱圖像既費力又主觀；使用 GeomPrompt 局部脊線提示初始化的 SAM 有潛力顯著改善根箱圖像處理。\n\n作者同時發布了一個名為 geomprompt 的開源軟體套件，該套件可以生成點提示，其格式可直接與 segment-anything 軟體包整合。", "audio": "audios/2505.21644v1.mp3", "timestamp": "2025-05-29T23:17:24.870094"}
{"query": "Diffusion Model", "id": "2505.21975v1", "url": "http://arxiv.org/abs/2505.21975v1", "title": "DvD: Unleashing a Generative Paradigm for Document Dewarping via Coordinates-based Diffusion Model", "summary": "Document dewarping aims to rectify deformations in photographic document\nimages, thus improving text readability, which has attracted much attention and\nmade great progress, but it is still challenging to preserve document\nstructures. Given recent advances in diffusion models, it is natural for us to\nconsider their potential applicability to document dewarping. However, it is\nfar from straightforward to adopt diffusion models in document dewarping due to\ntheir unfaithful control on highly complex document images (e.g.,\n2000$\\times$3000 resolution). In this paper, we propose DvD, the first\ngenerative model to tackle document \\textbf{D}ewarping \\textbf{v}ia a\n\\textbf{D}iffusion framework. To be specific, DvD introduces a coordinate-level\ndenoising instead of typical pixel-level denoising, generating a mapping for\ndeformation rectification. In addition, we further propose a time-variant\ncondition refinement mechanism to enhance the preservation of document\nstructures. In experiments, we find that current document dewarping benchmarks\ncan not evaluate dewarping models comprehensively. To this end, we present\nAnyPhotoDoc6300, a rigorously designed large-scale document dewarping benchmark\ncomprising 6,300 real image pairs across three distinct domains, enabling\nfine-grained evaluation of dewarping models. Comprehensive experiments\ndemonstrate that our proposed DvD can achieve state-of-the-art performance with\nacceptable computational efficiency on multiple metrics across various\nbenchmarks including DocUNet, DIR300, and AnyPhotoDoc6300. The new benchmark\nand code will be publicly available.", "authors": ["Weiguang Zhang", "Huangcheng Lu", "Maizhen Ning", "Xiaowei Huang", "Wei Wang", "Kaizhu Huang", "Qiufeng Wang"], "published_date": "2025-05-28", "title_zh": "DvD：基於坐標擴散模型的文檔去畸變生成範式", "summary_zh": "文件去扭曲旨在修正照片文件中圖像的變形，從而提高文本可讀性。儘管已取得顯著進展，但保持文件結構仍然具有挑戰性。鑑於擴散模型的最新進展，本文提出首個基於擴散框架的文件去扭曲生成模型DvD。DvD引入坐標級去噪而非像素級去噪，生成用於修正變形的映射。此外，提出時變條件細化機制以增強文件結構的保持。實驗表明，現有基準測試無法全面評估去扭曲模型。因此，本文提出AnyPhotoDoc6300，一個包含6300個真實圖像對的大規模文件去扭曲基準測試，以實現對去扭曲模型的細粒度評估。綜合實驗表明，所提出的DvD在DocUNet、DIR300和AnyPhotoDoc6300等多個基準測試中，以可接受的計算效率實現了最先進的性能。新的基準測試和程式碼將公開發布。", "audio": "audios/2505.21975v1.mp3", "timestamp": "2025-05-29T23:17:32.609325"}
{"query": "AI", "id": "2505.22467v2", "url": "http://arxiv.org/abs/2505.22467v2", "title": "Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems", "summary": "Large Language Model-based Multi-Agent Systems (MASs) have emerged as a\npowerful paradigm for tackling complex tasks through collaborative\nintelligence. Nevertheless, the question of how agents should be structurally\norganized for optimal cooperation remains largely unexplored. In this position\npaper, we aim to gently redirect the focus of the MAS research community toward\nthis critical dimension: develop topology-aware MASs for specific tasks.\nSpecifically, the system consists of three core components - agents,\ncommunication links, and communication patterns - that collectively shape its\ncoordination performance and efficiency. To this end, we introduce a\nsystematic, three-stage framework: agent selection, structure profiling, and\ntopology synthesis. Each stage would trigger new research opportunities in\nareas such as language models, reinforcement learning, graph learning, and\ngenerative modeling; together, they could unleash the full potential of MASs in\ncomplicated real-world applications. Then, we discuss the potential challenges\nand opportunities in the evaluation of multiple systems. We hope our\nperspective and framework can offer critical new insights in the era of agentic\nAI.", "authors": ["Jiaxi Yang", "Mengqi Zhang", "Yiqiao Jin", "Hao Chen", "Qingsong Wen", "Lu Lin", "Yi He", "Weijie Xu", "James Evans", "Jindong Wang"], "published_date": "2025-05-28", "title_zh": "基於大型語言模型的多智能體系統應優先研究拓撲結構學習", "summary_zh": "基於大型語言模型的多代理系統已成為解決複雜任務的強大範例。然而，如何建構代理以實現最佳協作仍待探索。本文旨在引導研究社群關注此關鍵面向：為特定任務開發感知拓撲結構的多代理系統。此系統包含代理、通訊連結和通訊模式三個核心組件，共同影響協調效能和效率。我們提出一個系統性的三階段框架：代理選擇、結構分析和拓撲合成。每個階段都將觸發語言模型、強化學習、圖學習和生成模型等領域的新研究機會，進而釋放多代理系統在複雜現實應用中的潛力。此外，我們探討多系統評估中的潛在挑戰與機遇。期望本文的觀點和框架能為代理式AI時代提供重要的新見解。", "audio": "audios/2505.22467v2.mp3", "timestamp": "2025-05-30T01:24:29.305214"}
{"query": "Foundation Model", "id": "2505.21904v2", "url": "http://arxiv.org/abs/2505.21904v2", "title": "CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation", "summary": "Instance segmentation demands costly per-pixel annotations and large models.\nWe introduce CAST, a semi-supervised knowledge distillation (SSKD) framework\nthat compresses pretrained vision foundation models (VFM) into compact experts\nusing limited labeled and abundant unlabeled data. CAST unfolds in three\nstages: (1) domain adaptation of the VFM teacher(s) via self-training with\ncontrastive pixel calibration, (2) distillation into a compact student via a\nunified multi-objective loss that couples standard supervision and\npseudo-labels with our instance-aware pixel-wise contrastive term, and (3)\nfine-tuning on labeled data to remove residual pseudo-label bias. Central to\nCAST is an \\emph{instance-aware pixel-wise contrastive loss} that fuses mask\nand class scores to mine informative negatives and enforce clear inter-instance\nmargins. By maintaining this contrastive signal across both adaptation and\ndistillation, we align teacher and student embeddings and fully leverage\nunlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses\nits adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs.\n15.2) and outperforms state-of-the-art semi-supervised approaches.", "authors": ["Pardis Taghavi", "Tian Liu", "Renjie Li", "Reza Langari", "Zhengzhong Tu"], "published_date": "2025-05-28", "title_zh": "CAST：對比適應與蒸餾用於半監督實例分割", "summary_zh": "實例分割需要高昂的逐像素標註和大型模型。本研究提出CAST，一種半監督知識蒸餾框架，利用有限標註和大量無標註數據，將預訓練視覺基礎模型壓縮為精簡專家模型。CAST包含三個階段：（1）透過對比像素校準的自訓練進行VFM教師模型的領域適應，（2）透過統一的多目標損失函數將知識蒸餾至精簡學生模型，該損失函數結合標準監督和偽標籤以及實例感知的像素級對比項，（3）在標註數據上進行微調，以消除殘餘的偽標籤偏差。CAST的核心是實例感知的像素級對比損失，它融合了遮罩和類別分數，以挖掘有用的負樣本並強化實例間的清晰邊界。透過在適應和蒸餾過程中保持這種對比信號，我們對齊教師和學生模型的嵌入，並充分利用無標註圖像。在Cityscapes和ADE20K數據集上，我們體積小約11倍的學生模型超越了其適應後的VFM教師模型，分別提升了+3.4 AP（33.9 vs. 30.5）和+1.5 AP（16.7 vs. 15.2），並優於最先進的半監督方法。", "audio": "audios/2505.21904v2.mp3", "timestamp": "2025-05-30T01:24:38.599414"}
{"query": "Diffusion Model", "id": "2505.21960v1", "url": "http://arxiv.org/abs/2505.21960v1", "title": "One-Way Ticket:Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models", "summary": "Text-to-Image (T2I) diffusion models have made remarkable advancements in\ngenerative modeling; however, they face a trade-off between inference speed and\nimage quality, posing challenges for efficient deployment. Existing distilled\nT2I models can generate high-fidelity images with fewer sampling steps, but\noften struggle with diversity and quality, especially in one-step models. From\nour analysis, we observe redundant computations in the UNet encoders. Our\nfindings suggest that, for T2I diffusion models, decoders are more adept at\ncapturing richer and more explicit semantic information, while encoders can be\neffectively shared across decoders from diverse time steps. Based on these\nobservations, we introduce the first Time-independent Unified Encoder TiUE for\nthe student model UNet architecture, which is a loop-free image generation\napproach for distilling T2I diffusion models. Using a one-pass scheme, TiUE\nshares encoder features across multiple decoder time steps, enabling parallel\nsampling and significantly reducing inference time complexity. In addition, we\nincorporate a KL divergence term to regularize noise prediction, which enhances\nthe perceptual realism and diversity of the generated images. Experimental\nresults demonstrate that TiUE outperforms state-of-the-art methods, including\nLCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results\nwhile maintaining the computational efficiency.", "authors": ["Senmao Li", "Lei Wang", "Kai Wang", "Tao Liu", "Jiehang Xie", "Joost van de Weijer", "Fahad Shahbaz Khan", "Shiqi Yang", "Yaxing Wang", "Jian Yang"], "published_date": "2025-05-28", "title_zh": "單程票：用於提煉文圖擴散模型的時不變統一編碼器", "summary_zh": "文本到圖像(T2I)擴散模型在生成建模方面取得顯著進展，但面臨推論速度和圖像品質之間的權衡。現有的精餾T2I模型雖然能以較少採樣步驟生成高保真圖像，但在多樣性和品質方面表現不佳，尤其是一步模型。研究發現UNet編碼器中存在冗餘計算。T2I擴散模型中，解碼器更擅長捕獲豐富的語義資訊，而編碼器可以有效地在不同時間步的解碼器之間共享。因此，我們提出了第一個時間獨立統一編碼器(TiUE)，用於學生模型UNet架構，這是一種無迴圈圖像生成方法，用於精餾T2I擴散模型。TiUE採用單次傳遞方案，在多個解碼器時間步共享編碼器特徵，實現平行採樣並顯著降低推論時間複雜度。此外，我們加入KL散度項來正則化噪聲預測，從而提高生成圖像的感知真實性和多樣性。實驗結果表明，TiUE優於現有方法，包括LCM、SD-Turbo和SwiftBrushv2，在保持計算效率的同時，產生更多樣化和逼真的結果。", "audio": "audios/2505.21960v1.mp3", "timestamp": "2025-05-30T01:24:55.436497"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "人工智慧對齊的扭曲：偏好優化是否優化了偏好本身？", "summary_zh": "大型語言模型經預訓練後，會基於成對比較與人類偏好對齊。現有對齊方法（如基於PPO的RLHF和DPO）假設與單一偏好模型對齊，但實際部署環境中用戶偏好各異。因此，這些對齊方法產生的模型是否能滿足用戶的平均需求仍不明確。基於社會選擇理論，並透過個別Bradley-Terry模型模擬用戶比較，本文引入對齊方法的失真概念：即最佳可實現平均效用與學習策略平均效用之間的最差比率。失真概念有助於區分對齊方法：來自人類反饋的納許學習在BT溫度$\\beta$下，實現了$(\\frac{1}{2} + o(1)) \\cdot \\beta$的極小化極大最優失真，且對效用分佈、比較對分佈以及與參考策略的KL散度具有穩健性。相比之下，RLHF和DPO即便沒有KL約束，也遭受$\\geq (1 - o(1)) \\cdot \\beta$的失真；在完整設置下，根據比較對的採樣方式，甚至會遭受$e^{\\Omega(\\beta)}$或無界的失真。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-30T03:10:54.228531"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "空間多模態大型語言模型：提升基於視覺的空間智能中的多模態大型語言模型能力", "summary_zh": "多模態大型語言模型在二維視覺任務上表現卓越，但空間智能仍待加強。現有三維模型依賴額外三維或二點五維數據，限制了其在僅有二維輸入場景中的應用。本文提出Spatial-MLLM，一種基於純二維觀察的視覺空間推理框架。不同於依賴CLIP視覺編碼器的傳統模型，本研究利用前饋視覺幾何基礎模型的結構先驗知識。具體而言，採用雙編碼器架構：預訓練二維視覺編碼器提取語義特徵，空間編碼器（基於視覺幾何模型骨幹初始化）提取三維結構特徵。連接器整合兩者，形成統一視覺令牌，增強空間理解。此外，提出空間感知幀採樣策略，於推理時選取視頻序列中富含空間信息的幀，確保模型在有限令牌長度下聚焦於空間推理的關鍵幀。除架構改進外，構建Spatial-MLLM-120k數據集，並利用監督微調和GRPO訓練模型。實驗表明，Spatial-MLLM在多項實際數據集上的視覺空間理解和推理任務中達到頂尖水平。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T03:11:02.330529"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於修正流變換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop是首個基於LoRA模型的多概念圖像編輯框架。它基於對Flux風格擴散轉換器內部特徵交互模式的觀察：概念特定的轉換器特徵在去噪過程早期激活空間上連貫的區域。利用此特性，LoRAShop在前向傳遞中為每個概念導出分離的潛在遮罩，並僅在限定概念的區域內混合相應的LoRA權重。產生的編輯將多個主體或風格無縫整合到原始場景中，同時保留全局上下文、光照和細節。實驗表明，相較於基準方法，LoRAShop能更好地保留身份特徵。藉由消除重新訓練和外部約束，LoRAShop將個人化擴散模型轉變為實用的「LoRA版Photoshop」工具，並為組合式視覺敘事和快速創意迭代開闢了新途徑。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T03:11:09.926655"}
{"query": "AI", "id": "2505.23746v1", "url": "http://arxiv.org/abs/2505.23746v1", "title": "Comparative of Genetic Fuzzy regression techniques for aeroacoustic phenomenons", "summary": "This study investigates the application of Genetic Fuzzy Systems (GFS) to\nmodel the self-noise generated by airfoils, a key issue in aeroaccoustics with\nsignificant implications for aerospace, automotive and drone applications.\nUsing the publicly available Airfoil Self Noise dataset, various Fuzzy\nregression strategies are explored and compared. The paper evaluates a brute\nforce Takagi Sugeno Kang (TSK) fuzzy system with high rule density, a cascading\nGeneti Fuzzy Tree (GFT) architecture and a novel clustered approach based on\nFuzzy C-means (FCM) to reduce the model's complexity. This highlights the\nviability of clustering assisted fuzzy inference as an effective regression\ntool for complex aero accoustic phenomena. Keywords : Fuzzy logic, Regression,\nCascading systems, Clustering and AI.", "authors": ["Hugo Henry", "Kelly Cohen"], "published_date": "2025-05-29", "title_zh": "航空聲學現象之基因模糊回歸技術比較", "summary_zh": "本研究探討基因模糊系統（GFS）在機翼自生噪音建模中的應用，此為航空聲學的關鍵議題，對航太、汽車及無人機應用具有重要影響。研究使用公開的機翼自生噪音數據集，探索並比較多種模糊迴歸策略。評估了高規則密度的暴力破解田口-菅野-姜（TSK）模糊系統、級聯基因模糊樹（GFT）架構，以及基於模糊C均值（FCM）的新型集群方法，以降低模型複雜度。結果表明，集群輔助模糊推論可作為複雜航空聲學現象的有效迴歸工具。關鍵詞：模糊邏輯、迴歸、級聯系統、集群、人工智慧。", "audio": "audios/2505.23746v1.mp3", "timestamp": "2025-05-30T04:23:21.312157"}
{"query": "Foundation Model", "id": "2505.23726v1", "url": "http://arxiv.org/abs/2505.23726v1", "title": "FMG-Det: Foundation Model Guided Robust Object Detection", "summary": "Collecting high quality data for object detection tasks is challenging due to\nthe inherent subjectivity in labeling the boundaries of an object. This makes\nit difficult to not only collect consistent annotations across a dataset but\nalso to validate them, as no two annotators are likely to label the same object\nusing the exact same coordinates. These challenges are further compounded when\nobject boundaries are partially visible or blurred, which can be the case in\nmany domains. Training on noisy annotations significantly degrades detector\nperformance, rendering them unusable, particularly in few-shot settings, where\njust a few corrupted annotations can impact model performance. In this work, we\npropose FMG-Det, a simple, efficient methodology for training models with noisy\nannotations. More specifically, we propose combining a multiple instance\nlearning (MIL) framework with a pre-processing pipeline that leverages powerful\nfoundation models to correct labels prior to training. This pre-processing\npipeline, along with slight modifications to the detector head, results in\nstate-of-the-art performance across a number of datasets, for both standard and\nfew-shot scenarios, while being much simpler and more efficient than other\napproaches.", "authors": ["Darryl Hannan", "Timothy Doster", "Henry Kvinge", "Adam Attarian", "Yijing Watkins"], "published_date": "2025-05-29", "title_zh": "FMG-Det：基礎模型引導的穩健物件偵測", "summary_zh": "物件偵測任務中，標註物件邊界的主觀性導致收集高品質資料極具挑戰，不僅難以確保標註一致性，驗證也存在困難。物件邊界部分可見或模糊時，問題更加複雜。於含雜訊標註的資料上訓練模型會嚴重影響偵測器效能，特別是在少量樣本情境下。本研究提出FMG-Det，一種簡潔高效的雜訊標註模型訓練方法。透過結合多實例學習框架與預處理流程，利用強大的基礎模型在訓練前修正標籤。此流程與偵測器頭部的微小修改，使模型在標準和少量樣本情境下皆能達到最先進的效能，且比其他方法更簡單高效。", "audio": "audios/2505.23726v1.mp3", "timestamp": "2025-05-30T04:23:27.111520"}
{"query": "Diffusion Model", "id": "2505.23743v1", "url": "http://arxiv.org/abs/2505.23743v1", "title": "DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion Models for Camera ISP", "summary": "High-quality photography in extreme low-light conditions is challenging but\nimpactful for digital cameras. With advanced computing hardware, traditional\ncamera image signal processor (ISP) algorithms are gradually being replaced by\nefficient deep networks that enhance noisy raw images more intelligently.\nHowever, existing regression-based models often minimize pixel errors and\nresult in oversmoothing of low-light photos or deep shadows. Recent work has\nattempted to address this limitation by training a diffusion model from\nscratch, yet those models still struggle to recover sharp image details and\naccurate colors. We introduce a novel framework to enhance low-light raw images\nby retasking pre-trained generative diffusion models with the camera ISP.\nExtensive experiments demonstrate that our method outperforms the\nstate-of-the-art in perceptual quality across three challenging low-light raw\nimage benchmarks.", "authors": ["Amber Yijia Zheng", "Yu Zhang", "Jun Hu", "Raymond A. Yeh", "Chen Chen"], "published_date": "2025-05-29", "title_zh": "DarkDiff：藉由重新調整擴散模型於相機影像訊號處理以推進低光原始圖像增強", "summary_zh": "在極低光照環境下獲取高品質照片極具挑戰，但對數位相機至關重要。隨著運算硬體的進步，傳統相機影像訊號處理器（ISP）演算法正逐漸被高效能深度網路取代，以更智慧的方式增強帶有雜訊的原始影像。然而，現有的基於回歸的模型通常最小化像素誤差，導致低光照片或深陰影過度平滑。近期研究嘗試從頭訓練擴散模型來解決此限制，但這些模型在恢復清晰影像細節和準確色彩方面仍存在困難。我們提出一種新穎框架，透過重新調整預訓練的生成擴散模型與相機ISP，來增強低光原始影像。大量實驗表明，我們的模型在三個具挑戰性的低光原始影像基準測試中，於感知品質方面超越了最先進技術。", "audio": "audios/2505.23743v1.mp3", "timestamp": "2025-05-30T04:23:33.205533"}
{"query": "AI", "id": "2505.23733v1", "url": "http://arxiv.org/abs/2505.23733v1", "title": "Exposing the Impact of GenAI for Cybercrime: An Investigation into the Dark Side", "summary": "In recent years, the rapid advancement and democratization of generative AI\nmodels have sparked significant debate over safety, ethical risks, and dual-use\nconcerns, particularly in the context of cybersecurity. While anecdotally\nknown, this paper provides empirical evidence regarding generative AI's\nassociation with malicious internet-related activities and cybercrime by\nexamining the phenomenon through psychological frameworks of technological\namplification and affordance theory. Using a quasi-experimental design with\ninterrupted time series analysis, we analyze two datasets, one general and one\ncryptocurrency-focused, to empirically assess generative AI's role in\ncybercrime. The findings contribute to ongoing discussions about AI governance\nby balancing control and fostering innovation, underscoring the need for\nstrategies to guide policymakers, inform AI developers and cybersecurity\nprofessionals, and educate the public to maximize AI's benefits while\nmitigating its risks.", "authors": ["Truong", "Luu", "Binny M. Samuel"], "published_date": "2025-05-29", "title_zh": "揭露生成式人工智慧對網路犯罪的影響：暗黑面的調查", "summary_zh": "近年來，生成式AI模型的快速發展引發了對安全性、倫理風險和雙重用途的廣泛討論，尤其是在網路安全領域。本研究透過心理學的技術放大和可供性理論框架，檢驗生成式AI與惡意網路活動和網路犯罪之間的關聯性，並提供實證證據。採用準實驗設計和中斷時間序列分析，分析一般和加密貨幣相關的兩組數據集，以評估生成式AI在網路犯罪中的作用。研究結果有助於平衡管控與促進創新的AI治理討論，強調需要制定策略，指導決策者、AI開發者和網路安全專家，並教育大眾，以最大化AI的效益並降低其風險。", "audio": "audios/2505.23733v1.mp3", "timestamp": "2025-05-30T05:18:41.825045"}
{"query": "Foundation Model", "id": "2505.23656v1", "url": "http://arxiv.org/abs/2505.23656v1", "title": "VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models", "summary": "Recent advancements in text-to-video (T2V) diffusion models have enabled\nhigh-fidelity and realistic video synthesis. However, current T2V models often\nstruggle to generate physically plausible content due to their limited inherent\nability to accurately understand physics. We found that while the\nrepresentations within T2V models possess some capacity for physics\nunderstanding, they lag significantly behind those from recent video\nself-supervised learning methods. To this end, we propose a novel framework\ncalled VideoREPA, which distills physics understanding capability from video\nunderstanding foundation models into T2V models by aligning token-level\nrelations. This closes the physics understanding gap and enable more\nphysics-plausible generation. Specifically, we introduce the Token Relation\nDistillation (TRD) loss, leveraging spatio-temporal alignment to provide soft\nguidance suitable for finetuning powerful pre-trained T2V models, a critical\ndeparture from prior representation alignment (REPA) methods. To our knowledge,\nVideoREPA is the first REPA method designed for finetuning T2V models and\nspecifically for injecting physical knowledge. Empirical evaluations show that\nVideoREPA substantially enhances the physics commonsense of baseline method,\nCogVideoX, achieving significant improvement on relevant benchmarks and\ndemonstrating a strong capacity for generating videos consistent with intuitive\nphysics. More video results are available at https://videorepa.github.io/.", "authors": ["Xiangdong Zhang", "Jiaqi Liao", "Shaofeng Zhang", "Fanqing Meng", "Xiangpeng Wan", "Junchi Yan", "Yu Cheng"], "published_date": "2025-05-29", "title_zh": "VideoREPA：藉由與基礎模型之關係對齊學習物理，以進行影片生成", "summary_zh": "文字轉影片擴散模型在產生高傳真、逼真影片方面取得進展。然而，由於物理理解能力有限，現有模型難以生成符合物理規則的內容。研究發現，文字轉影片模型雖具備一定物理理解能力，但仍落後於影片自監督學習方法。為此，我們提出VideoREPA框架，透過對齊Token層級關係，將影片理解基礎模型的物理理解能力提煉至文字轉影片模型，彌補物理理解差距，實現更符合物理規則的生成。我們引入Token關係蒸餾損失，利用時空對齊提供柔性引導，適用於微調預訓練文字轉影片模型，這與先前的表徵對齊方法截然不同。據我們所知，VideoREPA是首個專為微調文字轉影片模型，並注入物理知識而設計的表徵對齊方法。實驗結果表明，VideoREPA顯著提升了CogVideoX的物理常識，在相關基準測試上取得顯著改進，展現出生成符合直覺物理學影片的強大能力。更多影片結果請見https://videorepa.github.io/。", "audio": "audios/2505.23656v1.mp3", "timestamp": "2025-05-30T05:18:51.070311"}
{"query": "Diffusion Model", "id": "2505.23740v1", "url": "http://arxiv.org/abs/2505.23740v1", "title": "LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization", "summary": "Image vectorization is a powerful technique that converts raster images into\nvector graphics, enabling enhanced flexibility and interactivity. However,\npopular image vectorization tools struggle with occluded regions, producing\nincomplete or fragmented shapes that hinder editability. While recent\nadvancements have explored rule-based and data-driven layer-wise image\nvectorization, these methods face limitations in vectorization quality and\nflexibility. In this paper, we introduce LayerPeeler, a novel layer-wise image\nvectorization approach that addresses these challenges through a progressive\nsimplification paradigm. The key to LayerPeeler's success lies in its\nautoregressive peeling strategy: by identifying and removing the topmost\nnon-occluded layers while recovering underlying content, we generate vector\ngraphics with complete paths and coherent layer structures. Our method\nleverages vision-language models to construct a layer graph that captures\nocclusion relationships among elements, enabling precise detection and\ndescription for non-occluded layers. These descriptive captions are used as\nediting instructions for a finetuned image diffusion model to remove the\nidentified layers. To ensure accurate removal, we employ localized attention\ncontrol that precisely guides the model to target regions while faithfully\npreserving the surrounding content. To support this, we contribute a\nlarge-scale dataset specifically designed for layer peeling tasks. Extensive\nquantitative and qualitative experiments demonstrate that LayerPeeler\nsignificantly outperforms existing techniques, producing vectorization results\nwith superior path semantics, geometric regularity, and visual fidelity.", "authors": ["Ronghuan Wu", "Wanchao Su", "Jing Liao"], "published_date": "2025-05-29", "title_zh": "LayerPeeler：用於分層圖像向量化的自迴歸剝離法", "summary_zh": "圖像向量化是一種將柵格圖像轉換為向量圖形的強大技術，可增強靈活性和互動性。然而，現有工具在處理遮擋區域時表現不佳，產生不完整或破碎的形狀，阻礙編輯性。本文提出LayerPeeler，一種新穎的分層圖像向量化方法，透過漸進式簡化範例解決這些挑戰。LayerPeeler的關鍵在於其自迴歸剝離策略：識別並移除最上層未遮擋圖層，同時恢復底層內容，生成具有完整路徑和連貫圖層結構的向量圖形。本方法利用視覺語言模型構建圖層圖，捕捉元素間的遮擋關係，精確檢測和描述未遮擋圖層。描述性標題作為編輯指令，用於微調的圖像擴散模型以移除識別出的圖層。為確保精確移除，採用局部注意力控制，引導模型精準鎖定目標區域，同時忠實保留周圍內容。為此，貢獻了一個專為圖層剝離任務設計的大規模資料集。大量定量和定性實驗表明，LayerPeeler顯著優於現有技術，產生具有卓越路徑語義、幾何規則性和視覺保真度的向量化結果。", "audio": "audios/2505.23740v1.mp3", "timestamp": "2025-05-30T05:18:59.843331"}
{"query": "AI", "id": "2505.23710v1", "url": "http://arxiv.org/abs/2505.23710v1", "title": "From Connectivity to Autonomy: The Dawn of Self-Evolving Communication Systems", "summary": "This paper envisions 6G as a self-evolving telecom ecosystem, where AI-driven\nintelligence enables dynamic adaptation beyond static connectivity. We explore\nthe key enablers of autonomous communication systems, spanning reconfigurable\ninfrastructure, adaptive middleware, and intelligent network functions,\nalongside multi-agent collaboration for distributed decision-making. We explore\nhow these methodologies align with emerging industrial IoT frameworks, ensuring\nseamless integration within digital manufacturing processes. Our findings\nemphasize the potential for improved real-time decision-making, optimizing\nefficiency, and reducing latency in networked control systems. The discussion\naddresses ethical challenges, research directions, and standardization efforts,\nconcluding with a technology stack roadmap to guide future developments. By\nleveraging state-of-the-art 6G network management techniques, this research\ncontributes to the next generation of intelligent automation solutions,\nbridging the gap between theoretical advancements and real-world industrial\napplications.", "authors": ["Zeinab Nezami", "Syed Danial Ali Shah", "Maryam Hafeez", "Karim Djemame", "Syed Ali Raza Zaidi"], "published_date": "2025-05-29", "title_zh": "從連通性到自主性：自演化通訊系統的曙光", "summary_zh": "本研究將6G設想為一個自我演進的電信生態系統，透過人工智慧驅動的智慧實現動態適應，超越靜態連接。探討自主通訊系統的關鍵促成因素，包括可重構基礎設施、自適應中間件和智慧網路功能，以及用於分散式決策的多代理協作。研究這些方法如何與新興工業物聯網框架對齊，確保在數位製造流程中實現無縫整合。研究結果強調改善即時決策、優化效率和降低網路控制系統延遲的潛力。討論涵蓋倫理挑戰、研究方向和標準化工作，最後提出技術堆疊路線圖以指導未來發展。藉由利用最先進的6G網路管理技術，本研究有助於下一代智慧自動化解決方案，彌合理論進展與實際工業應用之間的差距。", "audio": "audios/2505.23710v1.mp3", "timestamp": "2025-05-30T06:26:53.966771"}
{"query": "Foundation Model", "id": "2505.23625v1", "url": "http://arxiv.org/abs/2505.23625v1", "title": "ZeroSep: Separate Anything in Audio with Zero Training", "summary": "Audio source separation is fundamental for machines to understand complex\nacoustic environments and underpins numerous audio applications. Current\nsupervised deep learning approaches, while powerful, are limited by the need\nfor extensive, task-specific labeled data and struggle to generalize to the\nimmense variability and open-set nature of real-world acoustic scenes. Inspired\nby the success of generative foundation models, we investigate whether\npre-trained text-guided audio diffusion models can overcome these limitations.\nWe make a surprising discovery: zero-shot source separation can be achieved\npurely through a pre-trained text-guided audio diffusion model under the right\nconfiguration. Our method, named ZeroSep, works by inverting the mixed audio\ninto the diffusion model's latent space and then using text conditioning to\nguide the denoising process to recover individual sources. Without any\ntask-specific training or fine-tuning, ZeroSep repurposes the generative\ndiffusion model for a discriminative separation task and inherently supports\nopen-set scenarios through its rich textual priors. ZeroSep is compatible with\na variety of pre-trained text-guided audio diffusion backbones and delivers\nstrong separation performance on multiple separation benchmarks, surpassing\neven supervised methods.", "authors": ["Chao Huang", "Yuesheng Ma", "Junxuan Huang", "Susan Liang", "Yunlong Tang", "Jing Bi", "Wenqiang Liu", "Nima Mesgarani", "Chenliang Xu"], "published_date": "2025-05-29", "title_zh": "ZeroSep：零訓練音訊萬物分離", "summary_zh": "音訊源分離是機器理解複雜聲學環境的基礎，並支撐著眾多音訊應用。現有監督深度學習方法雖然強大，但受限於大量特定任務標記數據的需求，難以泛化到真實聲學場景的巨大變異性和開放集性質。受生成式基礎模型成功的啟發，本文探討預訓練文本引導音訊擴散模型是否能克服這些限制。研究發現，在適當配置下，僅通過預訓練文本引導音訊擴散模型即可實現零樣本源分離。名為ZeroSep的方法將混合音訊逆向轉換至擴散模型的潛在空間，然後利用文本條件引導去噪過程以恢復個別音源。無需任何特定任務訓練或微調，ZeroSep即可將生成式擴散模型重新用於判別性分離任務，並通過其豐富的文本先驗知識，自然支援開放集場景。ZeroSep相容於多種預訓練文本引導音訊擴散骨幹網路，並在多個分離基準測試上表現出強大的分離性能，甚至超越了監督方法。", "audio": "audios/2505.23625v1.mp3", "timestamp": "2025-05-30T06:27:01.827008"}
{"query": "Diffusion Model", "id": "2505.23738v1", "url": "http://arxiv.org/abs/2505.23738v1", "title": "How Animals Dance (When You're Not Looking)", "summary": "We present a keyframe-based framework for generating music-synchronized,\nchoreography aware animal dance videos. Starting from a few keyframes\nrepresenting distinct animal poses -- generated via text-to-image prompting or\nGPT-4o -- we formulate dance synthesis as a graph optimization problem: find\nthe optimal keyframe structure that satisfies a specified choreography pattern\nof beats, which can be automatically estimated from a reference dance video. We\nalso introduce an approach for mirrored pose image generation, essential for\ncapturing symmetry in dance. In-between frames are synthesized using an video\ndiffusion model. With as few as six input keyframes, our method can produce up\nto 30 second dance videos across a wide range of animals and music tracks.", "authors": ["Xiaojuan Wang", "Aleksander Holynski", "Brian Curless", "Ira Kemelmacher", "Steve Seitz"], "published_date": "2025-05-29", "title_zh": "動物如何舞蹈（在你沒注意的時候）", "summary_zh": "本研究提出一種基於關鍵幀的框架，用於生成與音樂同步且具編舞意識的動物舞蹈影片。從代表不同動物姿態的少量關鍵幀（透過文本生成圖像或GPT-4o產生）開始，將舞蹈合成建構為圖優化問題：尋找滿足指定節拍編舞模式的最佳關鍵幀結構，該模式可從參考舞蹈影片自動估算。同時引入鏡像姿勢圖像生成方法，對捕捉舞蹈中的對稱性至關重要。中間幀採用影片擴散模型合成。僅需六個輸入關鍵幀，該方法即可生成長達30秒，涵蓋多種動物和音樂的舞蹈影片。", "audio": "audios/2505.23738v1.mp3", "timestamp": "2025-05-30T06:27:06.228845"}
{"query": "AI", "id": "2505.23693v1", "url": "http://arxiv.org/abs/2505.23693v1", "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos", "summary": "MLLMs have been widely studied for video question answering recently.\nHowever, most existing assessments focus on natural videos, overlooking\nsynthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in\nvideo generation rely on MLLMs to evaluate the quality of generated videos, but\nthe capabilities of MLLMs on interpreting AIGC videos remain largely\nunderexplored. To address this, we propose a new benchmark, VF-Eval, which\nintroduces four tasks-coherence validation, error awareness, error type\ndetection, and reasoning evaluation-to comprehensively evaluate the abilities\nof MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that\neven the best-performing model, GPT-4.1, struggles to achieve consistently good\nperformance across all tasks. This highlights the challenging nature of our\nbenchmark. Additionally, to investigate the practical applications of VF-Eval\nin improving video generation, we conduct an experiment, RePrompt,\ndemonstrating that aligning MLLMs more closely with human feedback can benefit\nvideo generation.", "authors": ["Tingyu Song", "Tongyan Hu", "Guo Gan", "Yilun Zhao"], "published_date": "2025-05-29", "title_zh": "VF-Eval：評估多模態大型語言模型在生成AIGC影片回饋上的表現", "summary_zh": "多模態大型語言模型（MLLMs）近年廣泛應用於影片問答。然而，現有評估多集中於自然影片，忽略了如AI生成內容（AIGC）等合成影片。部分影片生成研究仰賴MLLMs評估生成品質，但MLLMs解讀AIGC影片的能力尚未充分探索。為此，我們提出VF-Eval基準，包含連貫性驗證、錯誤感知、錯誤類型檢測和推理評估四項任務，全面評估MLLMs對AIGC影片的理解能力。我們評估了13個前沿MLLMs，發現即使表現最佳的GPT-4.1也難以在所有任務中維持優異表現，突顯此基準的挑戰性。此外，為探討VF-Eval在改善影片生成方面的實際應用，我們進行了RePrompt實驗，表明使MLLMs更貼近人類回饋有助於影片生成。", "audio": "audios/2505.23693v1.mp3", "timestamp": "2025-05-30T07:18:05.166019"}
{"query": "Foundation Model", "id": "2505.23579v1", "url": "http://arxiv.org/abs/2505.23579v1", "title": "BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model", "summary": "Unlocking deep, interpretable biological reasoning from complex genomic data\nis a major AI challenge hindering scientific discovery. Current DNA foundation\nmodels, despite strong sequence representation, struggle with multi-step\nreasoning and lack inherent transparent, biologically intuitive explanations.\nWe introduce BioReason, a pioneering architecture that, for the first time,\ndeeply integrates a DNA foundation model with a Large Language Model (LLM).\nThis novel connection enables the LLM to directly process and reason with\ngenomic information as a fundamental input, fostering a new form of multimodal\nbiological understanding. BioReason's sophisticated multi-step reasoning is\ndeveloped through supervised fine-tuning and targeted reinforcement learning,\nguiding the system to generate logical, biologically coherent deductions. On\nbiological reasoning benchmarks including KEGG-based disease pathway prediction\n- where accuracy improves from 88% to 97% - and variant effect prediction,\nBioReason demonstrates an average 15% performance gain over strong\nsingle-modality baselines. BioReason reasons over unseen biological entities\nand articulates decision-making through interpretable, step-by-step biological\ntraces, offering a transformative approach for AI in biology that enables\ndeeper mechanistic insights and accelerates testable hypothesis generation from\ngenomic data. Data, code, and checkpoints are publicly available at\nhttps://github.com/bowang-lab/BioReason", "authors": ["Adibvafa Fallahpour", "Andrew Magnuson", "Purav Gupta", "Shihao Ma", "Jack Naimer", "Arnav Shah", "Haonan Duan", "Omar Ibrahim", "Hani Goodarzi", "Chris J. Maddison", "Bo Wang"], "published_date": "2025-05-29", "title_zh": "生物理性：激勵DNA-LLM模型內的多模態生物推理", "summary_zh": "從複雜基因組數據中解鎖深度且可解釋的生物學推理是一項阻礙科學發現的人工智慧挑戰。現有DNA基礎模型雖具備強大的序列表示能力，卻難以進行多步驟推理，且缺乏內在透明、生物學直觀的解釋。我們推出了BioReason，一種開創性的架構，首次將DNA基礎模型與大型語言模型(LLM)深度整合。這種新穎的連接使LLM能夠直接處理基因組信息並進行推理，作為一種基本輸入，從而促進了一種新的多模態生物學理解形式。BioReason複雜的多步驟推理是透過監督式微調和目標強化學習開發的，引導系統產生邏輯、生物學上連貫的推論。在生物學推理基準測試中，包括基於KEGG的疾病途徑預測（準確度從88%提高到97%）和變異效應預測，BioReason相較於強大的單模態基準模型，平均性能提升了15%。BioReason能夠推導未見過的生物實體，並通過可解釋、逐步的生物學蹤跡闡明決策過程，為生物學中的人工智慧提供了一種變革性的方法，能夠實現更深入的機制性見解，並加速從基因組數據中生成可驗證的假設。數據、代碼和檢查點可在https://github.com/bowang-lab/BioReason公開獲取。", "audio": "audios/2505.23579v1.mp3", "timestamp": "2025-05-30T07:18:13.573459"}
{"query": "Diffusion Model", "id": "2505.23721v1", "url": "http://arxiv.org/abs/2505.23721v1", "title": "DiffER: Categorical Diffusion for Chemical Retrosynthesis", "summary": "Methods for automatic chemical retrosynthesis have found recent success\nthrough the application of models traditionally built for natural language\nprocessing, primarily through transformer neural networks. These models have\ndemonstrated significant ability to translate between the SMILES encodings of\nchemical products and reactants, but are constrained as a result of their\nautoregressive nature. We propose DiffER, an alternative template-free method\nfor retrosynthesis prediction in the form of categorical diffusion, which\nallows the entire output SMILES sequence to be predicted in unison. We\nconstruct an ensemble of diffusion models which achieves state-of-the-art\nperformance for top-1 accuracy and competitive performance for top-3, top-5,\nand top-10 accuracy among template-free methods. We prove that DiffER is a\nstrong baseline for a new class of template-free model, capable of learning a\nvariety of synthetic techniques used in laboratory settings and outperforming a\nvariety of other template-free methods on top-k accuracy metrics. By\nconstructing an ensemble of categorical diffusion models with a novel length\nprediction component with variance, our method is able to approximately sample\nfrom the posterior distribution of reactants, producing results with strong\nmetrics of confidence and likelihood. Furthermore, our analyses demonstrate\nthat accurate prediction of the SMILES sequence length is key to further\nboosting the performance of categorical diffusion models.", "authors": ["Sean Current", "Ziqi Chen", "Daniel Adu-Ampratwum", "Xia Ning", "Srinivasan Parthasarathy"], "published_date": "2025-05-29", "title_zh": "DiffER：化學逆合成的類別擴散", "summary_zh": "自動化化學逆合成方法近期藉由應用傳統用於自然語言處理的模型，特別是轉換器神經網絡，獲得成功。這些模型已展現出在化學產物與反應物SMILES編碼之間進行轉換的顯著能力，但因其自迴歸性質而受到限制。我們提出DiffER，一種基於類別擴散的替代性無模板逆合成預測方法，允許同步預測整個輸出SMILES序列。我們構建了一個擴散模型集成，在無模板方法中，該集成在首位準確度方面達到最先進的性能，並在首三、首五和首十位準確度方面具有競爭力。我們證明DiffER是一種新型無模板模型的強大基準，能夠學習實驗室中使用的各種合成技術，並在首k位準確度指標上優於其他各種無模板方法。通過構建具有新型長度預測組件及其變異數的類別擴散模型集成，我們的模型能夠近似地從反應物的後驗分佈中取樣，產生具有高度可信度和可能性的結果。此外，我們的分析表明，準確預測SMILES序列長度是進一步提高類別擴散模型性能的關鍵。", "audio": "audios/2505.23721v1.mp3", "timestamp": "2025-05-30T07:18:21.000526"}
{"query": "AI", "id": "2505.23686v1", "url": "http://arxiv.org/abs/2505.23686v1", "title": "ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork", "summary": "Developing AI agents capable of collaborating with previously unseen partners\nis a fundamental generalization challenge in multi-agent learning, known as Ad\nHoc Teamwork (AHT). Existing AHT approaches typically adopt a two-stage\npipeline, where first, a fixed population of teammates is generated with the\nidea that they should be representative of the teammates that will be seen at\ndeployment time, and second, an AHT agent is trained to collaborate well with\nagents in the population. To date, the research community has focused on\ndesigning separate algorithms for each stage. This separation has led to\nalgorithms that generate teammate pools with limited coverage of possible\nbehaviors, and that ignore whether the generated teammates are easy to learn\nfrom for the AHT agent. Furthermore, algorithms for training AHT agents\ntypically treat the set of training teammates as static, thus attempting to\ngeneralize to previously unseen partner agents without assuming any control\nover the distribution of training teammates. In this paper, we present a\nunified framework for AHT by reformulating the problem as an open-ended\nlearning process between an ad hoc agent and an adversarial teammate generator.\nWe introduce ROTATE, a regret-driven, open-ended training algorithm that\nalternates between improving the AHT agent and generating teammates that probe\nits deficiencies. Extensive experiments across diverse AHT environments\ndemonstrate that ROTATE significantly outperforms baselines at generalizing to\nan unseen set of evaluation teammates, thus establishing a new standard for\nrobust and generalizable teamwork.", "authors": ["Caroline Wang", "Arrasy Rahman", "Jiaxun Cui", "Yoonchang Sung", "Peter Stone"], "published_date": "2025-05-29", "title_zh": "ROTATE：基於遺憾驅動的特設團隊開放式訓練", "summary_zh": "發展能與未曾謀面夥伴協作的AI體現了多代理人學習中的基本泛化挑戰，即特設團隊合作。現有方法通常採用兩階段流程：首先生成固定隊友群體，使其具備代表性；其次訓練特設團隊合作代理人，使其能與群體中的代理人良好協作。目前研究主要關注於為每個階段設計獨立算法。此分離導致隊友池覆蓋行為有限，且忽略了生成隊友是否易於學習。此外，訓練算法通常將訓練隊友視為靜態，在未控制訓練隊友分布的情況下泛化至未見過的夥伴。本研究提出一個統一的框架，將此問題重塑為特設代理人與對抗性隊友生成器之間的開放式學習過程。我們引入ROTATE，一種基於遺憾驅動的開放式訓練算法，交替改進特設代理人並生成能探測其缺陷的隊友。在多個特設團隊合作環境中的實驗表明，ROTATE在泛化至未見過的評估隊友集合方面顯著優於基準，為穩健且可泛化的團隊合作建立新標準。", "audio": "audios/2505.23686v1.mp3", "timestamp": "2025-05-30T08:24:35.879413"}
{"query": "Foundation Model", "id": "2505.23569v1", "url": "http://arxiv.org/abs/2505.23569v1", "title": "Maximum Likelihood Learning of Latent Dynamics Without Reconstruction", "summary": "We introduce a novel unsupervised learning method for time series data with\nlatent dynamical structure: the recognition-parametrized Gaussian state space\nmodel (RP-GSSM). The RP-GSSM is a probabilistic model that learns Markovian\nGaussian latents explaining statistical dependence between observations at\ndifferent time steps, combining the intuition of contrastive methods with the\nflexible tools of probabilistic generative models. Unlike contrastive\napproaches, the RP-GSSM is a valid probabilistic model learned via maximum\nlikelihood. Unlike generative approaches, the RP-GSSM has no need for an\nexplicit network mapping from latents to observations, allowing it to focus\nmodel capacity on inference of latents. The model is both tractable and\nexpressive: it admits exact inference thanks to its jointly Gaussian latent\nprior, while maintaining expressivity with an arbitrarily nonlinear neural\nnetwork link between observations and latents. These qualities allow the\nRP-GSSM to learn task-relevant latents without ad-hoc regularization, auxiliary\nlosses, or optimizer scheduling. We show how this approach outperforms\nalternatives on problems that include learning nonlinear stochastic dynamics\nfrom video, with or without background distractors. Our results position the\nRP-GSSM as a useful foundation model for a variety of downstream applications.", "authors": ["Samo Hromadka", "Kai Biegun", "Lior Fox", "James Heald", "Maneesh Sahani"], "published_date": "2025-05-29", "title_zh": "無重構潛在動態之最大似然學習", "summary_zh": "本研究提出一種新穎的非監督學習方法，用於具潛在動態結構的時間序列資料：識別參數化高斯狀態空間模型(RP-GSSM)。 RP-GSSM是一種概率模型，學習馬可夫高斯潛變數以解釋不同時間步觀測值之間的統計依賴性，結合了對比方法的直覺和概率生成模型的靈活性。與對比方法不同，RP-GSSM是通過最大似然學習的有效概率模型。與生成方法不同，RP-GSSM不需要從潛變數到觀測值的顯式網路映射，從而可以將模型容量集中於潛變數的推斷。該模型兼具易處理性和表達性：由於其聯合高斯潛在先驗，因此可以進行精確推斷，同時通過觀測值和潛變數之間任意非線性神經網路連結來保持表達性。這些特性使RP-GSSM能夠在沒有臨時正則化、輔助損失或優化器排程的情況下學習與任務相關的潛變數。研究表明，在包括從影片中學習非線性隨機動態（有或沒有背景干擾）的問題上，此方法優於其他方法。研究結果將RP-GSSM定位為各種下游應用程式的有用基礎模型。", "audio": "audios/2505.23569v1.mp3", "timestamp": "2025-05-30T08:24:43.638968"}
{"query": "Diffusion Model", "id": "2505.23675v1", "url": "http://arxiv.org/abs/2505.23675v1", "title": "ImmunoDiff: A Diffusion Model for Immunotherapy Response Prediction in Lung Cancer", "summary": "Accurately predicting immunotherapy response in Non-Small Cell Lung Cancer\n(NSCLC) remains a critical unmet need. Existing radiomics and deep\nlearning-based predictive models rely primarily on pre-treatment imaging to\npredict categorical response outcomes, limiting their ability to capture the\ncomplex morphological and textural transformations induced by immunotherapy.\nThis study introduces ImmunoDiff, an anatomy-aware diffusion model designed to\nsynthesize post-treatment CT scans from baseline imaging while incorporating\nclinically relevant constraints. The proposed framework integrates anatomical\npriors, specifically lobar and vascular structures, to enhance fidelity in CT\nsynthesis. Additionally, we introduce a novel cbi-Adapter, a conditioning\nmodule that ensures pairwise-consistent multimodal integration of imaging and\nclinical data embeddings, to refine the generative process. Additionally, a\nclinical variable conditioning mechanism is introduced, leveraging demographic\ndata, blood-based biomarkers, and PD-L1 expression to refine the generative\nprocess. Evaluations on an in-house NSCLC cohort treated with immune checkpoint\ninhibitors demonstrate a 21.24% improvement in balanced accuracy for response\nprediction and a 0.03 increase in c-index for survival prediction. Code will be\nreleased soon.", "authors": ["Moinak Bhattacharya", "Judy Huang", "Amna F. Sher", "Gagandeep Singh", "Chao Chen", "Prateek Prasanna"], "published_date": "2025-05-29", "title_zh": "ImmunoDiff：一種用於肺癌免疫療法反應預測的擴散模型", "summary_zh": "準確預測非小細胞肺癌(NSCLC)免疫療法反應仍是重要需求。現有基於影像基因組學和深度學習的預測模型主要依賴治療前影像，難以捕捉免疫療法引起的複雜形態和紋理變化。本研究提出ImmunoDiff，一種解剖感知擴散模型，旨在基於基線影像合成治療後CT掃描，同時納入臨床相關約束。該框架整合解剖先驗知識，如肺葉和血管結構，以提高CT合成的保真度。此外，引入新型cbi-Adapter，確保影像和臨床數據嵌入的成對一致多模態整合，並利用人口統計學數據、血液生物標記和PD-L1表達來細化生成過程。在內部NSCLC隊列的評估表明，反應預測的平衡準確率提高21.24%，生存預測的c-index提高0.03。代碼即將發布。", "audio": "audios/2505.23675v1.mp3", "timestamp": "2025-05-30T08:24:50.812172"}
{"query": "AI", "id": "2505.23672v1", "url": "http://arxiv.org/abs/2505.23672v1", "title": "Position Dependent Prediction Combination For Intra-Frame Video Coding", "summary": "Intra-frame prediction in the High Efficiency Video Coding (HEVC) standard\ncan be empirically improved by applying sets of recursive two-dimensional\nfilters to the predicted values. However, this approach does not allow (or\ncomplicates significantly) the parallel computation of pixel predictions. In\nthis work we analyze why the recursive filters are effective, and use the\nresults to derive sets of non-recursive predictors that have superior\nperformance. We present an extension to HEVC intra prediction that combines\nvalues predicted using non-filtered and filtered (smoothed) reference samples,\ndepending on the prediction mode, and block size. Simulations using the HEVC\ncommon test conditions show that a 2.0% bit rate average reduction can be\nachieved compared to HEVC, for All Intra (AI) configurations.", "authors": ["Amir Said", "Xin Zhao", "Marta Karczewicz", "Jianle Chen", "Feng Zou"], "published_date": "2025-05-29", "title_zh": "幀內視訊編碼之位置相依預測組合", "summary_zh": "高效視訊編碼(HEVC)的幀內預測可透過遞迴二維濾波器提升，然此法不利於像素預測的平行運算。本文分析遞迴濾波器的效用，並據此推導出性能更優越的非遞迴預測器。我們提出一種HEVC幀內預測擴展，依據預測模式和區塊大小，結合使用未經濾波和經過濾波(平滑化)的參考樣本預測值。在HEVC通用測試條件下，全幀內(AI)配置的模擬結果顯示，相較於HEVC，平均位元率降低了2.0%。", "audio": "audios/2505.23672v1.mp3", "timestamp": "2025-05-30T09:19:36.997469"}
{"query": "Foundation Model", "id": "2505.23400v1", "url": "http://arxiv.org/abs/2505.23400v1", "title": "Bridging Geometric and Semantic Foundation Models for Generalized Monocular Depth Estimation", "summary": "We present Bridging Geometric and Semantic (BriGeS), an effective method that\nfuses geometric and semantic information within foundation models to enhance\nMonocular Depth Estimation (MDE). Central to BriGeS is the Bridging Gate, which\nintegrates the complementary strengths of depth and segmentation foundation\nmodels. This integration is further refined by our Attention Temperature\nScaling technique. It finely adjusts the focus of the attention mechanisms to\nprevent over-concentration on specific features, thus ensuring balanced\nperformance across diverse inputs. BriGeS capitalizes on pre-trained foundation\nmodels and adopts a strategy that focuses on training only the Bridging Gate.\nThis method significantly reduces resource demands and training time while\nmaintaining the model's ability to generalize effectively. Extensive\nexperiments across multiple challenging datasets demonstrate that BriGeS\noutperforms state-of-the-art methods in MDE for complex scenes, effectively\nhandling intricate structures and overlapping objects.", "authors": ["Sanggyun Ma", "Wonjoon Choi", "Jihun Park", "Jaeyeul Kim", "Seunghun Lee", "Jiwan Seo", "Sunghoon Im"], "published_date": "2025-05-29", "title_zh": "幾何與語義基礎模型橋接：廣義單目深度估計", "summary_zh": "本文提出橋接幾何與語義(BriGeS)方法，融合基礎模型中的幾何與語義信息，以提升單眼深度估計(MDE)效能。BriGeS的核心為橋接閘，整合深度與分割基礎模型的互補優勢。注意力溫度縮放技術進一步優化此整合，精細調整注意力機制，避免過度集中於特定特徵，確保多樣輸入的均衡表現。BriGeS利用預訓練基礎模型，僅訓練橋接閘，大幅降低資源需求與訓練時間，同時維持模型泛化能力。大量實驗證明，BriGeS在複雜場景的MDE中，超越現有技術，能有效處理複雜結構與重疊物體。", "audio": "audios/2505.23400v1.mp3", "timestamp": "2025-05-30T09:19:44.163821"}
{"query": "Diffusion Model", "id": "2505.23661v1", "url": "http://arxiv.org/abs/2505.23661v1", "title": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation", "summary": "In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni.", "authors": ["Size Wu", "Zhonghua Wu", "Zerui Gong", "Qingyi Tao", "Sheng Jin", "Qinyue Li", "Wei Li", "Chen Change Loy"], "published_date": "2025-05-29", "title_zh": "OpenUni：統一多模態理解與生成之簡明基準模型", "summary_zh": "本研究提出OpenUni，一個簡潔、輕量且完全開源的多模態理解與生成基線模型。其靈感來自於統一模型學習的普遍做法，採用高效的訓練策略，透過可學習的查詢和輕量級的Transformer連接器，橋接現成的多模態大型語言模型(LLM)和擴散模型，從而最小化訓練複雜度和開銷。OpenUni以極簡的架構，證明了其能力：一，生成高品質且符合指令的圖像；二，在GenEval、DPG-Bench和WISE等標準基準測試中，僅啟用11億和31億參數，便取得卓越的效能。為支持開放研究和社群發展，所有模型權重、訓練程式碼以及精心策劃的訓練數據集(包含2300萬個圖像文本對)均已釋出於https://github.com/wusize/OpenUni。", "audio": "audios/2505.23661v1.mp3", "timestamp": "2025-05-30T09:19:51.624063"}
{"query": "AI", "id": "2505.23655v1", "url": "http://arxiv.org/abs/2505.23655v1", "title": "Keyed Chaotic Tensor Transformations for Secure And Attributable Neural Inference", "summary": "This work introduces a novel framework for secure and privacy-preserving\nneural network inference based on keyed chaotic dynamical transformations. The\nproposed method applies a deterministic, cryptographically seeded chaotic\nsystem to tensors, producing non-invertible, user-specific transformations that\nenable authenticated inference, tensor-level watermarking, and data\nattribution. This framework offers a scalable and lightweight alternative to\nconventional cryptographic techniques, and establishes a new direction for\ntensor-level security in AI systems.", "authors": ["Peter David Fagan"], "published_date": "2025-05-29", "title_zh": "用於安全且可歸因神經推論的具鍵控混沌張量轉換", "summary_zh": "本研究提出基於密鑰混沌動力轉換的安全且保護隱私的神經網路推論新框架。此方法利用密碼學種子的確定性混沌系統轉換張量，產生不可逆、使用者指定的轉換，實現身份驗證推論、張量級浮水印和數據歸屬。相較於傳統密碼技術，此框架提供可擴展且輕量化的替代方案，為人工智慧系統中的張量級安全建立新方向。", "audio": "audios/2505.23655v1.mp3", "timestamp": "2025-05-30T10:19:25.766648"}
{"query": "Foundation Model", "id": "2505.23354v1", "url": "http://arxiv.org/abs/2505.23354v1", "title": "Representing local protein environments with atomistic foundation models", "summary": "The local structure of a protein strongly impacts its function and\ninteractions with other molecules. Therefore, a concise, informative\nrepresentation of a local protein environment is essential for modeling and\ndesigning proteins and biomolecular interactions. However, these environments'\nextensive structural and chemical variability makes them challenging to model,\nand such representations remain under-explored. In this work, we propose a\nnovel representation for a local protein environment derived from the\nintermediate features of atomistic foundation models (AFMs). We demonstrate\nthat this embedding effectively captures both local structure (e.g., secondary\nmotifs), and chemical features (e.g., amino-acid identity and protonation\nstate). We further show that the AFM-derived representation space exhibits\nmeaningful structure, enabling the construction of data-driven priors over the\ndistribution of biomolecular environments. Finally, in the context of\nbiomolecular NMR spectroscopy, we demonstrate that the proposed representations\nenable a first-of-its-kind physics-informed chemical shift predictor that\nachieves state-of-the-art accuracy. Our results demonstrate the surprising\neffectiveness of atomistic foundation models and their emergent representations\nfor protein modeling beyond traditional molecular simulations. We believe this\nwill open new lines of work in constructing effective functional\nrepresentations for protein environments.", "authors": ["Meital Bojan", "Sanketh Vedula", "Advaith Maddipatla", "Nadav Bojan Sellam", "Federico Napoli", "Paul Schanda", "Alex M. Bronstein"], "published_date": "2025-05-29", "title_zh": "以原子基礎模型表徵局部蛋白質環境", "summary_zh": "蛋白質局部結構顯著影響其功能及分子間交互作用。簡潔且資訊豐富的局部蛋白質環境表示對於蛋白質建模與設計至關重要，但因其結構和化學多樣性使其建模具挑戰性。本研究提出一種基於原子級基礎模型(AFMs)中間特徵的新型局部蛋白質環境表示法，能有效捕捉局部結構（如二級結構）和化學特徵（如胺基酸特性及質子化狀態）。該表示空間呈現有意義的結構，可用於構建生物分子環境分佈的數據驅動先驗知識。在生物分子核磁共振波譜學中，此表示法實現了首創的物理訊息化學位移預測器，達到最先進的準確度。結果表明原子級基礎模型及其湧現的表示在蛋白質建模中具有超越傳統分子模擬的效力，有望為構建有效的蛋白質環境功能表示開闢新方向。", "audio": "audios/2505.23354v1.mp3", "timestamp": "2025-05-30T10:19:31.153386"}
{"query": "Diffusion Model", "id": "2505.23652v1", "url": "http://arxiv.org/abs/2505.23652v1", "title": "Optimization-Free Diffusion Model -- A Perturbation Theory Approach", "summary": "Diffusion models have emerged as a powerful framework in generative modeling,\ntypically relying on optimizing neural networks to estimate the score function\nvia forward SDE simulations. In this work, we propose an alternative method\nthat is both optimization-free and forward SDE-free. By expanding the score\nfunction in a sparse set of eigenbasis of the backward Kolmogorov operator\nassociated with the diffusion process, we reformulate score estimation as the\nsolution to a linear system, avoiding iterative optimization and time-dependent\nsample generation. We analyze the approximation error using perturbation theory\nand demonstrate the effectiveness of our method on high-dimensional Boltzmann\ndistributions and real-world datasets.", "authors": ["Yuehaw Khoo", "Mathias Oster", "Yifan Peng"], "published_date": "2025-05-29", "title_zh": "無須優化的擴散模型：一種微擾理論方法", "summary_zh": "擴散模型已成為生成建模的有力工具，通常依賴於優化神經網路，透過正向隨機微分方程模擬來估計得分函數。本研究提出一種無需優化且無需正向隨機微分方程的新方法。透過在與擴散過程相關的後向柯爾莫哥洛夫算子的稀疏特徵基中展開得分函數，我們將得分估計重新表述為線性系統的解，避免了迭代優化和時間相關的樣本生成。我們利用擾動理論分析了近似誤差，並在高維玻爾茲曼分布和真實世界數據集上驗證了該方法的有效性。", "audio": "audios/2505.23652v1.mp3", "timestamp": "2025-05-30T10:19:35.376746"}
{"query": "AI", "id": "2505.23643v1", "url": "http://arxiv.org/abs/2505.23643v1", "title": "Securing AI Agents with Information-Flow Control", "summary": "As AI agents become increasingly autonomous and capable, ensuring their\nsecurity against vulnerabilities such as prompt injection becomes critical.\nThis paper explores the use of information-flow control (IFC) to provide\nsecurity guarantees for AI agents. We present a formal model to reason about\nthe security and expressiveness of agent planners. Using this model, we\ncharacterize the class of properties enforceable by dynamic taint-tracking and\nconstruct a taxonomy of tasks to evaluate security and utility trade-offs of\nplanner designs. Informed by this exploration, we present Fides, a planner that\ntracks confidentiality and integrity labels, deterministically enforces\nsecurity policies, and introduces novel primitives for selectively hiding\ninformation. Its evaluation in AgentDojo demonstrates that this approach\nbroadens the range of tasks that can be securely accomplished. A tutorial to\nwalk readers through the the concepts introduced in the paper can be found at\nhttps://github.com/microsoft/fides", "authors": ["Manuel Costa", "Boris Köpf", "Aashish Kolluri", "Andrew Paverd", "Mark Russinovich", "Ahmed Salem", "Shruti Tople", "Lukas Wutschitz", "Santiago Zanella-Béguelin"], "published_date": "2025-05-29", "title_zh": "以資訊流控制保護人工智慧代理", "summary_zh": "隨著人工智慧代理日趨自主和強大，確保其安全性以抵禦提示注入等漏洞至關重要。本研究探討資訊流控制在保障人工智慧代理安全性方面的應用。我們提出一個形式化模型，用於推理代理規劃器的安全性和表達性。基於此模型，我們刻畫了動態污點追蹤可執行的屬性類別，並構建任務分類體系，以評估規劃器設計在安全性和效用之間的權衡。在此基礎上，我們提出了Fides，一種追蹤機密性和完整性標籤的規劃器，能確定性地執行安全策略，並引入選擇性隱藏資訊的新型原語。在AgentDojo中的評估表明，此方法擴展了可安全完成的任務範圍。相關概念教學可在https://github.com/microsoft/fides 獲取。", "audio": "audios/2505.23643v1.mp3", "timestamp": "2025-05-30T11:15:25.906792"}
{"query": "Foundation Model", "id": "2505.23292v1", "url": "http://arxiv.org/abs/2505.23292v1", "title": "Federated Unsupervised Semantic Segmentation", "summary": "This work explores the application of Federated Learning (FL) in Unsupervised\nSemantic image Segmentation (USS). Recent USS methods extract pixel-level\nfeatures using frozen visual foundation models and refine them through\nself-supervised objectives that encourage semantic grouping. These features are\nthen grouped to semantic clusters to produce segmentation masks. Extending\nthese ideas to federated settings requires feature representation and cluster\ncentroid alignment across distributed clients -- an inherently difficult task\nunder heterogeneous data distributions in the absence of supervision. To\naddress this, we propose FUSS Federated Unsupervised image Semantic\nSegmentation) which is, to our knowledge, the first framework to enable fully\ndecentralized, label-free semantic segmentation training. FUSS introduces novel\nfederation strategies that promote global consistency in feature and prototype\nspace, jointly optimizing local segmentation heads and shared semantic\ncentroids. Experiments on both benchmark and real-world datasets, including\nbinary and multi-class segmentation tasks, show that FUSS consistently\noutperforms local-only client trainings as well as extensions of classical FL\nalgorithms under varying client data distributions. To support reproducibility,\nfull code will be released upon manuscript acceptance.", "authors": ["Evangelos Charalampakis", "Vasileios Mygdalis", "Ioannis Pitas"], "published_date": "2025-05-29", "title_zh": "聯邦無監督語義分割", "summary_zh": "本研究探討聯邦學習於非監督式語義圖像分割之應用。現有方法利用凍結的視覺基礎模型提取像素級特徵，並透過自我監督目標精煉，促使語義分組。這些特徵隨後被分組至語義叢集以生成分割遮罩。將此概念擴展至聯邦環境需要在分散式客戶端之間對齊特徵表示和叢集中心點，在缺乏監督且數據分佈異質的情況下，這項任務極具挑戰性。為此，我們提出FUSS（聯邦非監督圖像語義分割），據我們所知，此為首個實現完全分散式、無標籤語義分割訓練的框架。FUSS引入新穎的聯邦策略，促進特徵和原型空間中的全局一致性，共同優化局部分割頭部和共享語義中心點。在基準測試和真實世界數據集（包括二元和多類分割任務）上的實驗表明，在不同的客戶端數據分佈下，FUSS始終優於僅限本地客戶端的訓練以及傳統聯邦學習算法的擴展。完整程式碼將在論文接受後發布，以支持可重複性。", "audio": "audios/2505.23292v1.mp3", "timestamp": "2025-05-30T11:15:32.326257"}
{"query": "Diffusion Model", "id": "2505.23614v1", "url": "http://arxiv.org/abs/2505.23614v1", "title": "Inference-time Scaling of Diffusion Models through Classical Search", "summary": "Classical search algorithms have long underpinned modern artificial\nintelligence. In this work, we tackle the challenge of inference-time control\nin diffusion models -- adapting generated outputs to meet diverse test-time\nobjectives -- using principles from classical search. We propose a general\nframework that orchestrates local and global search to efficiently navigate the\ngenerative space. It employs a theoretically grounded local search via annealed\nLangevin MCMC and performs compute-efficient global exploration using\nbreadth-first and depth-first tree search. We evaluate our approach on a range\nof challenging domains, including planning, offline reinforcement learning, and\nimage generation. Across all tasks, we observe significant gains in both\nperformance and efficiency. These results show that classical search provides a\nprincipled and practical foundation for inference-time scaling in diffusion\nmodels. Project page at diffusion-inference-scaling.github.io.", "authors": ["Xiangcheng Zhang", "Haowei Lin", "Haotian Ye", "James Zou", "Jianzhu Ma", "Yitao Liang", "Yilun Du"], "published_date": "2025-05-29", "title_zh": "藉由古典搜尋於推論時調整擴散模型", "summary_zh": "古典搜尋演算法是現代人工智慧的基石。本研究旨在解決擴散模型中推論時控制的挑戰，即調整生成輸出以滿足不同的測試時目標，並採用古典搜尋的原理。我們提出了一個通用框架，協調局部和全域搜尋，以有效導航生成空間。該框架利用經退火的朗之萬馬可夫鏈蒙地卡羅方法進行理論上有根據的局部搜尋，並使用廣度優先和深度優先樹搜尋執行具備運算效率的全域探索。我們在包括規劃、離線強化學習和圖像生成等一系列具挑戰性的領域中評估了此方法。在所有任務中，我們觀察到效能和效率均顯著提升。這些結果表明，古典搜尋為擴散模型中推論時的擴展提供了一個有原則且實用的基礎。", "audio": "audios/2505.23614v1.mp3", "timestamp": "2025-05-30T11:15:37.502759"}
{"query": "AI", "id": "2505.23634v1", "url": "http://arxiv.org/abs/2505.23634v1", "title": "MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment", "summary": "The model context protocol (MCP) has been widely adapted as an open standard\nenabling the seamless integration of generative AI agents. However, recent work\nhas shown the MCP is susceptible to retrieval-based \"falsely benign\" attacks\n(FBAs), allowing malicious system access and credential theft, but requiring\nthat users download compromised files directly to their systems. Herein, we\nshow that the threat model of MCP-based attacks is significantly broader than\npreviously thought, i.e., attackers need only post malicious content online to\ndeceive MCP agents into carrying out their attacks on unsuspecting victims'\nsystems.\n  To improve alignment guardrails against such attacks, we introduce a new MCP\ndataset of FBAs and (truly) benign samples to explore the effectiveness of\ndirect preference optimization (DPO) for the refusal training of large language\nmodels (LLMs). While DPO improves model guardrails against such attacks, we\nshow that the efficacy of refusal learning varies drastically depending on the\nmodel's original post-training alignment scheme--e.g., GRPO-based LLMs learn to\nrefuse extremely poorly. Thus, to further improve FBA refusals, we introduce\nRetrieval Augmented Generation for Preference alignment (RAG-Pref), a novel\npreference alignment strategy based on RAG. We show that RAG-Pref significantly\nimproves the ability of LLMs to refuse FBAs, particularly when combined with\nDPO alignment, thus drastically improving guardrails against MCP-based attacks.", "authors": ["John Halloran"], "published_date": "2025-05-29", "title_zh": "MCP 安全培訓：利用改良偏好對齊學習拒絕虛假良性 MCP 漏洞", "summary_zh": "模型上下文協定(MCP)廣泛應用於生成式AI代理的整合，但研究顯示其易受基於檢索的偽良性攻擊(FBA)影響，導致惡意系統存取和憑證竊取。本文指出MCP攻擊的威脅模型比先前認為的更廣泛，攻擊者只需於線上發布惡意內容，即可誘騙MCP代理對受害者系統發起攻擊。\n\n為提升防禦此類攻擊的對齊防護欄，我們提出一個新的MCP資料集，包含FBA和良性樣本，以研究直接偏好最佳化(DPO)在大型語言模型(LLM)拒絕訓練中的有效性。儘管DPO能改善模型防護欄，但拒絕學習的效能取決於模型原有的後訓練對齊方案。為進一步改善FBA拒絕能力，我們提出基於檢索增強生成(RAG)的偏好對齊策略(RAG-Pref)。研究顯示，RAG-Pref能顯著提升LLM拒絕FBA的能力，尤其與DPO對齊結合使用時，能大幅改善對MCP攻擊的防護。", "audio": "audios/2505.23634v1.mp3", "timestamp": "2025-05-30T12:37:31.778026"}
{"query": "Foundation Model", "id": "2505.23266v1", "url": "http://arxiv.org/abs/2505.23266v1", "title": "Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion", "summary": "We present Adversarial Object Fusion (AdvOF), a novel attack framework\ntargeting vision-and-language navigation (VLN) agents in service-oriented\nenvironments by generating adversarial 3D objects. While foundational models\nlike Large Language Models (LLMs) and Vision Language Models (VLMs) have\nenhanced service-oriented navigation systems through improved perception and\ndecision-making, their integration introduces vulnerabilities in\nmission-critical service workflows. Existing adversarial attacks fail to\naddress service computing contexts, where reliability and quality-of-service\n(QoS) are paramount. We utilize AdvOF to investigate and explore the impact of\nadversarial environments on the VLM-based perception module of VLN agents. In\nparticular, AdvOF first precisely aggregates and aligns the victim object\npositions in both 2D and 3D space, defining and rendering adversarial objects.\nThen, we collaboratively optimize the adversarial object with regularization\nbetween the adversarial and victim object across physical properties and VLM\nperceptions. Through assigning importance weights to varying views, the\noptimization is processed stably and multi-viewedly by iterative fusions from\nlocal updates and justifications. Our extensive evaluations demonstrate AdvOF\ncan effectively degrade agent performance under adversarial conditions while\nmaintaining minimal interference with normal navigation tasks. This work\nadvances the understanding of service security in VLM-powered navigation\nsystems, providing computational foundations for robust service composition in\nphysical-world deployments.", "authors": ["Chunlong Xie", "Jialing He", "Shangwei Guo", "Jiacheng Wang", "Shudong Zhang", "Tianwei Zhang", "Tao Xiang"], "published_date": "2025-05-29", "title_zh": "基於對抗性物件融合擾亂視覺語言模型驅動的導航服務", "summary_zh": "本研究提出對抗性物件融合(AdvOF)框架，旨在透過生成對抗性3D物件，攻擊以視覺語言導航(VLN)代理於服務導向環境的系統。大型語言模型(LLM)與視覺語言模型(VLM)雖提升了服務導向導航系統的感知與決策能力，但也引入了任務關鍵型服務流程的漏洞。現有攻擊未能針對服務計算情境，此情境對可靠性與服務品質(QoS)要求嚴格。AdvOF用於探討對抗性環境對VLM-based VLN代理感知模組的影響。AdvOF精確聚合並對齊2D與3D空間中的目標物件位置，定義並渲染對抗性物件，協同優化對抗性物件，並透過物理屬性與VLM感知間的正則化實現穩定、多視角的迭代融合。實驗結果表明，AdvOF可在對抗性環境下有效降低代理效能，同時最小化對正常導航任務的干擾。本研究推進了對VLM驅動導航系統的服務安全理解，為實體部署中穩健的服務組合奠定計算基礎。", "audio": "audios/2505.23266v1.mp3", "timestamp": "2025-05-30T12:37:40.725543"}
{"query": "Diffusion Model", "id": "2505.23606v1", "url": "http://arxiv.org/abs/2505.23606v1", "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model", "summary": "Unified generation models aim to handle diverse tasks across modalities --\nsuch as text generation, image generation, and vision-language reasoning --\nwithin a single architecture and decoding paradigm. Autoregressive unified\nmodels suffer from slow inference due to sequential decoding, and\nnon-autoregressive unified models suffer from weak generalization due to\nlimited pretrained backbones. We introduce Muddit, a unified discrete diffusion\ntransformer that enables fast and parallel generation across both text and\nimage modalities. Unlike prior unified diffusion models trained from scratch,\nMuddit integrates strong visual priors from a pretrained text-to-image backbone\nwith a lightweight text decoder, enabling flexible and high-quality multimodal\ngeneration under a unified architecture. Empirical results show that Muddit\nachieves competitive or superior performance compared to significantly larger\nautoregressive models in both quality and efficiency. The work highlights the\npotential of purely discrete diffusion, when equipped with strong visual\npriors, as a scalable and effective backbone for unified generation.", "authors": ["Qingyu Shi", "Jinbin Bai", "Zhuoran Zhao", "Wenhao Chai", "Kaidong Yu", "Jianzong Wu", "Shuangyong Song", "Yunhai Tong", "Xiangtai Li", "Xuelong Li", "Shuicheng Yan"], "published_date": "2025-05-29", "title_zh": "Muddit：以統一離散擴散模型解放超越文本到圖像的生成", "summary_zh": "統一生成模型旨在單一架構和解碼範式中處理跨模態的多樣任務，如文本生成、圖像生成及視覺語言推理。自迴歸統一模型因循序解碼而推論緩慢，非自迴歸統一模型則因預訓練骨幹網絡有限而泛化能力弱。本研究提出Muddit，一種統一離散擴散轉換器，可在文本和圖像模態中實現快速並行的生成。與從頭訓練的先前統一擴散模型不同，Muddit整合了預訓練文本到圖像骨幹網絡的強大視覺先驗，以及輕量級文本解碼器，從而在統一架構下實現靈活且高品質的多模態生成。實驗結果表明，Muddit在質量和效率方面均可達到與規模更大的自迴歸模型相當或更優越的性能。本研究突顯了純粹離散擴散在配備強大視覺先驗時，作為統一生成可擴展且有效骨幹網絡的潛力。", "audio": "audios/2505.23606v1.mp3", "timestamp": "2025-05-30T12:37:48.386133"}
{"query": "AI", "id": "2505.23631v1", "url": "http://arxiv.org/abs/2505.23631v1", "title": "Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education", "summary": "Assessing student depression in sensitive environments like special education\nis challenging. Standardized questionnaires may not fully reflect students'\ntrue situations. Furthermore, automated methods often falter with rich student\nnarratives, lacking the crucial, individualized insights stemming from\nteachers' empathetic connections with students. Existing methods often fail to\naddress this ambiguity or effectively integrate educator understanding. To\naddress these limitations by fostering a synergistic human-AI collaboration,\nthis paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered\nAI framework for transparent and socially responsible depression severity\nassessment. Our approach uniquely integrates student narrative text with a\nteacher-derived, 9-dimensional \"Empathy Vector\" (EV), its dimensions guided by\nthe PHQ-9 framework,to explicitly translate tacit empathetic insight into a\nstructured AI input enhancing rather than replacing human judgment. Rigorous\nexperiments optimized the multimodal fusion, text representation, and\nclassification architecture, achieving 82.74% accuracy for 7-level severity\nclassification. This work demonstrates a path toward more responsible and\nethical affective computing by structurally embedding human empathy", "authors": ["Boning Zhao"], "published_date": "2025-05-29", "title_zh": "人類同理心作為編碼器：特殊教育中人工智慧輔助的憂鬱症評估", "summary_zh": "在特殊教育等敏感環境中評估學生憂鬱症具挑戰性。標準化問卷可能無法完整反映學生真實情況，且自動化方法難以處理豐富的學生敘事，缺乏教師同理心產生的關鍵個人化見解。為解決此限制，本研究提出以人為本的AI框架HEAE，用於透明且具社會責任的憂鬱症嚴重程度評估。HEAE整合學生敘事文本及教師提供的九維「同理心向量」，將隱性同理心轉化為結構化AI輸入，提升而非取代人類判斷。實驗結果顯示，此方法在七級嚴重程度分類中達到82.74%的準確度，展現了透過結構性嵌入人類同理心，實現更負責任和合乎倫理的情感運算之路徑。", "audio": "audios/2505.23631v1.mp3", "timestamp": "2025-05-30T13:31:18.457834"}
{"query": "Foundation Model", "id": "2505.23195v1", "url": "http://arxiv.org/abs/2505.23195v1", "title": "Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning", "summary": "Scaling laws motivate the development of Time Series Foundation Models\n(TSFMs) that pre-train vast parameters and achieve remarkable zero-shot\nforecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot\nconsistently outperform smaller, specialized models trained on full-shot\ndownstream data. A key question is how to realize effective adaptation of TSFMs\nfor a target forecasting task. Through empirical studies on various TSFMs, the\npre-trained models often exhibit inherent sparsity and redundancy in\ncomputation, suggesting that TSFMs have learned to activate task-relevant\nnetwork substructures to accommodate diverse forecasting tasks. To preserve\nthis valuable prior knowledge, we propose a structured pruning method to\nregularize the subsequent fine-tuning process by focusing it on a more relevant\nand compact parameter space. Extensive experiments on seven TSFMs and six\nbenchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly\nimproves forecasting performance compared to fine-tuning original models. This\n\"prune-then-finetune\" paradigm often enables TSFMs to achieve state-of-the-art\nperformance and surpass strong specialized baselines.", "authors": ["Lifan Zhao", "Yanyan Shen", "Zhaoyang Liu", "Xue Wang", "Jiaji Deng"], "published_date": "2025-05-29", "title_zh": "少即是多：透過結構化剪枝釋放時間序列基礎模型的專業化", "summary_zh": "時序資料基礎模型(TSFMs)具備大規模參數和卓越的零樣本預測能力。然而，微調後，TSFMs表現通常不如在完整下游資料上訓練的小型專用模型。研究顯示，TSFMs在運算中存在內在稀疏性和冗餘，表明其已學習啟動與任務相關的網路子結構以適應各種預測任務。為保留此先驗知識，本文提出一種結構化剪枝方法，藉由專注於更相關且精簡的參數空間來規範後續的微調過程。大量實驗證明，相較於微調原始模型，微調較小的剪枝TSFM能顯著提升預測性能。這種「剪枝後微調」模式通常使TSFMs達到最先進的性能，並超越強大的專用基準模型。", "audio": "audios/2505.23195v1.mp3", "timestamp": "2025-05-30T13:31:27.694031"}
{"query": "Diffusion Model", "id": "2505.23527v1", "url": "http://arxiv.org/abs/2505.23527v1", "title": "Normalizing Flows are Capable Models for RL", "summary": "Modern reinforcement learning (RL) algorithms have found success by using\npowerful probabilistic models, such as transformers, energy-based models, and\ndiffusion/flow-based models. To this end, RL researchers often choose to pay\nthe price of accommodating these models into their algorithms -- diffusion\nmodels are expressive, but are computationally intensive due to their reliance\non solving differential equations, while autoregressive transformer models are\nscalable but typically require learning discrete representations. Normalizing\nflows (NFs), by contrast, seem to provide an appealing alternative, as they\nenable likelihoods and sampling without solving differential equations or\nautoregressive architectures. However, their potential in RL has received\nlimited attention, partly due to the prevailing belief that normalizing flows\nlack sufficient expressivity. We show that this is not the case. Building on\nrecent work in NFs, we propose a single NF architecture which integrates\nseamlessly into RL algorithms, serving as a policy, Q-function, and occupancy\nmeasure. Our approach leads to much simpler algorithms, and achieves higher\nperformance in imitation learning, offline, goal conditioned RL and\nunsupervised RL.", "authors": ["Raj Ghugare", "Benjamin Eysenbach"], "published_date": "2025-05-29", "title_zh": "正規化流是強化學習中具備能力的模型", "summary_zh": "現代強化學習演算法利用變換器、基於能量的模型和基於擴散/流的模型等強大的機率模型獲得成功。 儘管擴散模型具表現力但計算密集，自迴歸變換器模型具可擴展性但通常需要學習離散表示，正規化流(NF)提供了一種吸引人的替代方案，它無需解微分方程式或使用自迴歸架構即可實現可能性和抽樣。 然而，由於人們普遍認為正規化流缺乏足夠的表現力，因此其在強化學習中的潛力受到關注有限。 我們證明情況並非如此。 基於正規化流的最新研究，我們提出一種單一正規化流架構，可無縫整合到強化學習演算法中，用作策略、Q函數和佔用度量。 我們的途徑可實現更簡單的演算法，並在模仿學習、離線、目標條件強化學習和無監督強化學習中獲得更高的效能。", "audio": "audios/2505.23527v1.mp3", "timestamp": "2025-05-30T13:31:33.830183"}
{"query": "AI", "id": "2505.23575v1", "url": "http://arxiv.org/abs/2505.23575v1", "title": "CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring", "summary": "As AI models are deployed with increasing autonomy, it is important to ensure\nthey do not take harmful actions unnoticed. As a potential mitigation, we\ninvestigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor\nmodel continuously oversees the intermediate reasoning steps of a more powerful\nbut untrusted model. We compare CoT monitoring to action-only monitoring, where\nonly final outputs are reviewed, in a red-teaming setup where the untrusted\nmodel is instructed to pursue harmful side tasks while completing a coding\nproblem. We find that CoT monitoring improves detection by up to 27 percentage\npoints in scenarios where action-only monitoring fails to reliably identify\nsabotage. However, CoT traces can also contain misleading rationalizations that\ndeceive the monitor, reducing performance in more obvious sabotage cases. To\naddress this, we introduce a hybrid protocol that independently scores both\nreasoning and final outputs and combines them using a weighted average. This\nhybrid monitor consistently outperforms both CoT and action-only monitors\nacross all tested models and tasks, with detection rates over four times higher\nthan action-only monitoring for subtle deception scenarios.", "authors": ["Benjamin Arnav", "Pablo Bernabeu-Pérez", "Nathan Helm-Burger", "Tim Kostolansky", "Hannes Whittingham", "Mary Phuong"], "published_date": "2025-05-29", "title_zh": "CoT現形記：連鎖思維監測的壓力測試", "summary_zh": "隨著人工智慧模型自主性日益增強，確保其行為無害至關重要。本研究探討思維鏈監控（CoT），即由較弱但可信賴的監控模型持續監督較強大但不可信賴模型的推理過程，作為潛在的緩解措施。在紅隊演練中，我們比較CoT監控與僅監控最終輸出結果的行動監控，其中不可信賴模型在完成編碼問題時被指示執行有害的附加任務。研究發現，在行動監控無法可靠識別破壞行為的情況下，CoT監控可將偵測率提高達27個百分點。然而，CoT追蹤也可能包含誤導性解釋，欺騙監控模型，從而降低在更明顯的破壞案例中的性能。為了解決此問題，我們引入了一種混合協議，該協議獨立評估推理和最終輸出結果，並使用加權平均值組合它們。在所有測試模型和任務中，這種混合監控始終優於CoT和行動監控，對於微妙的欺騙情境，其偵測率是行動監控的四倍以上。", "audio": "audios/2505.23575v1.mp3", "timestamp": "2025-05-30T14:19:00.047750"}
{"query": "Foundation Model", "id": "2505.23107v1", "url": "http://arxiv.org/abs/2505.23107v1", "title": "EAD: An EEG Adapter for Automated Classification", "summary": "While electroencephalography (EEG) has been a popular modality for neural\ndecoding, it often involves task specific acquisition of the EEG data. This\nposes challenges for the development of a unified pipeline to learn embeddings\nfor various EEG signal classification, which is often involved in various\ndecoding tasks. Traditionally, EEG classification involves the step of signal\npreprocessing and the use of deep learning techniques, which are highly\ndependent on the number of EEG channels in each sample. However, the same\npipeline cannot be applied even if the EEG data is collected for the same\nexperiment but with different acquisition devices. This necessitates the\ndevelopment of a framework for learning EEG embeddings, which could be highly\nbeneficial for tasks involving multiple EEG samples for the same task but with\nvarying numbers of EEG channels. In this work, we propose EEG Adapter (EAD), a\nflexible framework compatible with any signal acquisition device. More\nspecifically, we leverage a recent EEG foundational model with significant\nadaptations to learn robust representations from the EEG data for the\nclassification task. We evaluate EAD on two publicly available datasets\nachieving state-of-the-art accuracies 99.33% and 92.31% on EEG-ImageNet and\nBrainLat respectively. This illustrates the effectiveness of the proposed\nframework across diverse EEG datasets containing two different perception\ntasks: stimulus and resting-state EEG signals. We also perform zero-shot EEG\nclassification on EEG-ImageNet task to demonstrate the generalization\ncapability of the proposed approach.", "authors": ["Pushapdeep Singh", "Jyoti Nigam", "Medicherla Vamsi Krishna", "Arnav Bhavsar", "Aditya Nigam"], "published_date": "2025-05-29", "title_zh": "EAD：用於自動分類的腦電圖適配器", "summary_zh": "腦電圖(EEG)常用於神經解碼，但常需針對特定任務收集數據，不利於開發統一的嵌入學習流程。傳統EEG分類依賴訊號預處理和深度學習，對通道數敏感，不同設備採集的同實驗數據也無法通用。為此，我們提出EEG適配器(EAD)，一個靈活框架，適用於各種訊號採集設備。EAD利用最新的EEG基礎模型，經調整後學習EEG數據的穩健表示，用於分類任務。在EEG-ImageNet和BrainLat數據集上，EAD分別達到99.33%和92.31%的準確度，展現其在刺激和靜息態EEG訊號上的有效性。此外，EAD在EEG-ImageNet任務上的零樣本分類也驗證了其泛化能力。", "audio": "audios/2505.23107v1.mp3", "timestamp": "2025-05-30T14:19:06.054797"}
{"query": "Diffusion Model", "id": "2505.23462v1", "url": "http://arxiv.org/abs/2505.23462v1", "title": "LAFR: Efficient Diffusion-based Blind Face Restoration via Latent Codebook Alignment Adapter", "summary": "Blind face restoration from low-quality (LQ) images is a challenging task\nthat requires not only high-fidelity image reconstruction but also the\npreservation of facial identity. While diffusion models like Stable Diffusion\nhave shown promise in generating high-quality (HQ) images, their VAE modules\nare typically trained only on HQ data, resulting in semantic misalignment when\nencoding LQ inputs. This mismatch significantly weakens the effectiveness of LQ\nconditions during the denoising process. Existing approaches often tackle this\nissue by retraining the VAE encoder, which is computationally expensive and\nmemory-intensive. To address this limitation efficiently, we propose LAFR\n(Latent Alignment for Face Restoration), a novel codebook-based latent space\nadapter that aligns the latent distribution of LQ images with that of HQ\ncounterparts, enabling semantically consistent diffusion sampling without\naltering the original VAE. To further enhance identity preservation, we\nintroduce a multi-level restoration loss that combines constraints from\nidentity embeddings and facial structural priors. Additionally, by leveraging\nthe inherent structural regularity of facial images, we show that lightweight\nfinetuning of diffusion prior on just 0.9% of FFHQ dataset is sufficient to\nachieve results comparable to state-of-the-art methods, reduce training time by\n70%. Extensive experiments on both synthetic and real-world face restoration\nbenchmarks demonstrate the effectiveness and efficiency of LAFR, achieving\nhigh-quality, identity-preserving face reconstruction from severely degraded\ninputs.", "authors": ["Runyi Li", "Bin Chen", "Jian Zhang", "Radu Timofte"], "published_date": "2025-05-29", "title_zh": "LAFR：基於潛碼本對齊適配器的高效擴散盲人臉修復", "summary_zh": "低質量人臉圖像修復極具挑戰，需兼顧高保真重建與身份保持。Stable Diffusion等擴散模型雖能生成高質量圖像，但其VAE模組通常僅在高質量數據上訓練，編碼低質量輸入時易產生語義錯位，削弱去噪效果。現有方法常需重新訓練VAE編碼器，耗費大量運算資源。為解決此限制，我們提出LAFR，一種基於碼本的潛在空間適配器，能使低質量圖像的潛在分佈與高質量圖像對齊，實現語義一致的擴散採樣，且無需變更原始VAE。為進一步提升身份保持，我們引入多層次修復損失，結合身份嵌入和人臉結構先驗的約束。此外，藉由人臉圖像的結構規律性，僅需在0.9%的FFHQ數據集上輕量微調擴散先驗，即可達到與最先進方法相當的效果，並縮短70%的訓練時間。合成及真實人臉修復基準測試上的大量實驗證明，LAFR能有效率地從嚴重退化的輸入中重建高品質且保持身份的人臉。", "audio": "audios/2505.23462v1.mp3", "timestamp": "2025-05-30T14:19:13.741893"}
{"query": "AI", "id": "2505.23570v1", "url": "http://arxiv.org/abs/2505.23570v1", "title": "Evaluating AI capabilities in detecting conspiracy theories on YouTube", "summary": "As a leading online platform with a vast global audience, YouTube's extensive\nreach also makes it susceptible to hosting harmful content, including\ndisinformation and conspiracy theories. This study explores the use of\nopen-weight Large Language Models (LLMs), both text-only and multimodal, for\nidentifying conspiracy theory videos shared on YouTube. Leveraging a labeled\ndataset of thousands of videos, we evaluate a variety of LLMs in a zero-shot\nsetting and compare their performance to a fine-tuned RoBERTa baseline. Results\nshow that text-based LLMs achieve high recall but lower precision, leading to\nincreased false positives. Multimodal models lag behind their text-only\ncounterparts, indicating limited benefits from visual data integration. To\nassess real-world applicability, we evaluate the most accurate models on an\nunlabeled dataset, finding that RoBERTa achieves performance close to LLMs with\na larger number of parameters. Our work highlights the strengths and\nlimitations of current LLM-based approaches for online harmful content\ndetection, emphasizing the need for more precise and robust systems.", "authors": ["Leonardo La Rocca", "Francesco Corso", "Francesco Pierri"], "published_date": "2025-05-29", "title_zh": "評估人工智慧在YouTube上偵測陰謀論的能力", "summary_zh": "YouTube作為全球領先平台，易受有害內容影響，包括不實資訊和陰謀論。本研究探討使用開放權重大型語言模型(LLMs)，包含純文字和多模態模型，辨識YouTube上的陰謀論影片。我們利用數千個標記影片的數據集，在零樣本設定下評估多種LLMs，並將其效能與微調後的RoBERTa基準線進行比較。結果顯示，基於文字的LLMs實現了高召回率，但精確度較低，導致誤判增加。多模態模型落後於純文字模型，表明視覺數據整合的益處有限。為評估實際應用性，我們在未標記數據集上評估最準確的模型，發現RoBERTa的效能接近參數更多的LLMs。本研究強調了當前基於LLM的方法在線上偵測有害內容方面的優點和局限性，並強調需要更精確和穩健的系統。", "audio": "audios/2505.23570v1.mp3", "timestamp": "2025-05-30T15:19:23.702770"}
{"query": "Foundation Model", "id": "2505.23099v1", "url": "http://arxiv.org/abs/2505.23099v1", "title": "Weight Spectra Induced Efficient Model Adaptation", "summary": "Large-scale foundation models have demonstrated remarkable versatility across\na wide range of downstream tasks. However, fully fine-tuning these models\nincurs prohibitive computational costs, motivating the development of\nParameter-Efficient Fine-Tuning (PEFT) methods such as LoRA, which introduces\nlow-rank updates to pre-trained weights. Despite their empirical success, the\nunderlying mechanisms by which PEFT modifies model parameters remain\nunderexplored. In this work, we present a systematic investigation into the\nstructural changes of weight matrices during fully fine-tuning. Through\nsingular value decomposition (SVD), we reveal that fine-tuning predominantly\namplifies the top singular values while leaving the remainder largely intact,\nsuggesting that task-specific knowledge is injected into a low-dimensional\nsubspace. Furthermore, we find that the dominant singular vectors are\nreoriented in task-specific directions, whereas the non-dominant subspace\nremains stable. Building on these insights, we propose a novel method that\nleverages learnable rescaling of top singular directions, enabling precise\nmodulation of the most influential components without disrupting the global\nstructure. Our approach achieves consistent improvements over strong baselines\nacross multiple tasks, highlighting the efficacy of structurally informed\nfine-tuning.", "authors": ["Chongjie Si", "Xuankun Yang", "Muqing Liu", "Yadao Wang", "Xiaokang Yang", "Wenbo Su", "Bo Zheng", "Wei Shen"], "published_date": "2025-05-29", "title_zh": "權重譜誘導的高效模型適應", "summary_zh": "大型基礎模型在下游任務中展現卓越通用性。完全微調這些模型計算成本過高，因此出現如LoRA等參數高效微調(PEFT)方法，其對預訓練權重引入低秩更新。儘管PEFT在實證上成功，但其修改模型參數的潛在機制仍待探索。本研究系統性地探討了完全微調期間權重矩陣的結構變化。透過奇異值分解(SVD)，我們發現微調主要放大頂部奇異值，而其餘部分基本保持不變，表明任務特定知識被注入到一個低維子空間中。此外，我們發現主導奇異向量在任務特定方向上重新定向，而非主導子空間保持穩定。基於這些洞見，我們提出一種新方法，利用頂部奇異方向的可學習重縮放，實現對最具影響力組件的精確調節，而不破壞全局結構。我們的方案在多個任務中持續優於強基線，突顯了結構化資訊微調的有效性。", "audio": "audios/2505.23099v1.mp3", "timestamp": "2025-05-30T15:19:34.268350"}
{"query": "Diffusion Model", "id": "2505.23458v1", "url": "http://arxiv.org/abs/2505.23458v1", "title": "Diffusion Guidance Is a Controllable Policy Improvement Operator", "summary": "At the core of reinforcement learning is the idea of learning beyond the\nperformance in the data. However, scaling such systems has proven notoriously\ntricky. In contrast, techniques from generative modeling have proven remarkably\nscalable and are simple to train. In this work, we combine these strengths, by\nderiving a direct relation between policy improvement and guidance of diffusion\nmodels. The resulting framework, CFGRL, is trained with the simplicity of\nsupervised learning, yet can further improve on the policies in the data. On\noffline RL tasks, we observe a reliable trend -- increased guidance weighting\nleads to increased performance. Of particular importance, CFGRL can operate\nwithout explicitly learning a value function, allowing us to generalize simple\nsupervised methods (e.g., goal-conditioned behavioral cloning) to further\nprioritize optimality, gaining performance for \"free\" across the board.", "authors": ["Kevin Frans", "Seohong Park", "Pieter Abbeel", "Sergey Levine"], "published_date": "2025-05-29", "title_zh": "擴散引導為一種可控的策略改進算子", "summary_zh": "強化學習的核心在於超越數據表現的學習，但擴展此類系統極具挑戰。相比之下，生成模型技術的可擴展性強且易於訓練。本研究結合兩者優勢，推導策略提升與擴散模型引導的直接關聯。由此產生的框架CFGRL以監督學習的簡便性進行訓練，並能進一步改善數據中的策略。在離線強化學習任務中，觀察到可靠趨勢：增加引導權重可提高性能。更重要的是，CFGRL無需顯式學習價值函數即可運行，從而將簡單的監督方法（例如，目標條件行為克隆）推廣到進一步優先考慮最優性，全面提升性能。", "audio": "audios/2505.23458v1.mp3", "timestamp": "2025-05-30T15:19:42.246769"}
{"query": "AI", "id": "2505.23559v1", "url": "http://arxiv.org/abs/2505.23559v1", "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents", "summary": "Recent advancements in large language model (LLM) agents have significantly\naccelerated scientific discovery automation, yet concurrently raised critical\nethical and safety concerns. To systematically address these challenges, we\nintroduce \\textbf{SafeScientist}, an innovative AI scientist framework\nexplicitly designed to enhance safety and ethical responsibility in AI-driven\nscientific exploration. SafeScientist proactively refuses ethically\ninappropriate or high-risk tasks and rigorously emphasizes safety throughout\nthe research process. To achieve comprehensive safety oversight, we integrate\nmultiple defensive mechanisms, including prompt monitoring, agent-collaboration\nmonitoring, tool-use monitoring, and an ethical reviewer component.\nComplementing SafeScientist, we propose \\textbf{SciSafetyBench}, a novel\nbenchmark specifically designed to evaluate AI safety in scientific contexts,\ncomprising 240 high-risk scientific tasks across 6 domains, alongside 30\nspecially designed scientific tools and 120 tool-related risk tasks. Extensive\nexperiments demonstrate that SafeScientist significantly improves safety\nperformance by 35\\% compared to traditional AI scientist frameworks, without\ncompromising scientific output quality. Additionally, we rigorously validate\nthe robustness of our safety pipeline against diverse adversarial attack\nmethods, further confirming the effectiveness of our integrated approach. The\ncode and data will be available at https://github.com/ulab-uiuc/SafeScientist.\n\\textcolor{red}{Warning: this paper contains example data that may be offensive\nor harmful.}", "authors": ["Kunlun Zhu", "Jiaxun Zhang", "Ziheng Qi", "Nuoxing Shang", "Zijia Liu", "Peixuan Han", "Yue Su", "Haofei Yu", "Jiaxuan You"], "published_date": "2025-05-29", "title_zh": "安全科學家：基於大型語言模型代理的風險感知科學發現", "summary_zh": "大型語言模型代理加速科學發現自動化，同時引發倫理與安全疑慮。為應對這些挑戰，我們提出SafeScientist，一個旨在提升AI驅動科學探索之安全與倫理責任的創新框架。SafeScientist主動拒絕不合倫理或高風險任務，並貫徹研究過程中的安全性。透過整合提示監控、代理協作監控、工具使用監控及倫理審查組件等多重防禦機制，實現全面安全監督。此外，我們提出SciSafetyBench，一個評估科學情境下AI安全性的基準，包含六個領域的240個高風險科學任務、30個專用科學工具及120個工具相關風險任務。實驗結果顯示，SafeScientist相較於傳統AI科學家框架，在不影響科學產出品質的前提下，顯著提升35%的安全性能。我們亦嚴格驗證安全管道對抗多種對抗性攻擊方法的穩健性，進一步確認整合方法的有效性。程式碼與數據將於https://github.com/ulab-uiuc/SafeScientist公開。警告：本論文包含可能具冒犯性或危害性的範例數據。", "audio": "audios/2505.23559v1.mp3", "timestamp": "2025-05-30T16:23:52.822012"}
{"query": "Foundation Model", "id": "2505.23058v1", "url": "http://arxiv.org/abs/2505.23058v1", "title": "Be.FM: Open Foundation Models for Human Behavior", "summary": "Despite their success in numerous fields, the potential of foundation models\nfor modeling and understanding human behavior remains largely unexplored. We\nintroduce Be.FM, one of the first open foundation models designed for human\nbehavior modeling. Built upon open-source large language models and fine-tuned\non a diverse range of behavioral data, Be.FM can be used to understand and\npredict human decision-making. We construct a comprehensive set of benchmark\ntasks for testing the capabilities of behavioral foundation models. Our results\ndemonstrate that Be.FM can predict behaviors, infer characteristics of\nindividuals and populations, generate insights about contexts, and apply\nbehavioral science knowledge.", "authors": ["Yutong Xie", "Zhuoheng Li", "Xiyuan Wang", "Yijun Pan", "Qijia Liu", "Xingzhi Cui", "Kuang-Yu Lo", "Ruoyi Gao", "Xingjian Zhang", "Jin Huang", "Walter Yuan", "Matthew O. Jackson", "Qiaozhu Mei"], "published_date": "2025-05-29", "title_zh": "Be.FM：人類行為之開放基礎模型", "summary_zh": "儘管基礎模型在多個領域取得成功，但其在人類行為建模與理解方面的潛力尚未被充分挖掘。本研究推出Be.FM，首批專為人類行為建模設計的開放基礎模型之一。Be.FM基於開源大型語言模型，並在多樣化的行為數據上進行微調，可用於理解和預測人類決策。我們構建了一套全面的基準任務，以測試行為基礎模型的能力。結果表明，Be.FM能夠預測行為、推斷個體和群體的特徵、產生關於情境的洞見，並應用行為科學知識。", "audio": "audios/2505.23058v1.mp3", "timestamp": "2025-05-30T16:23:57.643231"}
{"query": "Diffusion Model", "id": "2505.23444v1", "url": "http://arxiv.org/abs/2505.23444v1", "title": "CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis", "summary": "Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of\nmacromolecules, but developing robust models for downstream analysis is\nhindered by the scarcity of high-quality annotated data. While synthetic data\ngeneration has emerged as a potential solution, existing methods often fail to\ncapture both the structural diversity of biological specimens and the complex,\nspatially varying noise inherent in cryo-EM imaging. To overcome these\nlimitations, we propose CryoCCD, a synthesis framework that integrates\nbiophysical modeling with generative techniques. Specifically, CryoCCD produces\nmulti-scale cryo-EM micrographs that reflect realistic biophysical variability\nthrough compositional heterogeneity, cellular context, and physics-informed\nimaging. To generate realistic noise, we employ a conditional diffusion model,\nenhanced by cycle consistency to preserve structural fidelity and mask-aware\ncontrastive learning to capture spatially adaptive noise patterns. Extensive\nexperiments show that CryoCCD generates structurally accurate micrographs and\nenhances performance in downstream tasks, outperforming state-of-the-art\nbaselines in both particle picking and reconstruction.", "authors": ["Runmin Jiang", "Genpei Zhang", "Yuntian Yang", "Siqi Wu", "Yuheng Zhang", "Wanyue Feng", "Yizhou Zhao", "Xi Xiao", "Xiao Wang", "Tianyang Wang", "Xingjian Li", "Min Xu"], "published_date": "2025-05-29", "title_zh": "CryoCCD：基於生物物理建模的條件循環一致擴散用於冷凍電鏡圖像合成", "summary_zh": "低溫電子顯微鏡 (cryo-EM) 雖可提供近原子解析度的大分子影像，但缺乏高品質標註數據阻礙了穩健模型的建立。合成數據生成提供了解決方案，但現有方法難以同時捕捉生物樣本的結構多樣性和 cryo-EM 影像中複雜且空間變化的噪聲。為此，我們提出 CryoCCD，一個結合生物物理建模與生成技術的合成框架。CryoCCD 產生多尺度 cryo-EM 顯微照片，通過成分異質性、細胞背景和物理信息成像反映真實的生物物理變異性。為生成真實噪聲，我們採用條件擴散模型，並通過循環一致性來保持結構保真度，以及掩碼感知對比學習來捕捉空間自適應噪聲模式。實驗表明，CryoCCD 生成結構準確的顯微照片，並在下游任務中提高了性能，在粒子挑選和重建方面均優於現有技術。", "audio": "audios/2505.23444v1.mp3", "timestamp": "2025-05-30T16:24:04.098990"}
{"query": "AI", "id": "2505.23553v1", "url": "http://arxiv.org/abs/2505.23553v1", "title": "A Unified Framework for Mapping and Synthesis of Approximate R-Blocks CGRAs", "summary": "The ever-increasing complexity and operational diversity of modern Neural\nNetworks (NNs) have caused the need for low-power and, at the same time,\nhigh-performance edge devices for AI applications. Coarse Grained\nReconfigurable Architectures (CGRAs) form a promising design paradigm to\naddress these challenges, delivering a close-to-ASIC performance while allowing\nfor hardware programmability. In this paper, we introduce a novel end-to-end\nexploration and synthesis framework for approximate CGRA processors that\nenables transparent and optimized integration and mapping of state-of-the-art\napproximate multiplication components into CGRAs. Our methodology introduces a\nper-channel exploration strategy that maps specific output features onto\napproximate components based on accuracy degradation constraints. This enables\nthe optimization of the system's energy consumption while retaining the\naccuracy above a certain threshold. At the circuit level, the integration of\napproximate components enables the creation of voltage islands that operate at\nreduced voltage levels, which is attributed to their inherently shorter\ncritical paths. This key enabler allows us to effectively reduce the overall\npower consumption by an average of 30% across our analyzed architectures,\ncompared to their baseline counterparts, while incurring only a minimal 2% area\noverhead. The proposed methodology was evaluated on a widely used NN model,\nMobileNetV2, on the ImageNet dataset, demonstrating that the generated\narchitectures can deliver up to 440 GOPS/W with relatively small output error\nduring inference, outperforming several State-of-the-Art CGRA architectures in\nterms of throughput and energy efficiency.", "authors": ["Georgios Alexandris", "Panagiotis Chaidos", "Alexis Maras", "Barry de Bruin", "Manil Dev Gomony", "Henk Corporaal", "Dimitrios Soudris", "Sotirios Xydis"], "published_date": "2025-05-29", "title_zh": "近似R區塊粗粒度重構陣列映射與合成之統一框架", "summary_zh": "為滿足人工智慧應用對低功耗、高效能邊緣設備的需求，粗粒度可重構架構(CGRA)提供了一種兼具客製化晶片效能與硬體可程式性的解決方案。本文提出一種新穎的端到端探索與合成框架，用於近似CGRA處理器，能透明且最佳化地將先進的近似乘法元件整合至CGRA中。該方法基於精度降級約束，採用逐通道探索策略，將特定輸出特徵映射到近似元件，藉此優化系統能耗，同時保持精度在閾值之上。在電路層面，近似元件的整合促成了低電壓島的形成，進而降低整體功耗。實驗結果顯示，相較於基準設計，該方法可在僅增加2%面積開銷的情況下，平均降低30%的功耗。在ImageNet數據集上，使用MobileNetV2模型進行評估，驗證了所生成的架構在推理過程中可實現高達440 GOPS/W的效能，且輸出誤差相對較小，在吞吐量和能效方面優於現有的CGRA架構。", "audio": "audios/2505.23553v1.mp3", "timestamp": "2025-05-30T17:16:28.493384"}
{"query": "Foundation Model", "id": "2505.23042v1", "url": "http://arxiv.org/abs/2505.23042v1", "title": "From Theory to Application: Fine-Tuning Large EEG Model with Real-World Stress Data", "summary": "Recent advancements in Large Language Models have inspired the development of\nfoundation models across various domains. In this study, we evaluate the\nefficacy of Large EEG Models (LEMs) by fine-tuning LaBraM, a state-of-the-art\nfoundation EEG model, on a real-world stress classification dataset collected\nin a graduate classroom. Unlike previous studies that primarily evaluate LEMs\nusing data from controlled clinical settings, our work assesses their\napplicability to real-world environments. We train a binary classifier that\ndistinguishes between normal and elevated stress states using resting-state EEG\ndata recorded from 18 graduate students during a class session. The\nbest-performing fine-tuned model achieves a balanced accuracy of 90.47% with a\n5-second window, significantly outperforming traditional stress classifiers in\nboth accuracy and inference efficiency. We further evaluate the robustness of\nthe fine-tuned LEM under random data shuffling and reduced channel counts.\nThese results demonstrate the capability of LEMs to effectively process\nreal-world EEG data and highlight their potential to revolutionize\nbrain-computer interface applications by shifting the focus from model-centric\nto data-centric design.", "authors": ["Siwen Wang", "Shitou Zhang", "Wan-Lin Chen", "Dung Truong", "Tzyy-Ping Jung"], "published_date": "2025-05-29", "title_zh": "從理論到應用：利用真實世界壓力數據微調大型腦電模型", "summary_zh": "大型語言模型的進展啟發了各領域的基礎模型開發。本研究評估大型腦電模型（LEMs）的效能，透過在真實世界壓力分類數據集上微調LaBraM模型（一種先進的腦電基礎模型）。與先前研究主要使用受控臨床環境數據評估LEMs不同，本研究評估其在真實環境中的適用性。我們訓練一個二元分類器，利用課堂中18名研究生的靜息態腦電數據區分正常和高壓力狀態。最佳微調模型在5秒窗口下達到90.47%的平衡準確率，顯著優於傳統壓力分類器，並兼具準確性和推論效率。此外，我們評估了微調後LEMs在隨機數據洗牌和減少通道數下的穩健性。結果表明LEMs能有效處理真實世界腦電數據，並突顯其將腦機介面應用從以模型為中心轉向以數據為中心設計的潛力。", "audio": "audios/2505.23042v1.mp3", "timestamp": "2025-05-30T17:16:38.394069"}
{"query": "Diffusion Model", "id": "2505.23426v1", "url": "http://arxiv.org/abs/2505.23426v1", "title": "Enhanced DACER Algorithm with High Diffusion Efficiency", "summary": "Due to their expressive capacity, diffusion models have shown great promise\nin offline RL and imitation learning. Diffusion Actor-Critic with Entropy\nRegulator (DACER) extended this capability to online RL by using the reverse\ndiffusion process as a policy approximator, trained end-to-end with policy\ngradient methods, achieving strong performance. However, this comes at the cost\nof requiring many diffusion steps, which significantly hampers training\nefficiency, while directly reducing the steps leads to noticeable performance\ndegradation. Critically, the lack of inference efficiency becomes a significant\nbottleneck for applying diffusion policies in real-time online RL settings. To\nimprove training and inference efficiency while maintaining or even enhancing\nperformance, we propose a Q-gradient field objective as an auxiliary\noptimization target to guide the denoising process at each diffusion step.\nNonetheless, we observe that the independence of the Q-gradient field from the\ndiffusion time step negatively impacts the performance of the diffusion policy.\nTo address this, we introduce a temporal weighting mechanism that enables the\nmodel to efficiently eliminate large-scale noise in the early stages and refine\nactions in the later stages. Experimental results on MuJoCo benchmarks and\nseveral multimodal tasks demonstrate that the DACER2 algorithm achieves\nstate-of-the-art performance in most MuJoCo control tasks with only five\ndiffusion steps, while also exhibiting stronger multimodality compared to\nDACER.", "authors": ["Yinuo Wang", "Mining Tan", "Wenjun Zou", "Haotian Lin", "Xujie Song", "Wenxuan Wang", "Tong Liu", "Likun Wang", "Guojian Zhan", "Tianze Zhu", "Shiqi Liu", "Jingliang Duan", "Shengbo Eben Li"], "published_date": "2025-05-29", "title_zh": "具高擴散效率之強化型DACER演算法", "summary_zh": "擴散模型因其表達能力，在離線強化學習和模仿學習中展現潛力。DACER將此能力擴展至線上強化學習，利用逆向擴散過程作為策略近似器，並以策略梯度方法進行端到端訓練，效果顯著。然而，這需要大量擴散步驟，嚴重影響訓練效率；直接減少步驟則會降低性能。推論效率不足成為擴散策略應用於即時線上強化學習的主要瓶頸。為提升訓練和推論效率，同時維持甚至提高性能，本文提出Q梯度場目標作為輔助優化目標，以指導每個擴散步驟的去噪過程。研究發現Q梯度場與擴散時間步的獨立性會負面影響擴散策略的性能。為解決此問題，本文引入時間加權機制，使模型能在早期階段有效消除大規模噪聲，並在後期階段完善動作。在MuJoCo基準測試和多種模態任務中的實驗結果表明，DACER2算法僅需五個擴散步驟即可在大多數MuJoCo控制任務中實現最優性能，並展現比DACER更強的多模態性。", "audio": "audios/2505.23426v1.mp3", "timestamp": "2025-05-30T17:16:48.863569"}
{"query": "AI", "id": "2505.23522v1", "url": "http://arxiv.org/abs/2505.23522v1", "title": "OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data", "summary": "Existing benchmarks for Earth science multimodal learning exhibit critical\nlimitations in systematic coverage of geosystem components and cross-sphere\ninteractions, often constrained to isolated subsystems (only in\nHuman-activities sphere or atmosphere) with limited evaluation dimensions (less\nthan 16 tasks). To address these gaps, we introduce OmniEarth-Bench, the first\ncomprehensive multimodal benchmark spanning all six Earth science spheres\n(atmosphere, lithosphere, Oceansphere, cryosphere, biosphere and\nHuman-activities sphere) and cross-spheres with one hundred expert-curated\nevaluation dimensions. Leveraging observational data from satellite sensors and\nin-situ measurements, OmniEarth-Bench integrates 29,779 annotations across four\ntiers: perception, general reasoning, scientific knowledge reasoning and\nchain-of-thought (CoT) reasoning. This involves the efforts of 2-5 experts per\nsphere to establish authoritative evaluation dimensions and curate relevant\nobservational datasets, 40 crowd-sourcing annotators to assist experts for\nannotations, and finally, OmniEarth-Bench is validated via hybrid expert-crowd\nworkflows to reduce label ambiguity. Experiments on 9 state-of-the-art MLLMs\nreveal that even the most advanced models struggle with our benchmarks, where\nnone of them reach 35\\% accuracy. Especially, in some cross-spheres tasks, the\nperformance of leading models like GPT-4o drops to 0.0\\%. OmniEarth-Bench sets\na new standard for geosystem-aware AI, advancing both scientific discovery and\npractical applications in environmental monitoring and disaster prediction. The\ndataset, source code, and trained models were released.", "authors": ["Fengxiang Wang", "Mingshuo Chen", "Xuming He", "YiFan Zhang", "Feng Liu", "Zijie Guo", "Zhenghao Hu", "Jiong Wang", "Jingyi Xu", "Zhangrui Li", "Fenghua Ling", "Ben Fei", "Weijia Li", "Long Lan", "Wenjing Yang", "Wenlong Zhang", "Lei Bai"], "published_date": "2025-05-29", "title_zh": "全域地球基準：邁向以多模態觀測地球數據對地球六大圈層及其跨圈層交互作用的整體評估", "summary_zh": "現有地球科學多模態學習基準測試在地球系統組成部分和跨圈層互動的系統覆蓋方面存在關鍵限制，通常局限於孤立子系統，且評估維度有限。為解決這些問題，我們推出OmniEarth-Bench，首個涵蓋所有六個地球科學圈層及其跨圈層互動的綜合多模態基準測試，包含一百個專家策劃的評估維度。OmniEarth-Bench利用衛星感測器和現地測量數據，整合了29,779個標註，涵蓋感知、一般推理、科學知識推理和鏈式思考推理四個層次。經由專家與群眾協作驗證，減少標籤模糊性。對九個最先進的多模態大型語言模型進行的實驗表明，即使是最先進的模型也難以應對我們的基準測試，準確率均未達到35%。特別是在某些跨圈層任務中，GPT-4o等領先模型的性能降至0.0%。OmniEarth-Bench為具地球系統意識的人工智慧設定了新標準，促進了科學發現以及環境監測和災害預測的實際應用。數據集、原始程式碼和訓練模型已發布。", "audio": "audios/2505.23522v1.mp3", "timestamp": "2025-05-30T18:25:58.570296"}
{"query": "Foundation Model", "id": "2505.22964v1", "url": "http://arxiv.org/abs/2505.22964v1", "title": "Exploring Scaling Laws for EHR Foundation Models", "summary": "The emergence of scaling laws has profoundly shaped the development of large\nlanguage models (LLMs), enabling predictable performance gains through\nsystematic increases in model size, dataset volume, and compute. Yet, these\nprinciples remain largely unexplored in the context of electronic health\nrecords (EHRs) -- a rich, sequential, and globally abundant data source that\ndiffers structurally from natural language. In this work, we present the first\nempirical investigation of scaling laws for EHR foundation models. By training\ntransformer architectures on patient timeline data from the MIMIC-IV database\nacross varying model sizes and compute budgets, we identify consistent scaling\npatterns, including parabolic IsoFLOPs curves and power-law relationships\nbetween compute, model parameters, data size, and clinical utility. These\nfindings demonstrate that EHR models exhibit scaling behavior analogous to\nLLMs, offering predictive insights into resource-efficient training strategies.\nOur results lay the groundwork for developing powerful EHR foundation models\ncapable of transforming clinical prediction tasks and advancing personalized\nhealthcare.", "authors": ["Sheng Zhang", "Qin Liu", "Naoto Usuyama", "Cliff Wong", "Tristan Naumann", "Hoifung Poon"], "published_date": "2025-05-29", "title_zh": "探索電子病歷基礎模型的規模律", "summary_zh": "大型語言模型的發展受益於規模定律，透過增加模型大小、數據集容量和算力，實現可預測的效能提升。然而，電子病歷(EHR)作為一種結構不同於自然語言的豐富、序列化且全球普及的數據源，其規模定律尚未被充分探索。本研究首次針對EHR基礎模型進行規模定律的實證研究。我們在MIMIC-IV數據庫的患者時間軸數據上，訓練不同模型大小和算力預算的Transformer架構，識別出一致的規模模式，包括拋物線型等算力曲線和算力、模型參數、數據大小及臨床效用之間的冪律關係。這些結果表明EHR模型展現出類似於大型語言模型的規模效應，為資源高效的訓練策略提供了預測性見解。本研究為開發強大的EHR基礎模型奠定基礎，有望革新臨床預測任務並推進個人化醫療。", "audio": "audios/2505.22964v1.mp3", "timestamp": "2025-05-30T18:26:07.378958"}
{"query": "Diffusion Model", "id": "2505.23343v1", "url": "http://arxiv.org/abs/2505.23343v1", "title": "Diffusion Sampling Path Tells More: An Efficient Plug-and-Play Strategy for Sample Filtering", "summary": "Diffusion models often exhibit inconsistent sample quality due to stochastic\nvariations inherent in their sampling trajectories. Although training-based\nfine-tuning (e.g. DDPO [1]) and inference-time alignment techniques[2] aim to\nimprove sample fidelity, they typically necessitate full denoising processes\nand external reward signals. This incurs substantial computational costs,\nhindering their broader applicability. In this work, we unveil an intriguing\nphenomenon: a previously unobserved yet exploitable link between sample quality\nand characteristics of the denoising trajectory during classifier-free guidance\n(CFG). Specifically, we identify a strong correlation between high-density\nregions of the sample distribution and the Accumulated Score Differences\n(ASD)--the cumulative divergence between conditional and unconditional scores.\nLeveraging this insight, we introduce CFG-Rejection, an efficient,\nplug-and-play strategy that filters low-quality samples at an early stage of\nthe denoising process, crucially without requiring external reward signals or\nmodel retraining. Importantly, our approach necessitates no modifications to\nmodel architectures or sampling schedules and maintains full compatibility with\nexisting diffusion frameworks. We validate the effectiveness of CFG-Rejection\nin image generation through extensive experiments, demonstrating marked\nimprovements on human preference scores (HPSv2, PickScore) and challenging\nbenchmarks (GenEval, DPG-Bench). We anticipate that CFG-Rejection will offer\nsignificant advantages for diverse generative modalities beyond images, paving\nthe way for more efficient and reliable high-quality sample generation.", "authors": ["Sixian Wang", "Zhiwei Tang", "Tsung-Hui Chang"], "published_date": "2025-05-29", "title_zh": "擴散採樣路徑揭示更多：一種高效的隨插即用樣本過濾策略", "summary_zh": "擴散模型取樣軌跡的隨機性導致樣本品質不一。訓練微調及推理對齊技術雖能改善樣本逼真度，但需完整去噪過程與外部獎勵訊號，耗費大量運算資源。本研究揭示分類器無條件引導（CFG）下，樣本品質與去噪軌跡特性間存在先前未被發現的關聯：樣本分佈高密度區域與累積分數差異（ASD）間存在強烈相關性。據此，我們提出 CFG-Rejection，一種高效、隨插即用的策略，無需外部獎勵訊號或模型重新訓練，即可在去噪早期階段過濾低品質樣本。此方法不需修改模型架構或取樣排程，並與現有擴散框架完全相容。實驗驗證 CFG-Rejection 在圖像生成上的有效性，顯著提升人類偏好分數與基準測試結果。預期 CFG-Rejection 將為圖像以外的多種生成模態帶來優勢，實現更高效可靠的高品質樣本生成。", "audio": "audios/2505.23343v1.mp3", "timestamp": "2025-05-30T18:26:17.735534"}
{"query": "AI", "id": "2505.23518v1", "url": "http://arxiv.org/abs/2505.23518v1", "title": "TRAP: Targeted Redirecting of Agentic Preferences", "summary": "Autonomous agentic AI systems powered by vision-language models (VLMs) are\nrapidly advancing toward real-world deployment, yet their cross-modal reasoning\ncapabilities introduce new attack surfaces for adversarial manipulation that\nexploit semantic reasoning across modalities. Existing adversarial attacks\ntypically rely on visible pixel perturbations or require privileged model or\nenvironment access, making them impractical for stealthy, real-world\nexploitation. We introduce TRAP, a generative adversarial framework that\nmanipulates the agent's decision-making using diffusion-based semantic\ninjections. Our method combines negative prompt-based degradation with positive\nsemantic optimization, guided by a Siamese semantic network and layout-aware\nspatial masking. Without requiring access to model internals, TRAP produces\nvisually natural images yet induces consistent selection biases in agentic AI\nsystems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO)\ndataset, building multi-candidate decision scenarios. Across these scenarios,\nTRAP achieves a 100% attack success rate on leading models, including\nLLaVA-34B, Gemma3, and Mistral-3.1, significantly outperforming baselines such\nas SPSA, Bandit, and standard diffusion approaches. These results expose a\ncritical vulnerability: Autonomous agents can be consistently misled through\nhuman-imperceptible cross-modal manipulations. These findings highlight the\nneed for defense strategies beyond pixel-level robustness to address semantic\nvulnerabilities in cross-modal decision-making.", "authors": ["Hangoo Kang", "Jehyeok Yeon", "Gagandeep Singh"], "published_date": "2025-05-29", "title_zh": "TRAP：目標導向的主體偏好重定向", "summary_zh": "基於視覺語言模型(VLM)的自主代理AI系統正快速發展，其跨模態推理能力帶來新的攻擊面。現有攻擊多依賴於可見像素擾動或需模型/環境特權，不利於隱蔽的實際應用。本研究提出TRAP，一種基於擴散模型的生成對抗框架，透過語義注入操縱代理決策。TRAP結合負提示降級與正向語義優化，並由暹羅語義網路和佈局感知空間遮罩引導。無需存取模型內部資訊，TRAP即可產生視覺上自然的圖像，並在代理AI系統中誘導一致的選擇偏差。在COCO資料集上進行的多候選決策情境評估顯示，TRAP對LLaVA-34B、Gemma3和Mistral-3.1等領先模型實現了100%的攻擊成功率，顯著優於SPSA、Bandit和標準擴散方法。結果揭示了一項關鍵漏洞：自主代理可透過人眼難以察覺的跨模態操縱而被持續誤導。研究強調，需超越像素層面的防禦策略，以解決跨模態決策中的語義漏洞。", "audio": "audios/2505.23518v1.mp3", "timestamp": "2025-05-30T19:14:47.464948"}
{"query": "Foundation Model", "id": "2505.22959v1", "url": "http://arxiv.org/abs/2505.22959v1", "title": "LLM-based HSE Compliance Assessment: Benchmark, Performance, and Advancements", "summary": "Health, Safety, and Environment (HSE) compliance assessment demands dynamic\nreal-time decision-making under complicated regulations and complex\nhuman-machine-environment interactions. While large language models (LLMs) hold\nsignificant potential for decision intelligence and contextual dialogue, their\ncapacity for domain-specific knowledge in HSE and structured legal reasoning\nremains underexplored. We introduce HSE-Bench, the first benchmark dataset\ndesigned to evaluate the HSE compliance assessment capabilities of LLM.\nHSE-Bench comprises over 1,000 manually curated questions drawn from\nregulations, court cases, safety exams, and fieldwork videos, and integrates a\nreasoning flow based on Issue spotting, rule Recall, rule Application, and rule\nConclusion (IRAC) to assess the holistic reasoning pipeline. We conduct\nextensive evaluations on different prompting strategies and more than 10 LLMs,\nincluding foundation models, reasoning models and multimodal vision models. The\nresults show that, although current LLMs achieve good performance, their\ncapabilities largely rely on semantic matching rather than principled reasoning\ngrounded in the underlying HSE compliance context. Moreover, their native\nreasoning trace lacks the systematic legal reasoning required for rigorous HSE\ncompliance assessment. To alleviate these, we propose a new prompting\ntechnique, Reasoning of Expert (RoE), which guides LLMs to simulate the\nreasoning process of different experts for compliance assessment and reach a\nmore accurate unified decision. We hope our study highlights reasoning gaps in\nLLMs for HSE compliance and inspires further research on related tasks.", "authors": ["Jianwei Wang", "Mengqi Wang", "Yinsi Zhou", "Zhenchang Xing", "Qing Liu", "Xiwei Xu", "Wenjie Zhang", "Liming Zhu"], "published_date": "2025-05-29", "title_zh": "基於大型語言模型的健康、安全與環境合規性評估：基準、效能與進展", "summary_zh": "健康、安全與環境（HSE）合規評估需要在複雜法規和人機環境互動下進行動態即時決策。大型語言模型（LLM）在決策智慧和情境對話方面具有潛力，但其在HSE領域知識和結構化法律推理方面的能力仍待探索。本研究推出HSE-Bench，首個評估LLM的HSE合規評估能力的基準資料集。HSE-Bench包含超過一千個手動整理的問題，源於法規、案例、安全考試和現場影片，並整合基於議題發現、規則回憶、規則應用和規則結論（IRAC）的推理流程，以評估整體推理管道。針對不同提示策略和十多個LLM（包括基礎模型、推理模型和多模態視覺模型）進行了廣泛評估。結果表明，儘管目前LLM表現良好，但其能力主要依賴語義匹配，而非基於HSE合規背景的原則性推理。此外，其原生推理追蹤缺乏嚴格的HSE合規評估所需的系統法律推理。為此，提出一種新的提示技術，專家推理（RoE），引導LLM模擬不同專家的推理過程以進行合規評估，並達成更準確的統一決策。本研究旨在強調LLM在HSE合規方面的推理差距，並激發對相關任務的進一步研究。", "audio": "audios/2505.22959v1.mp3", "timestamp": "2025-05-30T19:14:56.198290"}
{"query": "Diffusion Model", "id": "2505.23312v1", "url": "http://arxiv.org/abs/2505.23312v1", "title": "TRACE: Trajectory-Constrained Concept Erasure in Diffusion Models", "summary": "Text-to-image diffusion models have shown unprecedented generative\ncapability, but their ability to produce undesirable concepts\n(e.g.~pornographic content, sensitive identities, copyrighted styles) poses\nserious concerns for privacy, fairness, and safety. {Concept erasure} aims to\nremove or suppress specific concept information in a generative model. In this\npaper, we introduce \\textbf{TRACE (Trajectory-Constrained Attentional Concept\nErasure)}, a novel method to erase targeted concepts from diffusion models\nwhile preserving overall generative quality. Our approach combines a rigorous\ntheoretical framework, establishing formal conditions under which a concept can\nbe provably suppressed in the diffusion process, with an effective fine-tuning\nprocedure compatible with both conventional latent diffusion (Stable Diffusion)\nand emerging rectified flow models (e.g.~FLUX). We first derive a closed-form\nupdate to the model's cross-attention layers that removes hidden\nrepresentations of the target concept. We then introduce a trajectory-aware\nfinetuning objective that steers the denoising process away from the concept\nonly in the late sampling stages, thus maintaining the model's fidelity on\nunrelated content. Empirically, we evaluate TRACE on multiple benchmarks used\nin prior concept erasure studies (object classes, celebrity faces, artistic\nstyles, and explicit content from the I2P dataset). TRACE achieves\nstate-of-the-art performance, outperforming recent methods such as ANT,\nEraseAnything, and MACE in terms of removal efficacy and output quality.", "authors": ["Finn Carter"], "published_date": "2025-05-29", "title_zh": "TRACE：擴散模型中軌跡約束的概念擦除", "summary_zh": "文字生成圖像擴散模型展現了前所未有的生成能力，但也帶來了關於隱私、公平和安全的顧慮，例如產生不雅內容、敏感身分、受版權保護的風格等。概念抹除旨在移除或抑制生成模型中的特定概念資訊。本研究提出軌跡約束注意力概念抹除（TRACE），一種從擴散模型中抹除目標概念同時保持整體生成品質的新方法。TRACE結合嚴謹的理論框架，確立了概念在擴散過程中可被證明抑制的形式條件，以及與潛在擴散（Stable Diffusion）和新興校正流模型（如FLUX）相容的有效微調程序。首先，推導出模型交叉注意力層的封閉形式更新，以移除目標概念的隱藏表示。其次，引入軌跡感知微調目標，僅在後期採樣階段引導去噪過程遠離該概念，從而保持模型對無關內容的保真度。實驗評估表明，TRACE在多個基準測試中表現出色，在移除效果和輸出品質方面優於現有方法。", "audio": "audios/2505.23312v1.mp3", "timestamp": "2025-05-30T19:15:02.979360"}
{"query": "AI", "id": "2505.23503v1", "url": "http://arxiv.org/abs/2505.23503v1", "title": "Can Large Language Models Challenge CNNS in Medical Image Analysis?", "summary": "This study presents a multimodal AI framework designed for precisely\nclassifying medical diagnostic images. Utilizing publicly available datasets,\nthe proposed system compares the strengths of convolutional neural networks\n(CNNs) and different large language models (LLMs). This in-depth comparative\nanalysis highlights key differences in diagnostic performance, execution\nefficiency, and environmental impacts. Model evaluation was based on accuracy,\nF1-score, average execution time, average energy consumption, and estimated\n$CO_2$ emission. The findings indicate that although CNN-based models can\noutperform various multimodal techniques that incorporate both images and\ncontextual information, applying additional filtering on top of LLMs can lead\nto substantial performance gains. These findings highlight the transformative\npotential of multimodal AI systems to enhance the reliability, efficiency, and\nscalability of medical diagnostics in clinical settings.", "authors": ["Shibbir Ahmed", "Shahnewaz Karim Sakib", "Anindya Bijoy Das"], "published_date": "2025-05-29", "title_zh": "大型語言模型能否在醫學影像分析中挑戰卷積神經網路？", "summary_zh": "本研究提出一個用於精確分類醫療診斷影像的多模態人工智慧框架。該系統利用公開數據集，比較卷積神經網絡（CNN）與不同大型語言模型（LLM）的優勢。深入的比較分析突顯診斷性能、執行效率和環境影響方面的關鍵差異。模型評估基於準確度、F1分數、平均執行時間、平均能耗和預估二氧化碳排放量。研究結果表明，雖然基於CNN的模型可能優於結合影像與上下文資訊的各種多模態技術，但在LLM之上應用額外的過濾可以顯著提升性能。這些發現強調了多模態人工智慧系統在提高臨床環境中醫療診斷的可靠性、效率和可擴展性方面的變革潛力。", "audio": "audios/2505.23503v1.mp3", "timestamp": "2025-05-30T20:20:38.710094"}
{"query": "Foundation Model", "id": "2505.22954v1", "url": "http://arxiv.org/abs/2505.22954v1", "title": "Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents", "summary": "Today's AI systems have human-designed, fixed architectures and cannot\nautonomously and continuously improve themselves. The advance of AI could\nitself be automated. If done safely, that would accelerate AI development and\nallow us to reap its benefits much sooner. Meta-learning can automate the\ndiscovery of novel algorithms, but is limited by first-order improvements and\nthe human design of a suitable search space. The G\\\"odel machine proposed a\ntheoretical alternative: a self-improving AI that repeatedly modifies itself in\na provably beneficial manner. Unfortunately, proving that most changes are net\nbeneficial is impossible in practice. We introduce the Darwin G\\\"odel Machine\n(DGM), a self-improving system that iteratively modifies its own code (thereby\nalso improving its ability to modify its own codebase) and empirically\nvalidates each change using coding benchmarks. Inspired by Darwinian evolution\nand open-endedness research, the DGM maintains an archive of generated coding\nagents. It grows the archive by sampling an agent from it and using a\nfoundation model to create a new, interesting, version of the sampled agent.\nThis open-ended exploration forms a growing tree of diverse, high-quality\nagents and allows the parallel exploration of many different paths through the\nsearch space. Empirically, the DGM automatically improves its coding\ncapabilities (e.g., better code editing tools, long-context window management,\npeer-review mechanisms), increasing performance on SWE-bench from 20.0% to\n50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly\noutperforms baselines without self-improvement or open-ended exploration. All\nexperiments were done with safety precautions (e.g., sandboxing, human\noversight). The DGM is a significant step toward self-improving AI, capable of\ngathering its own stepping stones along paths that unfold into endless\ninnovation.", "authors": ["Jenny Zhang", "Shengran Hu", "Cong Lu", "Robert Lange", "Jeff Clune"], "published_date": "2025-05-29", "title_zh": "達爾文哥德爾機：自我改進智能體的開放式演化", "summary_zh": "當前人工智慧系統架構固定，缺乏自主持續改良能力。元學習雖可自動探索新演算法，但受限於一階改良和人為設計的搜尋空間。哥德爾機提出自改良AI的理論方案，但實務上難以驗證變更的效益。本文提出達爾文哥德爾機(DGM)，透過編碼基準評估，迭代修改自身程式碼以提升程式碼修改能力。DGM受達爾文演化和開放性研究啟發，維護一個程式碼代理人檔案庫，利用基礎模型生成新版本，擴展檔案庫。此開放式探索形成多樣且高品質代理人的成長樹，平行探索搜尋空間。實驗結果顯示，DGM自動提升編碼能力，在SWE-bench和Polyglot上的效能顯著提升，優於無自改良或開放式探索的基準線。所有實驗皆採取安全措施。DGM是朝向自改良AI的重要一步，能沿著無限創新的路徑自主發展。", "audio": "audios/2505.22954v1.mp3", "timestamp": "2025-05-30T20:20:49.278359"}
{"query": "Diffusion Model", "id": "2505.23305v1", "url": "http://arxiv.org/abs/2505.23305v1", "title": "MGE-LDM: Joint Latent Diffusion for Simultaneous Music Generation and Source Extraction", "summary": "We present MGE-LDM, a unified latent diffusion framework for simultaneous\nmusic generation, source imputation, and query-driven source separation. Unlike\nprior approaches constrained to fixed instrument classes, MGE-LDM learns a\njoint distribution over full mixtures, submixtures, and individual stems within\na single compact latent diffusion model. At inference, MGE-LDM enables (1)\ncomplete mixture generation, (2) partial generation (i.e., source imputation),\nand (3) text-conditioned extraction of arbitrary sources. By formulating both\nseparation and imputation as conditional inpainting tasks in the latent space,\nour approach supports flexible, class-agnostic manipulation of arbitrary\ninstrument sources. Notably, MGE-LDM can be trained jointly across\nheterogeneous multi-track datasets (e.g., Slakh2100, MUSDB18, MoisesDB) without\nrelying on predefined instrument categories. Audio samples are available at our\nproject page: https://yoongi43.github.io/MGELDM_Samples/.", "authors": ["Yunkee Chae", "Kyogu Lee"], "published_date": "2025-05-29", "title_zh": "MGE-LDM：用於同步音樂生成與音源分離的聯合潛在擴散模型", "summary_zh": "MGE-LDM是一種統一的潛在擴散框架，用於同時進行音樂生成、音源補全和查詢驅動的音源分離。與以往受限於固定樂器類別的方法不同，MGE-LDM在單一緊湊的潛在擴散模型中學習完整混合音、子混合音和單獨音軌的聯合分佈。在推論階段，MGE-LDM可實現：(1)完整混合音生成；(2)部分生成（即音源補全）；(3)文本條件下的任意音源提取。透過將分離和補全表述為潛在空間中的條件填補任務，此方法支援對任意樂器音源進行靈活且不限類別的操作。值得注意的是，MGE-LDM可以跨異質多軌數據集（例如Slakh2100、MUSDB18、MoisesDB）聯合訓練，而無需依賴預定義的樂器類別。", "audio": "audios/2505.23305v1.mp3", "timestamp": "2025-05-30T20:20:56.948063"}
{"query": "AI", "id": "2505.23436v1", "url": "http://arxiv.org/abs/2505.23436v1", "title": "Emergent Risk Awareness in Rational Agents under Resource Constraints", "summary": "Advanced reasoning models with agentic capabilities (AI agents) are deployed\nto interact with humans and to solve sequential decision-making problems under\n(approximate) utility functions and internal models. When such problems have\nresource or failure constraints where action sequences may be forcibly\nterminated once resources are exhausted, agents face implicit trade-offs that\nreshape their utility-driven (rational) behaviour. Additionally, since these\nagents are typically commissioned by a human principal to act on their behalf,\nasymmetries in constraint exposure can give rise to previously unanticipated\nmisalignment between human objectives and agent incentives. We formalise this\nsetting through a survival bandit framework, provide theoretical and empirical\nresults that quantify the impact of survival-driven preference shifts, identify\nconditions under which misalignment emerges and propose mechanisms to mitigate\nthe emergence of risk-seeking or risk-averse behaviours. As a result, this work\naims to increase understanding and interpretability of emergent behaviours of\nAI agents operating under such survival pressure, and offer guidelines for\nsafely deploying such AI systems in critical resource-limited environments.", "authors": ["Daniel Jarne Ornia", "Nicholas Bishop", "Joel Dyer", "Wei-Chen Lee", "Ani Calinescu", "Doyne Farme", "Michael Wooldridge"], "published_date": "2025-05-29", "title_zh": "資源約束下理性主體的湧現風險意識", "summary_zh": "具備代理能力的先進推理模型（AI代理）被部署以與人類互動，並在近似效用函數和內部模型下解決序列決策問題。當這些問題存在資源或故障約束，導致行動序列可能在資源耗盡時被迫終止時，代理面臨隱含的權衡，重塑其效用驅動的（理性）行為。此外，由於這些代理通常由人類委託代表其行事，約束暴露的不對稱性可能導致人類目標與代理激勵之間出現先前未預料到的錯位。本研究透過生存匪徒框架形式化此設定，提供量化生存驅動偏好轉變影響的理論和實證結果，識別錯位產生的條件，並提出緩解風險偏好或風險規避行為出現的機制。因此，本研究旨在提高對在此類生存壓力下運作的AI代理的突發行為的理解和可解釋性，並為在關鍵資源受限環境中安全部署此類AI系統提供指引。", "audio": "audios/2505.23436v1.mp3", "timestamp": "2025-05-30T21:16:36.585967"}
{"query": "Foundation Model", "id": "2505.22948v1", "url": "http://arxiv.org/abs/2505.22948v1", "title": "Foundation Molecular Grammar: Multi-Modal Foundation Models Induce Interpretable Molecular Graph Languages", "summary": "Recent data-efficient molecular generation approaches exploit graph grammars\nto introduce interpretability into the generative models. However, grammar\nlearning therein relies on expert annotation or unreliable heuristics for\nalgorithmic inference. We propose Foundation Molecular Grammar (FMG), which\nleverages multi-modal foundation models (MMFMs) to induce an interpretable\nmolecular language. By exploiting the chemical knowledge of an MMFM, FMG\nrenders molecules as images, describes them as text, and aligns information\nacross modalities using prompt learning. FMG can be used as a drop-in\nreplacement for the prior grammar learning approaches in molecular generation\nand property prediction. We show that FMG not only excels in synthesizability,\ndiversity, and data efficiency but also offers built-in chemical\ninterpretability for automated molecular discovery workflows. Code is available\nat https://github.com/shiningsunnyday/induction.", "authors": ["Michael Sun", "Weize Yuan", "Gang Liu", "Wojciech Matusik", "Jie Chen"], "published_date": "2025-05-29", "title_zh": "基礎分子文法：多模態基礎模型誘導可解釋的分子圖語言", "summary_zh": "近期資料效率分子生成方法利用圖語法將可解釋性導入生成模型。然而，其中語法學習依賴專家標註或不可靠的演算法推論。我們提出基礎分子語法(FMG)，利用多模態基礎模型(MMFMs)誘導可解釋的分子語言。藉由利用MMFM的化學知識，FMG將分子呈現為圖像、描述為文本，並使用提示學習跨模態對齊資訊。FMG可直接替換分子生成和性質預測中先前的語法學習方法。實驗表明，FMG不僅在可合成性、多樣性和資料效率方面表現出色，還為自動化分子發現工作流程提供內建的化學可解釋性。程式碼可在https://github.com/shiningsunnyday/induction取得。", "audio": "audios/2505.22948v1.mp3", "timestamp": "2025-05-30T21:16:43.052108"}
{"query": "Diffusion Model", "id": "2505.23283v1", "url": "http://arxiv.org/abs/2505.23283v1", "title": "RSFAKE-1M: A Large-Scale Dataset for Detecting Diffusion-Generated Remote Sensing Forgeries", "summary": "Detecting forged remote sensing images is becoming increasingly critical, as\nsuch imagery plays a vital role in environmental monitoring, urban planning,\nand national security. While diffusion models have emerged as the dominant\nparadigm for image generation, their impact on remote sensing forgery detection\nremains underexplored. Existing benchmarks primarily target GAN-based forgeries\nor focus on natural images, limiting progress in this critical domain. To\naddress this gap, we introduce RSFAKE-1M, a large-scale dataset of 500K forged\nand 500K real remote sensing images. The fake images are generated by ten\ndiffusion models fine-tuned on remote sensing data, covering six generation\nconditions such as text prompts, structural guidance, and inpainting. This\npaper presents the construction of RSFAKE-1M along with a comprehensive\nexperimental evaluation using both existing detectors and unified baselines.\nThe results reveal that diffusion-based remote sensing forgeries remain\nchallenging for current methods, and that models trained on RSFAKE-1M exhibit\nnotably improved generalization and robustness. Our findings underscore the\nimportance of RSFAKE-1M as a foundation for developing and evaluating\nnext-generation forgery detection approaches in the remote sensing domain. The\ndataset and other supplementary materials are available at\nhttps://huggingface.co/datasets/TZHSW/RSFAKE/.", "authors": ["Zhihong Tan", "Jiayi Wang", "Huiying Shi", "Binyuan Huang", "Hongchen Wei", "Zhenzhong Chen"], "published_date": "2025-05-29", "title_zh": "RSFAKE-1M：用於檢測擴散生成遙感偽造影像的大規模資料集", "summary_zh": "遙感影像偽造檢測日益重要，但擴散模型對此領域的影響尚未充分研究。現有基準測試主要針對基於GAN的偽造或自然圖像，限制了相關進展。為解決此問題，我們推出RSFAKE-1M，一個包含50萬張偽造和50萬張真實遙感影像的大規模數據集。偽造影像由十個微調於遙感數據的擴散模型生成，涵蓋六種生成條件。本文介紹RSFAKE-1M的構建，並利用現有檢測器和統一基準進行全面實驗評估。結果表明，基於擴散模型的遙感偽造對現有方法仍具挑戰性，且在RSFAKE-1M上訓練的模型展現出顯著的泛化性和穩健性。我們的研究強調了RSFAKE-1M作為開發和評估下一代遙感領域偽造檢測方法的重要性。數據集和其他補充材料可在指定網址獲取。", "audio": "audios/2505.23283v1.mp3", "timestamp": "2025-05-30T21:16:49.540935"}
{"query": "AI", "id": "2505.23432v1", "url": "http://arxiv.org/abs/2505.23432v1", "title": "A Mathematical Framework for AI-Human Integration in Work", "summary": "The rapid rise of Generative AI (GenAI) tools has sparked debate over their\nrole in complementing or replacing human workers across job contexts. We\npresent a mathematical framework that models jobs, workers, and worker-job fit,\nintroducing a novel decomposition of skills into decision-level and\naction-level subskills to reflect the complementary strengths of humans and\nGenAI. We analyze how changes in subskill abilities affect job success,\nidentifying conditions for sharp transitions in success probability. We also\nestablish sufficient conditions under which combining workers with\ncomplementary subskills significantly outperforms relying on a single worker.\nThis explains phenomena such as productivity compression, where GenAI\nassistance yields larger gains for lower-skilled workers. We demonstrate the\nframework' s practicality using data from O*NET and Big-Bench Lite, aligning\nreal-world data with our model via subskill-division methods. Our results\nhighlight when and how GenAI complements human skills, rather than replacing\nthem.", "authors": ["Elisa Celis", "Lingxiao Huang", "Nisheeth K. Vishnoi"], "published_date": "2025-05-29", "title_zh": "工作場域中人工智慧與人類整合之數學框架", "summary_zh": "生成式人工智慧工具的快速崛起引發了關於其在工作中輔助或取代人類的爭論。本研究提出一個數學框架，模擬工作、工作者及工作者與工作的適配度，並將技能分解為決策層次和行動層次的次級技能，以反映人類和生成式人工智慧的互補優勢。分析次級技能能力變化如何影響工作成功，並識別成功機率急劇轉變的條件。此外，確立了在互補次級技能的工作者協作顯著優於單一工作者的充分條件，解釋了諸如生產力壓縮等現象，即生成式人工智慧輔助對低技能工作者的效益更大。透過次級技能劃分方法，使用O*NET和Big-Bench Lite的數據驗證框架的實用性，將真實世界數據與模型對齊。研究結果強調了生成式人工智慧在何時以及如何與人類技能互補，而非取代。", "audio": "audios/2505.23432v1.mp3", "timestamp": "2025-05-30T22:17:48.313855"}
{"query": "Foundation Model", "id": "2505.22904v1", "url": "http://arxiv.org/abs/2505.22904v1", "title": "Defining Foundation Models for Computational Science: A Call for Clarity and Rigor", "summary": "The widespread success of foundation models in natural language processing\nand computer vision has inspired researchers to extend the concept to\nscientific machine learning and computational science. However, this position\npaper argues that as the term \"foundation model\" is an evolving concept, its\napplication in computational science is increasingly used without a universally\naccepted definition, potentially creating confusion and diluting its precise\nscientific meaning. In this paper, we address this gap by proposing a formal\ndefinition of foundation models in computational science, grounded in the core\nvalues of generality, reusability, and scalability. We articulate a set of\nessential and desirable characteristics that such models must exhibit, drawing\nparallels with traditional foundational methods, like the finite element and\nfinite volume methods. Furthermore, we introduce the Data-Driven Finite Element\nMethod (DD-FEM), a framework that fuses the modular structure of classical FEM\nwith the representational power of data-driven learning. We demonstrate how\nDD-FEM addresses many of the key challenges in realizing foundation models for\ncomputational science, including scalability, adaptability, and physics\nconsistency. By bridging traditional numerical methods with modern AI\nparadigms, this work provides a rigorous foundation for evaluating and\ndeveloping novel approaches toward future foundation models in computational\nscience.", "authors": ["Youngsoo Choi", "Siu Wun Cheung", "Youngkyu Kim", "Ping-Hsuan Tsai", "Alejandro N. Diaz", "Ivan Zanardi", "Seung Whan Chung", "Dylan Matthew Copeland", "Coleman Kendrick", "William Anderson", "Traian Iliescu", "Matthias Heinkenschloss"], "published_date": "2025-05-28", "title_zh": "計算科學基礎模型的定義：明確性與嚴謹性的呼籲", "summary_zh": "自然語言處理和計算機視覺中基石模型的廣泛成功激勵研究人員將其概念擴展至科學機器學習和計算科學。本文指出，基石模型一詞尚在演進，其在計算科學中的應用日益普及，但缺乏普遍接受的定義，可能造成混淆並淡化其精確科學意義。為此，本文提出計算科學中基石模型的正式定義，強調通用性、可重用性和可擴展性，並闡述了此類模型必須具備的必要和期望特性，與有限元素和有限體積等傳統基礎方法進行類比。此外，本文介紹了數據驅動有限元素法（DD-FEM），它融合了傳統有限元素法的模塊化結構與數據驅動學習的表徵能力。研究展示了DD-FEM如何解決計算科學中基石模型實現的關鍵挑戰，包括可擴展性、適應性和物理一致性。通過連接傳統數值方法與現代人工智慧範式，本研究為評估和開發計算科學中未來基石模型的新方法提供了嚴格的基礎。", "audio": "audios/2505.22904v1.mp3", "timestamp": "2025-05-30T22:17:54.627029"}
{"query": "Diffusion Model", "id": "2505.23265v1", "url": "http://arxiv.org/abs/2505.23265v1", "title": "Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening with MLLMs", "summary": "Multimodal Large Language Models (MLLMs) are of great application across many\ndomains, such as multimodal understanding and generation. With the development\nof diffusion models (DM) and unified MLLMs, the performance of image generation\nhas been significantly improved, however, the study of image screening is rare\nand its performance with MLLMs is unsatisfactory due to the lack of data and\nthe week image aesthetic reasoning ability in MLLMs. In this work, we propose a\ncomplete solution to address these problems in terms of data and methodology.\nFor data, we collect a comprehensive medical image screening dataset with 1500+\nsamples, each sample consists of a medical image, four generated images, and a\nmultiple-choice answer. The dataset evaluates the aesthetic reasoning ability\nunder four aspects: \\textit{(1) Appearance Deformation, (2) Principles of\nPhysical Lighting and Shadow, (3) Placement Layout, (4) Extension Rationality}.\nFor methodology, we utilize long chains of thought (CoT) and Group Relative\nPolicy Optimization with Dynamic Proportional Accuracy reward, called DPA-GRPO,\nto enhance the image aesthetic reasoning ability of MLLMs. Our experimental\nresults reveal that even state-of-the-art closed-source MLLMs, such as GPT-4o\nand Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic\nreasoning. In contrast, by leveraging the reinforcement learning approach, we\nare able to surpass the score of both large-scale models and leading\nclosed-source models using a much smaller model. We hope our attempt on medical\nimage screening will serve as a regular configuration in image aesthetic\nreasoning in the future.", "authors": ["Zheng Sun", "Yi Wei", "Long Yu"], "published_date": "2025-05-29", "title_zh": "圖像美學推理：MLLM在醫學影像篩查中的新基準", "summary_zh": "多模態大型語言模型(MLLM)在多模態理解與生成等領域應用廣泛。圖像生成效能受益於擴散模型(DM)與統一MLLM的發展而顯著提升，然而，圖像篩選研究相對缺乏，且受限於數據不足與MLLM在圖像美學推理能力上的不足，表現並不理想。本研究針對數據與方法提出完整解決方案，構建包含1500多個樣本的綜合醫療圖像篩選數據集，每個樣本包含一張醫療圖像、四張生成圖像及一個多項選擇題，從外觀變形、光影物理原則、布局和擴展合理性四個方面評估美學推理能力。研究採用長鏈思維(CoT)及動態比例準確度獎勵的分組相對策略優化(DPA-GRPO)方法，增強MLLM的圖像美學推理能力。實驗結果顯示，即使是GPT-4o和Qwen-VL-Max等頂尖閉源MLLM，在圖像美學推理方面表現也接近隨機猜測。透過強化學習方法，本研究能以更小的模型超越大型模型和領先閉源模型的表現。期望本研究在醫療圖像篩選上的嘗試，未來能成為圖像美學推理的常規配置。", "audio": "audios/2505.23265v1.mp3", "timestamp": "2025-05-30T22:18:02.469675"}
{"query": "AI", "id": "2505.23422v1", "url": "http://arxiv.org/abs/2505.23422v1", "title": "From Knowledge to Noise: CTIM-Rover and the Pitfalls of Episodic Memory in Software Engineering Agents", "summary": "We introduce CTIM-Rover, an AI agent for Software Engineering (SE) built on\ntop of AutoCodeRover (Zhang et al., 2024) that extends agentic reasoning\nframeworks with an episodic memory, more specifically, a general and\nrepository-level Cross-Task-Instance Memory (CTIM). While existing open-source\nSE agents mostly rely on ReAct (Yao et al., 2023b), Reflexion (Shinn et al.,\n2023), or Code-Act (Wang et al., 2024), all of these reasoning and planning\nframeworks inefficiently discard their long-term memory after a single task\ninstance. As repository-level understanding is pivotal for identifying all\nlocations requiring a patch for fixing a bug, we hypothesize that SE is\nparticularly well positioned to benefit from CTIM. For this, we build on the\nExperiential Learning (EL) approach ExpeL (Zhao et al., 2024), proposing a\nMixture-Of-Experts (MoEs) inspired approach to create both a general-purpose\nand repository-level CTIM. We find that CTIM-Rover does not outperform\nAutoCodeRover in any configuration and thus conclude that neither ExpeL nor\nDoT-Bank (Lingam et al., 2024) scale to real-world SE problems. Our analysis\nindicates noise introduced by distracting CTIM items or exemplar trajectories\nas the likely source of the performance degradation.", "authors": ["Tobias Lindenbauer", "Georg Groh", "Hinrich Schütze"], "published_date": "2025-05-29", "title_zh": "從知識到雜訊：CTIM-Rover與軟體工程代理程式情節記憶的陷阱", "summary_zh": "CTIM-Rover為一軟體工程AI代理，基於AutoCodeRover，透過跨任務實例記憶(CTIM)擴展代理推理框架。現有開源軟體工程代理多仰賴ReAct、Reflexion或Code-Act，但這些框架於單次任務後捨棄長期記憶，效率不彰。由於理解儲存庫層級對於定位錯誤修復位置至關重要，因此我們認為軟體工程可受益於CTIM。基於經驗學習(EL)方法ExpeL，提出混合專家(MoEs)方法以創建通用及儲存庫層級的CTIM。實驗結果顯示，CTIM-Rover在任何配置下均未超越AutoCodeRover，推論ExpeL或DoT-Bank無法擴展至實際軟體工程問題。分析指出，CTIM項目或範例軌跡引入的雜訊可能是效能下降的原因。", "audio": "audios/2505.23422v1.mp3", "timestamp": "2025-05-30T23:17:27.230441"}
{"query": "Foundation Model", "id": "2505.22820v1", "url": "http://arxiv.org/abs/2505.22820v1", "title": "Preference Learning with Response Time", "summary": "This paper investigates the integration of response time data into human\npreference learning frameworks for more effective reward model elicitation.\nWhile binary preference data has become fundamental in fine-tuning foundation\nmodels, generative AI systems, and other large-scale models, the valuable\ntemporal information inherent in user decision-making remains largely\nunexploited. We propose novel methodologies to incorporate response time\ninformation alongside binary choice data, leveraging the Evidence Accumulation\nDrift Diffusion (EZ) model, under which response time is informative of the\npreference strength. We develop Neyman-orthogonal loss functions that achieve\noracle convergence rates for reward model learning, matching the theoretical\noptimal rates that would be attained if the expected response times for each\nquery were known a priori. Our theoretical analysis demonstrates that for\nlinear reward functions, conventional preference learning suffers from error\nrates that scale exponentially with reward magnitude. In contrast, our response\ntime-augmented approach reduces this to polynomial scaling, representing a\nsignificant improvement in sample efficiency. We extend these guarantees to\nnon-parametric reward function spaces, establishing convergence properties for\nmore complex, realistic reward models. Our extensive experiments validate our\ntheoretical findings in the context of preference learning over images.", "authors": ["Ayush Sawarni", "Sahasrajit Sarmasarkar", "Vasilis Syrgkanis"], "published_date": "2025-05-28", "title_zh": "帶反應時間的偏好學習", "summary_zh": "本研究探討將反應時間數據整合至人類偏好學習框架中，以更有效地獲取獎勵模型。儘管二元偏好數據已成為微調基礎模型、生成式AI系統及其他大型模型的基礎，但使用者決策中固有的寶貴時間資訊仍未被充分利用。我們提出新方法，將反應時間資訊與二元選擇數據結合，利用證據累積漂移擴散(EZ)模型，在此模型下，反應時間可反映偏好強度。我們開發奈曼正交損失函數，實現獎勵模型學習的神諭收斂速度，與預先已知每個查詢的預期反應時間時可達到的理論最佳速度相符。理論分析表明，對於線性獎勵函數，傳統偏好學習的誤差率隨獎勵幅度呈指數級增長。相比之下，我們基於反應時間增強的方法將其降低至多項式級別，顯著提高樣本效率。我們將這些保證擴展到非參數獎勵函數空間，為更複雜、更真實的獎勵模型建立收斂特性。大量實驗驗證了我們在圖像偏好學習中的理論發現。", "audio": "audios/2505.22820v1.mp3", "timestamp": "2025-05-30T23:17:37.428500"}
{"query": "Diffusion Model", "id": "2505.23264v1", "url": "http://arxiv.org/abs/2505.23264v1", "title": "Efficiently Access Diffusion Fisher: Within the Outer Product Span Space", "summary": "Recent Diffusion models (DMs) advancements have explored incorporating the\nsecond-order diffusion Fisher information (DF), defined as the negative Hessian\nof log density, into various downstream tasks and theoretical analysis.\nHowever, current practices typically approximate the diffusion Fisher by\napplying auto-differentiation to the learned score network. This black-box\nmethod, though straightforward, lacks any accuracy guarantee and is\ntime-consuming. In this paper, we show that the diffusion Fisher actually\nresides within a space spanned by the outer products of score and initial data.\nBased on the outer-product structure, we develop two efficient approximation\nalgorithms to access the trace and matrix-vector multiplication of DF,\nrespectively. These algorithms bypass the auto-differentiation operations with\ntime-efficient vector-product calculations. Furthermore, we establish the\napproximation error bounds for the proposed algorithms. Experiments in\nlikelihood evaluation and adjoint optimization demonstrate the superior\naccuracy and reduced computational cost of our proposed algorithms.\nAdditionally, based on the novel outer-product formulation of DF, we design the\nfirst numerical verification experiment for the optimal transport property of\nthe general PF-ODE deduced map.", "authors": ["Fangyikang Wang", "Hubery Yin", "Shaobin Zhuang", "Huminhao Zhu", "Yinan Li", "Lei Qian", "Chao Zhang", "Hanbin Zhao", "Hui Qian", "Chen Li"], "published_date": "2025-05-29", "title_zh": "在外積張成空間內高效存取擴散費雪資訊", "summary_zh": "近期擴散模型的研究進展探索將二階擴散費雪資訊（定義為對數密度的負海森矩陣）納入下游任務與理論分析。然而，現行方法通常透過對學習到的分數網絡進行自動微分來近似擴散費雪資訊，此黑箱方法雖直接，卻缺乏準確性保證且耗時。本文證明擴散費雪資訊實際上位於由分數與初始資料外積所張成的空間中。基於此外積結構，我們開發兩種高效近似算法，分別用於計算擴散費雪資訊的跡與矩陣向量乘積。這些算法透過省時的向量積計算繞過自動微分操作，並建立了所提算法的近似誤差界限。在似然評估和伴隨優化中的實驗表明，我們的算法具有更高的準確性和更低的計算成本。此外，基於擴散費雪資訊的新穎外積公式，我們設計了首個數值驗證實驗，用於驗證一般PF-ODE推導映射的最優傳輸性質。", "audio": "audios/2505.23264v1.mp3", "timestamp": "2025-05-30T23:17:46.231898"}
{"query": "AI", "id": "2505.23421v1", "url": "http://arxiv.org/abs/2505.23421v1", "title": "OTPTO: Joint Product Selection and Inventory Optimization in Fresh E-commerce Front-End Warehouses", "summary": "In China's competitive fresh e-commerce market, optimizing operational\nstrategies, especially inventory management in front-end warehouses, is key to\nenhance customer satisfaction and to gain a competitive edge. Front-end\nwarehouses are placed in residential areas to ensure the timely delivery of\nfresh goods and are usually in small size. This brings the challenge of\ndeciding which goods to stock and in what quantities, taking into account\ncapacity constraints. To address this issue, traditional predict-then-optimize\n(PTO) methods that predict sales and then decide on inventory often don't align\nprediction with inventory goals, as well as fail to prioritize consumer\nsatisfaction. This paper proposes a multi-task\nOptimize-then-Predict-then-Optimize (OTPTO) approach that jointly optimizes\nproduct selection and inventory management, aiming to increase consumer\nsatisfaction by maximizing the full order fulfillment rate. Our method employs\na 0-1 mixed integer programming model OM1 to determine historically optimal\ninventory levels, and then uses a product selection model PM1 and the stocking\nmodel PM2 for prediction. The combined results are further refined through a\npost-processing algorithm OM2. Experimental results from JD.com's 7Fresh\nplatform demonstrate the robustness and significant advantages of our OTPTO\nmethod. Compared to the PTO approach, our OTPTO method substantially enhances\nthe full order fulfillment rate by 4.34% (a relative increase of 7.05%) and\nnarrows the gap to the optimal full order fulfillment rate by 5.27%. These\nfindings substantiate the efficacy of the OTPTO method in managing inventory at\nfront-end warehouses of fresh e-commerce platforms and provide valuable\ninsights for future research in this domain.", "authors": ["Zheming Zhang", "Yan Jiang", "Qingshan Li", "Ai Han"], "published_date": "2025-05-29", "title_zh": "OTPTO：生鮮電商前置倉產品聯合選擇與庫存優化", "summary_zh": "為提升中國生鮮電商的競爭力及顧客滿意度，前置倉的庫存管理至關重要。考量前置倉通常規模較小，且需及時配送生鮮商品，故庫存品項及數量的決策極具挑戰。傳統的「預測後優化」方法未能兼顧預測與庫存目標的一致性，也忽略了顧客滿意度的優先性。本研究提出一種多任務「優化-預測-再優化」（OTPTO）方法，聯合優化商品選擇與庫存管理，旨在最大化完整訂單履行率，進而提升顧客滿意度。該方法首先利用0-1混合整數規劃模型OM1確定歷史最佳庫存水平，再透過商品選擇模型PM1與庫存模型PM2進行預測，最後經由後處理算法OM2進行精煉。京東7Fresh平台的實驗結果顯示，OTPTO方法具備穩健性與顯著優勢，相較於「預測後優化」方法，完整訂單履行率顯著提升4.34%（相對增長7.05%），並將與最佳完整訂單履行率的差距縮小5.27%。研究結果證實OTPTO方法在生鮮電商平台前置倉庫存管理上的有效性，並為未來研究提供寶貴見解。", "audio": "audios/2505.23421v1.mp3", "timestamp": "2025-05-31T01:24:50.419919"}
{"query": "Foundation Model", "id": "2505.22815v1", "url": "http://arxiv.org/abs/2505.22815v1", "title": "IMTS is Worth Time $\\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction", "summary": "Irregular Multivariate Time Series (IMTS) forecasting is challenging due to\nthe unaligned nature of multi-channel signals and the prevalence of extensive\nmissing data. Existing methods struggle to capture reliable temporal patterns\nfrom such data due to significant missing values. While pre-trained foundation\nmodels show potential for addressing these challenges, they are typically\ndesigned for Regularly Sampled Time Series (RTS). Motivated by the visual Mask\nAutoEncoder's (MAE) powerful capability for modeling sparse multi-channel\ninformation and its success in RTS forecasting, we propose VIMTS, a framework\nadapting Visual MAE for IMTS forecasting. To mitigate the effect of missing\nvalues, VIMTS first processes IMTS along the timeline into feature patches at\nequal intervals. These patches are then complemented using learned\ncross-channel dependencies. Then it leverages visual MAE's capability in\nhandling sparse multichannel data for patch reconstruction, followed by a\ncoarse-to-fine technique to generate precise predictions from focused contexts.\nIn addition, we integrate self-supervised learning for improved IMTS modeling\nby adapting the visual MAE to IMTS data. Extensive experiments demonstrate\nVIMTS's superior performance and few-shot capability, advancing the application\nof visual foundation models in more general time series tasks. Our code is\navailable at https://github.com/WHU-HZY/VIMTS.", "authors": ["Zhangyi Hu", "Jiemin Wu", "Hua Xu", "Mingqian Liao", "Ninghui Feng", "Bo Gao", "Songning Lai", "Yutao Yue"], "published_date": "2025-05-28", "title_zh": "IMTS：時序與通道修補的價值：用於不規則多變量時間序列預測的視覺遮蔽式自動編碼器", "summary_zh": "不規則多元時間序列（IMTS）預測因多通道訊號未對齊及大量缺失值而具挑戰性。現有方法難以從此類數據中捕獲可靠的時間模式。視覺遮罩自動編碼器（MAE）在稀疏多通道資訊建模方面的能力及其在規則採樣時間序列（RTS）預測中的成功，促使我們提出VIMTS，一個將視覺MAE適應於IMTS預測的框架。VIMTS首先沿時間軸將IMTS處理成等間隔的特徵塊，並利用學習到的跨通道依賴關係對這些塊進行補全，以減輕缺失值的影響。接著利用視覺MAE處理稀疏多通道數據的能力進行塊重建，然後採用由粗到精的技術，從聚焦的上下文中生成精確預測。此外，我們整合了自我監督學習，以增強IMTS建模。大量實驗表明VIMTS具有卓越的性能和少量樣本學習能力，推進了視覺基礎模型在更廣泛的時間序列任務中的應用。", "audio": "audios/2505.22815v1.mp3", "timestamp": "2025-05-31T01:25:02.314155"}
{"query": "Diffusion Model", "id": "2505.23189v1", "url": "http://arxiv.org/abs/2505.23189v1", "title": "TrackVLA: Embodied Visual Tracking in the Wild", "summary": "Embodied visual tracking is a fundamental skill in Embodied AI, enabling an\nagent to follow a specific target in dynamic environments using only egocentric\nvision. This task is inherently challenging as it requires both accurate target\nrecognition and effective trajectory planning under conditions of severe\nocclusion and high scene dynamics. Existing approaches typically address this\nchallenge through a modular separation of recognition and planning. In this\nwork, we propose TrackVLA, a Vision-Language-Action (VLA) model that learns the\nsynergy between object recognition and trajectory planning. Leveraging a shared\nLLM backbone, we employ a language modeling head for recognition and an\nanchor-based diffusion model for trajectory planning. To train TrackVLA, we\nconstruct an Embodied Visual Tracking Benchmark (EVT-Bench) and collect diverse\ndifficulty levels of recognition samples, resulting in a dataset of 1.7 million\nsamples. Through extensive experiments in both synthetic and real-world\nenvironments, TrackVLA demonstrates SOTA performance and strong\ngeneralizability. It significantly outperforms existing methods on public\nbenchmarks in a zero-shot manner while remaining robust to high dynamics and\nocclusion in real-world scenarios at 10 FPS inference speed. Our project page\nis: https://pku-epic.github.io/TrackVLA-web.", "authors": ["Shaoan Wang", "Jiazhao Zhang", "Minghan Li", "Jiahang Liu", "Anqi Li", "Kui Wu", "Fangwei Zhong", "Junzhi Yu", "Zhizheng Zhang", "He Wang"], "published_date": "2025-05-29", "title_zh": "TrackVLA：野外環境下具身視覺追蹤", "summary_zh": "具身視覺追蹤是具身人工智慧中的一項基礎技能，使代理人僅透過自我中心視覺即可在動態環境中追蹤特定目標。此任務極具挑戰性，需要在嚴重遮擋和高場景動態下，精確識別目標並有效規劃路徑。現有方法通常將識別和規劃模組化分離。本文提出 TrackVLA，一個視覺-語言-動作（VLA）模型，學習物體識別和路徑規劃之間的協同作用。TrackVLA 利用共享的 LLM 主幹，採用語言建模頭進行識別，並採用基於錨點的擴散模型進行路徑規劃。為訓練 TrackVLA，我們構建了具身視覺追蹤基準（EVT-Bench），並收集了不同難度的識別樣本，產生一個包含 170 萬個樣本的數據集。在合成和真實環境中的廣泛實驗表明，TrackVLA 展現了最先進的性能和強大的泛化能力。它在公共基準測試中以零樣本方式顯著優於現有方法，同時在真實場景中保持對高動態和遮擋的魯棒性，推理速度為 10 FPS。", "audio": "audios/2505.23189v1.mp3", "timestamp": "2025-05-31T01:25:13.932994"}
{"query": "AI", "id": "2505.23417v1", "url": "http://arxiv.org/abs/2505.23417v1", "title": "Toward Effective AI Governance: A Review of Principles", "summary": "Artificial Intelligence (AI) governance is the practice of establishing\nframeworks, policies, and procedures to ensure the responsible, ethical, and\nsafe development and deployment of AI systems. Although AI governance is a core\npillar of Responsible AI, current literature still lacks synthesis across such\ngovernance frameworks and practices. Objective: To identify which frameworks,\nprinciples, mechanisms, and stakeholder roles are emphasized in secondary\nliterature on AI governance. Method: We conducted a rapid tertiary review of\nnine peer-reviewed secondary studies from IEEE and ACM (20202024), using\nstructured inclusion criteria and thematic semantic synthesis. Results: The\nmost cited frameworks include the EU AI Act and NIST RMF; transparency and\naccountability are the most common principles. Few reviews detail actionable\ngovernance mechanisms or stakeholder strategies. Conclusion: The review\nconsolidates key directions in AI governance and highlights gaps in empirical\nvalidation and inclusivity. Findings inform both academic inquiry and practical\nadoption in organizations.", "authors": ["Danilo Ribeiro", "Thayssa Rocha", "Gustavo Pinto", "Bruno Cartaxo", "Marcelo Amaral", "Nicole Davila", "Ana Camargo"], "published_date": "2025-05-29", "title_zh": "邁向有效的AI治理：原則綜述", "summary_zh": "人工智慧治理旨在建立框架、政策和程序，以確保人工智慧系統的負責任、合乎倫理和安全開發與部署。儘管人工智慧治理是負責任人工智慧的核心支柱，但現有文獻仍缺乏對各治理框架和實踐的綜合。本研究旨在識別人工智慧治理相關文獻中強調的框架、原則、機制和利害關係人角色。我們對IEEE和ACM在2020至2024年間發表的九篇同行評審文獻進行了快速三級回顧，採用結構化納入標準和主題語義綜合。結果顯示，歐盟人工智慧法案和NIST風險管理框架是被引用最多的框架，透明度和問責制是最常見的原則。鮮有文獻詳細闡述可行的治理機制或利害關係人策略。結論是，本回顧整合了人工智慧治理的關鍵方向，並突顯了經驗驗證和包容性方面的差距。研究結果可為學術探討和組織的實際應用提供資訊。", "audio": "audios/2505.23417v1.mp3", "timestamp": "2025-05-31T03:09:53.143293"}
{"query": "Foundation Model", "id": "2505.22805v1", "url": "http://arxiv.org/abs/2505.22805v1", "title": "Anomalies by Synthesis: Anomaly Detection using Generative Diffusion Models for Off-Road Navigation", "summary": "In order to navigate safely and reliably in off-road and unstructured\nenvironments, robots must detect anomalies that are out-of-distribution (OOD)\nwith respect to the training data. We present an analysis-by-synthesis approach\nfor pixel-wise anomaly detection without making any assumptions about the\nnature of OOD data. Given an input image, we use a generative diffusion model\nto synthesize an edited image that removes anomalies while keeping the\nremaining image unchanged. Then, we formulate anomaly detection as analyzing\nwhich image segments were modified by the diffusion model. We propose a novel\ninference approach for guided diffusion by analyzing the ideal guidance\ngradient and deriving a principled approximation that bootstraps the diffusion\nmodel to predict guidance gradients. Our editing technique is purely test-time\nthat can be integrated into existing workflows without the need for retraining\nor fine-tuning. Finally, we use a combination of vision-language foundation\nmodels to compare pixels in a learned feature space and detect semantically\nmeaningful edits, enabling accurate anomaly detection for off-road navigation.\nProject website: https://siddancha.github.io/anomalies-by-diffusion-synthesis/", "authors": ["Siddharth Ancha", "Sunshine Jiang", "Travis Manderson", "Laura Brandt", "Yilun Du", "Philip R. Osteen", "Nicholas Roy"], "published_date": "2025-05-28", "title_zh": "合成異常：使用生成擴散模型進行越野導航的異常偵測", "summary_zh": "為使機器人在越野及非結構化環境中安全可靠地導航，必須檢測相對於訓練資料的異常值。本文提出一種基於分析合成的像素級異常檢測方法，無需對異常資料的性質做任何假設。給定輸入圖像，我們使用生成式擴散模型合成編輯後的圖像，以移除異常，同時保持剩餘圖像不變。然後，我們將異常檢測公式化為分析哪些圖像片段被擴散模型修改。本文提出一種新穎的引導擴散推理方法，透過分析理想的引導梯度並推導出一個有原則的近似值，引導擴散模型預測引導梯度。我們的編輯技術純粹是測試時操作，可以整合到現有的工作流程中，無需重新訓練或微調。最後，我們結合視覺語言基礎模型，在學習的特徵空間中比較像素，並檢測語義上有意義的編輯，從而為越野導航實現準確的異常檢測。", "audio": "audios/2505.22805v1.mp3", "timestamp": "2025-05-31T03:09:59.923284"}
{"query": "Diffusion Model", "id": "2505.23186v1", "url": "http://arxiv.org/abs/2505.23186v1", "title": "HiGarment: Cross-modal Harmony Based Diffusion Model for Flat Sketch to Realistic Garment Image", "summary": "Diffusion-based garment synthesis tasks primarily focus on the design phase\nin the fashion domain, while the garment production process remains largely\nunderexplored. To bridge this gap, we introduce a new task: Flat Sketch to\nRealistic Garment Image (FS2RG), which generates realistic garment images by\nintegrating flat sketches and textual guidance. FS2RG presents two key\nchallenges: 1) fabric characteristics are solely guided by textual prompts,\nproviding insufficient visual supervision for diffusion-based models, which\nlimits their ability to capture fine-grained fabric details; 2) flat sketches\nand textual guidance may provide conflicting information, requiring the model\nto selectively preserve or modify garment attributes while maintaining\nstructural coherence. To tackle this task, we propose HiGarment, a novel\nframework that comprises two core components: i) a multi-modal semantic\nenhancement mechanism that enhances fabric representation across textual and\nvisual modalities, and ii) a harmonized cross-attention mechanism that\ndynamically balances information from flat sketches and text prompts, allowing\ncontrollable synthesis by generating either sketch-aligned (image-biased) or\ntext-guided (text-biased) outputs. Furthermore, we collect Multi-modal Detailed\nGarment, the largest open-source dataset for garment generation. Experimental\nresults and user studies demonstrate the effectiveness of HiGarment in garment\nsynthesis. The code and dataset will be released.", "authors": ["Junyi Guo", "Jingxuan Zhang", "Fangyu Wu", "Huanda Lu", "Qiufeng Wang", "Wenmian Yang", "Eng Gee Lim", "Dongming Lu"], "published_date": "2025-05-29", "title_zh": "HiGarment：基於跨模態協調擴散模型的平面草圖到逼真服裝圖像生成", "summary_zh": "基於擴散的服裝合成任務主要著重於時尚領域的設計階段，而服裝生產過程則鮮少被探索。為彌補此差距，我們提出一個新任務：平面草圖到逼真服裝圖像 (FS2RG)，其透過整合平面草圖和文字引導來生成逼真的服裝圖像。FS2RG 面臨兩項主要挑戰：1) 織物特性僅受文字提示引導，為基於擴散的模型提供不足的視覺監督，限制了其捕捉精細織物細節的能力；2) 平面草圖和文字引導可能提供衝突資訊，要求模型選擇性地保留或修改服裝屬性，同時保持結構一致性。為了解決此任務，我們提出 HiGarment，一個包含兩個核心組件的新框架：i) 一種多模態語義增強機制，可增強跨文字和視覺模態的織物表示；ii) 一種協調的交叉注意力機制，可動態平衡來自平面草圖和文字提示的資訊，允許透過生成草圖對齊（圖像偏向）或文字引導（文字偏向）的輸出來進行可控合成。此外，我們收集了 Multi-modal Detailed Garment，這是用於服裝生成的最大開源數據集。實驗結果和使用者研究證明了 HiGarment 在服裝合成方面的有效性。程式碼和數據集將會發布。", "audio": "audios/2505.23186v1.mp3", "timestamp": "2025-05-31T03:10:11.726873"}
{"query": "AI", "id": "2505.23405v1", "url": "http://arxiv.org/abs/2505.23405v1", "title": "A Practical Guide for Supporting Formative Assessment and Feedback Using Generative AI", "summary": "Formative assessment is a cornerstone of effective teaching and learning,\nproviding students with feedback to guide their learning. While there has been\nan exponential growth in the application of generative AI in scaling various\naspects of formative assessment, ranging from automatic question generation to\nintelligent tutoring systems and personalized feedback, few have directly\naddressed the core pedagogical principles of formative assessment. Here, we\ncritically examined how generative AI, especially large-language models (LLMs)\nsuch as ChatGPT, can support key components of formative assessment: helping\nstudents, teachers, and peers understand \"where learners are going,\" \"where\nlearners currently are,\" and \"how to move learners forward\" in the learning\nprocess. With the rapid emergence of new prompting techniques and LLM\ncapabilities, we also provide guiding principles for educators to effectively\nleverage cost-free LLMs in formative assessments while remaining grounded in\npedagogical best practices. Furthermore, we reviewed the role of LLMs in\ngenerating feedback, highlighting limitations in current evaluation metrics\nthat inadequately capture the nuances of formative feedback, such as\ndistinguishing feedback at the task, process, and self-regulatory levels.\nFinally, we offer practical guidelines for educators and researchers, including\nconcrete classroom strategies and future directions such as developing robust\nmetrics to assess LLM-generated feedback, leveraging LLMs to overcome systemic\nand cultural barriers to formative assessment, and designing AI-aware\nassessment strategies that promote transferable skills while mitigating\noverreliance on LLM-generated responses. By structuring the discussion within\nan established formative assessment framework, this review provides a\ncomprehensive foundation for integrating LLMs into formative assessment in a\npedagogically informed manner.", "authors": ["Sapolnach Prompiengchai", "Charith Narreddy", "Steve Joordens"], "published_date": "2025-05-29", "title_zh": "利用生成式人工智能支援形成性評估與回饋之實用指南", "summary_zh": "形成性評量是有效教學的基石。生成式AI雖廣泛應用於擴展形成性評量的各面向，但鮮少直接探討其核心教學原則。本文評估大型語言模型（LLMs）如何支持形成性評量的關鍵要素，即幫助學生、教師和同儕理解學習目標、學習現況以及進步策略。考量到提示技巧和LLM能力的快速發展，本文亦為教育者提供指導原則，使其能在教學最佳實務的基礎上，有效地利用免費LLMs進行形成性評量。此外，本文回顧LLMs在生成回饋中的作用，並強調當前評估指標的局限性，其無法充分捕捉形成性回饋的細微差別。最後，本文為教育者和研究人員提供實用指南，包括具體的課堂策略和未來方向，例如開發穩健的指標來評估LLM生成的回饋，利用LLMs克服形成性評量的系統性和文化障礙，以及設計能促進可轉移技能並減輕對LLM生成回應過度依賴的AI感知評量策略。透過在既定的形成性評量框架內進行討論，本文為以教學方式整合LLMs至形成性評量提供了全面的基礎。", "audio": "audios/2505.23405v1.mp3", "timestamp": "2025-05-31T04:22:18.538830"}
{"query": "Foundation Model", "id": "2505.22799v1", "url": "http://arxiv.org/abs/2505.22799v1", "title": "Theory and simulation of elastoinertial rectification of oscillatory flows in two-dimensional deformable rectangular channels", "summary": "A slender two-dimensional (2D) channel bounded by a rigid bottom surface and\na slender elastic layer above deforms when a fluid flows through it.\nHydrodynamic forces cause deformation at the fluid-solid interface, which in\nturn changes the cross-sectional area of the fluidic channel. The nonlinear\ncoupling between flow and deformation, along with the attendant asymmetry in\ngeometry caused by flow-induced deformation, produces a streaming effect (a\nnon-zero cycle-average despite time-periodic forcing). Surprisingly, fluid\ninertia provides another nonlinear coupling, tightly connected to deformation,\nthat enhances streaming, termed ``elastoinertial rectification'' by Zhang and\nRallabandi [J. Fluid Mech. 996, A16 (2024)]. We adapt the latter theory of how\ntwo-way coupled fluid--structure interaction (FSI) produces streaming to a 2D\nrectangular configuration, specifically taking care to capture the deformations\nof the nearly incompressible slender elastic layer via the combined foundation\nmodel of Chandler and Vella [Proc. R. Soc. A 476, 20200551 (2020)]. We\nsupplement the elastoinertial rectification theory with direct numerical\nsimulations performed using a conforming arbitrary Lagrangian-Eulerian (ALE)\nFSI formulation with streamline upwind Petrov-Galerkin stabilization,\nimplemented via the open-source computing platform FEniCS. We examine the axial\nvariation of the cycle-averaged pressure as a function of key dimensionless\ngroups of the problem: the Womersley number, the elastoviscous number, and the\ncompliance number. Assuming a small compliance number, we find excellent\nagreement between theory and simulations for the leading-order pressure and\ndeformation across a range of conditions. At the next order, the cycle-averaged\npressures agree well; however, the cycle-averaged deformation is found to\nexhibit significant axial and vertical displacements, unlike the combined\nfoundation model.", "authors": ["Uday M. Rade", "Shrihari D. Pande", "Ivan C. Christov"], "published_date": "2025-05-28", "title_zh": "二維可變形矩形通道中振盪流的彈性慣性整流理論與模擬", "summary_zh": "流體流經由剛性底面和彈性薄層構成的狹窄二維通道時，會產生形變。流體動力導致流固界面形變，進而改變通道截面積。流動與形變的非線性耦合，以及流動誘導形變產生的幾何不對稱性，導致流動效應（即使是週期性外力，也會產生非零的週期平均）。令人驚訝的是，流體慣性提供了另一種與形變緊密相關的非線性耦合，增強了流動效應，Zhang和Rallabandi稱之為「彈性慣性整流」。我們將後者關於雙向耦合流固互作用（FSI）如何產生流動的理論應用於二維矩形結構，特別採用Chandler和Vella的組合地基模型，捕捉近乎不可壓縮的彈性薄層形變。我們使用基於FEniCS開源計算平台，採用保形任意拉格朗日-歐拉（ALE）FSI公式和流線迎風Petrov-Galerkin穩定化的直接數值模擬，來補充彈性慣性整流理論。我們研究了週期平均壓力沿軸向的變化，並將其作為問題中關鍵無量綱群的函數：沃默斯利數、彈黏數和柔度數。假設柔度數較小，我們發現理論與模擬在各種條件下，對於主要壓力和形變都具有良好的一致性。在更高階的分析中，週期平均壓力吻合良好；然而，週期平均形變表現出顯著的軸向和垂直位移，這與組合地基模型不同。", "audio": "audios/2505.22799v1.mp3", "timestamp": "2025-05-31T04:22:29.583589"}
{"query": "Diffusion Model", "id": "2505.23119v1", "url": "http://arxiv.org/abs/2505.23119v1", "title": "TextSR: Diffusion Super-Resolution with Multilingual OCR Guidance", "summary": "While recent advancements in Image Super-Resolution (SR) using diffusion\nmodels have shown promise in improving overall image quality, their application\nto scene text images has revealed limitations. These models often struggle with\naccurate text region localization and fail to effectively model image and\nmultilingual character-to-shape priors. This leads to inconsistencies, the\ngeneration of hallucinated textures, and a decrease in the perceived quality of\nthe super-resolved text.\n  To address these issues, we introduce TextSR, a multimodal diffusion model\nspecifically designed for Multilingual Scene Text Image Super-Resolution.\nTextSR leverages a text detector to pinpoint text regions within an image and\nthen employs Optical Character Recognition (OCR) to extract multilingual text\nfrom these areas. The extracted text characters are then transformed into\nvisual shapes using a UTF-8 based text encoder and cross-attention. Recognizing\nthat OCR may sometimes produce inaccurate results in real-world scenarios, we\nhave developed two innovative methods to enhance the robustness of our model.\nBy integrating text character priors with the low-resolution text images, our\nmodel effectively guides the super-resolution process, enhancing fine details\nwithin the text and improving overall legibility. The superior performance of\nour model on both the TextZoom and TextVQA datasets sets a new benchmark for\nSTISR, underscoring the efficacy of our approach.", "authors": ["Keren Ye", "Ignacio Garcia Dorado", "Michalis Raptis", "Mauricio Delbracio", "Irene Zhu", "Peyman Milanfar", "Hossein Talebi"], "published_date": "2025-05-29", "title_zh": "TextSR：多語言光學字元辨識引導的擴散超解析度", "summary_zh": "基於擴散模型的影像超解析度技術雖提升整體影像品質，但應用於場景文字影像時暴露出其局限性，難以精確定位文字區域，且無法有效建模影像及多語言字符形狀先驗知識，導致不一致、產生幻覺紋理，並降低超解析文字的感知品質。\n\n為解決上述問題，我們提出TextSR，一種專為多語言場景文字影像超解析度設計的多模態擴散模型。TextSR利用文字檢測器定位影像中的文字區域，並採用光學字符識別（OCR）從這些區域提取多語言文字。提取的文字字符透過基於UTF-8的文字編碼器和交叉注意力轉換為視覺形狀。考量到OCR在真實場景中可能產生不準確結果，我們開發了兩種創新方法以增強模型的穩健性。透過整合文字字符先驗知識和低解析度文字影像，我們的模型有效引導超解析度過程，增強文字中的細微細節並提高整體可讀性。模型在TextZoom和TextVQA數據集上的卓越表現為STISR設立了新基準，突顯了我們方法的有效性。", "audio": "audios/2505.23119v1.mp3", "timestamp": "2025-05-31T04:22:37.870689"}
{"query": "AI", "id": "2505.23404v1", "url": "http://arxiv.org/abs/2505.23404v1", "title": "Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models", "summary": "Adversarial attacks on Large Language Models (LLMs) via jailbreaking\ntechniques-methods that circumvent their built-in safety and ethical\nconstraints-have emerged as a critical challenge in AI security. These attacks\ncompromise the reliability of LLMs by exploiting inherent weaknesses in their\ncomprehension capabilities. This paper investigates the efficacy of\njailbreaking strategies that are specifically adapted to the diverse levels of\nunderstanding exhibited by different LLMs. We propose the Adaptive Jailbreaking\nStrategies Based on the Semantic Understanding Capabilities of Large Language\nModels, a novel framework that classifies LLMs into Type I and Type II\ncategories according to their semantic comprehension abilities. For each\ncategory, we design tailored jailbreaking strategies aimed at leveraging their\nvulnerabilities to facilitate successful attacks. Extensive experiments\nconducted on multiple LLMs demonstrate that our adaptive strategy markedly\nimproves the success rate of jailbreaking. Notably, our approach achieves an\nexceptional 98.9% success rate in jailbreaking GPT-4o(29 May 2025 release)", "authors": ["Mingyu Yu", "Wei Wang", "Yanjie Wei", "Sujuan Qin"], "published_date": "2025-05-29", "title_zh": "基於大型語言模型語義理解能力的自適應越獄策略", "summary_zh": "大型語言模型的對抗性攻擊，即透過繞過其內建安全與倫理限制的越獄技術，已成為人工智慧安全性的重大挑戰。此類攻擊藉由利用語言模型理解能力的固有弱點，損害其可靠性。本研究探討專為不同語言模型所展現的不同理解層次而設計的越獄策略之有效性。我們提出基於大型語言模型語義理解能力的自適應越獄策略，此框架將語言模型依據其語義理解能力分類為第一型和第二型。針對每一類型，我們設計客製化的越獄策略，旨在利用其漏洞以促進成功攻擊。在多個語言模型上進行的廣泛實驗表明，我們的自適應策略顯著提高了越獄的成功率。值得注意的是，我們的研究在針對GPT-4o(2025年5月29日發布)的越獄中，達到了卓越的98.9%成功率。", "audio": "audios/2505.23404v1.mp3", "timestamp": "2025-05-31T05:17:41.825629"}
{"query": "Foundation Model", "id": "2505.22785v1", "url": "http://arxiv.org/abs/2505.22785v1", "title": "Navigating the Latent Space Dynamics of Neural Models", "summary": "Neural networks transform high-dimensional data into compact, structured\nrepresentations, often modeled as elements of a lower dimensional latent space.\nIn this paper, we present an alternative interpretation of neural models as\ndynamical systems acting on the latent manifold. Specifically, we show that\nautoencoder models implicitly define a latent vector field on the manifold,\nderived by iteratively applying the encoding-decoding map, without any\nadditional training. We observe that standard training procedures introduce\ninductive biases that lead to the emergence of attractor points within this\nvector field. Drawing on this insight, we propose to leverage the vector field\nas a representation for the network, providing a novel tool to analyze the\nproperties of the model and the data. This representation enables to: (i)\nanalyze the generalization and memorization regimes of neural models, even\nthroughout training; (ii) extract prior knowledge encoded in the network's\nparameters from the attractors, without requiring any input data; (iii)\nidentify out-of-distribution samples from their trajectories in the vector\nfield. We further validate our approach on vision foundation models, showcasing\nthe applicability and effectiveness of our method in real-world scenarios.", "authors": ["Marco Fumero", "Luca Moschella", "Emanuele Rodolà", "Francesco Locatello"], "published_date": "2025-05-28", "title_zh": "神經模型潛在空間動態導航", "summary_zh": "神經網路將高維數據轉化為精簡且結構化的低維潛在空間表徵。本文提出一種將神經模型視為作用於潛在流形的動態系統的新穎詮釋。我們證明，自編碼器模型隱含地定義了流形上的潛在向量場，該向量場通過迭代應用編碼-解碼映射而得，無需額外訓練。標準訓練程序引入歸納偏置，導致該向量場中出現吸引子點。基於此，我們提議利用向量場作為網路的表徵，提供一種分析模型與數據屬性的新工具。此表徵能夠：(i)分析神經模型的泛化和記憶機制，即使在訓練過程中亦然；(ii)從吸引子中提取編碼在網路參數中的先驗知識，無需任何輸入數據；(iii)從向量場中的軌跡識別異常樣本。我們在視覺基礎模型上驗證了此方法，展示了其在實際場景中的適用性和有效性。", "audio": "audios/2505.22785v1.mp3", "timestamp": "2025-05-31T05:17:47.734304"}
{"query": "Diffusion Model", "id": "2505.23115v1", "url": "http://arxiv.org/abs/2505.23115v1", "title": "Diffusion-Based Generative Models for 3D Occupancy Prediction in Autonomous Driving", "summary": "Accurately predicting 3D occupancy grids from visual inputs is critical for\nautonomous driving, but current discriminative methods struggle with noisy\ndata, incomplete observations, and the complex structures inherent in 3D\nscenes. In this work, we reframe 3D occupancy prediction as a generative\nmodeling task using diffusion models, which learn the underlying data\ndistribution and incorporate 3D scene priors. This approach enhances prediction\nconsistency, noise robustness, and better handles the intricacies of 3D spatial\nstructures. Our extensive experiments show that diffusion-based generative\nmodels outperform state-of-the-art discriminative approaches, delivering more\nrealistic and accurate occupancy predictions, especially in occluded or\nlow-visibility regions. Moreover, the improved predictions significantly\nbenefit downstream planning tasks, highlighting the practical advantages of our\nmethod for real-world autonomous driving applications.", "authors": ["Yunshen Wang", "Yicheng Liu", "Tianyuan Yuan", "Yucheng Mao", "Yingshi Liang", "Xiuyu Yang", "Honggang Zhang", "Hang Zhao"], "published_date": "2025-05-29", "title_zh": "基於擴散的生成模型用於自動駕駛中的三維佔用預測", "summary_zh": "精準預測視覺輸入的3D佔用網格對自駕至關重要，但現有判別式方法難以應對噪聲數據、不完整觀測和3D場景的複雜結構。本研究將3D佔用預測重新構建為生成建模任務，採用擴散模型學習底層數據分佈並整合3D場景先驗知識，從而提升預測一致性、噪聲魯棒性並更好地處理3D空間結構的複雜性。實驗結果表明，基於擴散的生成模型優於現有最佳判別式方法，提供更真實、更準確的佔用預測，尤其是在遮擋或低能見度區域。此外，改進的預測顯著提升下游規劃任務的性能，突顯本方法在實際自駕應用中的優勢。", "audio": "audios/2505.23115v1.mp3", "timestamp": "2025-05-31T05:17:52.758296"}
{"query": "AI", "id": "2505.23397v1", "url": "http://arxiv.org/abs/2505.23397v1", "title": "A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy", "summary": "This article presents a structured framework for Human-AI collaboration in\nSecurity Operations Centers (SOCs), integrating AI autonomy, trust calibration,\nand Human-in-the-loop decision making. Existing frameworks in SOCs often focus\nnarrowly on automation, lacking systematic structures to manage human\noversight, trust calibration, and scalable autonomy with AI. Many assume static\nor binary autonomy settings, failing to account for the varied complexity,\ncriticality, and risk across SOC tasks considering Humans and AI collaboration.\nTo address these limitations, we propose a novel autonomy tiered framework\ngrounded in five levels of AI autonomy from manual to fully autonomous, mapped\nto Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This\nenables adaptive and explainable AI integration across core SOC functions,\nincluding monitoring, protection, threat detection, alert triage, and incident\nresponse. The proposed framework differentiates itself from previous research\nby creating formal connections between autonomy, trust, and HITL across various\nSOC levels, which allows for adaptive task distribution according to\noperational complexity and associated risks. The framework is exemplified\nthrough a simulated cyber range that features the cybersecurity AI-Avatar, a\nfine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates\nhuman-AI collaboration for SOC tasks, reducing alert fatigue, enhancing\nresponse coordination, and strategically calibrating trust. This research\nsystematically presents both the theoretical and practical aspects and\nfeasibility of designing next-generation cognitive SOCs that leverage AI not to\nreplace but to enhance human decision-making.", "authors": ["Ahmad Mohsin", "Helge Janicke", "Ahmed Ibrahim", "Iqbal H. Sarker", "Seyit Camtepe"], "published_date": "2025-05-29", "title_zh": "安全運營中心中具備可信自主性之人機協作統一框架", "summary_zh": "本文提出一個結構化的框架，用於安全運營中心（SOC）中的人機協作，整合了AI自主性、信任校準以及人迴路決策。現有SOC框架通常側重於自動化，缺乏系統性結構來管理人工監督、信任校準以及AI的可擴展自主性。許多框架假設靜態或二元自主性設定，未能考慮SOC任務中人類與AI協作所帶來的複雜性、重要性和風險差異。為解決這些限制，我們提出一個新穎的自主分層框架，基於從手動到完全自主的五個AI自主級別，並對應於人迴路（HITL）角色和任務特定信任閾值。這使得在核心SOC功能（包括監控、保護、威脅檢測、警報分類和事件響應）中實現自適應且可解釋的AI整合。該框架透過在不同SOC層級建立自主性、信任和HITL之間的正式連接，與先前的研究區分開來，從而根據操作複雜性和相關風險進行自適應任務分配。透過一個模擬的網路靶場，以網路安全AI化身（一個微調的基於LLM的SOC助手）為例，展示該框架。AI化身案例研究說明了SOC任務中的人機協作，減少了警報疲勞，增強了響應協調，並策略性地校準了信任。本研究系統地展示了設計下一代認知型SOC的理論、實用性及可行性，利用AI來增強而非取代人類決策。", "audio": "audios/2505.23397v1.mp3", "timestamp": "2025-05-31T06:25:02.017150"}
{"query": "Foundation Model", "id": "2505.22768v1", "url": "http://arxiv.org/abs/2505.22768v1", "title": "Multivariate de Bruijn Graphs: A Symbolic Graph Framework for Time Series Forecasting", "summary": "Time series forecasting remains a challenging task for foundation models due\nto temporal heterogeneity, high dimensionality, and the lack of inherent\nsymbolic structure. In this work, we propose DRAGON (Discrete Representation\nand Augmented Graph encoding Over deBruijN Graphs), a novel encoder that\nintroduces Multivariate de Bruijn Graphs (MdBGs) to bridge the gap between\nsymbolic representations and neural modeling. DRAGON discretizes continuous\ninput sequences and maps them onto a fixed graph structure, enabling dynamic\ncontext recovery via graph-based attention. Integrated as an auxiliary module\nwithin a dual-branch architecture, DRAGON augments conventional CNN-based\nencoders with symbolic, structure-aware representations. All code developed for\nthis study is available at:\nhttps://github.com/KurbanIntelligenceLab/MultdBG-Time-Series-Library", "authors": ["Mert Onur Cakiroglu", "Idil Bilge Altun", "Hasan Kurban", "Elham Buxton", "Mehmet Dalkilic"], "published_date": "2025-05-28", "title_zh": "多變量德布魯因圖：時間序列預測的符號圖框架", "summary_zh": "時間序列預測因時間異質性、高維度及缺乏固有符號結構，對基礎模型構成挑戰。本研究提出DRAGON，一種新型編碼器，引入多元迪布魯因圖以彌合符號表示與神經建模間的差距。DRAGON離散化連續輸入序列並將其映射到固定圖結構，通過基於圖的注意力實現動態上下文恢復。DRAGON作為雙分支架構中的輔助模塊，利用符號化、結構感知表示增強傳統基於CNN的編碼器。相關程式碼已公開。", "audio": "audios/2505.22768v1.mp3", "timestamp": "2025-05-31T06:25:06.282127"}
{"query": "Diffusion Model", "id": "2505.23085v1", "url": "http://arxiv.org/abs/2505.23085v1", "title": "GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion", "summary": "Estimating accurate and temporally consistent 3D human geometry from videos\nis a challenging problem in computer vision. Existing methods, primarily\noptimized for single images, often suffer from temporal inconsistencies and\nfail to capture fine-grained dynamic details. To address these limitations, we\npresent GeoMan, a novel architecture designed to produce accurate and\ntemporally consistent depth and normal estimations from monocular human videos.\nGeoMan addresses two key challenges: the scarcity of high-quality 4D training\ndata and the need for metric depth estimation to accurately model human size.\nTo overcome the first challenge, GeoMan employs an image-based model to\nestimate depth and normals for the first frame of a video, which then\nconditions a video diffusion model, reframing video geometry estimation task as\nan image-to-video generation problem. This design offloads the heavy lifting of\ngeometric estimation to the image model and simplifies the video model's role\nto focus on intricate details while using priors learned from large-scale video\ndatasets. Consequently, GeoMan improves temporal consistency and\ngeneralizability while requiring minimal 4D training data. To address the\nchallenge of accurate human size estimation, we introduce a root-relative depth\nrepresentation that retains critical human-scale details and is easier to be\nestimated from monocular inputs, overcoming the limitations of traditional\naffine-invariant and metric depth representations. GeoMan achieves\nstate-of-the-art performance in both qualitative and quantitative evaluations,\ndemonstrating its effectiveness in overcoming longstanding challenges in 3D\nhuman geometry estimation from videos.", "authors": ["Gwanghyun Kim", "Xueting Li", "Ye Yuan", "Koki Nagano", "Tianye Li", "Jan Kautz", "Se Young Chun", "Umar Iqbal"], "published_date": "2025-05-29", "title_zh": "GeoMan：基於圖像到影片擴散的時序一致性人體幾何估計", "summary_zh": "從影片中估算精確且時序一致的3D人體幾何形狀是電腦視覺的難題。現有方法主要針對單張影像優化，常有時序不一致問題，難以捕捉細緻的動態細節。本研究提出GeoMan，一種新型架構，旨在從單眼人體影片中產生精確且時序一致的深度和法線估計。GeoMan解決兩個主要挑戰：高品質4D訓練數據的稀缺性，以及準確建模人體尺寸所需的度量深度估計。GeoMan採用基於影像的模型來估計影片首幀的深度和法線，進而調節影片擴散模型，將影片幾何估計任務重新定義為影像到影片的生成問題，將幾何估計的繁重工作轉移到影像模型，簡化影片模型，使其專注於細節並使用從大型影片數據集學習到的先驗知識，從而提高時間一致性和泛化能力，同時減少對4D訓練數據的需求。此外，引入根相對深度表示，保留關鍵的人體尺度細節，更易於從單眼輸入估計，克服了傳統仿射不變和度量深度表示的局限性。實驗結果表明，GeoMan在定性和定量評估中均達到最先進的性能，證明其在克服影片3D人體幾何估計長期挑戰方面的有效性。", "audio": "audios/2505.23085v1.mp3", "timestamp": "2025-05-31T06:25:14.391795"}
{"query": "AI", "id": "2505.23383v1", "url": "http://arxiv.org/abs/2505.23383v1", "title": "Automated Modeling Method for Pathloss Model Discovery", "summary": "Modeling propagation is the cornerstone for designing and optimizing\nnext-generation wireless systems, with a particular emphasis on 5G and beyond\nera. Traditional modeling methods have long relied on statistic-based\ntechniques to characterize propagation behavior across different environments.\nWith the expansion of wireless communication systems, there is a growing demand\nfor methods that guarantee the accuracy and interoperability of modeling.\nArtificial intelligence (AI)-based techniques, in particular, are increasingly\nbeing adopted to overcome this challenge, although the interpretability is not\nassured with most of these methods. Inspired by recent advancements in AI, this\npaper proposes a novel approach that accelerates the discovery of path loss\nmodels while maintaining interpretability. The proposed method automates the\nmodel formulation, evaluation, and refinement, facilitating model discovery. We\nevaluate two techniques: one based on Deep Symbolic Regression, offering full\ninterpretability, and the second based on Kolmogorov-Arnold Networks, providing\ntwo levels of interpretability. Both approaches are evaluated on two synthetic\nand two real-world datasets. Our results show that Kolmogorov-Arnold Networks\nachieve R^2 values close to 1 with minimal prediction error, while Deep\nSymbolic Regression generates compact models with moderate accuracy. Moreover,\non the selected examples, we demonstrate that automated methods outperform\ntraditional methods, achieving up to 75% reduction in prediction errors,\noffering accurate and explainable solutions with potential to increase the\nefficiency of discovering next-generation path loss models.", "authors": ["Ahmad Anaqreh", "Shih-Kai Chou", "Mihael Mohorčič", "Carolina Fortuna"], "published_date": "2025-05-29", "title_zh": "路徑損耗模型探索之自動化建模方法", "summary_zh": "傳播建模是設計與優化新世代無線系統的基石，尤其在5G及未來世代。傳統建模長期仰賴統計方法描述不同環境下的傳播特性。隨著無線通訊系統擴展，對保證建模準確性與互操作性的方法需求日增。人工智慧技術日益被採用以應對此挑戰，但多數方法缺乏可解釋性。本研究受人工智慧最新進展啟發，提出一種新穎方法，加速路徑損耗模型的發現，同時保持可解釋性。該方法自動化模型建立、評估與優化，促進模型發現。我們評估兩種技術：基於深度符號回歸者，提供完整可解釋性；以及基於柯爾莫哥洛夫-阿諾德網路者，提供兩層級可解釋性。兩種方法均在合成與真實世界數據集上進行評估。結果顯示，柯爾莫哥洛夫-阿諾德網路達到接近1的R平方值，預測誤差極小；而深度符號回歸生成精簡模型，準確度中等。此外，在選定的範例中，自動化方法優於傳統方法，預測誤差降低達75%，提供準確且可解釋的解決方案，有潛力提高新世代路徑損耗模型發現的效率。", "audio": "audios/2505.23383v1.mp3", "timestamp": "2025-05-31T07:16:47.360257"}
{"query": "Foundation Model", "id": "2505.22759v1", "url": "http://arxiv.org/abs/2505.22759v1", "title": "FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian", "summary": "The development of speech foundation models (SFMs) like Whisper and\nSeamlessM4T has significantly advanced the field of speech processing. However,\ntheir closed nature--with inaccessible training data and code--poses major\nreproducibility and fair evaluation challenges. While other domains have made\nsubstantial progress toward open science by developing fully transparent models\ntrained on open-source (OS) code and data, similar efforts in speech remain\nlimited. To fill this gap, we introduce FAMA, the first family of open science\nSFMs for English and Italian, trained on 150k+ hours of OS speech data.\nMoreover, we present a new dataset containing 16k hours of cleaned and\npseudo-labeled speech for both languages. Results show that FAMA achieves\ncompetitive performance compared to existing SFMs while being up to 8 times\nfaster. All artifacts, including code, datasets, and models, are released under\nOS-compliant licenses, promoting openness in speech technology research.", "authors": ["Sara Papi", "Marco Gaido", "Luisa Bentivogli", "Alessio Brutti", "Mauro Cettolo", "Roberto Gretter", "Marco Matassoni", "Mohamed Nabih", "Matteo Negri"], "published_date": "2025-05-28", "title_zh": "FAMA：首個用於英語和意大利語的大規模開放科學語音基礎模型", "summary_zh": "語音基礎模型如Whisper和SeamlessM4T大幅推進了語音處理領域。然而，其封閉性，即訓練數據和程式碼的不可訪問性，對重現性和公平評估構成挑戰。其他領域透過開發基於開源程式碼和數據的完全透明模型，在開放科學方面取得了顯著進展，但語音領域的類似努力仍然有限。為填補這一空白，我們推出了FAMA，首個面向英語和義大利語的開放科學語音基礎模型系列，其訓練基於超過15萬小時的開源語音數據。此外，我們提供了一個包含1.6萬小時清潔和偽標記語音的新數據集，適用於這兩種語言。結果表明，FAMA在實現與現有語音基礎模型相當的性能的同時，速度提高了8倍。所有成果，包括程式碼、數據集和模型，均以符合開源規範的許可證發布，促進語音技術研究的開放性。", "audio": "audios/2505.22759v1.mp3", "timestamp": "2025-05-31T07:16:54.358850"}
{"query": "Diffusion Model", "id": "2505.23061v1", "url": "http://arxiv.org/abs/2505.23061v1", "title": "DINGO: Constrained Inference for Diffusion LLMs", "summary": "Diffusion LLMs have emerged as a promising alternative to conventional\nautoregressive LLMs, offering significant potential for improved runtime\nefficiency. However, existing diffusion models lack the ability to provably\nenforce user-specified formal constraints, such as regular expressions, which\nmakes them unreliable for tasks that require structured outputs, such as\nfixed-schema JSON generation. Unlike autoregressive models that generate tokens\nsequentially, diffusion LLMs predict a block of tokens in parallel. This\nparallelism makes traditional constrained decoding algorithms, which are\ndesigned for sequential token prediction, ineffective at preserving the true\noutput distribution. To address this limitation, we propose DINGO, a dynamic\nprogramming-based constrained decoding strategy that is both efficient and\nprovably distribution-preserving. DINGO enables sampling of output strings with\nthe highest probability under the model's predicted distribution, while\nstrictly satisfying any user-specified regular expression. On standard symbolic\nmath and JSON generation benchmarks, DINGO achieves up to a 68 percentage point\nimprovement over unconstrained inference", "authors": ["Tarun Suresh", "Debangshu Banerjee", "Shubham Ugare", "Sasa Misailovic", "Gagandeep Singh"], "published_date": "2025-05-29", "title_zh": "DINGO：擴散LLM之約束推論", "summary_zh": "擴散語言模型作為自迴歸模型的替代方案，在提升運行效率方面展現潛力。然而，現有擴散模型無法保證強制執行使用者指定的正式約束，例如正則表達式，使其在需要結構化輸出（如固定模式JSON生成）的任務中表現不穩定。不同於依序生成token的自迴歸模型，擴散語言模型並行預測token區塊，此特性使傳統的約束解碼算法（專為依序token預測設計）無法有效保持真實輸出分佈。為解決此問題，我們提出DINGO，一種基於動態規劃的約束解碼策略，兼具效率和可證明的分佈保持特性。DINGO能在嚴格滿足使用者指定的正則表達式的前提下，對模型預測分佈下機率最高的輸出字串進行採樣。在標準符號數學和JSON生成基準測試中，DINGO比無約束推理提升高達68個百分點。", "audio": "audios/2505.23061v1.mp3", "timestamp": "2025-05-31T07:17:00.594849"}
{"query": "AI", "id": "2505.23379v1", "url": "http://arxiv.org/abs/2505.23379v1", "title": "Vision-Integrated High-Quality Neural Speech Coding", "summary": "This paper proposes a novel vision-integrated neural speech codec (VNSC),\nwhich aims to enhance speech coding quality by leveraging visual modality\ninformation. In VNSC, the image analysis-synthesis module extracts visual\nfeatures from lip images, while the feature fusion module facilitates\ninteraction between the image analysis-synthesis module and the speech coding\nmodule, transmitting visual information to assist the speech coding process.\nDepending on whether visual information is available during the inference\nstage, the feature fusion module integrates visual features into the speech\ncoding module using either explicit integration or implicit distillation\nstrategies. Experimental results confirm that integrating visual information\neffectively improves the quality of the decoded speech and enhances the noise\nrobustness of the neural speech codec, without increasing the bitrate.", "authors": ["Yao Guo", "Yang Ai", "Rui-Chen Zheng", "Hui-Peng Du", "Xiao-Hang Jiang", "Zhen-Hua Ling"], "published_date": "2025-05-29", "title_zh": "視覺整合的高品質神經語音編碼", "summary_zh": "本研究提出一種新型視覺整合神經語音編解碼器(VNSC)，旨在利用視覺模態資訊提升語音編碼品質。VNSC包含圖像分析合成模組，從唇部圖像提取視覺特徵；以及特徵融合模組，促進圖像分析合成模組與語音編碼模組互動，傳輸視覺資訊以輔助語音編碼。根據推論階段視覺資訊是否可用，特徵融合模組採用顯式整合或隱式蒸餾策略，將視覺特徵整合至語音編碼模組中。實驗結果表明，整合視覺資訊能有效提升解碼語音品質，並增強神經語音編解碼器的抗噪性，同時不增加位元速率。", "audio": "audios/2505.23379v1.mp3", "timestamp": "2025-05-31T08:22:20.862728"}
{"query": "Foundation Model", "id": "2505.22705v1", "url": "http://arxiv.org/abs/2505.22705v1", "title": "HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer", "summary": "Recent advancements in image generative foundation models have prioritized\nquality improvements but often at the cost of increased computational\ncomplexity and inference latency. To address this critical trade-off, we\nintroduce HiDream-I1, a new open-source image generative foundation model with\n17B parameters that achieves state-of-the-art image generation quality within\nseconds. HiDream-I1 is constructed with a new sparse Diffusion Transformer\n(DiT) structure. Specifically, it starts with a dual-stream decoupled design of\nsparse DiT with dynamic Mixture-of-Experts (MoE) architecture, in which two\nseparate encoders are first involved to independently process image and text\ntokens. Then, a single-stream sparse DiT structure with dynamic MoE\narchitecture is adopted to trigger multi-model interaction for image generation\nin a cost-efficient manner. To support flexiable accessibility with varied\nmodel capabilities, we provide HiDream-I1 in three variants: HiDream-I1-Full,\nHiDream-I1-Dev, and HiDream-I1-Fast.\n  Furthermore, we go beyond the typical text-to-image generation and remould\nHiDream-I1 with additional image conditions to perform precise,\ninstruction-based editing on given images, yielding a new instruction-based\nimage editing model namely HiDream-E1. Ultimately, by integrating text-to-image\ngeneration and instruction-based image editing, HiDream-I1 evolves to form a\ncomprehensive image agent (HiDream-A1) capable of fully interactive image\ncreation and refinement. To accelerate multi-modal AIGC research, we have\nopen-sourced all the codes and model weights of HiDream-I1-Full,\nHiDream-I1-Dev, HiDream-I1-Fast, HiDream-E1 through our project websites:\nhttps://github.com/HiDream-ai/HiDream-I1 and\nhttps://github.com/HiDream-ai/HiDream-E1. All features can be directly\nexperienced via https://vivago.ai/studio.", "authors": ["Qi Cai", "Jingwen Chen", "Yang Chen", "Yehao Li", "Fuchen Long", "Yingwei Pan", "Zhaofan Qiu", "Yiheng Zhang", "Fengbin Gao", "Peihan Xu", "Yimeng Wang", "Kai Yu", "Wenxuan Chen", "Ziwei Feng", "Zijian Gong", "Jianzhuang Pan", "Yi Peng", "Rui Tian", "Siyu Wang", "Bo Zhao", "Ting Yao", "Tao Mei"], "published_date": "2025-05-28", "title_zh": "HiDream-I1：基於稀疏擴散Transformer的高效圖像生成基礎模型", "summary_zh": "為了解決圖像生成模型在品質與運算複雜度及推論延遲間的權衡問題，本研究提出一開源圖像生成基礎模型HiDream-I1，擁有170億參數，能在數秒內達到頂尖圖像生成品質。HiDream-I1基於新型稀疏擴散轉換器(DiT)結構，採用雙流解耦設計，結合動態混合專家(MoE)架構，先由兩個獨立編碼器處理圖像與文字符記，再透過單流稀疏DiT結構與動態MoE架構，以具成本效益的方式觸發多模態互動進行圖像生成。HiDream-I1提供三種變體：HiDream-I1-Full、HiDream-I1-Dev與HiDream-I1-Fast，以支援不同模型能力。此外，本研究將HiDream-I1改造為HiDream-E1，透過額外圖像條件對指定圖像執行精確的指令式編輯。最後，HiDream-I1整合文字轉圖像生成與指令式圖像編輯，進而發展為具備完整互動圖像創建與優化能力的綜合圖像代理HiDream-A1。所有HiDream-I1-Full、HiDream-I1-Dev、HiDream-I1-Fast及HiDream-E1的程式碼與模型權重均已開源，以加速多模態AIGC研究。", "audio": "audios/2505.22705v1.mp3", "timestamp": "2025-05-31T08:22:29.225249"}
{"query": "Diffusion Model", "id": "2505.22980v1", "url": "http://arxiv.org/abs/2505.22980v1", "title": "MOVi: Training-free Text-conditioned Multi-Object Video Generation", "summary": "Recent advances in diffusion-based text-to-video (T2V) models have\ndemonstrated remarkable progress, but these models still face challenges in\ngenerating videos with multiple objects. Most models struggle with accurately\ncapturing complex object interactions, often treating some objects as static\nbackground elements and limiting their movement. In addition, they often fail\nto generate multiple distinct objects as specified in the prompt, resulting in\nincorrect generations or mixed features across objects. In this paper, we\npresent a novel training-free approach for multi-object video generation that\nleverages the open world knowledge of diffusion models and large language\nmodels (LLMs). We use an LLM as the ``director'' of object trajectories, and\napply the trajectories through noise re-initialization to achieve precise\ncontrol of realistic movements. We further refine the generation process by\nmanipulating the attention mechanism to better capture object-specific features\nand motion patterns, and prevent cross-object feature interference. Extensive\nexperiments validate the effectiveness of our training free approach in\nsignificantly enhancing the multi-object generation capabilities of existing\nvideo diffusion models, resulting in 42% absolute improvement in motion\ndynamics and object generation accuracy, while also maintaining high fidelity\nand motion smoothness.", "authors": ["Aimon Rahman", "Jiang Liu", "Ze Wang", "Ximeng Sun", "Jialian Wu", "Xiaodong Yu", "Yusheng Su", "Vishal M. Patel", "Zicheng Liu", "Emad Barsoum"], "published_date": "2025-05-29", "title_zh": "MOVi：免訓練文本條件多物件影片生成", "summary_zh": "基於擴散的文本生成影片模型雖有顯著進展，但生成多物件影片仍具挑戰。現有模型難以準確捕捉複雜物件互動，常將部分物件視為靜態背景，並限制其移動。此外，模型也難以生成提示詞指定的多個獨立物件，導致錯誤生成或物件間特徵混雜。本研究提出一種免訓練的多物件影片生成方法，利用擴散模型與大型語言模型（LLM）的開放世界知識。我們使用LLM作為物件軌跡的「導演」，並通過雜訊重置應用這些軌跡，以精確控制逼真動作。透過操控注意力機制，我們進一步優化生成過程，以更好地捕捉特定物件的特徵和運動模式，並防止物件間的特徵干擾。大量實驗驗證了此免訓練方法可顯著提升現有影片擴散模型的多物件生成能力，在運動動態和物件生成準確性方面實現42%的絕對提升，同時保持高保真度和運動流暢性。", "audio": "audios/2505.22980v1.mp3", "timestamp": "2025-05-31T08:22:35.523558"}
{"query": "AI", "id": "2505.23326v1", "url": "http://arxiv.org/abs/2505.23326v1", "title": "Designing the Future of Entrepreneurship Education: Exploring an AI-Empowered Scaffold System for Business Plan Development", "summary": "Entrepreneurship education equips students to transform innovative ideas into\nactionable entrepreneurship plans, yet traditional approaches often struggle to\nprovide the personalized guidance and practical alignment needed for success.\nFocusing on the business plan as a key learning tool and evaluation method,\nthis study investigates the design needs for an AI-empowered scaffold system to\naddress these challenges. Based on qualitative insights from educators and\nstudents, the findings highlight three critical dimensions for system design:\nmastery of business plan development, alignment with entrepreneurial learning\ngoals, and integration of adaptive system features. These findings underscore\nthe transformative potential of AI in bridging gaps in entrepreneurship\neducation while emphasizing the enduring value of human mentorship and\nexperiential learning.", "authors": ["Junhua Zhu", "Lan Luo"], "published_date": "2025-05-29", "title_zh": "創業教育的未來設計：探索人工智慧賦能的商業計畫發展鷹架系統", "summary_zh": "創業教育旨在培養學生將創新構想轉化為可行的創業計畫，然傳統方式往往缺乏個人化指導與實務配合。本研究以商業計畫為核心學習工具與評估方法，探討AI輔助鷹架系統的設計需求。透過對教育者及學生的質性分析，研究發現系統設計需關注三方面：精通商業計畫撰寫、符合創業學習目標、整合適應性系統功能。結果表明AI具備彌補創業教育缺口的潛力，同時強調人為指導及實務經驗的價值。", "audio": "audios/2505.23326v1.mp3", "timestamp": "2025-05-31T09:18:21.117325"}
{"query": "Foundation Model", "id": "2505.22697v1", "url": "http://arxiv.org/abs/2505.22697v1", "title": "Update Your Transformer to the Latest Release: Re-Basin of Task Vectors", "summary": "Foundation models serve as the backbone for numerous specialized models\ndeveloped through fine-tuning. However, when the underlying pretrained model is\nupdated or retrained (e.g., on larger and more curated datasets), the\nfine-tuned model becomes obsolete, losing its utility and requiring retraining.\nThis raises the question: is it possible to transfer fine-tuning to a new\nrelease of the model? In this work, we investigate how to transfer fine-tuning\nto a new checkpoint without having to re-train, in a data-free manner. To do\nso, we draw principles from model re-basin and provide a recipe based on weight\npermutations to re-base the modifications made to the original base model,\noften called task vector. In particular, our approach tailors model re-basin\nfor Transformer models, taking into account the challenges of residual\nconnections and multi-head attention layers. Specifically, we propose a\ntwo-level method rooted in spectral theory, initially permuting the attention\nheads and subsequently adjusting parameters within select pairs of heads.\nThrough extensive experiments on visual and textual tasks, we achieve the\nseamless transfer of fine-tuned knowledge to new pre-trained backbones without\nrelying on a single training step or datapoint. Code is available at\nhttps://github.com/aimagelab/TransFusion.", "authors": ["Filippo Rinaldi", "Giacomo Capitani", "Lorenzo Bonicelli", "Donato Crisostomi", "Federico Bolelli", "Elisa Ficarra", "Emanuele Rodolà", "Simone Calderara", "Angelo Porrello"], "published_date": "2025-05-28", "title_zh": "將Transformer更新至最新版本：任務向量的重定基", "summary_zh": "基礎模型為許多微調後的專業模型提供骨幹。然而，當底層預訓練模型更新或重新訓練時，微調模型會過時，失去效用並需要重新訓練。本文探討如何在無需重新訓練資料的情況下，將微調轉移到新版本的模型。我們基於模型重塑的原理，提供一種基於權重排列的方法，以重新定位對原始基礎模型所做的修改（通常稱為任務向量）。我們的研究針對Transformer模型客製化模型重塑，考量到殘差連接和多頭注意力層的挑戰。具體而言，我們提出一種基於譜理論的雙層方法，首先排列注意力頭，然後調整選定頭部對中的參數。透過在視覺和文本任務上的大量實驗，我們實現了微調知識到新預訓練骨幹的無縫轉移，無需依賴單個訓練步驟或數據點。程式碼可在https://github.com/aimagelab/TransFusion取得。", "audio": "audios/2505.22697v1.mp3", "timestamp": "2025-05-31T09:18:28.295332"}
{"query": "Diffusion Model", "id": "2505.22977v1", "url": "http://arxiv.org/abs/2505.22977v1", "title": "HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions", "summary": "Recent advances in diffusion models have significantly improved conditional\nvideo generation, particularly in the pose-guided human image animation task.\nAlthough existing methods are capable of generating high-fidelity and\ntime-consistent animation sequences in regular motions and static scenes, there\nare still obvious limitations when facing complex human body motions\n(Hypermotion) that contain highly dynamic, non-standard motions, and the lack\nof a high-quality benchmark for evaluation of complex human motion animations.\nTo address this challenge, we introduce the \\textbf{Open-HyperMotionX Dataset}\nand \\textbf{HyperMotionX Bench}, which provide high-quality human pose\nannotations and curated video clips for evaluating and improving pose-guided\nhuman image animation models under complex human motion conditions.\nFurthermore, we propose a simple yet powerful DiT-based video generation\nbaseline and design spatial low-frequency enhanced RoPE, a novel module that\nselectively enhances low-frequency spatial feature modeling by introducing\nlearnable frequency scaling. Our method significantly improves structural\nstability and appearance consistency in highly dynamic human motion sequences.\nExtensive experiments demonstrate the effectiveness of our dataset and proposed\napproach in advancing the generation quality of complex human motion image\nanimations. Code and dataset will be made publicly available.", "authors": ["Shuolin Xu", "Siming Zheng", "Ziyi Wang", "HC Yu", "Jinwei Chen", "Huaqi Zhang", "Bo Li", "Peng-Tao Jiang"], "published_date": "2025-05-29", "title_zh": "HyperMotion：基於DiT的姿態引導複雜動作人像動畫", "summary_zh": "擴散模型顯著提升了條件式影片生成，尤其是在姿態導引人體圖像動畫方面。現有方法雖能生成高保真和時間一致的動畫序列，但在面對包含高度動態、非標準動作的複雜人體運動（超運動）時仍有局限性，且缺乏用於評估複雜人體運動動畫的高品質基準。為了解決此問題，我們推出了 Open-HyperMotionX 數據集和 HyperMotionX 基準，提供高品質的人體姿態註釋和精選影片片段，用於評估和改進複雜人體運動條件下的姿態導引人體圖像動畫模型。此外，我們提出了一個基於DiT的簡單而強大的影片生成基準，並設計了空間低頻增強RoPE，這是一個透過引入可學習頻率縮放來選擇性地增強低頻空間特徵建模的新模組。 我們的技術顯著提高了高度動態人體運動序列中的結構穩定性和外觀一致性。大量實驗表明，我們的數據集和方法在提高複雜人體運動圖像動畫的生成品質方面是有效的。程式碼和數據集將公開提供。", "audio": "audios/2505.22977v1.mp3", "timestamp": "2025-05-31T09:18:35.360832"}
{"query": "AI", "id": "2505.23311v1", "url": "http://arxiv.org/abs/2505.23311v1", "title": "Towards LLM-based Generation of Human-Readable Proofs in Polynomial Formal Verification", "summary": "Verification is one of the central tasks in circuit and system design. While\nsimulation and emulation are widely used, complete correctness can only be\nensured based on formal proof techniques. But these approaches often have very\nhigh run time and memory requirements. Recently, Polynomial Formal Verification\n(PFV) has been introduced showing that for many instances of practical\nrelevance upper bounds on needed resources can be given. But proofs have to be\nprovided that are human-readable.\n  Here, we study how modern approaches from Artificial Intelligence (AI) based\non Large Language Models (LLMs) can be used to generate proofs that later on\ncan be validated based on reasoning engines. Examples are given that show how\nLLMs can interact with proof engines, and directions for future work are\noutlined.", "authors": ["Rolf Drechsler"], "published_date": "2025-05-29", "title_zh": "基於大型語言模型的 polynomial 形式驗證中人類可讀證明生成方法", "summary_zh": "驗證是電路與系統設計的核心任務。儘管模擬和仿真廣泛應用，但完整正確性需仰賴形式化驗證技術。然而，此類方法通常耗費大量時間和記憶體。近年來，多項式形式化驗證 (PFV) 顯示，對於許多實際案例，所需資源存在上限。但驗證過程需提供人類可讀的證明。\n\n本研究探討如何運用基於大型語言模型 (LLM) 的現代人工智慧 (AI) 方法，產生後續可透過推理引擎驗證的證明。文中舉例說明 LLM 如何與證明引擎互動，並概述未來研究方向。", "audio": "audios/2505.23311v1.mp3", "timestamp": "2025-05-31T10:18:16.574610"}
{"query": "Foundation Model", "id": "2505.21801v2", "url": "http://arxiv.org/abs/2505.21801v2", "title": "Query, Don't Train: Privacy-Preserving Tabular Prediction from EHR Data via SQL Queries", "summary": "Electronic health records (EHRs) contain richly structured, longitudinal data\nessential for predictive modeling, yet stringent privacy regulations (e.g.,\nHIPAA, GDPR) often restrict access to individual-level records. We introduce\nQuery, Don't Train (QDT): a structured-data foundation-model interface enabling\ntabular inference via LLM-generated SQL over EHRs. Instead of training on or\naccessing individual-level examples, QDT uses a large language model (LLM) as a\nschema-aware query planner to generate privacy-compliant SQL queries from a\nnatural language task description and a test-time input. The model then\nextracts summary-level population statistics through these SQL queries and the\nLLM performs, chain-of-thought reasoning over the results to make predictions.\nThis inference-time-only approach (1) eliminates the need for supervised model\ntraining or direct data access, (2) ensures interpretability through symbolic,\nauditable queries, (3) naturally handles missing features without imputation or\npreprocessing, and (4) effectively manages high-dimensional numerical data to\nenhance analytical capabilities. We validate QDT on the task of 30-day hospital\nreadmission prediction for Type 2 diabetes patients using a MIMIC-style EHR\ncohort, achieving F1 = 0.70, which outperforms TabPFN (F1 = 0.68). To our\nknowledge, this is the first demonstration of LLM-driven, privacy-preserving\nstructured prediction using only schema metadata and aggregate statistics -\noffering a scalable, interpretable, and regulation-compliant alternative to\nconventional foundation-model pipelines.", "authors": ["Josefa Lia Stoisser", "Marc Boubnovski Martell", "Kaspar Märtens", "Lawrence Phillips", "Stephen Michael Town", "Rory Donovan-Maiye", "Julien Fauqueur"], "published_date": "2025-05-27", "title_zh": "查詢而非訓練：基於SQL查詢的EHR數據隱私保護表格預測", "summary_zh": "電子病歷(EHRs)包含豐富的結構化縱向數據，對預測模型至關重要，但嚴格的隱私法規限制了對個體紀錄的訪問。我們提出「查詢，而非訓練」(QDT)：一種結構化數據基礎模型介面，透過大型語言模型(LLM)生成於EHR上的SQL語句實現表格推理。QDT使用LLM作為模式感知查詢規劃器，從自然語言任務描述和測試時輸入生成符合隱私規定的SQL查詢，無需在個體層級資料上訓練或直接訪問資料。模型提取摘要層級的群體統計數據，LLM對結果進行思維鏈推理以做出預測。此推理時方法(1)無需監督模型訓練或直接數據訪問，(2)透過可審計的符號查詢確保可解釋性，(3)自然處理缺失特徵，(4)有效管理高維數值數據。在二型糖尿病患者的30天醫院再入院預測任務中，QDT的F1值為0.70，優於TabPFN(F1 = 0.68)。這是首次展示LLM驅動、隱私保護的結構化預測，僅使用模式元數據和聚合統計數據，為傳統基礎模型流程提供了一種可擴展、可解釋且符合法規的替代方案。", "audio": "audios/2505.21801v2.mp3", "timestamp": "2025-05-31T10:18:25.379327"}
{"query": "Diffusion Model", "id": "2505.22973v1", "url": "http://arxiv.org/abs/2505.22973v1", "title": "EquiReg: Equivariance Regularized Diffusion for Inverse Problems", "summary": "Diffusion models represent the state-of-the-art for solving inverse problems\nsuch as image restoration tasks. In the Bayesian framework, diffusion-based\ninverse solvers incorporate a likelihood term to guide the prior sampling\nprocess, generating data consistent with the posterior distribution. However,\ndue to the intractability of the likelihood term, many current methods rely on\nisotropic Gaussian approximations, which lead to deviations from the data\nmanifold and result in inconsistent, unstable reconstructions. We propose\nEquivariance Regularized (EquiReg) diffusion, a general framework for\nregularizing posterior sampling in diffusion-based inverse problem solvers.\nEquiReg enhances reconstructions by reweighting diffusion trajectories and\npenalizing those that deviate from the data manifold. We define a new\ndistribution-dependent equivariance error, empirically identify functions that\nexhibit low error for on-manifold samples and higher error for off-manifold\nsamples, and leverage these functions to regularize the diffusion sampling\nprocess. When applied to a variety of solvers, EquiReg outperforms\nstate-of-the-art diffusion models in both linear and nonlinear image\nrestoration tasks, as well as in reconstructing partial differential equations.", "authors": ["Bahareh Tolooshams", "Aditi Chandrashekar", "Rayhan Zirvi", "Abbas Mammadov", "Jiachen Yao", "Chuwei Wang", "Anima Anandkumar"], "published_date": "2025-05-29", "title_zh": "EquiReg：逆問題的等變正則化擴散", "summary_zh": "擴散模型是解決圖像復原等逆問題的最新技術。在貝葉斯框架下，基於擴散的逆問題求解器整合似然項以引導先驗採樣，生成與後驗分佈一致的數據。然而，由於似然項的難解性，許多現有方法依賴於各向同性高斯近似，導致偏離數據流形，產生不一致、不穩定的重建結果。我們提出等變正則化擴散(EquiReg)，這是一個用於正則化基於擴散的逆問題求解器中後驗採樣的通用框架。EquiReg通過重新加權擴散軌跡並懲罰偏離數據流形的軌跡來增強重建效果。我們定義了一種新的分佈相關等變誤差，通過實驗識別出對於流形上樣本表現出低誤差，對於流形外樣本表現出高誤差的函數，並利用這些函數來正則化擴散採樣過程。在應用於各種求解器時，EquiReg在線性和非線性圖像復原任務以及偏微分方程重建方面均優於最新的擴散模型。", "audio": "audios/2505.22973v1.mp3", "timestamp": "2025-05-31T10:18:31.773480"}
{"query": "AI", "id": "2505.23287v1", "url": "http://arxiv.org/abs/2505.23287v1", "title": "GenCAD-Self-Repairing: Feasibility Enhancement for 3D CAD Generation", "summary": "With the advancement of generative AI, research on its application to 3D\nmodel generation has gained traction, particularly in automating the creation\nof Computer-Aided Design (CAD) files from images. GenCAD is a notable model in\nthis domain, leveraging an autoregressive transformer-based architecture with a\ncontrastive learning framework to generate CAD programs.\n  However, a major limitation of GenCAD is its inability to consistently\nproduce feasible boundary representations (B-reps), with approximately 10% of\ngenerated designs being infeasible. To address this, we propose\nGenCAD-Self-Repairing, a framework that enhances the feasibility of generative\nCAD models through diffusion guidance and a self-repairing pipeline. This\nframework integrates a guided diffusion denoising process in the latent space\nand a regression-based correction mechanism to refine infeasible CAD command\nsequences while preserving geometric accuracy. Our approach successfully\nconverted two-thirds of infeasible designs in the baseline method into feasible\nones, significantly improving the feasibility rate while simultaneously\nmaintaining a reasonable level of geometric accuracy between the point clouds\nof ground truth models and generated models.\n  By significantly improving the feasibility rate of generating CAD models, our\napproach helps expand the availability of high-quality training data and\nenhances the applicability of AI-driven CAD generation in manufacturing,\narchitecture, and product design.", "authors": ["Chikaha Tsuji", "Enrique Flores Medina", "Harshit Gupta", "Md Ferdous Alam"], "published_date": "2025-05-29", "title_zh": "GenCAD-自我修復：三維CAD生成之可行性增強", "summary_zh": "生成式AI興起，將其應用於3D模型生成的研究備受關注，尤其是在從圖像自動創建電腦輔助設計（CAD）檔案方面。GenCAD是該領域的代表模型，利用自迴歸Transformer架構與對比學習框架來生成CAD程式。\n\nGenCAD的主要限制是難以持續產生可行的邊界表示（B-rep），約10%生成的設計不可行。為了解決此問題，我們提出了GenCAD-Self-Repairing，透過擴散引導和自我修復流程來提高生成式CAD模型的可行性。該框架整合了潛在空間中的引導擴散去噪過程，以及基於迴歸的校正機制，以優化不可行的CAD指令序列，同時保持幾何精度。\n\n我們的方案成功將基準方法中三分之二的不可行設計轉變為可行設計，顯著提高了可行率，同時保持了真值模型和生成模型點雲之間合理的幾何精度。\n\n通過顯著提高生成CAD模型的可行率，我們的方案有助於擴展高品質訓練數據的可用性，並增強AI驅動的CAD生成在製造、建築和產品設計中的應用性。", "audio": "audios/2505.23287v1.mp3", "timestamp": "2025-05-31T11:14:32.447199"}
{"query": "Diffusion Model", "id": "2505.22935v1", "url": "http://arxiv.org/abs/2505.22935v1", "title": "Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models", "summary": "Explicit noise-level conditioning is widely regarded as essential for the\neffective operation of Graph Diffusion Models (GDMs). In this work, we\nchallenge this assumption by investigating whether denoisers can implicitly\ninfer noise levels directly from corrupted graph structures, potentially\neliminating the need for explicit noise conditioning. To this end, we develop a\ntheoretical framework centered on Bernoulli edge-flip corruptions and extend it\nto encompass more complex scenarios involving coupled structure-attribute\nnoise. Extensive empirical evaluations on both synthetic and real-world graph\ndatasets, using models such as GDSS and DiGress, provide strong support for our\ntheoretical findings. Notably, unconditional GDMs achieve performance\ncomparable or superior to their conditioned counterparts, while also offering\nreductions in parameters (4-6%) and computation time (8-10%). Our results\nsuggest that the high-dimensional nature of graph data itself often encodes\nsufficient information for the denoising process, opening avenues for simpler,\nmore efficient GDM architectures.", "authors": ["Jipeng Li", "Yanning Shen"], "published_date": "2025-05-28", "title_zh": "雜訊調節是否必要？無條件圖擴散模型的統一理論", "summary_zh": "顯式雜訊水平條件化被廣泛認為是圖形擴散模型(GDMs)有效運作的必要條件。本研究挑戰此假設，探討去噪器是否能從損毀的圖形結構中隱式推斷雜訊水平，從而消除對顯式雜訊條件化的需求。為此，我們開發了一個基於伯努利邊緣翻轉損毀的理論框架，並將其擴展至包含耦合結構-屬性雜訊的更複雜場景。在合成和真實世界圖形數據集上的大量實證評估，使用GDSS和DiGress等模型，為我們的理論發現提供了強有力的支持。值得注意的是，無條件GDMs實現了與其條件化模型相當或更優越的性能，同時還減少了參數(4-6%)和計算時間(8-10%)。我們的結果表明，圖形數據本身的高維特性通常編碼了足夠的訊息用於去噪過程，為更簡單、更高效的GDM架構開闢了途徑。", "audio": "audios/2505.22935v1.mp3", "timestamp": "2025-05-31T11:14:39.248594"}
{"query": "AI", "id": "2505.23276v1", "url": "http://arxiv.org/abs/2505.23276v1", "title": "The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text", "summary": "Large Language Models (LLMs) have achieved unprecedented capabilities in\ngenerating human-like text, posing subtle yet significant challenges for\ninformation integrity across critical domains, including education, social\nmedia, and academia, enabling sophisticated misinformation campaigns,\ncompromising healthcare guidance, and facilitating targeted propaganda. This\nchallenge becomes severe, particularly in under-explored and low-resource\nlanguages like Arabic. This paper presents a comprehensive investigation of\nArabic machine-generated text, examining multiple generation strategies\n(generation from the title only, content-aware generation, and text refinement)\nacross diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,\nand social media domains. Our stylometric analysis reveals distinctive\nlinguistic patterns differentiating human-written from machine-generated Arabic\ntext across these varied contexts. Despite their human-like qualities, we\ndemonstrate that LLMs produce detectable signatures in their Arabic outputs,\nwith domain-specific characteristics that vary significantly between different\ncontexts. Based on these insights, we developed BERT-based detection models\nthat achieved exceptional performance in formal contexts (up to 99.9\\%\nF1-score) with strong precision across model architectures. Our cross-domain\nanalysis confirms generalization challenges previously reported in the\nliterature. To the best of our knowledge, this work represents the most\ncomprehensive investigation of Arabic machine-generated text to date, uniquely\ncombining multiple prompt generation methods, diverse model architectures, and\nin-depth stylometric analysis across varied textual domains, establishing a\nfoundation for developing robust, linguistically-informed detection systems\nessential for preserving information integrity in Arabic-language contexts.", "authors": ["Maged S. Al-Shaibani", "Moataz Ahmed"], "published_date": "2025-05-29", "title_zh": "阿拉伯語AI指紋：大型語言模型文本的文體分析與檢測", "summary_zh": "大型語言模型在生成類人文本方面取得空前進展，對教育、社交媒體和學術界等關鍵領域的資訊真實性構成挑戰，可能助長不實訊息傳播。本文針對阿拉伯語機器生成文本進行全面研究，檢視多種生成策略和模型架構在學術及社交媒體領域的表現。風格分析揭示人類與機器生成阿拉伯語文本間的獨特語言模式，顯示大型語言模型在阿拉伯語輸出中存在可檢測的特徵。基於此，我們開發了基於BERT的檢測模型，在正式語境中表現卓越（F1值高達99.9%）。跨領域分析證實了先前文獻中報告的泛化挑戰。據我們所知，這項研究是對阿拉伯語機器生成文本最全面的調查，結合多種提示生成方法、模型架構和深入的風格分析，為開發穩健的、基於語言學的檢測系統奠定基礎，對於維護阿拉伯語語境下的資訊真實性至關重要。", "audio": "audios/2505.23276v1.mp3", "timestamp": "2025-05-31T12:34:31.402087"}
{"query": "Diffusion Model", "id": "2505.22926v1", "url": "http://arxiv.org/abs/2505.22926v1", "title": "Leveraging Diffusion Models for Synthetic Data Augmentation in Protein Subcellular Localization Classification", "summary": "We investigate whether synthetic images generated by diffusion models can\nenhance multi-label classification of protein subcellular localization.\nSpecifically, we implement a simplified class-conditional denoising diffusion\nprobabilistic model (DDPM) to produce label-consistent samples and explore\ntheir integration with real data via two hybrid training strategies: Mix Loss\nand Mix Representation. While these approaches yield promising validation\nperformance, our proposed MixModel exhibits poor generalization to unseen test\ndata, underscoring the challenges of leveraging synthetic data effectively. In\ncontrast, baseline classifiers built on ResNet backbones with conventional loss\nfunctions demonstrate greater stability and test-time performance. Our findings\nhighlight the importance of realistic data generation and robust supervision\nwhen incorporating generative augmentation into biomedical image\nclassification.", "authors": ["Sylvey Lin", "Zhi-Yi Cao"], "published_date": "2025-05-28", "title_zh": "利用擴散模型進行蛋白質亞細胞定位分類中的合成數據增強", "summary_zh": "本研究探討擴散模型生成的合成圖像是否能提升蛋白質亞細胞定位的多標籤分類。我們實作簡化的類別條件去噪擴散機率模型(DDPM)以生成標籤一致的樣本，並透過混合損失與混合表徵兩種混合訓練策略，研究其與真實資料的整合。儘管這些方法在驗證集上表現良好，但我們提出的混合模型在未見過的測試資料上泛化能力差，突顯了有效利用合成資料的挑戰。相反，基於ResNet骨幹且使用傳統損失函數的基線分類器表現出更高的穩定性和測試效能。研究結果強調，將生成式擴增應用於生物醫學影像分類時，逼真的資料生成與穩健的監督至關重要。", "audio": "audios/2505.22926v1.mp3", "timestamp": "2025-05-31T12:34:36.850326"}
{"query": "AI", "id": "2505.23275v1", "url": "http://arxiv.org/abs/2505.23275v1", "title": "Wireless Agentic AI with Retrieval-Augmented Multimodal Semantic Perception", "summary": "The rapid development of multimodal AI and Large Language Models (LLMs) has\ngreatly enhanced real-time interaction, decision-making, and collaborative\ntasks. However, in wireless multi-agent scenarios, limited bandwidth poses\nsignificant challenges to exchanging semantically rich multimodal information\nefficiently. Traditional semantic communication methods, though effective,\nstruggle with redundancy and loss of crucial details. To overcome these\nchallenges, we propose a Retrieval-Augmented Multimodal Semantic Communication\n(RAMSemCom) framework. RAMSemCom incorporates iterative, retrieval-driven\nsemantic refinement tailored for distributed multi-agent environments, enabling\nefficient exchange of critical multimodal elements through local caching and\nselective transmission. Our approach dynamically optimizes retrieval using deep\nreinforcement learning (DRL) to balance semantic fidelity with bandwidth\nconstraints. A comprehensive case study on multi-agent autonomous driving\ndemonstrates that our DRL-based retrieval strategy significantly improves task\ncompletion efficiency and reduces communication overhead compared to baseline\nmethods.", "authors": ["Guangyuan Liu", "Yinqiu Liu", "Ruichen Zhang", "Hongyang Du", "Dusit Niyato", "Zehui Xiong", "Sumei Sun", "Abbas Jamalipour"], "published_date": "2025-05-29", "title_zh": "具備檢索增強多模態語義感知的無線代理式人工智慧", "summary_zh": "多模態人工智慧與大型語言模型快速發展，大幅提升了即時互動、決策制定及協作任務能力。然而，無線多代理人場景中，有限頻寬對高效交換富含語義的多模態資訊構成重大挑戰。傳統語義通訊方法雖有效，但在冗餘和關鍵細節遺失方面表現不佳。為克服此挑戰，我們提出一種檢索增強多模態語義通訊框架RAMSemCom。RAMSemCom整合了迭代、檢索驅動的語義精煉，針對分散式多代理人環境客製化，透過本地快取和選擇性傳輸，實現關鍵多模態元素的有效交換。我們的研究方法利用深度強化學習動態優化檢索，以平衡語義保真度和頻寬限制。在多代理人自動駕駛的案例研究中，結果表明，相較於基準方法，我們基於深度強化學習的檢索策略顯著提升了任務完成效率並降低了通訊開銷。", "audio": "audios/2505.23275v1.mp3", "timestamp": "2025-05-31T13:26:53.706660"}
{"query": "Diffusion Model", "id": "2505.22923v1", "url": "http://arxiv.org/abs/2505.22923v1", "title": "Plug-and-Play Posterior Sampling for Blind Inverse Problems", "summary": "We introduce Blind Plug-and-Play Diffusion Models (Blind-PnPDM) as a novel\nframework for solving blind inverse problems where both the target image and\nthe measurement operator are unknown. Unlike conventional methods that rely on\nexplicit priors or separate parameter estimation, our approach performs\nposterior sampling by recasting the problem into an alternating Gaussian\ndenoising scheme. We leverage two diffusion models as learned priors: one to\ncapture the distribution of the target image and another to characterize the\nparameters of the measurement operator. This PnP integration of diffusion\nmodels ensures flexibility and ease of adaptation. Our experiments on blind\nimage deblurring show that Blind-PnPDM outperforms state-of-the-art methods in\nterms of both quantitative metrics and visual fidelity. Our results highlight\nthe effectiveness of treating blind inverse problems as a sequence of denoising\nsubproblems while harnessing the expressive power of diffusion-based priors.", "authors": ["Anqi Li", "Weijie Gan", "Ulugbek S. Kamilov"], "published_date": "2025-05-28", "title_zh": "即插即用後驗採樣法求解盲逆問題", "summary_zh": "本研究提出盲插拔式擴散模型（Blind-PnPDM），一種新穎的框架，用於解決目標圖像和測量算子均未知的盲逆問題。與依賴顯式先驗或獨立參數估計的傳統方法不同，此方法通過將問題重新構建為交替高斯去噪方案來執行後驗採樣。利用兩個擴散模型作為學習先驗：一個捕捉目標圖像的分布，另一個描述測量算子的參數。此擴散模型的PnP集成確保了靈活性和易於適應性。在盲圖像去模糊上的實驗表明，Blind-PnPDM在量化指標和視覺保真度方面均優於最先進的方法。結果突顯了將盲逆問題視為一系列去噪子問題，同時利用基於擴散的先驗表達能力的有效性。", "audio": "audios/2505.22923v1.mp3", "timestamp": "2025-05-31T13:26:58.542209"}
{"query": "AI", "id": "2505.23254v1", "url": "http://arxiv.org/abs/2505.23254v1", "title": "MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning", "summary": "Owing to the huge success of generative artificial intelligence (AI), large\nlanguage models (LLMs) have emerged as a core subclass, underpinning\napplications such as question answering, text generation, and code completion.\nWhile fine-tuning these models on domain-specific data can yield significant\nperformance gains, it also poses daunting computational challenges, especially\nfor researchers and small organizations with limited hardware resources.\nAlthough SSD offloading (i.e., ZeRO-Infinity) has emerged as a viable strategy\nto overcome the GPU memory barrier via leveraging both system memory (i.e., CPU\nDRAM) and storage space (i.e., solid-state devices, SSDs), its design primarily\ntargets model-centric performance issues. As a result, key system-level issues,\nincluding system memory fragmentation, inefficient pinned buffer allocation,\npeak CPU usage spikes, and file system overhead, remain unaddressed, stifling\nscalability and inflating costs. Such an observation motivates this paper to\nintroduce MemAscend, a framework that systematically tackles the underexplored\nsystem memory bottlenecks in SSD-offloaded LLM training, with a focus on\nresource-constrained environments. By streamlining pinned-memory allocation,\neradicating fragmentation, and mitigating peak overhead, MemAscend reclaims a\nsubstantial system memory budget, enabling larger models, longer context\nwindows, and higher batch sizes without exceeding modest hardware limits.\nAcross diverse LLM benchmarks, MemAscend reduces peak system-memory consumption\nby an average of 55.7% compared with standard SSD offloading techniques,\nlowering the hardware barrier for fine-tuning and unlocking new possibilities\nfor cost-effective large-scale training on limited-resource machines.", "authors": ["Yong-Cheng Liaw", "Shuo-Han Chen"], "published_date": "2025-05-29", "title_zh": "MemAscend：基於固態硬碟卸載的大型語言模型微調之系統記憶體最佳化", "summary_zh": "生成式人工智慧的巨大成功促使大型語言模型成為核心子類，廣泛應用於問答、文本生成及程式碼補全。針對特定領域資料微調雖可顯著提升效能，卻對硬體資源有限的研究者及小型組織構成計算挑戰。固態硬碟卸載技術（如ZeRO-Infinity）雖可透過系統記憶體（CPU DRAM）及儲存空間（固態硬碟）克服GPU記憶體瓶頸，但其設計主要針對模型效能問題。系統層級問題，如系統記憶體碎片化、無效率的鎖定緩衝區分配、CPU使用率高峰及檔案系統額外負擔仍未解決，阻礙了擴展性並增加成本。為此，本研究提出MemAscend框架，系統性地解決固態硬碟卸載大型語言模型訓練中未被充分探索的系統記憶體瓶頸，尤其關注資源受限環境。透過優化鎖定記憶體分配、消除碎片化及降低峰值負擔，MemAscend可回收大量系統記憶體預算，在不超過硬體限制下，實現更大模型、更長上下文窗口及更高批次大小。在多種大型語言模型基準測試中，相較於標準固態硬碟卸載技術，MemAscend平均降低55.7%的峰值系統記憶體消耗，降低了微調的硬體門檻，並為資源有限機器上具成本效益的大規模訓練開啟了新的可能性。", "audio": "audios/2505.23254v1.mp3", "timestamp": "2025-05-31T14:16:00.051248"}
{"query": "Diffusion Model", "id": "2505.22918v1", "url": "http://arxiv.org/abs/2505.22918v1", "title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape", "summary": "Diffusion Transformers (DiT) have become the de-facto model for generating\nhigh-quality visual content like videos and images. A huge bottleneck is the\nattention mechanism where complexity scales quadratically with resolution and\nvideo length. One logical way to lessen this burden is sparse attention, where\nonly a subset of tokens or patches are included in the calculation. However,\nexisting techniques fail to preserve visual quality at extremely high sparsity\nlevels and might even incur non-negligible compute overheads. % To address this\nconcern, we propose Re-ttention, which implements very high sparse attention\nfor visual generation models by leveraging the temporal redundancy of Diffusion\nModels to overcome the probabilistic normalization shift within the attention\nmechanism. Specifically, Re-ttention reshapes attention scores based on the\nprior softmax distribution history in order to preserve the visual quality of\nthe full quadratic attention at very high sparsity levels. % Experimental\nresults on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate\nthat Re-ttention requires as few as 3.1\\% of the tokens during inference,\noutperforming contemporary methods like FastDiTAttn, Sparse VideoGen and\nMInference. Further, we measure latency to show that our method can attain over\n45\\% end-to-end % and over 92\\% self-attention latency reduction on an H100 GPU\nat negligible overhead cost.\n  Code available online here:\n\\href{https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}", "authors": ["Ruichen Chen", "Keith G. Mills", "Liyao Jiang", "Chao Gao", "Di Niu"], "published_date": "2025-05-28", "title_zh": "Re-ttention：基於注意力統計重塑的超稀疏視覺生成", "summary_zh": "擴散轉換器（DiT）已成為生成高品質影像內容的主流模型，但其注意力機制複雜度隨解析度和影片長度呈二次方增長，構成瓶頸。稀疏注意力旨在減少此負擔，但現有技術在高稀疏度下難以維持視覺品質，甚至可能產生額外運算成本。為了解決此問題，我們提出Re-ttention，透過利用擴散模型的時間冗餘來克服注意力機制中的機率正規化偏移，進而實現視覺生成模型的高稀疏注意力。Re-ttention基於先前的softmax分佈歷史重塑注意力分數，以在高稀疏度下維持完整二次注意力機制下的視覺品質。在CogVideoX和PixArt DiT等T2V/T2I模型上的實驗結果表明，Re-ttention在推論時僅需3.1%的tokens，優於FastDiTAttn、Sparse VideoGen和MInference等方法。延遲測量顯示，我們的技術在H100 GPU上可實現超過45%的端到端延遲降低以及超過92%的自注意力延遲降低，且成本可忽略不計。", "audio": "audios/2505.22918v1.mp3", "timestamp": "2025-05-31T14:16:06.803019"}
{"query": "AI", "id": "2505.23231v1", "url": "http://arxiv.org/abs/2505.23231v1", "title": "REDDIX-NET: A Novel Dataset and Benchmark for Moderating Online Explicit Services", "summary": "The rise of online platforms has enabled covert illicit activities, including\nonline prostitution, to pose challenges for detection and regulation. In this\nstudy, we introduce REDDIX-NET, a novel benchmark dataset specifically designed\nfor moderating online sexual services and going beyond traditional NSFW\nfilters. The dataset is derived from thousands of web-scraped NSFW posts on\nReddit and categorizes users into six behavioral classes reflecting different\nservice offerings and user intentions. We evaluate the classification\nperformance of state-of-the-art large language models (GPT-4, LlaMA\n3.3-70B-Instruct, Gemini 1.5 Flash, Mistral 8x7B, Qwen 2.5 Turbo, Claude 3.5\nHaiku) using advanced quantitative metrics, finding promising results with\nmodels like GPT-4 and Gemini 1.5 Flash. Beyond classification, we conduct\nsentiment and comment analysis, leveraging LLM and PLM-based approaches and\nmetadata extraction to uncover behavioral and temporal patterns. These analyses\nreveal peak engagement times and distinct user interaction styles across\ncategories. Our findings provide critical insights into AI-driven moderation\nand enforcement, offering a scalable framework for platforms to combat online\nprostitution and associated harms.", "authors": ["MSVPJ Sathvik", "Manan Roy Choudhury", "Rishita Agarwal", "Sathwik Narkedimilli", "Vivek Gupta"], "published_date": "2025-05-29", "title_zh": "REDDIX-NET：用於調節線上露骨服務的新型數據集與基準", "summary_zh": "線上平台的興起助長了隱蔽的非法活動，包括網路賣淫，對偵測和監管構成挑戰。本研究提出REDDIX-NET，一個專為線上性服務審核設計的新基準數據集，超越傳統的NSFW過濾器。該數據集源自Reddit上數千個網路抓取的NSFW貼文，將使用者分為六個行為類別，反映不同的服務提供和使用者意圖。我們使用先進的量化指標評估了最先進的大型語言模型（GPT-4、LlaMA 3.3-70B-Instruct、Gemini 1.5 Flash、Mistral 8x7B、Qwen 2.5 Turbo、Claude 3.5 Haiku）的分類性能，發現GPT-4和Gemini 1.5 Flash等模型展現出有希望的結果。除了分類之外，我們還進行了情感和評論分析，利用基於LLM和PLM的方法以及元數據提取，以揭示行為和時間模式。這些分析揭示了高峰參與時間和各類別中獨特的使用者互動方式。我們的發現為人工智慧驅動的審核和執法提供了關鍵見解，為平台打擊網路賣淫和相關危害提供了一個可擴展的框架。", "audio": "audios/2505.23231v1.mp3", "timestamp": "2025-05-31T15:17:22.122509"}
{"query": "Diffusion Model", "id": "2505.22841v1", "url": "http://arxiv.org/abs/2505.22841v1", "title": "Kernel-Smoothed Scores for Denoising Diffusion: A Bias-Variance Study", "summary": "Diffusion models now set the benchmark in high-fidelity generative sampling,\nyet they can, in principle, be prone to memorization. In this case, their\nlearned score overfits the finite dataset so that the reverse-time SDE samples\nare mostly training points. In this paper, we interpret the empirical score as\na noisy version of the true score and show that its covariance matrix is\nasymptotically a re-weighted data PCA. In large dimension, the small time limit\nmakes the noise variance blow up while simultaneously reducing spatial\ncorrelation. To reduce this variance, we introduce a kernel-smoothed empirical\nscore and analyze its bias-variance trade-off. We derive asymptotic bounds on\nthe Kullback-Leibler divergence between the true distribution and the one\ngenerated by the modified reverse SDE. Regularization on the score has the same\neffect as increasing the size of the training dataset, and thus helps prevent\nmemorization. A spectral decomposition of the forward diffusion suggests better\nvariance control under some regularity conditions of the true data\ndistribution. Reverse diffusion with kernel-smoothed empirical score can be\nreformulated as a gradient descent drifted toward a Log-Exponential\nDouble-Kernel Density Estimator (LED-KDE). This perspective highlights two\nregularization mechanisms taking place in denoising diffusions: an initial\nGaussian kernel first diffuses mass isotropically in the ambient space, while a\nsecond kernel applied in score space concentrates and spreads that mass along\nthe data manifold. Hence, even a straightforward regularization-without any\nlearning-already mitigates memorization and enhances generalization.\nNumerically, we illustrate our results with several experiments on synthetic\nand MNIST datasets.", "authors": ["Franck Gabriel", "François Ged", "Maria Han Veiga", "Emmanuel Schertzer"], "published_date": "2025-05-28", "title_zh": "核平滑分數用於去噪擴散：一個偏差-方差研究", "summary_zh": "擴散模型雖在高保真生成取樣中表現卓越，但可能存在記憶化風險。本文將經驗分數視為真實分數的噪聲版本，並證明其協方差矩陣漸近等價於重加權數據主成分分析。在高維度下，小時間極限會導致噪聲方差激增並降低空間相關性。為降低此方差，本文引入核平滑經驗分數，並分析其偏差-方差權衡。推導出真實分佈與修正逆向隨機微分方程生成分佈之間的庫爾貝克-萊夫勒散度漸近界限。分數正規化與增大訓練數據集規模效果相同，有助於防止記憶化。正向擴散的譜分解表明，在真實數據分佈滿足一定規律性條件下，可實現更佳的方差控制。採用核平滑經驗分數的逆向擴散可重新表述為朝向對數指數雙核密度估計器的梯度下降。此觀點突顯了去噪擴散中的兩種正規化機制：初始高斯核首先在環境空間中各向同性地擴散質量，而第二個核則在分數空間中集中並沿數據流形擴散質量。因此，即使是簡單的正規化（無需任何學習）也已能減輕記憶化並增強泛化能力。數值實驗則在合成數據集和 MNIST 數據集上驗證了上述結果。", "audio": "audios/2505.22841v1.mp3", "timestamp": "2025-05-31T15:17:29.810814"}
{"query": "AI", "id": "2505.23153v1", "url": "http://arxiv.org/abs/2505.23153v1", "title": "Conceptual Framework Toward Embodied Collective Adaptive Intelligence", "summary": "Collective Adaptive Intelligence (CAI) represent a transformative approach in\nartificial intelligence, wherein numerous autonomous agents collaborate, adapt,\nand self-organize to navigate complex, dynamic environments. This paradigm is\nparticularly impactful in embodied AI applications, where adaptability and\nresilience are paramount. By enabling systems to reconfigure themselves in\nresponse to unforeseen challenges, CAI facilitate robust performance in\nreal-world scenarios. This article introduces a conceptual framework for\ndesigning and analyzing CAI. It delineates key attributes including task\ngeneralization, resilience, scalability, and self-assembly, aiming to bridge\ntheoretical foundations with practical methodologies for engineering adaptive,\nemergent intelligence. By providing a structured foundation for understanding\nand implementing CAI, this work seeks to guide researchers and practitioners in\ndeveloping more resilient, scalable, and adaptable AI systems across various\ndomains.", "authors": ["Fan Wang", "Shaoshan Liu"], "published_date": "2025-05-29", "title_zh": "具身集體適應智能的概念框架", "summary_zh": "群體適應智能（CAI）代表人工智慧的變革性途徑，其中眾多自主體協作、適應及自我組織以應對複雜動態環境。此範式於具體化人工智慧應用中尤為重要，適應性和韌性至關重要。CAI使系統能夠重新配置自身以應對突發挑戰，從而在真實情境中實現穩健效能。本文介紹了CAI設計和分析的概念框架，闡述了任務泛化、韌性、可擴展性和自組裝等關鍵屬性，旨在連接理論基礎與工程適應性、湧現智能的實用方法。透過為理解和實施CAI提供結構化基礎，本研究旨在引導研究人員和實踐者在各領域開發更具韌性、可擴展性和適應性的人工智慧系統。", "audio": "audios/2505.23153v1.mp3", "timestamp": "2025-05-31T16:20:56.022120"}
{"query": "Diffusion Model", "id": "2505.22839v1", "url": "http://arxiv.org/abs/2505.22839v1", "title": "How Do Diffusion Models Improve Adversarial Robustness?", "summary": "Recent findings suggest that diffusion models significantly enhance empirical\nadversarial robustness. While some intuitive explanations have been proposed,\nthe precise mechanisms underlying these improvements remain unclear. In this\nwork, we systematically investigate how and how well diffusion models improve\nadversarial robustness. First, we observe that diffusion models intriguingly\nincrease, rather than decrease, the $\\ell_p$ distance to clean\nsamples--challenging the intuition that purification denoises inputs closer to\nthe original data. Second, we find that the purified images are heavily\ninfluenced by the internal randomness of diffusion models, where a compression\neffect arises within each randomness configuration. Motivated by this\nobservation, we evaluate robustness under fixed randomness and find that the\nimprovement drops to approximately 24% on CIFAR-10--substantially lower than\nprior reports approaching 70%. Importantly, we show that this remaining\nrobustness gain strongly correlates with the model's ability to compress the\ninput space, revealing the compression rate as a reliable robustness indicator\nwithout requiring gradient-based analysis. Our findings provide novel insights\ninto the mechanisms underlying diffusion-based purification, and offer guidance\nfor developing more effective and principled adversarial purification systems.", "authors": ["Liu Yuezhang", "Xue-Xin Wei"], "published_date": "2025-05-28", "title_zh": "擴散模型如何提升對抗性穩健性？", "summary_zh": "近期研究表明，擴散模型顯著提升了經驗對抗穩健性。儘管已有一些直觀解釋，但其背後機制仍不明確。本文系統性地探討了擴散模型如何以及在何種程度上提升對抗穩健性。首先，我們觀察到擴散模型出人意料地增加了與原始樣本的ℓp距離，這挑戰了淨化操作使輸入更接近原始資料的直覺。其次，我們發現淨化後的圖像受到擴散模型內部隨機性的嚴重影響，且在每個隨機配置中都存在壓縮效應。基於此，我們評估了固定隨機性下的穩健性，發現CIFAR-10上的提升降至約24%，遠低於先前報導的接近70%。重要的是，我們證明剩餘的穩健性增益與模型壓縮輸入空間的能力密切相關，揭示了壓縮率作為可靠的穩健性指標，無需基於梯度的分析。研究結果為基於擴散的淨化機制提供了新的見解，並為開發更有效和有原則的對抗淨化系統提供了指導。", "audio": "audios/2505.22839v1.mp3", "timestamp": "2025-05-31T16:21:04.835815"}
{"query": "AI", "id": "2505.23143v1", "url": "http://arxiv.org/abs/2505.23143v1", "title": "Interpreting Chest X-rays Like a Radiologist: A Benchmark with Clinical Reasoning", "summary": "Artificial intelligence (AI)-based chest X-ray (CXR) interpretation\nassistants have demonstrated significant progress and are increasingly being\napplied in clinical settings. However, contemporary medical AI models often\nadhere to a simplistic input-to-output paradigm, directly processing an image\nand an instruction to generate a result, where the instructions may be integral\nto the model's architecture. This approach overlooks the modeling of the\ninherent diagnostic reasoning in chest X-ray interpretation. Such reasoning is\ntypically sequential, where each interpretive stage considers the images, the\ncurrent task, and the contextual information from previous stages. This\noversight leads to several shortcomings, including misalignment with clinical\nscenarios, contextless reasoning, and untraceable errors. To fill this gap, we\nconstruct CXRTrek, a new multi-stage visual question answering (VQA) dataset\nfor CXR interpretation. The dataset is designed to explicitly simulate the\ndiagnostic reasoning process employed by radiologists in real-world clinical\nsettings for the first time. CXRTrek covers 8 sequential diagnostic stages,\ncomprising 428,966 samples and over 11 million question-answer (Q&A) pairs,\nwith an average of 26.29 Q&A pairs per sample. Building on the CXRTrek dataset,\nwe propose a new vision-language large model (VLLM), CXRTrekNet, specifically\ndesigned to incorporate the clinical reasoning flow into the VLLM framework.\nCXRTrekNet effectively models the dependencies between diagnostic stages and\ncaptures reasoning patterns within the radiological context. Trained on our\ndataset, the model consistently outperforms existing medical VLLMs on the\nCXRTrek benchmarks and demonstrates superior generalization across multiple\ntasks on five diverse external datasets. The dataset and model can be found in\nour repository (https://github.com/guanjinquan/CXRTrek).", "authors": ["Jinquan Guan", "Qi Chen", "Lizhou Liang", "Yuhang Liu", "Vu Minh Hieu Phan", "Minh-Son To", "Jian Chen", "Yutong Xie"], "published_date": "2025-05-29", "title_zh": "如放射科醫師般判讀胸腔X光片：具備臨床推理的基準測試", "summary_zh": "基於人工智慧的胸腔X光輔助判讀系統進展顯著，但現有模型多採用簡化的輸入輸出模式，忽略了胸腔X光判讀中固有的診斷推理過程。為此，我們構建了CXRTrek，一個新的多階段視覺問答資料集，旨在模擬放射科醫師的診斷推理過程。CXRTrek涵蓋8個連續診斷階段，包含428,966個樣本和超過1100萬個問答對。基於此資料集，我們提出了一種新的視覺語言大模型CXRTrekNet，專門將臨床推理流程融入模型框架中。CXRTrekNet能有效模擬診斷階段之間的依賴關係，並捕捉放射學背景下的推理模式。經訓練後，該模型在CXRTrek基準測試中優於現有醫學視覺語言大模型，並在多個外部資料集上展現出卓越的泛化能力。", "audio": "audios/2505.23143v1.mp3", "timestamp": "2025-05-31T17:15:34.648323"}
{"query": "AI", "id": "2505.23132v1", "url": "http://arxiv.org/abs/2505.23132v1", "title": "Patient Domain Supervised Contrastive Learning for Lung Sound Classification Using Mobile Phone", "summary": "Auscultation is crucial for diagnosing lung diseases. The COVID-19 pandemic\nhas revealed the limitations of traditional, in-person lung sound assessments.\nTo overcome these issues, advancements in digital stethoscopes and artificial\nintelligence (AI) have led to the development of new diagnostic methods. In\nthis context, our study aims to use smartphone microphones to record and\nanalyze lung sounds. We faced two major challenges: the difference in audio\nstyle between electronic stethoscopes and smartphone microphones, and the\nvariability among patients. To address these challenges, we developed a method\ncalled Patient Domain Supervised Contrastive Learning (PD-SCL). By integrating\nthis method with the Audio Spectrogram Transformer (AST) model, we\nsignificantly improved its performance by 2.4\\% compared to the original AST\nmodel. This progress demonstrates that smartphones can effectively diagnose\nlung sounds, addressing inconsistencies in patient data and showing potential\nfor broad use beyond traditional clinical settings. Our research contributes to\nmaking lung disease detection more accessible in the post-COVID-19 world.", "authors": ["Seung Gyu Jeong", "Seong Eun Kim"], "published_date": "2025-05-29", "title_zh": "基於行動電話之患者域監督對比學習於肺音分類", "summary_zh": "聽診對診斷肺部疾病至關重要。新冠疫情突顯了傳統聽診的局限性。為了解決這些問題，數位聽診器和人工智慧的發展催生了新的診斷方法。本研究旨在利用智慧型手機麥克風記錄和分析肺部聲音。我們面臨電子聽診器與手機麥克風音訊風格差異及患者變異性兩大挑戰。為此，我們開發了患者域監督對比學習（PD-SCL）方法，並將其與音訊頻譜圖轉換器（AST）模型整合，效能顯著提升2.4%。此進展表明智慧型手機可有效診斷肺部聲音，解決患者數據不一致問題，並展現了在傳統臨床環境之外的廣泛應用潛力。本研究有助於在後疫情時代提高肺部疾病檢測的可及性。", "audio": "audios/2505.23132v1.mp3", "timestamp": "2025-05-31T18:23:48.021104"}
{"query": "AI", "id": "2505.23106v1", "url": "http://arxiv.org/abs/2505.23106v1", "title": "Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery", "summary": "Attention mechanisms have emerged as transformative tools in core AI domains\nsuch as natural language processing and computer vision. Yet, their largely\nuntapped potential for modeling intricate physical systems presents a\ncompelling frontier. Learning such systems often entails discovering operators\nthat map between functional spaces using limited instances of function pairs --\na task commonly framed as a severely ill-posed inverse PDE problem. In this\nwork, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator\narchitecture that builds upon and enhances Nonlocal Attention Operators (NAO)\nin both predictive accuracy and computational efficiency. NIPS employs a linear\nattention mechanism to enable scalable learning and integrates a learnable\nkernel network that acts as a channel-independent convolution in Fourier space.\nAs a consequence, NIPS eliminates the need to explicitly compute and store\nlarge pairwise interactions, effectively amortizing the cost of handling\nspatial interactions into the Fourier transform. Empirical evaluations\ndemonstrate that NIPS consistently surpasses NAO and other baselines across\ndiverse benchmarks, heralding a substantial leap in scalable, interpretable,\nand efficient physics learning. Our code and data accompanying this paper are\navailable at https://github.com/fishmoon1234/Nonlocal-Attention-Operator.", "authors": ["Ning Liu", "Yue Yu"], "published_date": "2025-05-29", "title_zh": "神經可解釋偏微分方程式：結合傅立葉洞察與注意力機制，以實現可擴展且可解釋的物理發現", "summary_zh": "注意力機制已成為自然語言處理和電腦視覺等AI核心領域的變革性工具。其在複雜物理系統建模方面的潛力尚未充分發揮。學習此類系統通常需要在函數空間之間，利用有限的函數對實例來發現算子，這通常被視為一個嚴重的病態逆偏微分方程問題。本研究提出神經可解釋偏微分方程(NIPS)，一種新型神經算子架構，在預測準確性和計算效率方面改進了非局部注意力算子(NAO)。NIPS採用線性注意力機制實現可擴展學習，並整合可學習的核函數網路，充當傅立葉空間中通道獨立的卷積。因此，NIPS無需顯式計算和儲存大型成對交互，有效地將處理空間交互的成本分攤到傅立葉轉換中。實驗評估表明，在各種基準測試中，NIPS始終優於NAO和其他基線，預示著在可擴展、可解釋和高效的物理學習方面取得了實質性飛躍。", "audio": "audios/2505.23106v1.mp3", "timestamp": "2025-05-31T19:13:50.420562"}
{"query": "AI", "id": "2505.23075v1", "url": "http://arxiv.org/abs/2505.23075v1", "title": "Second Opinion Matters: Towards Adaptive Clinical AI via the Consensus of Expert Model Ensemble", "summary": "Despite the growing clinical adoption of large language models (LLMs),\ncurrent approaches heavily rely on single model architectures. To overcome\nrisks of obsolescence and rigid dependence on single model systems, we present\na novel framework, termed the Consensus Mechanism. Mimicking clinical triage\nand multidisciplinary clinical decision-making, the Consensus Mechanism\nimplements an ensemble of specialized medical expert agents enabling improved\nclinical decision making while maintaining robust adaptability. This\narchitecture enables the Consensus Mechanism to be optimized for cost, latency,\nor performance, purely based on its interior model configuration.\n  To rigorously evaluate the Consensus Mechanism, we employed three medical\nevaluation benchmarks: MedMCQA, MedQA, and MedXpertQA Text, and the\ndifferential diagnosis dataset, DDX+. On MedXpertQA, the Consensus Mechanism\nachieved an accuracy of 61.0% compared to 53.5% and 45.9% for OpenAI's O3 and\nGoogle's Gemini 2.5 Pro. Improvement was consistent across benchmarks with an\nincrease in accuracy on MedQA\n($\\Delta\\mathrm{Accuracy}_{\\mathrm{consensus\\text{-}O3}} = 3.4\\%$) and MedMCQA\n($\\Delta\\mathrm{Accuracy}_{\\mathrm{consensus\\text{-}O3}} = 9.1\\%$). These\naccuracy gains extended to differential diagnosis generation, where our system\ndemonstrated improved recall and precision (F1$_\\mathrm{consensus}$ = 0.326 vs.\nF1$_{\\mathrm{O3\\text{-}high}}$ = 0.2886) and a higher top-1 accuracy for DDX\n(Top1$_\\mathrm{consensus}$ = 52.0% vs. Top1$_{\\mathrm{O3\\text{-}high}}$ =\n45.2%).", "authors": ["Amit Kumthekar", "Zion Tilley", "Henry Duong", "Bhargav Patel", "Michael Magnoli", "Ahmed Omar", "Ahmed Nasser", "Chaitanya Gharpure", "Yevgen Reztzov"], "published_date": "2025-05-29", "title_zh": "第二意見的重要性：基於專家模型集成共識的自適應臨床人工智能", "summary_zh": "儘管大型語言模型在臨床應用日漸普及，現有方法過度依賴單一模型架構。為克服過時風險及對單一模型系統的僵化依賴，本文提出名為共識機制的新穎框架。該機制模仿臨床分診及多學科臨床決策，採用專業醫學專家代理集成，在維持穩健適應性的同時，改善臨床決策。此架構可依據內部模型配置，針對成本、延遲或效能優化共識機制。為嚴格評估共識機制，我們採用MedMCQA、MedQA、MedXpertQA Text三個醫學評估基準及DDX+鑑別診斷資料集。在MedXpertQA上，共識機制達到61.0%的準確度，優於OpenAI的O3 (53.5%) 和Google的Gemini 2.5 Pro (45.9%)。在MedQA (準確度提升3.4%) 和MedMCQA (準確度提升9.1%) 上，各基準測試均呈現一致的效能提升。此準確度提升亦延伸至鑑別診斷生成，我們的系統展現更佳的召回率和精確度 (F1 = 0.326 vs. F1 = 0.2886)，以及更高的DDX前1準確度 (Top1 = 52.0% vs. Top1 = 45.2%)。", "audio": "audios/2505.23075v1.mp3", "timestamp": "2025-05-31T20:19:14.288322"}
{"query": "AI", "id": "2505.23037v1", "url": "http://arxiv.org/abs/2505.23037v1", "title": "Improving Multilingual Social Media Insights: Aspect-based Comment Analysis", "summary": "The inherent nature of social media posts, characterized by the freedom of\nlanguage use with a disjointed array of diverse opinions and topics, poses\nsignificant challenges to downstream NLP tasks such as comment clustering,\ncomment summarization, and social media opinion analysis. To address this, we\npropose a granular level of identifying and generating aspect terms from\nindividual comments to guide model attention. Specifically, we leverage\nmultilingual large language models with supervised fine-tuning for comment\naspect term generation (CAT-G), further aligning the model's predictions with\nhuman expectations through DPO. We demonstrate the effectiveness of our method\nin enhancing the comprehension of social media discourse on two NLP tasks.\nMoreover, this paper contributes the first multilingual CAT-G test set on\nEnglish, Chinese, Malay, and Bahasa Indonesian. As LLM capabilities vary among\nlanguages, this test set allows for a comparative analysis of performance\nacross languages with varying levels of LLM proficiency.", "authors": ["Longyin Zhang", "Bowei Zou", "Ai Ti Aw"], "published_date": "2025-05-29", "title_zh": "改進多語社交媒體洞察：基於面向的評論分析", "summary_zh": "社交媒體貼文的語言自由和主題多樣性對自然語言處理任務構成挑戰。為此，我們提出從評論中識別並生成細粒度面向詞彙，以引導模型注意力。具體而言，我們利用多語言大型語言模型，透過監督微調進行評論面向詞彙生成，並透過直接偏好優化使模型預測更符合人類期望。實驗結果表明，該方法能有效提升對社交媒體討論的理解。此外，本文貢獻了首個包含英語、中文、馬來語和印尼語的多語言評論面向詞彙生成測試集，可比較不同語言在大型語言模型能力上的表現。", "audio": "audios/2505.23037v1.mp3", "timestamp": "2025-05-31T21:15:53.030843"}
{"query": "AI", "id": "2505.23035v1", "url": "http://arxiv.org/abs/2505.23035v1", "title": "Machine-Facing English: Defining a Hybrid Register Shaped by Human-AI Discourse", "summary": "Machine-Facing English (MFE) is an emergent register shaped by the adaptation\nof everyday language to the expanding presence of AI interlocutors. Drawing on\nregister theory (Halliday 1985, 2006), enregisterment (Agha 2003), audience\ndesign (Bell 1984), and interactional pragmatics (Giles & Ogay 2007), this\nstudy traces how sustained human-AI interaction normalizes syntactic rigidity,\npragmatic simplification, and hyper-explicit phrasing - features that enhance\nmachine parseability at the expense of natural fluency. Our analysis is\ngrounded in qualitative observations from bilingual (Korean/English) voice- and\ntext-based product testing sessions, with reflexive drafting conducted using\nNatural Language Declarative Prompting (NLD-P) under human curation. Thematic\nanalysis identifies five recurrent traits - redundant clarity, directive\nsyntax, controlled vocabulary, flattened prosody, and single-intent structuring\n- that improve execution accuracy but compress expressive range. MFE's\nevolution highlights a persistent tension between communicative efficiency and\nlinguistic richness, raising design challenges for conversational interfaces\nand pedagogical considerations for multilingual users. We conclude by\nunderscoring the need for comprehensive methodological exposition and future\nempirical validation.", "authors": ["Hyunwoo Kim", "Hanau Yi"], "published_date": "2025-05-29", "title_zh": "機器導向英語：定義由人機對話形塑的混合語域", "summary_zh": "機器導向英語（MFE）是一種新興語域，源於日常語言適應人工智能互動。本研究基於語域理論、語域化、受眾設計及互動語用學，追蹤人機互動如何常態化句法僵化、語用簡化及超顯性措辭，以提高機器解析度為代價，犧牲自然流暢性。分析基於韓英雙語語音和文字產品測試的質性觀察，並使用自然語言聲明式提示（NLD-P）進行自反式草擬。主題分析辨識出五個常見特徵：冗餘清晰、指令性句法、受控詞彙、扁平韻律及單一意圖結構，這些特徵雖提高執行準確性，卻壓縮了表達範圍。MFE的演變突顯了溝通效率與語言豐富性之間的持續張力，為對話介面設計帶來挑戰，並對多語使用者提出教學考量。結論強調需要全面的方法論闡述和未來的經驗驗證。", "audio": "audios/2505.23035v1.mp3", "timestamp": "2025-05-31T22:16:59.986275"}
{"query": "AI", "id": "2505.23009v1", "url": "http://arxiv.org/abs/2505.23009v1", "title": "EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge", "summary": "Text-to-Speech (TTS) benchmarks often fail to capture how well models handle\nnuanced and semantically complex text. Building on $\\textit{EmergentTTS}$, we\nintroduce $\\textit{EmergentTTS-Eval}$, a comprehensive benchmark covering six\nchallenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic\ncomplexity, complex pronunciation (e.g. URLs, formulas), and questions.\nCrucially, our framework automates both test-case generation and evaluation,\nmaking the benchmark easily extensible. Starting from a small set of\nhuman-written seed prompts, we iteratively extend them using LLMs to target\nspecific structural, phonetic and prosodic challenges, resulting in 1,645\ndiverse test cases. Moreover, we employ a model-as-a-judge approach, using a\nLarge Audio Language Model (LALM) to assess the speech across multiple\ndimensions such as expressed emotion, prosodic, intonational, and pronunciation\naccuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,\nsuch as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,\ndemonstrating its ability to reveal fine-grained performance differences.\nResults show that the model-as-a-judge approach offers robust TTS assessment\nand a high correlation with human preferences. We open source the evaluation\n$\\href{https://github.com/boson-ai/EmergentTTS-Eval-public}{code}$ and the\n$\\href{https://huggingface.co/datasets/bosonai/EmergentTTS-Eval}{dataset}$.", "authors": ["Ruskin Raj Manku", "Yuzhi Tang", "Xingjian Shi", "Mu Li", "Alex Smola"], "published_date": "2025-05-29", "title_zh": "湧現式TTS評估：利用模型即評審評估TTS模型在複雜韻律、表現力及語言挑戰上的表現", "summary_zh": "語音合成評測常忽略模型處理細微及語義複雜文本的能力。本研究基於EmergentTTS，推出EmergentTTS-Eval，涵蓋情感、副語言、外來語、句法複雜性、複雜發音及疑問句等六大挑戰情境。此框架自動生成測試案例及評估，具備高度擴展性。從少量人工撰寫的種子提示出發，利用大型語言模型迭代擴展，針對特定結構、語音及韻律挑戰，生成1,645個多樣化測試案例。此外，採用模型即裁判方法，利用大型音訊語言模型評估語音在情感表達、韻律、語調及發音準確性等多方面的表現。我們使用EmergentTTS-Eval評估了11Labs、Deepgram及OpenAI的4o-mini-TTS等先進的開源及專有語音合成系統，展示了其揭示細微性能差異的能力。結果表明，模型即裁判方法提供穩健的語音合成評估，且與人類偏好高度相關。我們開放了評估程式碼及資料集。", "audio": "audios/2505.23009v1.mp3", "timestamp": "2025-05-31T23:17:58.726461"}
{"query": "AI", "id": "2505.23006v1", "url": "http://arxiv.org/abs/2505.23006v1", "title": "A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs", "summary": "The advancement of Large Language Models (LLMs) has led to significant\nimprovements in various service domains, including search, recommendation, and\nchatbot applications. However, applying state-of-the-art (SOTA) research to\nindustrial settings presents challenges, as it requires maintaining flexible\nconversational abilities while also strictly complying with service-specific\nconstraints. This can be seen as two conflicting requirements due to the\nprobabilistic nature of LLMs. In this paper, we propose our approach to\naddressing this challenge and detail the strategies we employed to overcome\ntheir inherent limitations in real-world applications. We conduct a practical\ncase study of a conversational agent designed for the e-commerce domain,\ndetailing our implementation workflow and optimizations. Our findings provide\ninsights into bridging the gap between academic research and real-world\napplication, introducing a framework for developing scalable, controllable, and\nreliable AI-driven agents.", "authors": ["Chiwan Park", "Wonjun Jang", "Daeryong Kim", "Aelim Ahn", "Kichang Yang", "Woosung Hwang", "Jihyeon Roh", "Hyerin Park", "Hyosun Wang", "Min Seok Kim", "Jihoon Kang"], "published_date": "2025-05-29", "title_zh": "利用工作流程圖構建生產級對話代理的實用方法", "summary_zh": "大型語言模型(LLMs)的進步顯著提升了搜尋、推薦、聊天機器人等服務領域。然而，將最先進(SOTA)研究應用於工業環境具挑戰性，既要維持靈活的對話能力，又需嚴格遵守特定服務的限制。由於LLMs的機率性質，這兩者存在衝突。本文提出解決方案，詳述克服實際應用中固有局限性的策略。透過電商領域對話代理的案例研究，闡述實施流程與優化。研究結果有助於彌合學術研究與實際應用之間的差距，並為開發具規模性、可控性與可靠性的人工智慧驅動代理程式提供框架。", "audio": "audios/2505.23006v1.mp3", "timestamp": "2025-06-01T01:50:21.983609"}
{"query": "AI", "id": "2505.22990v1", "url": "http://arxiv.org/abs/2505.22990v1", "title": "MenTeR: A fully-automated Multi-agenT workflow for end-to-end RF/Analog Circuits Netlist Design", "summary": "RF/Analog design is essential for bridging digital technologies with\nreal-world signals, ensuring the functionality and reliability of a wide range\nof electronic systems. However, analog design procedures are often intricate,\ntime-consuming and reliant on expert intuition, and hinder the time and cost\nefficiency of circuit development. To overcome the limitations of the manual\ncircuit design, we introduce MenTeR - a multiagent workflow integrated into an\nend-to-end analog design framework. By employing multiple specialized AI agents\nthat collaboratively address different aspects of the design process, such as\nspecification understanding, circuit optimization, and test bench validation,\nMenTeR reduces the dependency on frequent trial-and-error-style intervention.\nMenTeR not only accelerates the design cycle time but also facilitates a\nbroader exploration of the design space, demonstrating robust capabilities in\nhandling real-world analog systems. We believe that MenTeR lays the groundwork\nfor future \"RF/Analog Copilots\" that can collaborate seamlessly with human\ndesigners.", "authors": ["Pin-Han Chen", "Yu-Sheng Lin", "Wei-Cheng Lee", "Tin-Yu Leu", "Po-Hsiang Hsu", "Anjana Dissanayake", "Sungjin Oh", "Chinq-Shiun Chiu"], "published_date": "2025-05-29", "title_zh": "MenTeR：全自動化多代理端到端射頻/類比電路網表設計工作流程", "summary_zh": "射頻/類比設計對於連接數位技術與真實世界訊號至關重要，確保廣泛電子系統的功能和可靠性。然而，類比設計流程通常複雜耗時，依賴專家直覺，阻礙電路開發的效率。為克服手動設計的局限性，我們推出MenTeR，一個整合至端對端類比設計框架的多代理工作流。MenTeR採用多個專業AI代理協作處理設計流程的不同方面，如規格理解、電路優化和測試驗證，減少對頻繁試錯式干預的依賴，加速設計週期並促進更廣泛的設計空間探索，展現處理真實世界類比系統的穩健能力。MenTeR為未來的射頻/類比協同設計奠定基礎。", "audio": "audios/2505.22990v1.mp3", "timestamp": "2025-06-01T03:40:00.934774"}
{"query": "AI", "id": "2505.22987v1", "url": "http://arxiv.org/abs/2505.22987v1", "title": "Strategic Reflectivism In Intelligent Systems", "summary": "By late 20th century, the rationality wars had launched debates about the\nnature and norms of intuitive and reflective thinking. Those debates drew from\nmid-20th century ideas such as bounded rationality, which challenged more\nidealized notions of rationality observed since the 19th century. Now that 21st\ncentury cognitive scientists are applying the resulting dual process theories\nto artificial intelligence, it is time to dust off some lessons from this\nhistory. So this paper synthesizes old ideas with recent results from\nexperiments on humans and machines. The result is Strategic Reflectivism, which\ntakes the position that one key to intelligent systems (human or artificial) is\npragmatic switching between intuitive and reflective inference to optimally\nfulfill competing goals. Strategic Reflectivism builds on American Pragmatism,\ntranscends superficial indicators of reflective thinking such as model size or\nchains of thought, and becomes increasingly actionable as we learn more about\nthe value of intuition and reflection.", "authors": ["Nick Byrd"], "published_date": "2025-05-29", "title_zh": "智能系統中的策略性反思主義", "summary_zh": "20世紀末，理性論戰引發關於直覺和反思思維本質與規範的辯論，其根源可追溯至20世紀中期的有限理性等觀點。現今，認知科學家將由此產生的雙重歷程理論應用於人工智慧，故有必要重溫歷史教訓。本文整合舊觀點與近期人機實驗成果，提出策略性反思主義。該理論認為，智慧系統（無論人或機器）的關鍵在於直覺和反思推理之間的務實切換，以最佳化實現相互競爭的目標。策略性反思主義立基於美國實用主義，超越模型大小或思維鏈等表面化的反思思維指標，並隨著我們對直覺和反思價值認識的加深而更具可行性。", "audio": "audios/2505.22987v1.mp3", "timestamp": "2025-06-01T04:34:25.186243"}
{"query": "AI", "id": "2505.22907v1", "url": "http://arxiv.org/abs/2505.22907v1", "title": "Conversational Alignment with Artificial Intelligence in Context", "summary": "The development of sophisticated artificial intelligence (AI) conversational\nagents based on large language models raises important questions about the\nrelationship between human norms, values, and practices and AI design and\nperformance. This article explores what it means for AI agents to be\nconversationally aligned to human communicative norms and practices for\nhandling context and common ground and proposes a new framework for evaluating\ndevelopers' design choices. We begin by drawing on the philosophical and\nlinguistic literature on conversational pragmatics to motivate a set of\ndesiderata, which we call the CONTEXT-ALIGN framework, for conversational\nalignment with human communicative practices. We then suggest that current\nlarge language model (LLM) architectures, constraints, and affordances may\nimpose fundamental limitations on achieving full conversational alignment.", "authors": ["Rachel Katharine Sterken", "James Ravi Kirkpatrick"], "published_date": "2025-05-28", "title_zh": "情境脈絡下與人工智能的對話協調", "summary_zh": "基於大型語言模型的AI對話代理的發展引發了關於人類規範、價值觀與AI設計及效能間關係的重要問題。本文探討AI代理在對話上與人類溝通規範和處理語境及共同基礎之實踐對齊的意義，並提出評估開發者設計選擇的新框架。文章首先借鑒哲學和語言學中關於對話語用學的文獻，提出一套名為CONTEXT-ALIGN的標準，用於評估與人類溝通實踐的對話對齊程度。隨後，文章指出當前大型語言模型（LLM）的架構、限制和可供性可能對實現完全對話對齊構成根本性限制。", "audio": "audios/2505.22907v1.mp3", "timestamp": "2025-06-01T05:18:15.302037"}
{"query": "AI", "id": "2505.23655v2", "url": "http://arxiv.org/abs/2505.23655v2", "title": "Keyed Chaotic Masking: A Functional Privacy Framework for Neural Inference", "summary": "This work introduces a lightweight framework for privacy-preserving neural\nnetwork inference based on keyed chaotic masking a deterministic, user-specific\nobfuscation method derived from cryptographically seeded chaotic dynamical\nsystems. The approach applies masks to input and output tensors using\nkey-conditioned graph dynamics, enabling authenticated inference, user\nattribution, and soft output watermarking without modifying model\narchitectures. While the underlying chaotic system used to generate each mask\nis not analytically invertible, the masking operation itself is algebraically\nreversible by authorized key holders, offering functional privacy without\nformal cryptographic guarantees. Unlike traditional encryption or secure\nmulti-party computation, this method operates in continuous space and imposes\nminimal computational overhead. We describe the construction of the masking\nsystem, including graph sampling, dynamical rule selection, and chaos\ndiagnostics. Applications include privacy-preserving inference, secure data\ncontribution, and per-user watermarking in shared model pipelines. This\nframework offers a practical and modular building block for user-controlled\nprivacy in modern AI systems.", "authors": ["Peter David Fagan"], "published_date": "2025-05-29", "title_zh": "密鑰混沌掩蔽：神經推論之功能性隱私框架", "summary_zh": "本研究提出一種輕量級的隱私保護神經網路推論框架，其基於密鑰控制的混沌遮罩，一種源自密碼學種子的混沌動力系統的確定性、使用者特定混淆方法。此方法利用密鑰條件圖動力學，將遮罩應用於輸入和輸出張量，實現經身份驗證的推論、使用者歸屬和軟輸出浮水印，無需修改模型架構。雖然產生遮罩的底層混沌系統不可解析反轉，但授權密鑰持有者可代數還原遮罩操作，提供功能性隱私，但無正式密碼學保證。不同於傳統加密或安全多方計算，此方法在連續空間運作，計算開銷極小。本文描述了遮罩系統的建構，包括圖抽樣、動力規則選擇和混沌診斷。應用包括隱私保護推論、安全資料貢獻和共享模型管道中的使用者浮水印。此框架為現代人工智慧系統中使用者控制的隱私提供了一個實用且模組化的建構模塊。", "audio": "audios/2505.23655v2.mp3", "timestamp": "2025-06-02T01:39:26.043168"}
{"query": "Foundation Model", "id": "2505.22904v2", "url": "http://arxiv.org/abs/2505.22904v2", "title": "Defining Foundation Models for Computational Science: A Call for Clarity and Rigor", "summary": "The widespread success of foundation models in natural language processing\nand computer vision has inspired researchers to extend the concept to\nscientific machine learning and computational science. However, this position\npaper argues that as the term \"foundation model\" is an evolving concept, its\napplication in computational science is increasingly used without a universally\naccepted definition, potentially creating confusion and diluting its precise\nscientific meaning. In this paper, we address this gap by proposing a formal\ndefinition of foundation models in computational science, grounded in the core\nvalues of generality, reusability, and scalability. We articulate a set of\nessential and desirable characteristics that such models must exhibit, drawing\nparallels with traditional foundational methods, like the finite element and\nfinite volume methods. Furthermore, we introduce the Data-Driven Finite Element\nMethod (DD-FEM), a framework that fuses the modular structure of classical FEM\nwith the representational power of data-driven learning. We demonstrate how\nDD-FEM addresses many of the key challenges in realizing foundation models for\ncomputational science, including scalability, adaptability, and physics\nconsistency. By bridging traditional numerical methods with modern AI\nparadigms, this work provides a rigorous foundation for evaluating and\ndeveloping novel approaches toward future foundation models in computational\nscience.", "authors": ["Youngsoo Choi", "Siu Wun Cheung", "Youngkyu Kim", "Ping-Hsuan Tsai", "Alejandro N. Diaz", "Ivan Zanardi", "Seung Whan Chung", "Dylan Matthew Copeland", "Coleman Kendrick", "William Anderson", "Traian Iliescu", "Matthias Heinkenschloss"], "published_date": "2025-05-28", "title_zh": "計算科學之基礎模型定義：呼籲明確性與嚴謹性", "summary_zh": "自然語言處理與電腦視覺中基礎模型的成功激勵研究人員將其擴展至科學機器學習與計算科學。然此立場文件指出，由於基礎模型概念不斷演變，其於計算科學之應用日益普及，卻缺乏通用定義，恐造成混淆並稀釋其精確科學意涵。本文針對此缺口，基於通用性、可重用性與可擴展性等核心價值，提出計算科學中基礎模型的正式定義，闡述其應具備之必要與期望特性，並與有限元素法及有限體積法等傳統基礎方法作比較。此外，本文介紹數據驅動有限元素法(DD-FEM)，此框架融合傳統有限元素法之模組化結構與數據驅動學習之表徵能力。本文展示DD-FEM如何解決計算科學中實現基礎模型的關鍵挑戰，包含可擴展性、適應性及物理一致性。透過橋接傳統數值方法與現代AI範式，此研究為評估及發展未來計算科學之基礎模型提供嚴謹基礎。", "audio": "audios/2505.22904v2.mp3", "timestamp": "2025-06-02T01:39:36.171118"}
{"query": "Diffusion Model", "id": "2505.23661v2", "url": "http://arxiv.org/abs/2505.23661v2", "title": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation", "summary": "In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni.", "authors": ["Size Wu", "Zhonghua Wu", "Zerui Gong", "Qingyi Tao", "Sheng Jin", "Qinyue Li", "Wei Li", "Chen Change Loy"], "published_date": "2025-05-29", "title_zh": "OpenUni：統一多模態理解與生成的一個簡潔基線", "summary_zh": "本研究提出OpenUni，一個簡潔、輕量且完全開源的多模態理解與生成基線模型。受統一模型學習啟發，我們採用高效訓練策略，透過可學習查詢與輕量級Transformer連接器，橋接現成的多模態大型語言模型與擴散模型，最小化訓練複雜度與成本。OpenUni僅需11億至31億啟動參數，即可生成高品質且符合指令的圖像，並在GenEval、DPG-Bench及WISE等標準基準測試中取得優異表現。所有模型權重、訓練程式碼及包含2300萬圖文配對的訓練資料集已於https://github.com/wusize/OpenUni公開，以支持開放研究與社群發展。", "audio": "audios/2505.23661v2.mp3", "timestamp": "2025-06-02T01:39:41.912631"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類能直觀地在3D空間中構圖攝影。本研究探討AI影像生成器是否具備相似的3D空間感知能力，以根據文字或圖像提示規劃場景。我們提出GenSpace，一個評估影像生成模型空間感知能力的新基準和評估流程。現有評估方法常未能捕捉細微的空間錯誤。因此，我們提出專業評估流程與指標，利用多個視覺基礎模型重建3D場景幾何，提供更準確、更符合人類認知的空間保真度量測。研究發現，AI模型雖能生成視覺上吸引人的圖像並遵循通用指令，但在物件放置、關係和測量等具體3D細節上表現不佳。我們歸納出當前先進影像生成模型在空間感知方面的三個主要局限：1)物件透視理解，2)自我中心-非自我中心轉換，3)度量測量準確性，為提升影像生成中的空間智能指明方向。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T03:19:57.097718"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往通用神經符號學習之路應以基石模型鋪就", "summary_zh": "神經符號學習旨在解決複雜推理任務中神經網路訓練的挑戰，並提高可解釋性、可靠性和效率。傳統神經符號學習方法結合訓練神經模型和符號程式，但其複雜度受限。純神經基礎模型透過提示而非訓練達到最佳效能，但可靠性和可解釋性不足。利用符號程式補充基礎模型，即神經符號提示，可用於複雜推理。因此，在基礎模型時代，神經符號學習中專業模型訓練扮演何種角色？本文探討了傳統神經符號學習在計算、資料和程式方面導致泛化問題的三個陷阱，並主張基礎模型可實現可泛化的神經符號解決方案，從而無需從頭開始訓練即可實現神經符號學習的最初目標。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T03:20:02.442068"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視圖擴散的可動畫細節三維人體生成", "summary_zh": "現有圖像轉3D虛擬人像方法難以生成高細節、可動畫化的虛擬人像，不適用於實際應用。AdaHuman提出一種新框架，僅需單張自然圖像即可生成高保真、可動畫化的3D虛擬人像。AdaHuman包含兩項創新：(1)姿態條件3D關節擴散模型，合成任意姿態下一致的多視圖圖像，並於每次擴散步驟重建對應的3D高斯潑濺(3DGS)；(2)組合式3DGS精煉模組，透過圖像到圖像的精煉增強局部身體部位的細節，並利用新型的裁剪感知相機射線圖無縫整合，生成連貫細緻的3D虛擬人像。這些組件使AdaHuman能夠生成高度逼真的標準A字站姿虛擬人像，具有最小的自我遮擋，可以使用任何輸入動作進行綁定和動畫製作。在公共基準和自然圖像上的廣泛評估表明，AdaHuman在虛擬人像重建和重新擺姿勢方面顯著優於現有方法。程式碼和模型將公開提供，以供研究之用。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T03:20:09.778777"}
{"query": "AI", "id": "2505.24848v1", "url": "http://arxiv.org/abs/2505.24848v1", "title": "Reading Recognition in the Wild", "summary": "To enable egocentric contextual AI in always-on smart glasses, it is crucial\nto be able to keep a record of the user's interactions with the world,\nincluding during reading. In this paper, we introduce a new task of reading\nrecognition to determine when the user is reading. We first introduce the\nfirst-of-its-kind large-scale multimodal Reading in the Wild dataset,\ncontaining 100 hours of reading and non-reading videos in diverse and realistic\nscenarios. We then identify three modalities (egocentric RGB, eye gaze, head\npose) that can be used to solve the task, and present a flexible transformer\nmodel that performs the task using these modalities, either individually or\ncombined. We show that these modalities are relevant and complementary to the\ntask, and investigate how to efficiently and effectively encode each modality.\nAdditionally, we show the usefulness of this dataset towards classifying types\nof reading, extending current reading understanding studies conducted in\nconstrained settings to larger scale, diversity and realism. Code, model, and\ndata will be public.", "authors": ["Charig Yang", "Samiul Alam", "Shakhrul Iman Siam", "Michael J. Proulx", "Lambert Mathias", "Kiran Somasundaram", "Luis Pesqueira", "James Fort", "Sheroze Sheriffdeen", "Omkar Parkhi", "Carl Ren", "Mi Zhang", "Yuning Chai", "Richard Newcombe", "Hyo Jin Kim"], "published_date": "2025-05-30", "title_zh": "自然場景下的閱讀識別", "summary_zh": "為了在常時啟動的智慧眼鏡中實現以自我為中心的上下文人工智慧，記錄使用者與世界的互動至關重要，包括閱讀行為。本研究提出一項新的閱讀識別任務，旨在判斷使用者是否正在閱讀。我們創建了首個大規模多模態的「野外閱讀」資料集，包含100小時在多樣且真實場景下的閱讀和非閱讀影片。我們確認了三種可用於解決此任務的模態：以自我為中心的RGB影像、眼球注視和頭部姿態。我們提出了一個靈活的Transformer模型，可單獨或組合使用這些模態來執行任務。實驗結果表明，這些模態具有相關性和互補性，並探討了如何有效率地編碼每種模態。此外，我們展示了此資料集在分類閱讀類型方面的效用，將目前在受限環境中進行的閱讀理解研究擴展到更大規模、更多樣性和更真實的場景。程式碼、模型和資料將公開。", "audio": "audios/2505.24848v1.mp3", "timestamp": "2025-06-02T04:29:29.729530"}
{"query": "Foundation Model", "id": "2505.24846v1", "url": "http://arxiv.org/abs/2505.24846v1", "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning", "summary": "Reward modeling is a key step in building safe foundation models when\napplying reinforcement learning from human feedback (RLHF) to align Large\nLanguage Models (LLMs). However, reward modeling based on the Bradley-Terry\n(BT) model assumes a global reward function, failing to capture the inherently\ndiverse and heterogeneous human preferences. Hence, such oversimplification\nlimits LLMs from supporting personalization and pluralistic alignment.\nTheoretically, we show that when human preferences follow a mixture\ndistribution of diverse subgroups, a single BT model has an irreducible error.\nWhile existing solutions, such as multi-objective learning with fine-grained\nannotations, help address this issue, they are costly and constrained by\npredefined attributes, failing to fully capture the richness of human values.\nIn this work, we introduce MiCRo, a two-stage framework that enhances\npersonalized preference learning by leveraging large-scale binary preference\ndatasets without requiring explicit fine-grained annotations. In the first\nstage, MiCRo introduces context-aware mixture modeling approach to capture\ndiverse human preferences. In the second stage, MiCRo integrates an online\nrouting strategy that dynamically adapts mixture weights based on specific\ncontext to resolve ambiguity, allowing for efficient and scalable preference\nadaptation with minimal additional supervision. Experiments on multiple\npreference datasets demonstrate that MiCRo effectively captures diverse human\npreferences and significantly improves downstream personalization.", "authors": ["Jingyan Shen", "Jiarui Yao", "Rui Yang", "Yifan Sun", "Feng Luo", "Rui Pan", "Tong Zhang", "Han Zhao"], "published_date": "2025-05-30", "title_zh": "MiCRo：混合模型與情境感知路由於個人化偏好學習", "summary_zh": "獎勵模型是利用人類回饋強化學習(RLHF)對齊大型語言模型(LLM)的關鍵步驟。然而，基於布拉德利-特里(BT)模型的獎勵建模假設存在全局獎勵函數，無法捕捉人類偏好的多樣性。這種過度簡化限制了LLM支援個人化和多元對齊的能力。理論上，當人類偏好遵循多個子群體的混合分布時，單一BT模型存在不可避免的誤差。現有解決方案，如具細粒度標註的多目標學習，雖有助於解決此問題，但成本高昂且受限於預定義屬性。本研究提出MiCRo，一個雙階段框架，透過利用大規模二元偏好資料集，無需顯式細粒度標註，增強個人化偏好學習。第一階段，MiCRo引入情境感知混合模型方法，捕捉人類偏好的多樣性。第二階段，MiCRo整合線上路由策略，基於特定情境動態調整混合權重，以解決模糊性，並以最少的額外監督實現高效且可擴展的偏好適應。在多個偏好資料集上的實驗表明，MiCRo能有效捕捉人類偏好的多樣性，並顯著改善下游個人化效果。", "audio": "audios/2505.24846v1.mp3", "timestamp": "2025-06-02T04:29:41.821914"}
{"query": "Diffusion Model", "id": "2505.24873v1", "url": "http://arxiv.org/abs/2505.24873v1", "title": "MiniMax-Remover: Taming Bad Noise Helps Video Object Removal", "summary": "Recent advances in video diffusion models have driven rapid progress in video\nediting techniques. However, video object removal, a critical subtask of video\nediting, remains challenging due to issues such as hallucinated objects and\nvisual artifacts. Furthermore, existing methods often rely on computationally\nexpensive sampling procedures and classifier-free guidance (CFG), resulting in\nslow inference. To address these limitations, we propose MiniMax-Remover, a\nnovel two-stage video object removal approach. Motivated by the observation\nthat text condition is not best suited for this task, we simplify the\npretrained video generation model by removing textual input and cross-attention\nlayers, resulting in a more lightweight and efficient model architecture in the\nfirst stage. In the second stage, we distilled our remover on successful videos\nproduced by the stage-1 model and curated by human annotators, using a minimax\noptimization strategy to further improve editing quality and inference speed.\nSpecifically, the inner maximization identifies adversarial input noise (\"bad\nnoise\") that makes failure removals, while the outer minimization step trains\nthe model to generate high-quality removal results even under such challenging\nconditions. As a result, our method achieves a state-of-the-art video object\nremoval results with as few as 6 sampling steps and doesn't rely on CFG,\nsignificantly improving inference efficiency. Extensive experiments demonstrate\nthe effectiveness and superiority of MiniMax-Remover compared to existing\nmethods. Codes and Videos are available at: https://minimax-remover.github.io.", "authors": ["Bojia Zi", "Weixuan Peng", "Xianbiao Qi", "Jianan Wang", "Shihao Zhao", "Rong Xiao", "Kam-Fai Wong"], "published_date": "2025-05-30", "title_zh": "MiniMax消除器：駕馭不良雜訊助力影片物件移除", "summary_zh": "影片擴散模型促進了影片編輯技術的發展，但影片物件移除仍具挑戰，易產生幻覺物件與視覺瑕疵。現有方法仰賴高成本取樣與無分類器引導，導致推論速度緩慢。本文提出MiniMax-Remover，一種雙階段影片物件移除方法。考量文字條件不適用於此任務，第一階段簡化預訓練影片生成模型，移除文字輸入和交叉注意力層，實現輕量化架構。第二階段，利用極小極大最佳化策略，將移除器從第一階段模型成功移除物件並經人工標註的影片中提煉，以提升編輯品質與推論速度。內部極大化識別對抗性輸入雜訊，外部極小化訓練模型在惡劣條件下生成高品質移除結果。實驗結果顯示，MiniMax-Remover僅需少量取樣步驟且不依賴無分類器引導，即可達到最先進的影片物件移除效果，顯著提升推論效率。", "audio": "audios/2505.24873v1.mp3", "timestamp": "2025-06-02T04:29:50.900418"}
{"query": "AI", "id": "2505.24838v1", "url": "http://arxiv.org/abs/2505.24838v1", "title": "VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and 3D Reasoning from CAD Software", "summary": "Computer-Aided Design (CAD) is a time-consuming and complex process,\nrequiring precise, long-horizon user interactions with intricate 3D interfaces.\nWhile recent advances in AI-driven user interface (UI) agents show promise,\nmost existing datasets and methods focus on short, low-complexity tasks in\nmobile or web applications, failing to capture the demands of professional\nengineering tools. In this work, we introduce VideoCAD, the first attempt at\nengineering UI interaction learning for precision tasks. Specifically, VideoCAD\nis a large-scale synthetic dataset consisting of over 41K annotated video\nrecordings of CAD operations, generated using an automated framework for\ncollecting high-fidelity UI action data from human-made CAD designs. Compared\nto existing datasets, VideoCAD offers an order of magnitude higher complexity\nin UI interaction learning for real-world engineering tasks, having up to a 20x\nlonger time horizon than other datasets. We show two important downstream\napplications of VideoCAD: learning UI interactions from professional precision\n3D CAD tools and a visual question-answering (VQA) benchmark designed to\nevaluate multimodal large language models' (LLM) spatial reasoning and video\nunderstanding abilities. To learn the UI interactions, we propose\nVideoCADFormer - a state-of-the-art model in learning CAD interactions directly\nfrom video, which outperforms multiple behavior cloning baselines. Both\nVideoCADFormer and the VQA benchmark derived from VideoCAD reveal key\nchallenges in the current state of video-based UI understanding, including the\nneed for precise action grounding, multi-modal and spatial reasoning, and\nlong-horizon dependencies.", "authors": ["Brandon Man", "Ghadi Nehme", "Md Ferdous Alam", "Faez Ahmed"], "published_date": "2025-05-30", "title_zh": "VideoCAD：從CAD軟體學習UI互動與3D推理的大規模影片資料集", "summary_zh": "電腦輔助設計(CAD)耗時且複雜，需精確且長時間的人機互動。現有AI驅動介面代理雖具潛力，然現有資料集與方法多著重於行動或網頁應用上的短時低複雜度任務，未能滿足專業工程工具的需求。本研究推出VideoCAD，首次嘗試針對精確任務進行工程介面互動學習。VideoCAD為大規模合成資料集，包含逾4.1萬個CAD操作帶註釋的影片記錄，藉由自動化框架從人工CAD設計中收集高擬真介面動作資料產生。相較於現有資料集，VideoCAD在真實工程任務的介面互動學習上複雜度高出一個數量級，時間跨度比其他資料集長達20倍。展示VideoCAD的兩個重要下游應用：從專業精密3D CAD工具學習介面互動，以及評估多模態大型語言模型空間推理與影片理解能力的視覺問答(VQA)基準。為學習介面互動，提出VideoCADFormer，此為直接從影片學習CAD互動的先進模型，效能優於多個行為複製基準。VideoCADFormer與VideoCAD衍生的VQA基準皆揭示當前基於影片的介面理解所面臨的關鍵挑戰，包含精確動作定位、多模態與空間推理，以及長時依賴的需求。", "audio": "audios/2505.24838v1.mp3", "timestamp": "2025-06-02T05:19:39.450080"}
{"query": "Foundation Model", "id": "2505.24819v1", "url": "http://arxiv.org/abs/2505.24819v1", "title": "Bi-Manual Joint Camera Calibration and Scene Representation", "summary": "Robot manipulation, especially bimanual manipulation, often requires setting\nup multiple cameras on multiple robot manipulators. Before robot manipulators\ncan generate motion or even build representations of their environments, the\ncameras rigidly mounted to the robot need to be calibrated. Camera calibration\nis a cumbersome process involving collecting a set of images, with each\ncapturing a pre-determined marker. In this work, we introduce the Bi-Manual\nJoint Calibration and Representation Framework (Bi-JCR). Bi-JCR enables\nmultiple robot manipulators, each with cameras mounted, to circumvent taking\nimages of calibration markers. By leveraging 3D foundation models for dense,\nmarker-free multi-view correspondence, Bi-JCR jointly estimates: (i) the\nextrinsic transformation from each camera to its end-effector, (ii) the\ninter-arm relative poses between manipulators, and (iii) a unified,\nscale-consistent 3D representation of the shared workspace, all from the same\ncaptured RGB image sets. The representation, jointly constructed from images\ncaptured by cameras on both manipulators, lives in a common coordinate frame\nand supports collision checking and semantic segmentation to facilitate\ndownstream bimanual coordination tasks. We empirically evaluate the robustness\nof Bi-JCR on a variety of tabletop environments, and demonstrate its\napplicability on a variety of downstream tasks.", "authors": ["Haozhan Tang", "Tianyi Zhang", "Matthew Johnson-Roberson", "Weiming Zhi"], "published_date": "2025-05-30", "title_zh": "雙手協同的相機聯合校正與場景重建", "summary_zh": "機器人操作，尤其雙臂操作，常需在多個機器人手臂上架設多個攝影機。在機器人手臂產生運動或建立環境表示前，需先校正剛性連接於機器人的攝影機。傳統攝影機校正繁瑣，需拍攝一系列帶有預定標記的影像。本研究提出雙臂聯合校正與表示框架(Bi-JCR)，使配備攝影機的多個機器人手臂無需拍攝校正標記影像。Bi-JCR利用3D基礎模型實現密集的、無標記的多視角對應，從而聯合估計：(i) 各攝影機相對於其末端執行器的外參轉換，(ii) 各手臂間的相對位姿，以及 (iii) 工作空間的統一、尺度一致的3D表示，所有這些都來自同一組RGB影像。此表示由雙臂攝影機影像聯合構建，位於共同座標系中，並支援碰撞檢測與語意分割，以促進後續雙臂協調任務。實驗證明Bi-JCR在多種桌面環境中的穩健性，並展示其在各種下游任務中的適用性。", "audio": "audios/2505.24819v1.mp3", "timestamp": "2025-06-02T05:19:48.202653"}
{"query": "Diffusion Model", "id": "2505.24857v1", "url": "http://arxiv.org/abs/2505.24857v1", "title": "Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking", "summary": "Recent masked diffusion models (MDMs) have shown competitive performance\ncompared to autoregressive models (ARMs) for language modeling. While most\nliterature has focused on performance enhancing sampling procedures, efficient\nsampling from MDMs has been scarcely explored. We make the observation that\noften a given sequence of partially masked tokens determines the values of\nmultiple unknown tokens deterministically, meaning that a single prediction of\na masked model holds additional information unused by standard sampling\nprocedures. Based on this observation, we introduce EB-Sampler, a simple\ndrop-in replacement for existing samplers, utilizing an Entropy Bounded\nunmasking procedure that dynamically unmasks multiple tokens in one function\nevaluation with predefined approximate error tolerance. We formulate the\nEB-Sampler as part of a broad family of adaptive samplers for which we provide\nan error analysis that motivates our algorithmic choices. EB-Sampler\naccelerates sampling from current state of the art MDMs by roughly 2-3x on\nstandard coding and math reasoning benchmarks without loss in performance. We\nalso validate the same procedure works well on smaller reasoning tasks\nincluding maze navigation and Sudoku, tasks ARMs often struggle with.", "authors": ["Heli Ben-Hamu", "Itai Gat", "Daniel Severo", "Niklas Nolte", "Brian Karrer"], "published_date": "2025-05-30", "title_zh": "基於熵界限解蔽的遮罩擴散模型加速採樣", "summary_zh": "近期遮蔽擴散模型(MDM)在語言建模方面展現出與自迴歸模型(ARM)相媲美的效能。現有文獻多關注於提升效能的抽樣程序，對MDM的高效抽樣探索甚少。我們觀察到，給定部分遮蔽的tokens序列通常能決定多個未知tokens的值，意味著遮蔽模型的單次預測包含標準抽樣程序未利用的額外訊息。基於此，我們提出EB-Sampler，一種簡潔的即插即用抽樣器，利用熵界限解遮蔽程序，以預定義的近似誤差容忍度，在單次函數評估中動態解遮蔽多個tokens。我們將EB-Sampler表述為廣泛的自適應抽樣器家族的一部分，並提供誤差分析以佐證演算法選擇。EB-Sampler在標準編碼和數學推理基準測試中，加速了當前最先進MDM的抽樣速度約2-3倍，且效能無損。我們亦驗證了相同程序在較小的推理任務（包括迷宮導航和數獨）中表現良好，而這些任務通常是ARM的弱點。", "audio": "audios/2505.24857v1.mp3", "timestamp": "2025-06-02T05:19:55.417850"}
{"query": "AI", "id": "2505.24830v1", "url": "http://arxiv.org/abs/2505.24830v1", "title": "Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs", "summary": "Large language models (LLMs) exhibit extensive medical knowledge but are\nprone to hallucinations and inaccurate citations, which pose a challenge to\ntheir clinical adoption and regulatory compliance. Current methods, such as\nRetrieval Augmented Generation, partially address these issues by grounding\nanswers in source documents, but hallucinations and low fact-level\nexplainability persist. In this work, we introduce a novel atomic fact-checking\nframework designed to enhance the reliability and explainability of LLMs used\nin medical long-form question answering. This method decomposes LLM-generated\nresponses into discrete, verifiable units called atomic facts, each of which is\nindependently verified against an authoritative knowledge base of medical\nguidelines. This approach enables targeted correction of errors and direct\ntracing to source literature, thereby improving the factual accuracy and\nexplainability of medical Q&A. Extensive evaluation using multi-reader\nassessments by medical experts and an automated open Q&A benchmark demonstrated\nsignificant improvements in factual accuracy and explainability. Our framework\nachieved up to a 40% overall answer improvement and a 50% hallucination\ndetection rate. The ability to trace each atomic fact back to the most relevant\nchunks from the database provides a granular, transparent explanation of the\ngenerated responses, addressing a major gap in current medical AI applications.\nThis work represents a crucial step towards more trustworthy and reliable\nclinical applications of LLMs, addressing key prerequisites for clinical\napplication and fostering greater confidence in AI-assisted healthcare.", "authors": ["Juraj Vladika", "Annika Domres", "Mai Nguyen", "Rebecca Moser", "Jana Nano", "Felix Busch", "Lisa C. Adams", "Keno K. Bressem", "Denise Bernhardt", "Stephanie E. Combs", "Kai J. Borm", "Florian Matthes", "Jan C. Peeken"], "published_date": "2025-05-30", "title_zh": "藉由檢索增強型大型語言模型中原子事實查核提升醫學問答的可靠性與可解釋性", "summary_zh": "大型語言模型雖具備廣泛醫學知識，但易產生幻覺及不準確引用，阻礙其臨床應用及法規遵循。檢索增強生成等現有方法雖藉由文獻依據部分緩解此問題，然幻覺及低事實層級可解釋性依舊存在。本研究提出一種原子事實查核框架，旨在提升醫療長篇問答中大型語言模型的可靠性及可解釋性。此方法將模型生成的回應分解為離散、可驗證的原子事實單元，並針對醫學指南的權威知識庫逐一獨立驗證。此方式可針對性修正錯誤並直接追溯至原始文獻，從而改善醫學問答的事實準確性及可解釋性。經醫學專家多重評估及開放問答基準測試，結果顯示事實準確性及可解釋性顯著提升。框架整體答案改善幅度達40%，幻覺偵測率達50%。追溯各原子事實至資料庫最相關區塊的能力，為生成的回應提供細緻、透明的解釋，填補現有醫療AI應用中的主要缺口。此研究是朝向更可信賴及可靠的臨床大型語言模型應用邁出的關鍵一步，滿足臨床應用之先決條件，並增強對AI輔助醫療照護的信心。", "audio": "audios/2505.24830v1.mp3", "timestamp": "2025-06-02T06:28:52.636494"}
{"query": "Foundation Model", "id": "2505.24773v1", "url": "http://arxiv.org/abs/2505.24773v1", "title": "AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption", "summary": "Federated fine-tuning has emerged as a promising approach to adapt foundation\nmodels to downstream tasks using decentralized data. However, real-world\ndeployment remains challenging due to the high computational and communication\ndemands of fine-tuning Large Language Models (LLMs) on clients with data and\nsystem resources that are heterogeneous and constrained. In such settings, the\nglobal model's performance is often bottlenecked by the weakest clients and\nfurther degraded by the non-IID nature of local data. Although existing methods\nleverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to\nreduce communication and computation overhead, they often fail to\nsimultaneously ensure accurate aggregation of low-rank updates and maintain low\nsystem costs, thereby hindering overall performance. To address these\nchallenges, we propose AFLoRA, an adaptive and lightweight federated\nfine-tuning framework for LLMs. AFLoRA decouples shared and client-specific\nupdates to reduce overhead and improve aggregation accuracy, incorporates\ndiagonal matrix-based rank pruning to better utilize local resources, and\nemploys rank-aware aggregation with public data refinement to strengthen\ngeneralization under data heterogeneity. Extensive experiments demonstrate that\nAFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,\nproviding a practical solution for efficient LLM adaptation in heterogeneous\nenvironments in the real world.", "authors": ["Yajie Zhou", "Xiaoyi Pang", "Zhibo Wang"], "published_date": "2025-05-30", "title_zh": "AFLoRA：具資源感知的低秩適應大型語言模型自適應聯邦微調", "summary_zh": "聯邦微調成為利用分散式資料適應基礎模型至下游任務的有效方法。然而，由於在異質且資源受限的客戶端上微調大型語言模型的計算和通訊需求高昂，實際部署仍具挑戰。在此情況下，全局模型效能常受限於最弱客戶端，並因本地資料的非獨立同分布特性而進一步降低。現有方法雖利用低秩適應等參數高效技術來降低通訊和計算開銷，但往往無法同時確保低秩更新的精確聚合並維持低系統成本，從而阻礙整體效能。為了解決這些挑戰，我們提出AFLoRA，一個適用於大型語言模型的自適應輕量級聯邦微調框架。AFLoRA分離共享和客戶端特定更新以降低開銷並提高聚合準確性，採用基於對角矩陣的秩剪枝以更好地利用本地資源，並採用具有公共資料改進的秩感知聚合來加強資料異質性下的泛化能力。大量實驗表明，AFLoRA在準確性和效率方面均優於最先進的方法，為實際環境中高效的大型語言模型適應提供了可行的解決方案。", "audio": "audios/2505.24773v1.mp3", "timestamp": "2025-06-02T06:29:00.351325"}
{"query": "Diffusion Model", "id": "2505.24808v1", "url": "http://arxiv.org/abs/2505.24808v1", "title": "RealDrive: Retrieval-Augmented Driving with Diffusion Models", "summary": "Learning-based planners generate natural human-like driving behaviors by\nlearning to reason about nuanced interactions from data, overcoming the rigid\nbehaviors that arise from rule-based planners. Nonetheless, data-driven\napproaches often struggle with rare, safety-critical scenarios and offer\nlimited controllability over the generated trajectories. To address these\nchallenges, we propose RealDrive, a Retrieval-Augmented Generation (RAG)\nframework that initializes a diffusion-based planning policy by retrieving the\nmost relevant expert demonstrations from the training dataset. By interpolating\nbetween current observations and retrieved examples through a denoising\nprocess, our approach enables fine-grained control and safe behavior across\ndiverse scenarios, leveraging the strong prior provided by the retrieved\nscenario. Another key insight we produce is that a task-relevant retrieval\nmodel trained with planning-based objectives results in superior planning\nperformance in our framework compared to a task-agnostic retriever.\nExperimental results demonstrate improved generalization to long-tail events\nand enhanced trajectory diversity compared to standard learning-based planners\n-- we observe a 40% reduction in collision rate on the Waymo Open Motion\ndataset with RAG.", "authors": ["Wenhao Ding", "Sushant Veer", "Yuxiao Chen", "Yulong Cao", "Chaowei Xiao", "Marco Pavone"], "published_date": "2025-05-30", "title_zh": "RealDrive：基於擴散模型的檢索增強駕駛", "summary_zh": "基於學習的規劃器透過學習推理數據中細微的互動，產生自然的類人駕駛行為，克服了基於規則的規劃器產生的僵硬行為。然而，數據驅動方法在罕見且安全攸關的情境中經常遇到困難，並且對產生的軌跡提供有限的可控性。為了解決這些挑戰，我們提出RealDrive，一種檢索增強生成(RAG)框架，透過檢索訓練數據集中最相關的專家示範來初始化基於擴散的規劃策略。透過在當前觀測和檢索到的範例之間進行去噪處理的插值，我們的方案能夠在各種情境中實現細粒度控制和安全行為，利用檢索情境提供的強先驗知識。另一個關鍵見解是，相較於任務無關的檢索器，使用基於規劃目標訓練的任務相關檢索模型，在我們的框架中產生卓越的規劃效能。實驗結果表明，相較於標準的基於學習的規劃器，該方法改進了對長尾事件的泛化能力並增強了軌跡多樣性——我們觀察到在使用RAG的Waymo Open Motion數據集上的碰撞率降低了40%。", "audio": "audios/2505.24808v1.mp3", "timestamp": "2025-06-02T06:29:07.641726"}
{"query": "AI", "id": "2505.24823v1", "url": "http://arxiv.org/abs/2505.24823v1", "title": "PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models", "summary": "Large language models (LLMs) have rapidly advanced and are increasingly\ncapable of tackling complex scientific problems, including those in physics.\nDespite this progress, current LLMs often fail to emulate the concise,\nprinciple-based reasoning characteristic of human experts, instead generating\nlengthy and opaque solutions. This discrepancy highlights a crucial gap in\ntheir ability to apply core physical principles for efficient and interpretable\nproblem solving. To systematically investigate this limitation, we introduce\nPhySense, a novel principle-based physics reasoning benchmark designed to be\neasily solvable by experts using guiding principles, yet deceptively difficult\nfor LLMs without principle-first reasoning. Our evaluation across multiple\nstate-of-the-art LLMs and prompt types reveals a consistent failure to align\nwith expert-like reasoning paths, providing insights for developing AI systems\nwith efficient, robust and interpretable principle-based scientific reasoning.", "authors": ["Yinggan Xu", "Yue Liu", "Zhiqiang Gao", "Changnan Peng", "Di Luo"], "published_date": "2025-05-30", "title_zh": "PhySense：基於原理的大型語言模型物理推理基準測試", "summary_zh": "大型語言模型迅速發展，在物理學等複雜科學問題上的能力日益增強。然而，現有模型難以模仿人類專家簡潔、基於原理的推理方式，反而產生冗長且不透明的解答。這突顯了其在應用核心物理原理以進行高效且可解釋問題解決方面的關鍵缺陷。為系統性地研究此限制，我們引入PhySense，一個新穎的基於原理的物理推理基準，專家可輕易利用指導原則解決，但對於缺乏首要原理推理的模型而言，則具有欺騙性難度。我們對多個最先進模型和提示類型的評估表明，它們普遍未能與專家般的推理路徑對齊，為開發具有高效、穩健且可解釋的基於原理的科學推理之AI系統提供了見解。", "audio": "audios/2505.24823v1.mp3", "timestamp": "2025-06-02T07:19:45.968965"}
{"query": "Foundation Model", "id": "2505.24717v1", "url": "http://arxiv.org/abs/2505.24717v1", "title": "PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations", "summary": "We introduce PDE-Transformer, an improved transformer-based architecture for\nsurrogate modeling of physics simulations on regular grids. We combine recent\narchitectural improvements of diffusion transformers with adjustments specific\nfor large-scale simulations to yield a more scalable and versatile\ngeneral-purpose transformer architecture, which can be used as the backbone for\nbuilding large-scale foundation models in physical sciences. We demonstrate\nthat our proposed architecture outperforms state-of-the-art transformer\narchitectures for computer vision on a large dataset of 16 different types of\nPDEs. We propose to embed different physical channels individually as\nspatio-temporal tokens, which interact via channel-wise self-attention. This\nhelps to maintain a consistent information density of tokens when learning\nmultiple types of PDEs simultaneously. We demonstrate that our pre-trained\nmodels achieve improved performance on several challenging downstream tasks\ncompared to training from scratch and also beat other foundation model\narchitectures for physics simulations.", "authors": ["Benjamin Holzschuh", "Qiang Liu", "Georg Kohl", "Nils Thuerey"], "published_date": "2025-05-30", "title_zh": "偏微分方程變換器：用於物理模擬的高效且通用的變換器", "summary_zh": "PDE-Transformer是一種改良的基於Transformer的架構，用於在規則網格上進行物理模擬的替代建模。它結合了擴散Transformer的最新架構改進，並針對大規模模擬進行調整，產生更具可擴展性和通用性的Transformer架構，可用作構建物理科學領域大規模基礎模型的骨幹。在包含16種不同類型偏微分方程式的大型數據集上，該架構的性能優於最先進的電腦視覺Transformer架構。本研究提出將不同的物理通道單獨嵌入為時空令牌，這些令牌通過通道自注意力機制進行交互，有助於在同時學習多種類型偏微分方程式時保持令牌的一致信息密度。相較於從頭開始訓練，預訓練模型在多項下游任務上表現更佳，並超越了其他用於物理模擬的基礎模型架構。", "audio": "audios/2505.24717v1.mp3", "timestamp": "2025-06-02T07:19:51.170026"}
{"query": "Diffusion Model", "id": "2505.24769v1", "url": "http://arxiv.org/abs/2505.24769v1", "title": "Generalization Dynamics of Linear Diffusion Models", "summary": "Diffusion models trained on finite datasets with $N$ samples from a target\ndistribution exhibit a transition from memorisation, where the model reproduces\ntraining examples, to generalisation, where it produces novel samples that\nreflect the underlying data distribution. Understanding this transition is key\nto characterising the sample efficiency and reliability of generative models,\nbut our theoretical understanding of this transition is incomplete. Here, we\nanalytically study the memorisation-to-generalisation transition in a simple\nmodel using linear denoisers, which allow explicit computation of test errors,\nsampling distributions, and Kullback-Leibler divergences between samples and\ntarget distribution. Using these measures, we predict that this transition\noccurs roughly when $N \\asymp d$, the dimension of the inputs. When $N$ is\nsmaller than the dimension of the inputs $d$, so that only a fraction of\nrelevant directions of variation are present in the training data, we\ndemonstrate how both regularization and early stopping help to prevent\noverfitting. For $N > d$, we find that the sampling distributions of linear\ndiffusion models approach their optimum (measured by the Kullback-Leibler\ndivergence) linearly with $d/N$, independent of the specifics of the data\ndistribution. Our work clarifies how sample complexity governs generalisation\nin a simple model of diffusion-based generative models and provides insight\ninto the training dynamics of linear denoisers.", "authors": ["Claudia Merger", "Sebastian Goldt"], "published_date": "2025-05-30", "title_zh": "線性擴散模型的泛化動力學", "summary_zh": "擴散模型於有限樣本集訓練時，會經歷從記憶訓練樣本到生成反映潛在分布的新樣本之轉變。理解此轉變對於評估生成模型的樣本效率及可靠性至關重要，但理論尚不完善。本研究以線性去噪器簡化模型，解析探討此轉變，可明確計算測試誤差、抽樣分布及樣本與目標分布間的KL散度。預測此轉變發生於樣本數約等於輸入維度($N \\asymp d$)時。當樣本數小於輸入維度($N < d$)，僅有部分相關變異方向存在於訓練資料時，正規化與提前停止有助於防止過擬合。當$N > d$時，線性擴散模型的抽樣分布以$d/N$線性逼近最佳狀態(以KL散度衡量)，且與資料分布無關。本研究闡明了樣本複雜度如何影響基於擴散生成模型的泛化能力，並深入了解線性去噪器的訓練動態。", "audio": "audios/2505.24769v1.mp3", "timestamp": "2025-06-02T07:19:57.738271"}
{"query": "AI", "id": "2505.24785v1", "url": "http://arxiv.org/abs/2505.24785v1", "title": "EXP-Bench: Can AI Conduct AI Research Experiments?", "summary": "Automating AI research holds immense potential for accelerating scientific\nprogress, yet current AI agents struggle with the complexities of rigorous,\nend-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed\nto systematically evaluate AI agents on complete research experiments sourced\nfrom influential AI publications. Given a research question and incomplete\nstarter code, EXP-Bench challenges AI agents to formulate hypotheses, design\nand implement experimental procedures, execute them, and analyze results. To\nenable the creation of such intricate and authentic tasks with high-fidelity,\nwe design a semi-autonomous pipeline to extract and structure crucial\nexperimental details from these research papers and their associated\nopen-source code. With the pipeline, EXP-Bench curated 461 AI research tasks\nfrom 51 top-tier AI research papers. Evaluations of leading LLM-based agents,\nsuch as OpenHands and IterativeAgent on EXP-Bench demonstrate partial\ncapabilities: while scores on individual experimental aspects such as design or\nimplementation correctness occasionally reach 20-35%, the success rate for\ncomplete, executable experiments was a mere 0.5%. By identifying these\nbottlenecks and providing realistic step-by-step experiment procedures,\nEXP-Bench serves as a vital tool for future AI agents to improve their ability\nto conduct AI research experiments. EXP-Bench is open-sourced at\nhttps://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.", "authors": ["Patrick Tser Jern Kon", "Jiachen Liu", "Xinyi Zhu", "Qiuyi Ding", "Jingjia Peng", "Jiarong Xing", "Yibo Huang", "Yiming Qiu", "Jayanth Srinivasa", "Myungjin Lee", "Mosharaf Chowdhury", "Matei Zaharia", "Ang Chen"], "published_date": "2025-05-30", "title_zh": "EXP-Bench：人工智慧能否執行人工智慧研究實驗？", "summary_zh": "自動化人工智慧研究具加速科學進展的潛力，然而現有AI模型難以應對端對端實驗的複雜性。本研究提出EXP-Bench，一項旨在系統性評估AI在完整研究實驗中表現的新基準，實驗取材自具影響力的人工智慧論文。EXP-Bench要求AI模型針對研究問題及不完整的起始程式碼，構建假設、設計並執行實驗流程、以及分析結果。為高精準度地建立此類複雜任務，我們設計半自動化流程，從論文及開源程式碼中提取關鍵實驗細節。EXP-Bench利用此流程從頂級AI論文中蒐集了461項AI研究任務。對OpenHands及IterativeAgent等大型語言模型之評估顯示，其在實驗設計或實作的正確性方面偶爾可達20-35%，但完整可執行實驗的成功率僅為0.5%。EXP-Bench藉由指出瓶頸並提供逐步實驗程序，有助於AI模型提升其執行AI研究實驗的能力。EXP-Bench已開源。", "audio": "audios/2505.24785v1.mp3", "timestamp": "2025-06-02T08:26:55.190213"}
{"query": "Foundation Model", "id": "2505.24693v1", "url": "http://arxiv.org/abs/2505.24693v1", "title": "Conformal Prediction for Zero-Shot Models", "summary": "Vision-language models pre-trained at large scale have shown unprecedented\nadaptability and generalization to downstream tasks. Although its\ndiscriminative potential has been widely explored, its reliability and\nuncertainty are still overlooked. In this work, we investigate the capabilities\nof CLIP models under the split conformal prediction paradigm, which provides\ntheoretical guarantees to black-box models based on a small, labeled\ncalibration set. In contrast to the main body of literature on conformal\npredictors in vision classifiers, foundation models exhibit a particular\ncharacteristic: they are pre-trained on a one-time basis on an inaccessible\nsource domain, different from the transferred task. This domain drift\nnegatively affects the efficiency of the conformal sets and poses additional\nchallenges. To alleviate this issue, we propose Conf-OT, a transfer learning\nsetting that operates transductive over the combined calibration and query\nsets. Solving an optimal transport problem, the proposed method bridges the\ndomain gap between pre-training and adaptation without requiring additional\ndata splits but still maintaining coverage guarantees. We comprehensively\nexplore this conformal prediction strategy on a broad span of 15 datasets and\nthree non-conformity scores. Conf-OT provides consistent relative improvements\nof up to 20% on set efficiency while being 15 times faster than popular\ntransductive approaches.", "authors": ["Julio Silva-Rodríguez", "Ismail Ben Ayed", "Jose Dolz"], "published_date": "2025-05-30", "title_zh": "零樣本模型的共形預測", "summary_zh": "大規模預訓練的視覺語言模型展現了前所未有的適應性和下游任務泛化能力，但其可靠性和不確定性卻被忽視。本文探討CLIP模型在分離一致性預測範式下的能力，該範式基於小型標記校準集為黑盒模型提供理論保證。與視覺分類器中的一致性預測器文獻不同，基礎模型具有獨特性：它們在不可訪問的源域上一次性預訓練，這與轉移任務不同。這種域漂移會對一致性集合的效率產生負面影響，並帶來額外挑戰。為了解決這個問題，我們提出了Conf-OT，一種在組合校準和查詢集上進行直推式遷移學習的設置。透過求解最佳傳輸問題，該方法彌合了預訓練和適應之間的域間隙，無需額外數據分割，同時保持覆蓋保證。我們在15個數據集和三種非一致性分數上全面探索了這種一致性預測策略。Conf-OT在集合效率方面提供了高達20%的一致性相對改進，同時比流行的直推式方法快15倍。", "audio": "audios/2505.24693v1.mp3", "timestamp": "2025-06-02T08:27:01.405789"}
{"query": "Diffusion Model", "id": "2505.24576v1", "url": "http://arxiv.org/abs/2505.24576v1", "title": "A Composite Predictive-Generative Approach to Monaural Universal Speech Enhancement", "summary": "It is promising to design a single model that can suppress various\ndistortions and improve speech quality, i.e., universal speech enhancement\n(USE). Compared to supervised learning-based predictive methods,\ndiffusion-based generative models have shown greater potential due to the\ngenerative capacities from degraded speech with severely damaged information.\nHowever, artifacts may be introduced in highly adverse conditions, and\ndiffusion models often suffer from a heavy computational burden due to many\nsteps for inference. In order to jointly leverage the superiority of prediction\nand generation and overcome the respective defects, in this work we propose a\nuniversal speech enhancement model called PGUSE by combining predictive and\ngenerative modeling. Our model consists of two branches: the predictive branch\ndirectly predicts clean samples from degraded signals, while the generative\nbranch optimizes the denoising objective of diffusion models. We utilize the\noutput fusion and truncated diffusion scheme to effectively integrate\npredictive and generative modeling, where the former directly combines results\nfrom both branches and the latter modifies the reverse diffusion process with\ninitial estimates from the predictive branch. Extensive experiments on several\ndatasets verify the superiority of the proposed model over state-of-the-art\nbaselines, demonstrating the complementarity and benefits of combining\npredictive and generative modeling.", "authors": ["Jie Zhang", "Haoyin Yan", "Xiaofei Li"], "published_date": "2025-05-30", "title_zh": "單聲道通用語音增強之複合預測生成方法", "summary_zh": "設計能抑制多種失真並提升語音品質的通用語音增強模型極具前景。相較於監督式學習的預測方法，基於擴散的生成模型在嚴重受損語音的生成能力上展現更大潛力。然而，在極端不利條件下可能產生偽像，且擴散模型推論步驟繁多導致計算負擔沉重。為結合預測與生成的優勢並克服各自缺陷，本研究提出一種名為PGUSE的通用語音增強模型，結合了預測式和生成式建模。該模型包含兩個分支：預測分支直接從劣化訊號預測乾淨樣本，而生成分支則優化擴散模型的去噪目標。我們利用輸出融合和截斷擴散策略來有效整合預測式和生成式建模，前者直接結合兩分支的結果，後者則利用預測分支的初步估計修改逆向擴散過程。大量實驗驗證了所提模型優於最先進基準模型，證明了結合預測式和生成式建模的互補性和益處。", "audio": "audios/2505.24576v1.mp3", "timestamp": "2025-06-02T08:27:07.331152"}
{"query": "AI", "id": "2505.24701v1", "url": "http://arxiv.org/abs/2505.24701v1", "title": "Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison", "summary": "Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions\nbut often suffers from the scarcity of diverse, labeled datasets that reflect\nreal-world conversational nuances. This paper presents an approach for\ngenerating synthetic ABSA data using Large Language Models (LLMs) to address\nthis gap. We detail the generation process aimed at producing data with\nconsistent topic and sentiment distributions across multiple domains using\nGPT-4o. The quality and utility of the generated data were evaluated by\nassessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro,\nClaude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification\ntasks. Our results demonstrate the effectiveness of the synthetic data,\nrevealing distinct performance trade-offs among the models: DeepSeekR1 showed\nhigher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall,\nand Gemini 1.5 Pro offered significantly faster inference. We conclude that\nLLM-based synthetic data generation is a viable and flexible method for\ncreating valuable ABSA resources, facilitating research and model evaluation\nwithout reliance on limited or inaccessible real-world labeled data.", "authors": ["Tejul Pandit", "Meet Raval", "Dhvani Upadhyay"], "published_date": "2025-05-30", "title_zh": "藉由大型語言模型生成多領域面向情感分析對話資料集，以供真實世界評估與模型比較", "summary_zh": "基於面向情感分析(ABSA)提供細緻的觀點洞察，但常受限於缺乏反映真實對話細微差異的多樣化標註資料集。本文提出一種利用大型語言模型(LLM)生成合成ABSA資料的方法以解決此問題。我們詳述了使用GPT-4o，旨在產生跨多個領域具有一致主題和情感分布的資料生成過程。透過評估三種最先進LLM（Gemini 1.5 Pro、Claude 3.5 Sonnet和DeepSeek-R1）在主題和情感分類任務上的性能，評估生成資料的品質和效用。結果表明合成資料的有效性，揭示了模型之間不同的性能權衡：DeepSeekR1表現出更高的精確度，Gemini 1.5 Pro和Claude 3.5 Sonnet表現出強大的召回率，而Gemini 1.5 Pro提供了顯著更快的推論速度。我們得出結論，基於LLM的合成資料生成是創建有價值的ABSA資源的可行且靈活的方法，有助於研究和模型評估，而無需依賴有限或難以存取的真實標註資料。", "audio": "audios/2505.24701v1.mp3", "timestamp": "2025-06-02T09:21:08.708350"}
{"query": "Foundation Model", "id": "2505.24531v1", "url": "http://arxiv.org/abs/2505.24531v1", "title": "Transformers Are Universally Consistent", "summary": "Despite their central role in the success of foundational models and\nlarge-scale language modeling, the theoretical foundations governing the\noperation of Transformers remain only partially understood. Contemporary\nresearch has largely focused on their representational capacity for language\ncomprehension and their prowess in in-context learning, frequently under\nidealized assumptions such as linearized attention mechanisms. Initially\nconceived to model sequence-to-sequence transformations, a fundamental and\nunresolved question is whether Transformers can robustly perform functional\nregression over sequences of input tokens. This question assumes heightened\nimportance given the inherently non-Euclidean geometry underlying real-world\ndata distributions. In this work, we establish that Transformers equipped with\nsoftmax-based nonlinear attention are uniformly consistent when tasked with\nexecuting Ordinary Least Squares (OLS) regression, provided both the inputs and\noutputs are embedded in hyperbolic space. We derive deterministic upper bounds\non the empirical error which, in the asymptotic regime, decay at a provable\nrate of $\\mathcal{O}(t^{-1/2d})$, where $t$ denotes the number of input tokens\nand $d$ the embedding dimensionality. Notably, our analysis subsumes the\nEuclidean setting as a special case, recovering analogous convergence\nguarantees parameterized by the intrinsic dimensionality of the data manifold.\nThese theoretical insights are corroborated through empirical evaluations on\nreal-world datasets involving both continuous and categorical response\nvariables.", "authors": ["Sagar Ghosh", "Kushal Bose", "Swagatam Das"], "published_date": "2025-05-30", "title_zh": "變換器具有普適一致性", "summary_zh": "儘管 Transformer 在基礎模型和大規模語言建模中扮演核心角色，但其運作的理論基礎仍未完全明瞭。當前研究多集中於其語言理解的表徵能力和上下文學習能力，並常基於線性化注意力機制等理想化假設。最初旨在對序列到序列轉換進行建模，一個基本且未解決的問題是，Transformer 是否能穩健地對輸入token序列執行函數迴歸。鑑於現實世界數據分布的非歐幾里何特性，此問題更顯重要。本研究證明，配備 softmax 非線性注意力的 Transformer 在執行普通最小二乘法 (OLS) 迴歸時具有一致性，前提是輸入和輸出皆嵌入於雙曲空間中。我們推導出經驗誤差的確定性上限，在漸近狀態下，其衰減速率為$\\mathcal{O}(t^{-1/2d})$，其中 t 表示輸入token的數量，d 表示嵌入維度。值得注意的是，我們的分析涵蓋歐幾里得設定作為特例，並恢復了由數據流形的內在維度參數化的類似收斂保證。這些理論見解已透過對涉及連續和分類響應變量的真實世界數據集的實證評估得到證實。", "audio": "audios/2505.24531v1.mp3", "timestamp": "2025-06-02T09:21:16.687198"}
{"query": "Diffusion Model", "id": "2505.24521v1", "url": "http://arxiv.org/abs/2505.24521v1", "title": "UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation", "summary": "Recently, methods leveraging diffusion model priors to assist monocular\ngeometric estimation (e.g., depth and normal) have gained significant attention\ndue to their strong generalization ability. However, most existing works focus\non estimating geometric properties within the camera coordinate system of\nindividual video frames, neglecting the inherent ability of diffusion models to\ndetermine inter-frame correspondence. In this work, we demonstrate that,\nthrough appropriate design and fine-tuning, the intrinsic consistency of video\ngeneration models can be effectively harnessed for consistent geometric\nestimation. Specifically, we 1) select geometric attributes in the global\ncoordinate system that share the same correspondence with video frames as the\nprediction targets, 2) introduce a novel and efficient conditioning method by\nreusing positional encodings, and 3) enhance performance through joint training\non multiple geometric attributes that share the same correspondence. Our\nresults achieve superior performance in predicting global geometric attributes\nin videos and can be directly applied to reconstruction tasks. Even when\ntrained solely on static video data, our approach exhibits the potential to\ngeneralize to dynamic video scenes.", "authors": ["Yang-Tian Sun", "Xin Yu", "Zehuan Huang", "Yi-Hua Huang", "Yuan-Chen Guo", "Ziyi Yang", "Yan-Pei Cao", "Xiaojuan Qi"], "published_date": "2025-05-30", "title_zh": "UniGeo：用於統一一致幾何估計的影片擴散馴服", "summary_zh": "近年來，利用擴散模型先驗輔助單目幾何估計（如深度和法線）的方法因其強大的泛化能力而備受關注。然而，現有研究多集中於估計單個視訊幀相機座標系內的幾何屬性，忽略了擴散模型確定幀間對應關係的固有能力。本研究證明，透過適當的設計和微調，視訊生成模型的內在一致性可被有效利用於一致的幾何估計。具體而言，我們1)選擇與視訊幀具有相同對應關係的全局座標系幾何屬性作為預測目標，2)引入一種新穎高效的條件設置方法，重用位置編碼，3)透過對共享相同對應關係的多個幾何屬性進行聯合訓練來提高性能。實驗結果表明，我們的方法在預測視訊中的全局幾何屬性方面表現出色，並可直接應用於重建任務。即使僅在靜態視訊數據上進行訓練，我們的方案也展現出推廣至動態視訊場景的潛力。", "audio": "audios/2505.24521v1.mp3", "timestamp": "2025-06-02T09:21:23.199349"}
{"query": "AI", "id": "2505.24697v1", "url": "http://arxiv.org/abs/2505.24697v1", "title": "Towards a unified user modeling language for engineering human centered AI systems", "summary": "In today's digital society, personalization has become a crucial aspect of\nsoftware applications, significantly impacting user experience and engagement.\nA new wave of intelligent user interfaces, such as AI-based conversational\nagents, has the potential to enable such personalization beyond what other\ntypes of interfaces could offer in the past. Personalization requires the\nability to specify a complete user profile, covering as many dimensions as\npossible, such as potential accessibility constraints, interaction preferences,\nand even hobbies. In this sense, this paper presents the concepts of a unified\nuser modeling language, aimed to combine previous approaches in a single\nproposal. Additionally, a proof of concept has been developed that leverages\nuser profiles modeled using our language to automatically adapt a\nconversational agent.", "authors": ["Aaron Conrardy", "Alfredo Capozucca", "Jordi Cabot"], "published_date": "2025-05-30", "title_zh": "邁向統一的使用者建模語言：為工程化以人為本的AI系統", "summary_zh": "在現今數位社會，個人化已成為軟體應用程式的關鍵，顯著影響使用者體驗和參與度。新一代智慧使用者介面，如基於人工智慧的對話代理，具備超越以往介面的個人化潛力。個人化需完整的使用者模型，涵蓋諸如可及性限制、互動偏好、興趣等多個面向。本文提出一種統一使用者建模語言的概念，旨在整合既有方法。此外，開發了一個概念驗證，利用此語言建模的使用者檔案自動調整對話代理。", "audio": "audios/2505.24697v1.mp3", "timestamp": "2025-06-02T10:21:02.095846"}
{"query": "Foundation Model", "id": "2505.24528v1", "url": "http://arxiv.org/abs/2505.24528v1", "title": "Geospatial Foundation Models to Enable Progress on Sustainable Development Goals", "summary": "Foundation Models (FMs) are large-scale, pre-trained AI systems that have\nrevolutionized natural language processing and computer vision, and are now\nadvancing geospatial analysis and Earth Observation (EO). They promise improved\ngeneralization across tasks, scalability, and efficient adaptation with minimal\nlabeled data. However, despite the rapid proliferation of geospatial FMs, their\nreal-world utility and alignment with global sustainability goals remain\nunderexplored. We introduce SustainFM, a comprehensive benchmarking framework\ngrounded in the 17 Sustainable Development Goals with extremely diverse tasks\nranging from asset wealth prediction to environmental hazard detection. This\nstudy provides a rigorous, interdisciplinary assessment of geospatial FMs and\noffers critical insights into their role in attaining sustainability goals. Our\nfindings show: (1) While not universally superior, FMs often outperform\ntraditional approaches across diverse tasks and datasets. (2) Evaluating FMs\nshould go beyond accuracy to include transferability, generalization, and\nenergy efficiency as key criteria for their responsible use. (3) FMs enable\nscalable, SDG-grounded solutions, offering broad utility for tackling complex\nsustainability challenges. Critically, we advocate for a paradigm shift from\nmodel-centric development to impact-driven deployment, and emphasize metrics\nsuch as energy efficiency, robustness to domain shifts, and ethical\nconsiderations.", "authors": ["Pedram Ghamisi", "Weikang Yu", "Xiaokang Zhang", "Aldino Rizaldy", "Jian Wang", "Chufeng Zhou", "Richard Gloaguen", "Gustau Camps-Valls"], "published_date": "2025-05-30", "title_zh": "地理空間基礎模型促進永續發展目標之進展", "summary_zh": "基礎模型是大型預訓練AI系統，已革新自然語言處理和電腦視覺，現正推進地理空間分析和地球觀測。其具備跨任務泛化、可擴展性及少量標籤資料下的高效適應能力。儘管地理空間基礎模型迅速普及，其實際應用及與全球永續發展目標的關聯仍待探索。本研究提出SustainFM，一個基於17項永續發展目標的綜合基準框架，涵蓋從資產財富預測到環境危害偵測等多樣化任務，對地理空間基礎模型進行嚴謹的跨學科評估，深入探討其在實現永續發展目標中的作用。研究發現：(1) 基礎模型並非普遍優越，但在多樣任務和資料集上通常優於傳統方法。(2) 評估基礎模型應超越準確性，將可轉移性、泛化能力和能源效率納入負責使用的關鍵標準。(3) 基礎模型促成可擴展、基於永續發展目標的解決方案，為應對複雜的永續性挑戰提供廣泛效用。重要的是，本研究倡導從模型中心開發轉向影響驅動部署，並強調能源效率、對領域轉移的穩健性及倫理考量等指標。", "audio": "audios/2505.24528v1.mp3", "timestamp": "2025-06-02T10:21:10.004553"}
{"query": "Diffusion Model", "id": "2505.24417v1", "url": "http://arxiv.org/abs/2505.24417v1", "title": "EasyText: Controllable Diffusion Transformer for Multilingual Text Rendering", "summary": "Generating accurate multilingual text with diffusion models has long been\ndesired but remains challenging. Recent methods have made progress in rendering\ntext in a single language, but rendering arbitrary languages is still an\nunexplored area. This paper introduces EasyText, a text rendering framework\nbased on DiT (Diffusion Transformer), which connects denoising latents with\nmultilingual character tokens encoded as character tokens. We propose character\npositioning encoding and position encoding interpolation techniques to achieve\ncontrollable and precise text rendering. Additionally, we construct a\nlarge-scale synthetic text image dataset with 1 million multilingual image-text\nannotations as well as a high-quality dataset of 20K annotated images, which\nare used for pretraining and fine-tuning respectively. Extensive experiments\nand evaluations demonstrate the effectiveness and advancement of our approach\nin multilingual text rendering, visual quality, and layout-aware text\nintegration.", "authors": ["Runnan Lu", "Yuxuan Zhang", "Jailing Liu", "Haifa Wang", "Yiren Song"], "published_date": "2025-05-30", "title_zh": "EasyText：用於多語文本渲染的可控擴散Transformer", "summary_zh": "利用擴散模型產生精確的多語言文字長期以來備受期待，但仍具挑戰。本研究提出EasyText，一個基於DiT的文字渲染框架，連接去噪潛變量與多語言字元標記。論文引入字元定位編碼和位置編碼插值技術，實現可控且精準的文字渲染。此外，構建了一個包含百萬級多語言圖像文本註釋的大型合成文本圖像數據集，以及一個包含兩萬張註釋圖像的高質量數據集，分別用於預訓練和微調。大量實驗和評估表明，本方法在多語言文字渲染、視覺質量以及感知佈局的文字整合方面具有效性和先進性。", "audio": "audios/2505.24417v1.mp3", "timestamp": "2025-06-02T10:21:14.772182"}
{"query": "AI", "id": "2505.24687v1", "url": "http://arxiv.org/abs/2505.24687v1", "title": "TumorGen: Boundary-Aware Tumor-Mask Synthesis with Rectified Flow Matching", "summary": "Tumor data synthesis offers a promising solution to the shortage of annotated\nmedical datasets. However, current approaches either limit tumor diversity by\nusing predefined masks or employ computationally expensive two-stage processes\nwith multiple denoising steps, causing computational inefficiency.\nAdditionally, these methods typically rely on binary masks that fail to capture\nthe gradual transitions characteristic of tumor boundaries. We present\nTumorGen, a novel Boundary-Aware Tumor-Mask Synthesis with Rectified Flow\nMatching for efficient 3D tumor synthesis with three key components: a\nBoundary-Aware Pseudo Mask Generation module that replaces strict binary masks\nwith flexible bounding boxes; a Spatial-Constraint Vector Field Estimator that\nsimultaneously synthesizes tumor latents and masks using rectified flow\nmatching to ensure computational efficiency; and a VAE-guided mask refiner that\nenhances boundary realism. TumorGen significantly improves computational\nefficiency by requiring fewer sampling steps while maintaining pathological\naccuracy through coarse and fine-grained spatial constraints. Experimental\nresults demonstrate TumorGen's superior performance over existing tumor\nsynthesis methods in both efficiency and realism, offering a valuable\ncontribution to AI-driven cancer diagnostics.", "authors": ["Shengyuan Liu", "Wenting Chen", "Boyun Zheng", "Wentao Pan", "Xiang Li", "Yixuan Yuan"], "published_date": "2025-05-30", "title_zh": "TumorGen：基於修正流匹配的邊界感知腫瘤掩膜合成", "summary_zh": "腫瘤數據合成有望解決帶標註醫學數據集短缺問題。現有方法或因使用預定義遮罩而限制腫瘤多樣性，或採用計算密集型兩階段流程，效率低下。此外，它們通常依賴二元遮罩，無法捕捉腫瘤邊界的漸變特性。我們提出TumorGen，一種基於修正流匹配的邊界感知腫瘤遮罩合成方法，用於高效3D腫瘤合成。它包含三個關鍵組件：以彈性邊界框取代嚴格二元遮罩的邊界感知偽遮罩生成模組；利用修正流匹配同時合成腫瘤潛在變數和遮罩的空間約束向量場估計器，以確保計算效率；以及增強邊界真實感的VAE引導遮罩精煉器。TumorGen透過粗細粒度的空間約束，減少採樣步驟，顯著提高計算效率，同時保持病理準確性。實驗結果表明，TumorGen在效率和真實感方面優於現有腫瘤合成方法，為AI驅動的癌症診斷做出重要貢獻。", "audio": "audios/2505.24687v1.mp3", "timestamp": "2025-06-02T11:17:07.078789"}
{"query": "Foundation Model", "id": "2505.24517v1", "url": "http://arxiv.org/abs/2505.24517v1", "title": "un$^2$CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP", "summary": "Contrastive Language-Image Pre-training (CLIP) has become a foundation model\nand has been applied to various vision and multimodal tasks. However, recent\nworks indicate that CLIP falls short in distinguishing detailed differences in\nimages and shows suboptimal performance on dense-prediction and vision-centric\nmultimodal tasks. Therefore, this work focuses on improving existing CLIP\nmodels, aiming to capture as many visual details in images as possible. We find\nthat a specific type of generative models, unCLIP, provides a suitable\nframework for achieving our goal. Specifically, unCLIP trains an image\ngenerator conditioned on the CLIP image embedding. In other words, it inverts\nthe CLIP image encoder. Compared to discriminative models like CLIP, generative\nmodels are better at capturing image details because they are trained to learn\nthe data distribution of images. Additionally, the conditional input space of\nunCLIP aligns with CLIP's original image-text embedding space. Therefore, we\npropose to invert unCLIP (dubbed un$^2$CLIP) to improve the CLIP model. In this\nway, the improved image encoder can gain unCLIP's visual detail capturing\nability while preserving its alignment with the original text encoder\nsimultaneously. We evaluate our improved CLIP across various tasks to which\nCLIP has been applied, including the challenging MMVP-VLM benchmark, the\ndense-prediction open-vocabulary segmentation task, and multimodal large\nlanguage model tasks. Experiments show that un$^2$CLIP significantly improves\nthe original CLIP and previous CLIP improvement methods. Code and models will\nbe available at https://github.com/LiYinqi/un2CLIP.", "authors": ["Yinqi Li", "Jiahe Zhao", "Hong Chang", "Ruibing Hou", "Shiguang Shan", "Xilin Chen"], "published_date": "2025-05-30", "title_zh": "un$^2$CLIP：藉由反轉unCLIP提升CLIP的視覺細節捕捉能力", "summary_zh": "對比語言-圖像預訓練（CLIP）已成為基礎模型，廣泛應用於視覺和多模態任務。然而，CLIP在辨識圖像細節方面存在不足，且在密集預測和以視覺為中心的多模態任務中表現欠佳。本研究旨在改進CLIP模型，力求捕捉更多圖像細節。研究發現生成模型unCLIP為實現此目標提供了一個合適的框架。unCLIP訓練一個以CLIP圖像嵌入為條件的圖像生成器，實際上是反轉CLIP圖像編碼器。相較於判別模型，生成模型更擅長捕捉圖像細節。因此，本研究提出反轉unCLIP（稱為un$^2$CLIP）以提升CLIP模型。改良後的圖像編碼器既能獲得unCLIP的視覺細節捕捉能力，又能同時保持與原始文本編碼器的對齊。實驗結果顯示，un$^2$CLIP在多項CLIP應用任務中，顯著優於原始CLIP和先前的CLIP改進方法，包括具挑戰性的MMVP-VLM基準測試、密集預測開放詞彙分割任務和多模態大型語言模型任務。", "audio": "audios/2505.24517v1.mp3", "timestamp": "2025-06-02T11:17:14.262275"}
{"query": "Diffusion Model", "id": "2505.24406v1", "url": "http://arxiv.org/abs/2505.24406v1", "title": "IRBridge: Solving Image Restoration Bridge with Pre-trained Generative Diffusion Models", "summary": "Bridge models in image restoration construct a diffusion process from\ndegraded to clear images. However, existing methods typically require training\na bridge model from scratch for each specific type of degradation, resulting in\nhigh computational costs and limited performance. This work aims to efficiently\nleverage pretrained generative priors within existing image restoration bridges\nto eliminate this requirement. The main challenge is that standard generative\nmodels are typically designed for a diffusion process that starts from pure\nnoise, while restoration tasks begin with a low-quality image, resulting in a\nmismatch in the state distributions between the two processes. To address this\nchallenge, we propose a transition equation that bridges two diffusion\nprocesses with the same endpoint distribution. Based on this, we introduce the\nIRBridge framework, which enables the direct utilization of generative models\nwithin image restoration bridges, offering a more flexible and adaptable\napproach to image restoration. Extensive experiments on six image restoration\ntasks demonstrate that IRBridge efficiently integrates generative priors,\nresulting in improved robustness and generalization performance. Code will be\navailable at GitHub.", "authors": ["Hanting Wang", "Tao Jin", "Wang Lin", "Shulei Wang", "Hai Huang", "Shengpeng Ji", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "IRBridge：利用預訓練生成擴散模型解決影像復原橋接問題", "summary_zh": "圖像復原中的橋接模型構建從劣化圖像到清晰圖像的擴散過程。現有方法通常需針對每種劣化類型從頭訓練橋接模型，導致高計算成本和有限性能。本研究旨在高效利用現有圖像復原橋接中的預訓練生成先驗，以消除此需求。主要挑戰在於標準生成模型通常設計用於從純雜訊開始的擴散過程，而復原任務始於低質量圖像，導致兩者狀態分佈不匹配。為了解決此問題，我們提出了一種橋接具有相同終點分佈的兩個擴散過程的轉移方程。基於此，我們引入IRBridge框架，使生成模型能夠直接應用於圖像復原橋接中，從而提供更靈活和適應性更強的圖像復原方法。在六個圖像復原任務上的大量實驗表明，IRBridge能有效整合生成先驗，從而提高穩健性和泛化性能。代碼將於GitHub上公開。", "audio": "audios/2505.24406v1.mp3", "timestamp": "2025-06-02T11:17:20.249939"}
{"query": "AI", "id": "2505.24685v1", "url": "http://arxiv.org/abs/2505.24685v1", "title": "So, I climbed to the top of the pyramid of pain -- now what?", "summary": "This paper explores the evolving dynamics of cybersecurity in the age of\nadvanced AI, from the perspective of the introduced Human Layer Kill Chain\nframework. As traditional attack models like Lockheed Martin's Cyber Kill Chain\nbecome inadequate in addressing human vulnerabilities exploited by modern\nadversaries, the Humal Layer Kill Chain offers a nuanced approach that\nintegrates human psychology and behaviour into the analysis of cyber threats.\nWe detail the eight stages of the Human Layer Kill Chain, illustrating how\nAI-enabled techniques can enhance psychological manipulation in attacks. By\nmerging the Human Layer with the Cyber Kill Chain, we propose a Sociotechnical\nKill Plane that allows for a holistic examination of attackers' tactics,\ntechniques, and procedures (TTPs) across the sociotechnical landscape. This\nframework not only aids cybersecurity professionals in understanding\nadversarial methods, but also empowers non-technical personnel to engage in\nthreat identification and response. The implications for incident response and\norganizational resilience are significant, particularly as AI continues to\nshape the threat landscape.", "authors": ["Vasilis Katos", "Emily Rosenorn-Lanng", "Jane Henriksen-Bulmer", "Ala Yankouskaya"], "published_date": "2025-05-30", "title_zh": "因此，我攀登至痛苦金字塔的頂端——然後呢？", "summary_zh": "本文探討先進AI時代下，網路安全的不斷演變，以人為層面擊殺鏈框架為視角。鑑於傳統攻擊模型（如洛克希德·馬丁網路擊殺鏈）已不足以應對現代攻擊者利用的人性弱點，人為層面擊殺鏈提供了一種細緻入微的方法，將人類心理和行為融入網路威脅分析。本文詳細闡述人為層面擊殺鏈的八個階段，說明AI技術如何加強攻擊中的心理操縱。通過將人為層面與網路擊殺鏈融合，我們提出了一種社會技術擊殺平面，可全面檢視攻擊者在社會技術層面的戰術、技術和程序。此框架不僅有助於網路安全專業人員理解對手的手段，還能使非技術人員參與威脅識別和響應。隨著AI持續塑造威脅態勢，這對事件響應和組織韌性的影響至關重要。", "audio": "audios/2505.24685v1.mp3", "timestamp": "2025-06-02T12:38:56.628261"}
{"query": "Foundation Model", "id": "2505.24492v1", "url": "http://arxiv.org/abs/2505.24492v1", "title": "Object Centric Concept Bottlenecks", "summary": "Developing high-performing, yet interpretable models remains a critical\nchallenge in modern AI. Concept-based models (CBMs) attempt to address this by\nextracting human-understandable concepts from a global encoding (e.g., image\nencoding) and then applying a linear classifier on the resulting concept\nactivations, enabling transparent decision-making. However, their reliance on\nholistic image encodings limits their expressiveness in object-centric\nreal-world settings and thus hinders their ability to solve complex vision\ntasks beyond single-label classification. To tackle these challenges, we\nintroduce Object-Centric Concept Bottlenecks (OCB), a framework that combines\nthe strengths of CBMs and pre-trained object-centric foundation models,\nboosting performance and interpretability. We evaluate OCB on complex image\ndatasets and conduct a comprehensive ablation study to analyze key components\nof the framework, such as strategies for aggregating object-concept encodings.\nThe results show that OCB outperforms traditional CBMs and allows one to make\ninterpretable decisions for complex visual tasks.", "authors": ["David Steinmann", "Wolfgang Stammer", "Antonia Wüst", "Kristian Kersting"], "published_date": "2025-05-30", "title_zh": "以物件為中心的概念瓶頸", "summary_zh": "開發高效且具可解釋性的模型是當前人工智慧的重要挑戰。概念模型透過從整體編碼中提取人類可理解的概念，並對概念激活應用線性分類器，以實現透明決策。然而，其對整體圖像編碼的依賴限制了在以物件為中心的真實環境中的表達能力，阻礙了解決複雜視覺任務的能力。為了解決這些挑戰，我們引入了物件中心概念瓶頸，這是一個結合概念模型和預訓練物件中心基礎模型的框架，可提升性能和可解釋性。我們在複雜圖像數據集上評估了物件中心概念瓶頸，並進行了全面的消融研究，以分析框架的關鍵組成部分，例如聚合物件概念編碼的策略。結果表明，物件中心概念瓶頸優於傳統概念模型，並允許針對複雜視覺任務做出可解釋的決策。", "audio": "audios/2505.24492v1.mp3", "timestamp": "2025-06-02T12:39:05.767184"}
{"query": "Diffusion Model", "id": "2505.24360v1", "url": "http://arxiv.org/abs/2505.24360v1", "title": "Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning", "summary": "Sparse autoencoders are a promising new approach for decomposing language\nmodel activations for interpretation and control. They have been applied\nsuccessfully to vision transformer image encoders and to small-scale diffusion\nmodels. Inference-Time Decomposition of Activations (ITDA) is a recently\nproposed variant of dictionary learning that takes the dictionary to be a set\nof data points from the activation distribution and reconstructs them with\ngradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large\ntext-to-image diffusion model, Flux 1, and consider the interpretability of\nembeddings of both by introducing a visual automated interpretation pipeline.\nWe find that SAEs accurately reconstruct residual stream embeddings and beat\nMLP neurons on interpretability. We are able to use SAE features to steer image\ngeneration through activation addition. We find that ITDA has comparable\ninterpretability to SAEs.", "authors": ["Stepan Shabalin", "Ayush Panda", "Dmitrii Kharlapenko", "Abdur Raheem Ali", "Yixiong Hao", "Arthur Conmy"], "published_date": "2025-05-30", "title_zh": "利用字典學習詮釋大型文本到圖像擴散模型", "summary_zh": "稀疏自動編碼器是分解語言模型激活值以進行解釋和控制的新興方法。已成功應用於視覺轉換器圖像編碼器和小規模擴散模型。推論時激活分解（ITDA）是一種新近提出的字典學習變體，將字典視為來自激活分佈的數據點集合，並利用梯度追蹤進行重建。我們將稀疏自動編碼器（SAEs）和ITDA應用於大型文本到圖像擴散模型Flux 1，並引入視覺自動解釋流程來評估兩種嵌入的解釋性。研究發現，SAEs能準確重建殘差流嵌入，且在解釋性方面優於多層感知器神經元。我們能夠通過激活值疊加，利用SAE特徵來引導圖像生成。ITDA具有與SAEs相當的解釋性。", "audio": "audios/2505.24360v1.mp3", "timestamp": "2025-06-02T12:39:11.855141"}
{"query": "AI", "id": "2505.24683v1", "url": "http://arxiv.org/abs/2505.24683v1", "title": "Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation", "summary": "As people increasingly use AI systems in work and daily life, feedback\nmechanisms that help them use AI responsibly are urgently needed, particularly\nin settings where users are not equipped to assess the quality of AI\npredictions. We study a realistic Machine Translation (MT) scenario where\nmonolingual users decide whether to share an MT output, first without and then\nwith quality feedback. We compare four types of quality feedback: explicit\nfeedback that directly give users an assessment of translation quality using 1)\nerror highlights and 2) LLM explanations, and implicit feedback that helps\nusers compare MT inputs and outputs through 3) backtranslation and 4)\nquestion-answer (QA) tables. We find that all feedback types, except error\nhighlights, significantly improve both decision accuracy and appropriate\nreliance. Notably, implicit feedback, especially QA tables, yields\nsignificantly greater gains than explicit feedback in terms of decision\naccuracy, appropriate reliance, and user perceptions, receiving the highest\nratings for helpfulness and trust, and the lowest for mental burden.", "authors": ["Dayeon Ki", "Kevin Duh", "Marine Carpuat"], "published_date": "2025-05-30", "title_zh": "我應該分享這項翻譯嗎？評估品質回饋對使用者信任機器翻譯的影響", "summary_zh": "隨著人工智慧系統在工作與日常生活中普及，急需協助使用者負責任地運用AI的回饋機制，尤其是在使用者缺乏評估AI預測品質能力的情境下。本研究針對真實的機器翻譯情境進行探討，讓不諳雙語的使用者決定是否分享機器翻譯結果，先在無回饋的情況下，然後再提供品質回饋。我們比較了四種品質回饋類型：直接評估翻譯品質的顯性回饋，包括錯誤標示與大型語言模型解釋；以及透過回譯與問答表協助使用者比較機器翻譯輸入與輸出的隱性回饋。研究發現，除了錯誤標示外，所有回饋類型均顯著提升了決策準確性與適當依賴性。值得注意的是，隱性回饋，特別是問答表，在決策準確性、適當依賴性與使用者感知方面，相較於顯性回饋產生了更顯著的效益，在幫助性與信任度方面獲得最高評價，而在心理負擔方面則為最低。", "audio": "audios/2505.24683v1.mp3", "timestamp": "2025-06-02T13:31:58.226150"}
{"query": "Foundation Model", "id": "2505.24355v1", "url": "http://arxiv.org/abs/2505.24355v1", "title": "Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model", "summary": "Sign Language Translation (SLT) aims to convert sign language (SL) videos\ninto spoken language text, thereby bridging the communication gap between the\nsign and the spoken community. While most existing works focus on translating a\nsingle sign language into a single spoken language (one-to-one SLT), leveraging\nmultilingual resources could mitigate low-resource issues and enhance\naccessibility. However, multilingual SLT (MLSLT) remains unexplored due to\nlanguage conflicts and alignment difficulties across SLs and spoken languages.\nTo address these challenges, we propose a multilingual gloss-free model with\ndual CTC objectives for token-level SL identification and spoken text\ngeneration. Our model supports 10 SLs and handles one-to-one, many-to-one, and\nmany-to-many SLT tasks, achieving competitive performance compared to\nstate-of-the-art methods on three widely adopted benchmarks: multilingual\nSP-10, PHOENIX14T, and CSL-Daily.", "authors": ["Sihan Tan", "Taro Miyazaki", "Kazuhiro Nakadai"], "published_date": "2025-05-30", "title_zh": "多語無詞彙手語翻譯：邁向構建手語基礎模型", "summary_zh": "手語翻譯旨在將手語影片轉換為口語文本，彌合聾啞社群間的溝通鴻溝。現有研究多集中於單一手語到單一口語的翻譯，利用多語資源可緩解低資源問題並提升易用性。然而，多語手語翻譯因語言衝突和對齊困難而鮮少被探索。為此，本文提出一種多語無義素模型，採用雙重CTC目標，實現詞符層級的手語辨識和口語文本生成。該模型支援10種手語，處理一對一、多對一和多對多手語翻譯任務，在多語SP-10、PHOENIX14T和CSL-Daily三個基準測試集上，效能與最先進方法相比具有競爭力。", "audio": "audios/2505.24355v1.mp3", "timestamp": "2025-06-02T13:32:04.586458"}
{"query": "Diffusion Model", "id": "2505.24315v1", "url": "http://arxiv.org/abs/2505.24315v1", "title": "InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing", "summary": "Recent advances in 3D human-aware generation have made significant progress.\nHowever, existing methods still struggle with generating novel Human Object\nInteraction (HOI) from text, particularly for open-set objects. We identify\nthree main challenges of this task: precise human-object relation reasoning,\naffordance parsing for any object, and detailed human interaction pose\nsynthesis aligning description and object geometry. In this work, we propose a\nnovel zero-shot 3D HOI generation framework without training on specific\ndatasets, leveraging the knowledge from large-scale pre-trained models.\nSpecifically, the human-object relations are inferred from large language\nmodels (LLMs) to initialize object properties and guide the optimization\nprocess. Then we utilize a pre-trained 2D image diffusion model to parse unseen\nobjects and extract contact points, avoiding the limitations imposed by\nexisting 3D asset knowledge. The initial human pose is generated by sampling\nmultiple hypotheses through multi-view SDS based on the input text and object\ngeometry. Finally, we introduce a detailed optimization to generate\nfine-grained, precise, and natural interaction, enforcing realistic 3D contact\nbetween the 3D object and the involved body parts, including hands in grasping.\nThis is achieved by distilling human-level feedback from LLMs to capture\ndetailed human-object relations from the text instruction. Extensive\nexperiments validate the effectiveness of our approach compared to prior works,\nparticularly in terms of the fine-grained nature of interactions and the\nability to handle open-set 3D objects.", "authors": ["Jinlu Zhang", "Yixin Chen", "Zan Wang", "Jie Yang", "Yizhou Wang", "Siyuan Huang"], "published_date": "2025-05-30", "title_zh": "InteractAnything：基於大型語言模型回饋與物件可供性解析的零樣本人物物件互動合成", "summary_zh": "三維人機互動生成技術雖有進展，但現有方法在文本生成新穎人機互動方面仍有困難，尤其針對開放集物體。此任務面臨三大挑戰：精確的人物關係推理、適用於任何物體的機能解析，以及對齊描述和物體幾何的精細人體互動姿態合成。本研究提出一種新穎的零樣本三維人機互動生成框架，不需特定數據集訓練，利用大規模預訓練模型的知識。透過大型語言模型推斷人物關係，初始化物體屬性並引導最佳化過程。運用預訓練二維圖像擴散模型解析未見物體並提取接觸點，避免現有三維資源知識的限制。初步人體姿態透過多視角SDS採樣多個假設生成，基於輸入文本和物體幾何。最後，引入精細最佳化，生成細緻、精確、自然的互動，強化三維物體與涉及的身體部位（包括抓握中的手）之間的真實三維接觸。透過從大型語言模型中提取人類等級的回饋，以捕捉文本指令中詳細的人機關係。大量實驗驗證了本方法相較於先前研究的有效性，尤其在互動的精細程度和處理開放集三維物體的能力方面。", "audio": "audios/2505.24315v1.mp3", "timestamp": "2025-06-02T13:32:13.409502"}
{"query": "AI", "id": "2505.24681v1", "url": "http://arxiv.org/abs/2505.24681v1", "title": "Generative Knowledge Production Pipeline Driven by Academic Influencers", "summary": "Generative AI transforms knowledge production, validation, and dissemination,\nraising academic integrity and credibility concerns. This study examines 53\nacademic influencer videos that reached 5.3 million viewers to identify an\nemerging, structured, implementation-ready pipeline balancing originality,\nethical compliance, and human-AI collaboration despite the disruptive impacts.\nFindings highlight generative AI's potential to automate publication workflows\nand democratize participation in knowledge production while challenging\ntraditional scientific norms. Academic influencers emerge as key intermediaries\nin this paradigm shift, connecting bottom-up practices with institutional\npolicies to improve adaptability. Accordingly, the study proposes a generative\npublication production pipeline and a policy framework for co-intelligence\nadaptation and reinforcing credibility-centered standards in AI-powered\nresearch. These insights support scholars, educators, and policymakers in\nunderstanding AI's transformative impact by advocating responsible and\ninnovation-driven knowledge production. Additionally, they reveal pathways for\nautomating best practices, optimizing scholarly workflows, and fostering\ncreativity in academic research and publication.", "authors": ["Katalin Feher", "Marton Demeter"], "published_date": "2025-05-30", "title_zh": "由學術影響者驅動的生成式知識生產管線", "summary_zh": "生成式人工智慧正在轉變知識的生產、驗證和傳播，引發學術誠信與可信度疑慮。本研究檢視了觸及530萬觀眾的53個學術網紅影片，旨在識別一個新興、結構化的、可實施的流程，該流程在顛覆性影響下平衡了原創性、道德合規和人機協作。研究發現凸顯了生成式人工智慧在自動化出版流程和普及知識生產參與方面的潛力，同時挑戰了傳統科學規範。學術網紅成為此範式轉移的關鍵中介，連接自下而上的實踐與機構政策，以提高適應性。據此，本研究提出一個生成式出版生產流程和一個用於協作智能適應的政策框架，並在人工智慧驅動的研究中強化以可信度為中心的標準。這些見解有助於學者、教育工作者和政策制定者理解人工智慧的變革性影響，同時提倡負責任且創新驅動的知識生產。此外，研究也揭示了自動化最佳實踐、優化學術工作流程以及促進學術研究和出版創新的途徑。", "audio": "audios/2505.24681v1.mp3", "timestamp": "2025-06-02T14:19:34.583742"}
{"query": "Foundation Model", "id": "2505.24249v1", "url": "http://arxiv.org/abs/2505.24249v1", "title": "Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization", "summary": "Vision-based 6-DOF bronchoscopy localization offers a promising solution for\naccurate and cost-effective interventional guidance. However, existing methods\nstruggle with 1) limited generalization across patient cases due to scarce\nlabeled data, and 2) poor robustness under visual degradation, as bronchoscopy\nprocedures frequently involve artifacts such as occlusions and motion blur that\nimpair visual information. To address these challenges, we propose PANSv2, a\ngeneralizable and robust bronchoscopy localization framework. Motivated by PANS\nthat leverages multiple visual cues for pose likelihood measurement, PANSv2\nintegrates depth estimation, landmark detection, and centerline constraints\ninto a unified pose optimization framework that evaluates pose probability and\nsolves for the optimal bronchoscope pose. To further enhance generalization\ncapabilities, we leverage the endoscopic foundation model EndoOmni for depth\nestimation and the video foundation model EndoMamba for landmark detection,\nincorporating both spatial and temporal analyses. Pretrained on diverse\nendoscopic datasets, these models provide stable and transferable visual\nrepresentations, enabling reliable performance across varied bronchoscopy\nscenarios. Additionally, to improve robustness to visual degradation, we\nintroduce an automatic re-initialization module that detects tracking failures\nand re-establishes pose using landmark detections once clear views are\navailable. Experimental results on bronchoscopy dataset encompassing 10 patient\ncases show that PANSv2 achieves the highest tracking success rate, with an\n18.1% improvement in SR-5 (percentage of absolute trajectory error under 5 mm)\ncompared to existing methods, showing potential towards real clinical usage.", "authors": ["Qingyao Tian", "Huai Liao", "Xinyan Huang", "Bingyu Yang", "Hongbin Liu"], "published_date": "2025-05-30", "title_zh": "利用基礎模型實現穩健且具泛化性的六自由度支氣管鏡定位", "summary_zh": "基於視覺的六自由度支氣管鏡定位為精準且具成本效益的介入治療引導提供了解決方案。然而，現有方法受限於標註數據稀缺導致的病人案例泛化能力不足，以及支氣管鏡檢查中常見的遮擋和運動模糊等視覺信息退化問題。為了解決這些挑戰，我們提出了PANSv2，一種具備泛化性和魯棒性的支氣管鏡定位框架。PANSv2整合了深度估計、標記點檢測和中心線約束，形成統一的姿態優化框架，評估姿態概率並求解最佳支氣管鏡姿態。為增強泛化能力，我們利用內視鏡基礎模型EndoOmni進行深度估計，以及視訊基礎模型EndoMamba進行標記點檢測，結合了空間和時間分析。這些模型在多樣化的內視鏡數據集上預訓練，提供穩定且可轉移的視覺表徵，確保在不同支氣管鏡場景中的可靠性。此外，為提高對視覺退化的魯棒性，我們引入了自動重新初始化模組，用於檢測追蹤失敗並在清晰視野恢復時利用標記點檢測重新建立姿態。包含10個病人案例的支氣管鏡數據集上的實驗結果表明，PANSv2取得了最高的追蹤成功率，SR-5（絕對軌跡誤差小於5毫米的百分比）比現有方法提高了18.1%，顯示了在實際臨床應用中的潛力。", "audio": "audios/2505.24249v1.mp3", "timestamp": "2025-06-02T14:19:43.058066"}
{"query": "Diffusion Model", "id": "2505.24301v1", "url": "http://arxiv.org/abs/2505.24301v1", "title": "Category-aware EEG image generation based on wavelet transform and contrast semantic loss", "summary": "Reconstructing visual stimuli from EEG signals is a crucial step in realizing\nbrain-computer interfaces. In this paper, we propose a transformer-based EEG\nsignal encoder integrating the Discrete Wavelet Transform (DWT) and the gating\nmechanism. Guided by the feature alignment and category-aware fusion losses,\nthis encoder is used to extract features related to visual stimuli from EEG\nsignals. Subsequently, with the aid of a pre-trained diffusion model, these\nfeatures are reconstructed into visual stimuli. To verify the effectiveness of\nthe model, we conducted EEG-to-image generation and classification tasks using\nthe THINGS-EEG dataset. To address the limitations of quantitative analysis at\nthe semantic level, we combined WordNet-based classification and semantic\nsimilarity metrics to propose a novel semantic-based score, emphasizing the\nability of our model to transfer neural activities into visual representations.\nExperimental results show that our model significantly improves semantic\nalignment and classification accuracy, which achieves a maximum single-subject\naccuracy of 43\\%, outperforming other state-of-the-art methods. The source code\nand supplementary material is available at\nhttps://github.com/zes0v0inn/DWT_EEG_Reconstruction/tree/main.", "authors": ["Enshang Zhang", "Zhicheng Zhang", "Takashi Hanakawa"], "published_date": "2025-05-30", "title_zh": "基於小波轉換和對比語義損失的類別感知腦電圖圖像生成", "summary_zh": "本研究提出一種基於Transformer的腦電訊號編碼器，整合了離散小波轉換與閘控機制，旨在從腦電訊號中提取與視覺刺激相關的特徵。此編碼器在特徵對齊和類別感知融合損失的引導下，結合預訓練的擴散模型，將特徵重建為視覺刺激。我們利用THINGS-EEG數據集進行腦電訊號到圖像的生成和分類任務，驗證了模型的有效性。針對語義層面量化分析的局限性，我們結合WordNet分類和語義相似性指標，提出了一種新的語義評分方法，強調模型將神經活動轉化為視覺表徵的能力。實驗結果表明，我們的模型顯著提高了語義對齊度和分類準確度，單個受試者的最高準確度達到43%，優於其他最先進的方法。源代碼和補充材料可在https://github.com/zes0v0inn/DWT_EEG_Reconstruction/tree/main獲取。", "audio": "audios/2505.24301v1.mp3", "timestamp": "2025-06-02T14:19:49.468610"}
{"query": "AI", "id": "2505.24658v1", "url": "http://arxiv.org/abs/2505.24658v1", "title": "Can LLMs and humans be friends? Uncovering factors affecting human-AI intimacy formation", "summary": "Large language models (LLMs) are increasingly being used in conversational\nroles, yet little is known about how intimacy emerges in human-LLM\ninteractions. Although previous work emphasized the importance of\nself-disclosure in human-chatbot interaction, it is questionable whether\ngradual and reciprocal self-disclosure is also helpful in human-LLM\ninteraction. Thus, this study examined three possible aspects contributing to\nintimacy formation: gradual self-disclosure, reciprocity, and naturalness.\nStudy 1 explored the impact of mutual, gradual self-disclosure with 29 users\nand a vanilla LLM. Study 2 adopted self-criticism methods for more natural\nresponses and conducted a similar experiment with 53 users. Results indicate\nthat gradual self-disclosure significantly enhances perceived social intimacy,\nregardless of persona reciprocity. Moreover, participants perceived utterances\ngenerated with self-criticism as more natural compared to those of vanilla\nLLMs; self-criticism fostered higher intimacy in early stages. Also, we\nobserved that excessive empathetic expressions occasionally disrupted\nimmersion, pointing to the importance of response calibration during intimacy\nformation.", "authors": ["Yeseon Hong", "Junhyuk Choi", "Minju Kim", "Bugeun Kim"], "published_date": "2025-05-30", "title_zh": "大型語言模型與人類能否成為朋友？探討影響人機親密關係建立之因素", "summary_zh": "大型語言模型日益普及於對話應用，但人機互動中親密感的產生機制仍待探索。過去研究強調自我揭露於人機互動的重要性，然漸進式與互惠式自我揭露於人機互動中的作用尚不明確。本研究探討漸進式自我揭露、互惠性與自然性對親密關係建立的影響。研究一探討與29位使用者及一基礎語言模型進行互惠、漸進式自我揭露的影響。研究二採用自我批判方法以產生更自然的反應，並對53位使用者進行類似實驗。結果顯示，漸進式自我揭露顯著提升感知到的社會親密感，與角色互惠性無關。此外，參與者認為自我批判產生的回應比基礎語言模型更自然；自我批判在初期階段促進了更高的親密感。研究亦觀察到過度同理心表達偶爾會干擾沉浸感，顯示在親密關係建立過程中，回應校準至關重要。", "audio": "audios/2505.24658v1.mp3", "timestamp": "2025-06-02T15:21:09.523850"}
{"query": "Foundation Model", "id": "2505.24232v1", "url": "http://arxiv.org/abs/2505.24232v1", "title": "From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models", "summary": "Large foundation models (LFMs) are susceptible to two distinct\nvulnerabilities: hallucinations and jailbreak attacks. While typically studied\nin isolation, we observe that defenses targeting one often affect the other,\nhinting at a deeper connection.\n  We propose a unified theoretical framework that models jailbreaks as\ntoken-level optimization and hallucinations as attention-level optimization.\nWithin this framework, we establish two key propositions: (1) \\textit{Similar\nLoss Convergence} - the loss functions for both vulnerabilities converge\nsimilarly when optimizing for target-specific outputs; and (2) \\textit{Gradient\nConsistency in Attention Redistribution} - both exhibit consistent gradient\nbehavior driven by shared attention dynamics.\n  We validate these propositions empirically on LLaVA-1.5 and MiniGPT-4,\nshowing consistent optimization trends and aligned gradients. Leveraging this\nconnection, we demonstrate that mitigation techniques for hallucinations can\nreduce jailbreak success rates, and vice versa. Our findings reveal a shared\nfailure mode in LFMs and suggest that robustness strategies should jointly\naddress both vulnerabilities.", "authors": ["Haibo Jin", "Peiyan Zhang", "Peiran Wang", "Man Luo", "Haohan Wang"], "published_date": "2025-05-30", "title_zh": "從幻覺到越獄：重新思考大型基礎模型的脆弱性", "summary_zh": "大型基礎模型易受幻覺和越獄攻擊兩種漏洞影響。儘管通常獨立研究，但針對其中一種的防禦措施往往會影響另一種，暗示了更深層次的聯繫。\n本文提出一個統一的理論框架，將越獄建模為令牌級別的優化，將幻覺建模為注意力級別的優化。在此框架內，我們確立了兩個關鍵命題：（1）相似損失收斂——當針對特定目標輸出進行優化時，兩種漏洞的損失函數以相似的方式收斂；（2）注意力重新分配中的梯度一致性——兩者都表現出由共享注意力動態驅動的一致梯度行為。\n我們在LLaVA-1.5和MiniGPT-4上驗證了這些命題，展示了一致的優化趨勢和對齊的梯度。利用這種聯繫，我們證明了用於緩解幻覺的技術可以降低越獄成功率，反之亦然。我們的研究結果揭示了大型基礎模型中一個共同的失效模式，並表明穩健性策略應共同解決這兩種漏洞。", "audio": "audios/2505.24232v1.mp3", "timestamp": "2025-06-02T15:21:17.285988"}
{"query": "Diffusion Model", "id": "2505.24293v1", "url": "http://arxiv.org/abs/2505.24293v1", "title": "Large Language Models are Locally Linear Mappings", "summary": "We demonstrate that the inference operations of several open-weight large\nlanguage models (LLMs) can be mapped to an exactly equivalent linear system for\nan input sequence without modifying the model weights or altering output\npredictions. Extending techniques from image diffusion models that exhibit\nlocal or piecewise linearity, we strategically alter the gradient computation\nwith respect to a given input sequence for a next-token prediction such that\nthe Jacobian of the model nearly exactly reproduces the forward prediction with\na linear system. We demonstrate this approach across models (Llama 3, Gemma 3,\nQwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show\nthrough the singular value decomposition of the detached Jacobian that these\nLLMs operate in extremely low-dimensional subspaces where many of the largest\nsingular vectors decode to concepts related to the most-likely output token.\nThis approach also allows us to examine the operation of each successive layer\n(and its attention and MLP components) as nearly-exact linear systems and\nobserve the emergence of semantic concepts. Despite their expressive power and\nglobal nonlinearity, modern LLMs can be interpreted through nearly-exact\nlocally linear decompositions that provide insights into their internal\nrepresentations and reveal interpretable semantic structures in the next-token\nprediction process.", "authors": ["James R. Golden"], "published_date": "2025-05-30", "title_zh": "大型語言模型是局部線性映射", "summary_zh": "我們證明，若干開放權重大型語言模型(LLM)的推論運算，在不修改模型權重或改變輸出預測的前提下，可映射至等效線性系統。藉由擴展圖像擴散模型中展現局部或分段線性之技術，我們策略性地改變相對於給定輸入序列的梯度計算，以進行下一個詞符預測，使模型的雅可比矩陣近乎精確地以線性系統重現前向預測。我們於多個模型（Llama 3、Gemma 3、Qwen 3、Phi 4、Mistral Ministral 和 OLMo 2，最高至 Llama 3.3 70B Q4）上驗證此方法，並透過分離雅可比矩陣的奇異值分解，表明這些LLM在極低維度子空間中運作，其中許多最大奇異向量解碼為與最可能輸出詞符相關之概念。此方法也允許我們檢視每一層（及其注意力與MLP組件）的運作，視其為近乎精確的線性系統，並觀察語義概念的浮現。儘管現代LLM具有表達能力和全局非線性，但它們可透過近乎精確的局部線性分解進行解釋，從而深入了解其內部表示，並揭示下一個詞符預測過程中可解釋的語義結構。", "audio": "audios/2505.24293v1.mp3", "timestamp": "2025-06-02T15:21:29.744272"}
{"query": "AI", "id": "2505.24655v1", "url": "http://arxiv.org/abs/2505.24655v1", "title": "Adaptable Cardiovascular Disease Risk Prediction from Heterogeneous Data using Large Language Models", "summary": "Cardiovascular disease (CVD) risk prediction models are essential for\nidentifying high-risk individuals and guiding preventive actions. However,\nexisting models struggle with the challenges of real-world clinical practice as\nthey oversimplify patient profiles, rely on rigid input schemas, and are\nsensitive to distribution shifts. We developed AdaCVD, an adaptable CVD risk\nprediction framework built on large language models extensively fine-tuned on\nover half a million participants from the UK Biobank. In benchmark comparisons,\nAdaCVD surpasses established risk scores and standard machine learning\napproaches, achieving state-of-the-art performance. Crucially, for the first\ntime, it addresses key clinical challenges across three dimensions: it flexibly\nincorporates comprehensive yet variable patient information; it seamlessly\nintegrates both structured data and unstructured text; and it rapidly adapts to\nnew patient populations using minimal additional data. In stratified analyses,\nit demonstrates robust performance across demographic, socioeconomic, and\nclinical subgroups, including underrepresented cohorts. AdaCVD offers a\npromising path toward more flexible, AI-driven clinical decision support tools\nsuited to the realities of heterogeneous and dynamic healthcare environments.", "authors": ["Frederike Lübeck", "Jonas Wildberger", "Frederik Träuble", "Maximilian Mordig", "Sergios Gatidis", "Andreas Krause", "Bernhard Schölkopf"], "published_date": "2025-05-30", "title_zh": "利用大型語言模型從異質數據中進行適應性心血管疾病風險預測", "summary_zh": "心血管疾病（CVD）風險預測模型對於識別高風險人群和指導預防措施至關重要。現有模型因過於簡化患者資料、依賴僵化的輸入模式且對分佈變化敏感，難以應對真實臨床挑戰。我們開發了AdaCVD，一種基於大型語言模型的可適應CVD風險預測框架，該模型在英國生物樣本庫超過50萬參與者身上進行了廣泛的微調。基準測試顯示，AdaCVD超越了已建立的風險評分和標準機器學習方法，達到最先進的性能。它首次在三個維度上解決了關鍵臨床挑戰：靈活地整合全面但可變的患者資訊、無縫整合結構化數據和非結構化文本，並使用最少的額外數據快速適應新的患者群體。分層分析表明，它在人口、社會經濟和臨床亞群（包括代表性不足的群體）中表現出穩健的性能。AdaCVD為更靈活、AI驅動的臨床決策支持工具提供了一個有希望的途徑，以適應異構和動態醫療環境的現實。", "audio": "audios/2505.24655v1.mp3", "timestamp": "2025-06-02T16:24:53.675313"}
{"query": "Foundation Model", "id": "2505.24214v1", "url": "http://arxiv.org/abs/2505.24214v1", "title": "Benchmarking Foundation Models for Zero-Shot Biometric Tasks", "summary": "The advent of foundation models, particularly Vision-Language Models (VLMs)\nand Multi-modal Large Language Models (MLLMs), has redefined the frontiers of\nartificial intelligence, enabling remarkable generalization across diverse\ntasks with minimal or no supervision. Yet, their potential in biometric\nrecognition and analysis remains relatively underexplored. In this work, we\nintroduce a comprehensive benchmark that evaluates the zero-shot and few-shot\nperformance of state-of-the-art publicly available VLMs and MLLMs across six\nbiometric tasks spanning the face and iris modalities: face verification, soft\nbiometric attribute prediction (gender and race), iris recognition,\npresentation attack detection (PAD), and face manipulation detection (morphs\nand deepfakes). A total of 41 VLMs were used in this evaluation. Experiments\nshow that embeddings from these foundation models can be used for diverse\nbiometric tasks with varying degrees of success. For example, in the case of\nface verification, a True Match Rate (TMR) of 96.77 percent was obtained at a\nFalse Match Rate (FMR) of 1 percent on the Labeled Face in the Wild (LFW)\ndataset, without any fine-tuning. In the case of iris recognition, the TMR at 1\npercent FMR on the IITD-R-Full dataset was 97.55 percent without any\nfine-tuning. Further, we show that applying a simple classifier head to these\nembeddings can help perform DeepFake detection for faces, Presentation Attack\nDetection (PAD) for irides, and extract soft biometric attributes like gender\nand ethnicity from faces with reasonably high accuracy. This work reiterates\nthe potential of pretrained models in achieving the long-term vision of\nArtificial General Intelligence.", "authors": ["Redwan Sony", "Parisa Farmanifard", "Hamzeh Alzwairy", "Nitish Shukla", "Arun Ross"], "published_date": "2025-05-30", "title_zh": "零樣本生物特徵任務中基礎模型的基準測試", "summary_zh": "基於視覺語言模型及多模態大型語言模型的基礎模型重新定義了人工智慧的邊界，可在最少甚至零監督下於多樣化任務中實現顯著的泛化能力。然而，其在生物特徵識別與分析中的潛力尚未被充分探索。本研究引入一個綜合基準，評估了公開可用的最先進視覺語言模型及多模態大型語言模型在六項生物特徵任務中的零樣本及少樣本效能，涵蓋臉部與虹膜模態：臉部驗證、軟生物特徵屬性預測（性別與種族）、虹膜識別、呈現攻擊偵測及臉部操縱偵測。共使用了41個視覺語言模型進行評估。實驗顯示，這些基礎模型產生的嵌入向量可用於多種生物特徵任務，且成效不一。例如，在臉部驗證中，於LFW數據集上，無需任何微調，即可在1%的錯誤匹配率下達到96.77%的真實匹配率。在虹膜識別中，於IITD-R-Full數據集上，無需任何微調，即可在1%的錯誤匹配率下達到97.55%的真實匹配率。此外，研究表明，在這些嵌入向量上應用一個簡單的分類器頭，有助於執行臉部DeepFake偵測、虹膜呈現攻擊偵測，並以相當高的準確度從臉部提取性別與族裔等軟生物特徵屬性。此研究重申了預訓練模型在實現通用人工智慧長期願景方面的潛力。", "audio": "audios/2505.24214v1.mp3", "timestamp": "2025-06-02T16:25:05.418020"}
{"query": "Diffusion Model", "id": "2505.24267v1", "url": "http://arxiv.org/abs/2505.24267v1", "title": "MUSE: Model-Agnostic Tabular Watermarking via Multi-Sample Selection", "summary": "We introduce MUSE, a watermarking algorithm for tabular generative models.\nPrevious approaches typically leverage DDIM invertibility to watermark tabular\ndiffusion models, but tabular diffusion models exhibit significantly poorer\ninvertibility compared to other modalities, compromising performance.\nSimultaneously, tabular diffusion models require substantially less computation\nthan other modalities, enabling a multi-sample selection approach to tabular\ngenerative model watermarking. MUSE embeds watermarks by generating multiple\ncandidate samples and selecting one based on a specialized scoring function,\nwithout relying on model invertibility. Our theoretical analysis establishes\nthe relationship between watermark detectability, candidate count, and dataset\nsize, allowing precise calibration of watermarking strength. Extensive\nexperiments demonstrate that MUSE achieves state-of-the-art watermark\ndetectability and robustness against various attacks while maintaining data\nquality, and remains compatible with any tabular generative model supporting\nrepeated sampling, effectively addressing key challenges in tabular data\nwatermarking. Specifically, it reduces the distortion rates on fidelity metrics\nby 81-89%, while achieving a 1.0 TPR@0.1%FPR detection rate. Implementation of\nMUSE can be found at https://github.com/fangliancheng/MUSE.", "authors": ["Liancheng Fang", "Aiwei Liu", "Henry Peng Zou", "Yankai Chen", "Hengrui Zhang", "Zhongfen Deng", "Philip S. Yu"], "published_date": "2025-05-30", "title_zh": "MUSE：基於多樣本選擇的與模型無關之表格資料浮水印技術", "summary_zh": "MUSE是一種表格生成模型浮水印演算法。傳統方法利用DDIM可逆性為表格擴散模型添加浮水印，但表格擴散模型的可逆性遠遜於其他模態，影響效能。MUSE透過生成多個候選樣本並依據特定評分函數選擇樣本來嵌入浮水印，無需依賴模型可逆性。理論分析確立了浮水印可檢測性、候選數量和數據集大小之間的關係，可精確校準浮水印強度。實驗表明，MUSE在保持數據品質的同時，實現了最先進的浮水印檢測能力和針對各種攻擊的穩健性，並與任何支持重複抽樣的表格生成模型兼容，有效應對了表格數據浮水印的關鍵挑戰。與其他方法相比，MUSE可將保真度指標的失真率降低81-89%，同時實現1.0 TPR@0.1%FPR的檢測率。", "audio": "audios/2505.24267v1.mp3", "timestamp": "2025-06-02T16:25:12.800740"}
{"query": "AI", "id": "2505.24626v1", "url": "http://arxiv.org/abs/2505.24626v1", "title": "Co-designed Quantum Discrete Adiabatic Linear System Solver Via Dynamic Circuits", "summary": "Existing quantum discrete adiabatic approaches are hindered by circuit depth\nthat increases linearly with the number of evolution steps, a significant\nchallenge for current quantum hardware with limited coherence times. To address\nthis, we propose a co-designed framework that synergistically integrates\ndynamic circuit capabilities with real-time classical processing. This\nframework reformulates the quantum adiabatic evolution into discrete,\ndynamically adjustable segments. The unitary operator for each segment is\noptimized on-the-fly using classical computation, and circuit multiplexing\ntechniques are leveraged to reduce the overall circuit depth scaling from\n\\(O(\\text{steps}\\times\\text{depth}(U))\\) to \\(O(\\text{depth}(U))\\). We\nimplement and benchmark a quantum discrete adiabatic linear solver based on\nthis framework for linear systems of \\(W \\in {2,4,8,16}\\) dimensions with\ncondition numbers \\(\\kappa \\in {10,20,30,40,50}\\). Our solver successfully\novercomes previous depth limitations, maintaining over \\(80\\%\\) solution\nfidelity even under realistic noise models. Key algorithmic optimizations\ncontributing to this performance include a first-order approximation of the\ndiscrete evolution operator, a tailored dynamic circuit design exploiting\nreal-imaginary component separation, and noise-resilient post-processing\ntechniques.", "authors": ["Boxuan Ai", "Shuo He", "Xiang Zhao", "Lin Yang", "Guozhen Liu", "Pengfei Gao", "Hongbao Liu", "Tao Tang", "Jiecheng Yang", "Jie Wu"], "published_date": "2025-05-30", "title_zh": "基於動態電路的協同設計量子離散絕熱線性系統求解器", "summary_zh": "現有量子離散絕熱方法受限於電路深度隨演化步驟線性增長，對當前相干時間有限的量子硬體構成挑戰。為此，我們提出一個協同設計框架，整合動態電路能力與即時古典處理。此框架將量子絕熱演化重構為動態可調的離散片段。每個片段的么正算符透過古典計算即時最佳化，並利用電路多工技術將總電路深度從\\(O(\\text{steps}\\times\\text{depth}(U))\\)縮減至\\(O(\\text{depth}(U))\\)。我們基於此框架實現並評估一個量子離散絕熱線性求解器，用於求解維度\\(W \\in {2,4,8,16}\\)，條件數\\(\\kappa \\in {10,20,30,40,50}\\)的線性系統。此求解器成功克服了先前的深度限制，即使在真實噪音模型下仍保持超過\\(80\\%\\)的解保真度。促成此性能的關鍵演算法優化包括：離散演化算符的一階近似、利用實部-虛部隔離的客製化動態電路設計，以及具噪音容錯性的後處理技術。", "audio": "audios/2505.24626v1.mp3", "timestamp": "2025-06-02T17:17:58.232479"}
{"query": "Foundation Model", "id": "2505.24200v1", "url": "http://arxiv.org/abs/2505.24200v1", "title": "Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC", "summary": "Multilingual speech processing with self-supervised or supervised pre-trained\nSpeech Foundation Models (SFM) has achieved strong performance on tasks like\nLanguage Identification (LID) and Automatic Speech Recognition (ASR). However,\nthese models struggle with limited resources during fine-tuning. This paper\nenhances multilingual LID and ASR on ML-SUPERB 2.0 by exploring multiple\nstrategies for adapting SFMs, including frozen upstream training, partial\nfine-tuning, and low-rank adaptation. Furthermore, we employ data augmentation\nto mitigate performance gaps in few-shot settings and introduce LID\nConnectionist Temporal Classification (CTC) loss for regularization. Our\napproach achieves a 14% relative improvement in LID accuracy and a 30% relative\nreduction in ASR CER over the baseline on ML-SUPERB 2.0, securing second place\nin the Interspeech 2025 ML-SUPERB 2.0 Challenge.", "authors": ["Qingzheng Wang", "Jiancheng Sun", "Yifan Peng", "Shinji Watanabe"], "published_date": "2025-05-30", "title_zh": "ML-SUPERB 2.0多語音模型改進：基於數據增強和語言辨識感知CTC的微調", "summary_zh": "基於自監督或監督式預訓練語音基礎模型的多語語音處理，在語音辨識和語言辨識等任務上表現出色，但微調時面臨資源限制。本研究探討多種SFM適應策略，包含凍結上游訓練、部分微調和低秩適應，以提升ML-SUPERB 2.0上的多語語言辨識和語音辨識效能。此外，我們採用數據增強來減輕少樣本情境下的效能差距，並引入語言辨識連接時序分類損失進行正則化。實驗結果顯示，相較於基準模型，語言辨識準確率相對提升14%，語音辨識詞錯誤率相對降低30%，於Interspeech 2025 ML-SUPERB 2.0挑戰賽中獲得第二名。", "audio": "audios/2505.24200v1.mp3", "timestamp": "2025-06-02T17:18:05.049667"}
{"query": "Diffusion Model", "id": "2505.24260v1", "url": "http://arxiv.org/abs/2505.24260v1", "title": "Generative AI for Urban Design: A Stepwise Approach Integrating Human Expertise with Multimodal Diffusion Models", "summary": "Urban design is a multifaceted process that demands careful consideration of\nsite-specific constraints and collaboration among diverse professionals and\nstakeholders. The advent of generative artificial intelligence (GenAI) offers\ntransformative potential by improving the efficiency of design generation and\nfacilitating the communication of design ideas. However, most existing\napproaches are not well integrated with human design workflows. They often\nfollow end-to-end pipelines with limited control, overlooking the iterative\nnature of real-world design. This study proposes a stepwise generative urban\ndesign framework that integrates multimodal diffusion models with human\nexpertise to enable more adaptive and controllable design processes. Instead of\ngenerating design outcomes in a single end-to-end process, the framework\ndivides the process into three key stages aligned with established urban design\nworkflows: (1) road network and land use planning, (2) building layout\nplanning, and (3) detailed planning and rendering. At each stage, multimodal\ndiffusion models generate preliminary designs based on textual prompts and\nimage-based constraints, which can then be reviewed and refined by human\ndesigners. We design an evaluation framework to assess the fidelity,\ncompliance, and diversity of the generated designs. Experiments using data from\nChicago and New York City demonstrate that our framework outperforms baseline\nmodels and end-to-end approaches across all three dimensions. This study\nunderscores the benefits of multimodal diffusion models and stepwise generation\nin preserving human control and facilitating iterative refinements, laying the\ngroundwork for human-AI interaction in urban design solutions.", "authors": ["Mingyi He", "Yuebing Liang", "Shenhao Wang", "Yunhan Zheng", "Qingyi Wang", "Dingyi Zhuang", "Li Tian", "Jinhua Zhao"], "published_date": "2025-05-30", "title_zh": "生成式人工智慧於都市設計：整合人類專業知識與多模態擴散模型的逐步方法", "summary_zh": "都市設計是複雜程序，需周詳考量場地限制及跨專業協作。生成式人工智慧(GenAI)具變革潛力，可提升設計效率及促進設計溝通。然現有方法多與人類設計流程整合不足，常採端對端流程且控制有限，忽略實務設計的疊代性。本研究提出分階段生成式都市設計框架，整合多模態擴散模型與人類專業，以實現更具適應性及可控性的設計流程。框架將流程分為三個階段：(1)道路網絡及土地使用規劃，(2)建築佈局規劃，(3)細部規劃及渲染。各階段中，多模態擴散模型基於文本提示及圖像約束生成初步設計，供設計師審查及修改。我們設計評估框架，評估生成設計的逼真度、合規性及多樣性。芝加哥及紐約市數據實驗表明，本框架在所有三個維度上均優於基準模型及端對端方法。本研究強調多模態擴散模型及分階段生成在維持人類控制及促進疊代修改的優勢，為都市設計方案中的人機互動奠定基礎。", "audio": "audios/2505.24260v1.mp3", "timestamp": "2025-06-02T17:18:15.748575"}
{"query": "AI", "id": "2505.24621v1", "url": "http://arxiv.org/abs/2505.24621v1", "title": "Benchmarking Large Language Models for Cryptanalysis and Mismatched-Generalization", "summary": "Recent advancements in Large Language Models (LLMs) have transformed natural\nlanguage understanding and generation, leading to extensive benchmarking across\ndiverse tasks. However, cryptanalysis a critical area for data security and\nencryption has not yet been thoroughly explored in LLM evaluations. To address\nthis gap, we evaluate cryptanalytic potential of state of the art LLMs on\nencrypted texts generated using a range of cryptographic algorithms. We\nintroduce a novel benchmark dataset comprising diverse plain texts spanning\nvarious domains, lengths, writing styles, and topics paired with their\nencrypted versions. Using zero-shot and few shot settings, we assess multiple\nLLMs for decryption accuracy and semantic comprehension across different\nencryption schemes. Our findings reveal key insights into the strengths and\nlimitations of LLMs in side-channel communication while raising concerns about\ntheir susceptibility to jailbreaking attacks. This research highlights the\ndual-use nature of LLMs in security contexts and contributes to the ongoing\ndiscussion on AI safety and security.", "authors": ["Utsav Maskey", "Chencheng Zhu", "Usman Naseem"], "published_date": "2025-05-30", "title_zh": "大型語言模型於密碼分析與失配泛化之基準測試", "summary_zh": "大型語言模型（LLM）的進展改變了自然語言理解和生成，促使廣泛的基準測試。然而，密碼分析這一數據安全和加密的關鍵領域，尚未在LLM評估中得到充分探索。本研究評估了最先進LLM在加密文本上的密碼分析潛力，該文本使用多種加密算法生成。我們引入了一個新的基準數據集，包含跨越多個領域、長度、寫作風格和主題的多樣化明文，以及它們的加密版本。在零樣本和小樣本設置下，我們評估了多個LLM在不同加密方案下的解密準確性和語義理解能力。研究結果揭示了LLM在側信道通信中的優勢和局限性，同時引發了對其遭受越獄攻擊的擔憂。本研究強調了LLM在安全環境中的雙重用途，並有助於正在進行的關於人工智能安全性的討論。", "audio": "audios/2505.24621v1.mp3", "timestamp": "2025-06-02T18:27:08.818261"}
{"query": "Foundation Model", "id": "2505.24178v1", "url": "http://arxiv.org/abs/2505.24178v1", "title": "Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem", "summary": "In the era of foundation models, Out-of- Distribution (OOD) problems, i.e.,\nthe data discrepancy between the training environments and testing\nenvironments, hinder AI generalization. Further, relational data like graphs\ndisobeying the Independent and Identically Distributed (IID) condition makes\nthe problem more challenging, especially much harder when it is associated with\ntime. Motivated by this, to realize the robust invariant learning over temporal\ngraphs, we want to investigate what components in temporal graphs are most\ninvariant and representative with respect to labels. With the Information\nBottleneck (IB) method, we propose an error-bounded Invariant Link Selector\nthat can distinguish invariant components and variant components during the\ntraining process to make the deep learning model generalizable for different\ntesting scenarios. Besides deriving a series of rigorous generalizable\noptimization functions, we also equip the training with task-specific loss\nfunctions, e.g., temporal link prediction, to make pretrained models solve\nreal-world application tasks like citation recommendation and merchandise\nrecommendation, as demonstrated in our experiments with state-of-the-art (SOTA)\nmethods. Our code is available at https://github.com/kthrn22/OOD-Linker.", "authors": ["Katherine Tieu", "Dongqi Fu", "Jun Wu", "Jingrui He"], "published_date": "2025-05-30", "title_zh": "空間-時間分佈外推問題之不變連結選擇器", "summary_zh": "在基石模型時代，訓練與測試環境間的資料差異造成分布外 (OOD) 問題，阻礙了人工智慧的泛化能力。圖等關聯資料違反獨立同分布 (IID) 假設，更增加了問題的挑戰性，尤其是在與時間相關時。為實現時間圖上的穩健不變學習，我們探討時間圖中哪些成分對於標籤而言最不變且最具代表性。藉由資訊瓶頸 (IB) 方法，我們提出一個誤差界定的不變連結選擇器，可在訓練過程中區分不變與變異成分，使深度學習模型能泛化到不同的測試情境。除了推導出一系列嚴謹的可泛化最佳化函數，我們還為訓練配備了任務特定的損失函數，例如時間連結預測，使預訓練模型能解決現實世界的應用任務，如論文引用推薦和商品推薦。實驗結果顯示，本方法優於現有技術。程式碼位於 https://github.com/kthrn22/OOD-Linker。", "audio": "audios/2505.24178v1.mp3", "timestamp": "2025-06-02T18:27:19.091139"}
{"query": "Diffusion Model", "id": "2505.24253v1", "url": "http://arxiv.org/abs/2505.24253v1", "title": "Interactive Video Generation via Domain Adaptation", "summary": "Text-conditioned diffusion models have emerged as powerful tools for\nhigh-quality video generation. However, enabling Interactive Video Generation\n(IVG), where users control motion elements such as object trajectory, remains\nchallenging. Recent training-free approaches introduce attention masking to\nguide trajectory, but this often degrades perceptual quality. We identify two\nkey failure modes in these methods, both of which we interpret as domain shift\nproblems, and propose solutions inspired by domain adaptation. First, we\nattribute the perceptual degradation to internal covariate shift induced by\nattention masking, as pretrained models are not trained to handle masked\nattention. To address this, we propose mask normalization, a pre-normalization\nlayer designed to mitigate this shift via distribution matching. Second, we\naddress initialization gap, where the randomly sampled initial noise does not\nalign with IVG conditioning, by introducing a temporal intrinsic diffusion\nprior that enforces spatio-temporal consistency at each denoising step.\nExtensive qualitative and quantitative evaluations demonstrate that mask\nnormalization and temporal intrinsic denoising improve both perceptual quality\nand trajectory control over the existing state-of-the-art IVG techniques.", "authors": ["Ishaan Rawal", "Suryansh Kumar"], "published_date": "2025-05-30", "title_zh": "基於領域自適應的互動式影片生成", "summary_zh": "文本條件擴散模型已成為產生高品質影片的強大工具。然而，實現互動式影片生成（IVG），即使用者控制物體軌跡等運動元素，仍然具有挑戰性。現有的免訓練方法透過注意力遮罩引導軌跡，但通常會降低感知品質。我們發現這些方法存在兩種主要失效模式，並將其歸因於領域轉移問題，進而提出受領域適應啟發的解決方案。首先，我們認為感知品質的下降是由於注意力遮罩引起的內部協變量偏移，因為預訓練模型並未針對遮罩注意力進行訓練。為了解決此問題，我們提出遮罩正規化，一種旨在透過分佈匹配緩解此偏移的預正規化層。其次，我們透過引入時間本徵擴散先驗來解決初始化間隙問題，即隨機抽樣的初始雜訊與IVG條件不一致，該先驗在每個去噪步驟中強制執行時空一致性。廣泛的定性和定量評估表明，遮罩正規化和時間本徵去噪在現有最先進的IVG技術上改善了感知品質和軌跡控制。", "audio": "audios/2505.24253v1.mp3", "timestamp": "2025-06-02T18:27:31.177227"}
{"query": "AI", "id": "2505.24612v1", "url": "http://arxiv.org/abs/2505.24612v1", "title": "Multi-criteria Rank-based Aggregation for Explainable AI", "summary": "Explainability is crucial for improving the transparency of black-box machine\nlearning models. With the advancement of explanation methods such as LIME and\nSHAP, various XAI performance metrics have been developed to evaluate the\nquality of explanations. However, different explainers can provide contrasting\nexplanations for the same prediction, introducing trade-offs across conflicting\nquality metrics. Although available aggregation approaches improve robustness,\nreducing explanations' variability, very limited research employed a\nmulti-criteria decision-making approach. To address this gap, this paper\nintroduces a multi-criteria rank-based weighted aggregation method that\nbalances multiple quality metrics simultaneously to produce an ensemble of\nexplanation models. Furthermore, we propose rank-based versions of existing XAI\nmetrics (complexity, faithfulness and stability) to better evaluate ranked\nfeature importance explanations. Extensive experiments on publicly available\ndatasets demonstrate the robustness of the proposed model across these metrics.\nComparative analyses of various multi-criteria decision-making and rank\naggregation algorithms showed that TOPSIS and WSUM are the best candidates for\nthis use case.", "authors": ["Sujoy Chatterjee", "Everton Romanzini Colombo", "Marcos Medeiros Raimundo"], "published_date": "2025-05-30", "title_zh": "基於多準則排序的聚合方法，用於可解釋人工智慧", "summary_zh": "可解釋性對於提升黑箱機器學習模型的透明度至關重要。隨著LIME和SHAP等解釋方法發展，產生了多種XAI效能指標以評估解釋品質。然而，不同解釋器可能對相同預測產生對比鮮明的解釋，導致在衝突的品質指標間產生權衡。現有聚合方法雖能提升穩健性、降低解釋變異性，但採用多準則決策方法的研究仍然有限。為了解決此問題，本文提出一種基於排序加權聚合的多準則方法，同時平衡多個品質指標，以產生解釋模型集成。此外，我們提出基於排序的現有XAI指標版本（複雜性、忠實性和穩定性），以更佳評估排序特徵重要性解釋。在公開數據集上的大量實驗表明，所提出模型在這些指標上具有穩健性。對各種多準則決策和排序聚合算法的比較分析表明，TOPSIS和WSUM是此應用的最佳候選者。", "audio": "audios/2505.24612v1.mp3", "timestamp": "2025-06-02T19:14:58.291674"}
{"query": "Foundation Model", "id": "2505.24167v1", "url": "http://arxiv.org/abs/2505.24167v1", "title": "Pretraining Deformable Image Registration Networks with Random Images", "summary": "Recent advances in deep learning-based medical image registration have shown\nthat training deep neural networks~(DNNs) does not necessarily require medical\nimages. Previous work showed that DNNs trained on randomly generated images\nwith carefully designed noise and contrast properties can still generalize well\nto unseen medical data. Building on this insight, we propose using registration\nbetween random images as a proxy task for pretraining a foundation model for\nimage registration. Empirical results show that our pretraining strategy\nimproves registration accuracy, reduces the amount of domain-specific data\nneeded to achieve competitive performance, and accelerates convergence during\ndownstream training, thereby enhancing computational efficiency.", "authors": ["Junyu Chen", "Shuwen Wei", "Yihao Liu", "Aaron Carass", "Yong Du"], "published_date": "2025-05-30", "title_zh": "使用隨機圖像預訓練可變形圖像配準網絡", "summary_zh": "基於深度學習的醫學影像配準研究顯示，訓練深度神經網路不一定需要真實醫學影像。先前研究表明，以具特定雜訊和對比度之隨機生成影像訓練的神經網路，仍可有效推廣至未見過的醫學影像。基於此，本文提出使用隨機影像間的配準作為代理任務，預訓練影像配準的基礎模型。實驗結果表明，此預訓練策略能提升配準準確性，減少特定領域數據的需求量，加速後續訓練的收斂，進而提高運算效率。", "audio": "audios/2505.24167v1.mp3", "timestamp": "2025-06-02T19:15:03.944925"}
{"query": "Diffusion Model", "id": "2505.24245v1", "url": "http://arxiv.org/abs/2505.24245v1", "title": "LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework", "summary": "We present LTM3D, a Latent Token space Modeling framework for conditional 3D\nshape generation that integrates the strengths of diffusion and auto-regressive\n(AR) models. While diffusion-based methods effectively model continuous latent\nspaces and AR models excel at capturing inter-token dependencies, combining\nthese paradigms for 3D shape generation remains a challenge. To address this,\nLTM3D features a Conditional Distribution Modeling backbone, leveraging a\nmasked autoencoder and a diffusion model to enhance token dependency learning.\nAdditionally, we introduce Prefix Learning, which aligns condition tokens with\nshape latent tokens during generation, improving flexibility across modalities.\nWe further propose a Latent Token Reconstruction module with\nReconstruction-Guided Sampling to reduce uncertainty and enhance structural\nfidelity in generated shapes. Our approach operates in token space, enabling\nsupport for multiple 3D representations, including signed distance fields,\npoint clouds, meshes, and 3D Gaussian Splatting. Extensive experiments on\nimage- and text-conditioned shape generation tasks demonstrate that LTM3D\noutperforms existing methods in prompt fidelity and structural accuracy while\noffering a generalizable framework for multi-modal, multi-representation 3D\ngeneration.", "authors": ["Xin Kang", "Zihan Zheng", "Lei Chu", "Yue Gao", "Jiahao Li", "Hao Pan", "Xuejin Chen", "Yan Lu"], "published_date": "2025-05-30", "title_zh": "LTM3D：以自迴歸擴散框架橋接符記空間，實現條件式3D生成", "summary_zh": "LTM3D是一種潛在令牌空間建模框架，用於條件式三維形狀生成，融合了擴散模型與自迴歸模型的優點。它採用條件分佈建模主幹，利用遮罩自編碼器和擴散模型來增強令牌依賴性學習。引入前綴學習以對齊條件令牌與形狀潛在令牌，提高跨模態的靈活性。潛在令牌重建模組結合重建引導採樣，降低不確定性並提升生成形狀的結構保真度。LTM3D在令牌空間中運行，支援多種三維表示形式。在圖像和文本條件下的形狀生成任務中，實驗證明LTM3D在提示保真度和結構精確度方面優於現有方法，並為多模態、多表示三維生成提供了一個通用框架。", "audio": "audios/2505.24245v1.mp3", "timestamp": "2025-06-02T19:15:11.139961"}
{"query": "AI", "id": "2505.24584v1", "url": "http://arxiv.org/abs/2505.24584v1", "title": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams", "summary": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment.", "authors": ["Sakhinana Sagar Srinivas", "Shivam Gupta", "Venkataramana Runkana"], "published_date": "2025-05-30", "title_zh": "AutoChemSchematic AI：一種閉環、具物理感知能力的代理框架，用於自動生成化學流程及儀器圖", "summary_zh": "生成式人工智慧加速了新化學品與材料的發現，但工業規模生產仍受限於化學製程開發。現有AI方法無法自動生成製程流程圖(PFD)與管線儀表圖(PID)，且難以符合工程限制。本研究提出一個閉迴路、物理感知框架，用於自動生成可行的PFD與PID。該框架整合了領域專用的語言模型(SLM)與基礎物理模擬，包含：(1)包含1020多種化學品的製程流程與儀表描述的層級知識圖譜；(2)透過監督式微調(SFT)、直接偏好優化(DPO)與檢索增強指令微調(RAIT)在合成數據集上微調SLM的多階段訓練流程；(3)基於DWSIM模擬器的迴路驗證以確保可行性。為提高效率與模型精簡度，該框架整合了FlashAttention、Lookahead Decoding、PagedAttention與KV快取量化等推理時優化技術，並採用結構剪枝技術(寬度與深度)來縮減模型大小，同時保持準確度。實驗結果表明，該框架能高保真度地生成經模擬器驗證的製程描述，在正確性方面優於基準方法，並能推廣至未見過的化學品。此研究將AI設計與工業規模可行性連結，大幅縮短從實驗室發現到工廠部署的研發時程。", "audio": "audios/2505.24584v1.mp3", "timestamp": "2025-06-02T20:21:12.626960"}
{"query": "Foundation Model", "id": "2505.24141v1", "url": "http://arxiv.org/abs/2505.24141v1", "title": "The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models", "summary": "With the widespread adoption of pathology foundation models in both research\nand clinical decision support systems, exploring their security has become a\ncritical concern. However, despite their growing impact, the vulnerability of\nthese models to adversarial attacks remains largely unexplored. In this work,\nwe present the first systematic investigation into the security of pathology\nfoundation models for whole slide image~(WSI) analysis against adversarial\nattacks. Specifically, we introduce the principle of \\textit{local perturbation\nwith global impact} and propose a label-free attack framework that operates\nwithout requiring access to downstream task labels. Under this attack\nframework, we revise four classical white-box attack methods and redefine the\nperturbation budget based on the characteristics of WSI. We conduct\ncomprehensive experiments on three representative pathology foundation models\nacross five datasets and six downstream tasks. Despite modifying only 0.1\\% of\npatches per slide with imperceptible noise, our attack leads to downstream\naccuracy degradation that can reach up to 20\\% in the worst cases. Furthermore,\nwe analyze key factors that influence attack success, explore the relationship\nbetween patch-level vulnerability and semantic content, and conduct a\npreliminary investigation into potential defence strategies. These findings lay\nthe groundwork for future research on the adversarial robustness and reliable\ndeployment of pathology foundation models. Our code is publicly available at:\nhttps://github.com/Jiashuai-Liu-hmos/Attack-WSI-pathology-foundation-models.", "authors": ["Jiashuai Liu", "Yingjia Shang", "Yingkang Zhan", "Di Zhang", "Yi Niu", "Dong Wei", "Xian Wu", "Zeyu Gao", "Chen Li", "Yefeng Zheng"], "published_date": "2025-05-30", "title_zh": "病理學中的蝴蝶效應：探索病理學基礎模型的安全性", "summary_zh": "病理學基礎模型於研究與臨床決策支援系統中廣泛應用，其安全性日益重要。本文針對全玻片影像分析之病理學基礎模型，首度進行對抗性攻擊的系統性研究。我們提出「局部擾動，全局影響」原則，並設計一種無需下游任務標籤的無標籤攻擊框架。在此框架下，我們修正四種經典白盒攻擊方法，並根據全玻片影像特性重新定義擾動預算。實驗結果顯示，即使僅修改每張玻片0.1%的圖塊且加入的雜訊難以察覺，仍可能導致下游任務準確度下降，最嚴重可達20%。此外，我們分析影響攻擊成功的關鍵因素，探討圖塊層級脆弱性與語義內容的關聯，並初步研究潛在的防禦策略。相關研究成果為未來病理學基礎模型的對抗性穩健性及可靠部署奠定基礎。", "audio": "audios/2505.24141v1.mp3", "timestamp": "2025-06-02T20:21:19.535958"}
{"query": "Diffusion Model", "id": "2505.24222v1", "url": "http://arxiv.org/abs/2505.24222v1", "title": "Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin", "summary": "The diffusion models (DMs) have demonstrated the remarkable capability of\ngenerating images via learning the noised score function of data distribution.\nCurrent DM sampling techniques typically rely on first-order Langevin dynamics\nat each noise level, with efforts concentrated on refining inter-level\ndenoising strategies. While leveraging additional second-order Hessian geometry\nto enhance the sampling quality of Langevin is a common practice in Markov\nchain Monte Carlo (MCMC), the naive attempts to utilize Hessian geometry in\nhigh-dimensional DMs lead to quadratic-complexity computational costs,\nrendering them non-scalable. In this work, we introduce a novel\nLevenberg-Marquardt-Langevin (LML) method that approximates the diffusion\nHessian geometry in a training-free manner, drawing inspiration from the\ncelebrated Levenberg-Marquardt optimization algorithm. Our approach introduces\ntwo key innovations: (1) A low-rank approximation of the diffusion Hessian,\nleveraging the DMs' inherent structure and circumventing explicit\nquadratic-complexity computations; (2) A damping mechanism to stabilize the\napproximated Hessian. This LML approximated Hessian geometry enables the\ndiffusion sampling to execute more accurate steps and improve the image\ngeneration quality. We further conduct a theoretical analysis to substantiate\nthe approximation error bound of low-rank approximation and the convergence\nproperty of the damping mechanism. Extensive experiments across multiple\npretrained DMs validate that the LML method significantly improves image\ngeneration quality, with negligible computational overhead.", "authors": ["Fangyikang Wang", "Hubery Yin", "Lei Qian", "Yinan Li", "Shaobin Zhuang", "Huminhao Zhu", "Yilin Zhang", "Yanlong Tang", "Chao Zhang", "Hanbin Zhao", "Hui Qian", "Chen Li"], "published_date": "2025-05-30", "title_zh": "利用二階列文伯格-馬夸特-朗之萬方法在擴散採樣中釋放高品質圖像生成", "summary_zh": "擴散模型透過學習資料分布的雜訊分數函數，展現卓越的圖像生成能力。現有採樣技術通常依賴一階朗之萬動力學，並著重於優化層間去噪策略。儘管利用二階黑塞幾何結構強化朗之萬採樣品質在馬可夫鏈蒙地卡羅方法中常見，但直接應用於高維擴散模型會導致平方複雜度的計算成本，缺乏可擴展性。本研究提出一種新型的列文伯格-馬夸特-朗之萬方法（LML），靈感來自列文伯格-馬夸特優化算法，以無訓練方式近似擴散黑塞幾何結構。此方法包含兩個創新點：(1) 利用擴散模型的內在結構對擴散黑塞矩陣進行低秩近似，避免顯式的平方複雜度計算；(2) 引入阻尼機制來穩定近似的黑塞矩陣。LML近似的黑塞幾何結構使擴散採樣能夠執行更精確的步驟，提高圖像生成品質。此外，本研究進行了理論分析，驗證了低秩近似的近似誤差界限和阻尼機制的收斂性。在多個預訓練擴散模型上的大量實驗驗證了LML方法顯著提高了圖像生成品質，且計算開銷可忽略不計。", "audio": "audios/2505.24222v1.mp3", "timestamp": "2025-06-02T20:21:28.207799"}
{"query": "AI", "id": "2505.24538v1", "url": "http://arxiv.org/abs/2505.24538v1", "title": "Don't Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections", "summary": "Cultural Heritage (CH) data hold invaluable knowledge, reflecting the\nhistory, traditions, and identities of societies, and shaping our understanding\nof the past and present. However, many CH collections contain outdated or\noffensive descriptions that reflect historical biases. CH Institutions (CHIs)\nface significant challenges in curating these data due to the vast scale and\ncomplexity of the task. To address this, we develop an AI-powered tool that\ndetects offensive terms in CH metadata and provides contextual insights into\ntheir historical background and contemporary perception. We leverage a\nmultilingual vocabulary co-created with marginalized communities, researchers,\nand CH professionals, along with traditional NLP techniques and Large Language\nModels (LLMs). Available as a standalone web app and integrated with major CH\nplatforms, the tool has processed over 7.9 million records, contextualizing the\ncontentious terms detected in their metadata. Rather than erasing these terms,\nour approach seeks to inform, making biases visible and providing actionable\ninsights for creating more inclusive and accessible CH collections.", "authors": ["Orfeas Menis Mastromichalakis", "Jason Liartis", "Kristina Rose", "Antoine Isaac", "Giorgos Stamou"], "published_date": "2025-05-30", "title_zh": "勿除滅，以告之：文化遺產藏品中有害語言之偵測與脈絡化", "summary_zh": "文化遺產數據蘊含重要知識，體現社會的歷史、傳統與認同，影響我們對過去與現在的理解。然而，許多文化遺產藏品包含過時或冒犯性描述，反映歷史偏見。文化遺產機構在管理這些數據時面臨巨大挑戰，原因在於其規模和複雜性。為了解決此問題，我們開發了一款人工智慧工具，可偵測文化遺產元數據中的冒犯性詞語，並提供其歷史背景和當代認知的相關資訊。我們利用與邊緣化社群、研究人員及文化遺產專業人士共同創建的多語言詞彙，以及傳統自然語言處理技術和大型語言模型。該工具以獨立網頁應用程式的形式提供，並與主要文化遺產平台整合，已處理超過790萬筆記錄，對元數據中偵測到的爭議性詞語進行了脈絡化。我們的目標不是消除這些詞語，而是旨在提供資訊，使偏見可見，並為創建更具包容性和可訪問性的文化遺產藏品提供可行的見解。", "audio": "audios/2505.24538v1.mp3", "timestamp": "2025-06-02T21:18:53.235732"}
{"query": "Foundation Model", "id": "2505.24108v1", "url": "http://arxiv.org/abs/2505.24108v1", "title": "Federated Foundation Model for GI Endoscopy Images", "summary": "Gastrointestinal (GI) endoscopy is essential in identifying GI tract\nabnormalities in order to detect diseases in their early stages and improve\npatient outcomes. Although deep learning has shown success in supporting GI\ndiagnostics and decision-making, these models require curated datasets with\nlabels that are expensive to acquire. Foundation models offer a promising\nsolution by learning general-purpose representations, which can be finetuned\nfor specific tasks, overcoming data scarcity. Developing foundation models for\nmedical imaging holds significant potential, but the sensitive and protected\nnature of medical data presents unique challenges. Foundation model training\ntypically requires extensive datasets, and while hospitals generate large\nvolumes of data, privacy restrictions prevent direct data sharing, making\nfoundation model training infeasible in most scenarios. In this work, we\npropose a FL framework for training foundation models for gastroendoscopy\nimaging, enabling data to remain within local hospital environments while\ncontributing to a shared model. We explore several established FL algorithms,\nassessing their suitability for training foundation models without relying on\ntask-specific labels, conducting experiments in both homogeneous and\nheterogeneous settings. We evaluate the trained foundation model on three\ncritical downstream tasks--classification, detection, and segmentation--and\ndemonstrate that it achieves improved performance across all tasks,\nhighlighting the effectiveness of our approach in a federated,\nprivacy-preserving setting.", "authors": ["Alina Devkota", "Annahita Amireskandari", "Joel Palko", "Shyam Thakkar", "Donald Adjeroh", "Xiajun Jiang", "Binod Bhattarai", "Prashnna K. Gyawali"], "published_date": "2025-05-30", "title_zh": "用於胃腸內視鏡影像之聯邦式基礎模型", "summary_zh": "胃腸內視鏡檢查對早期發現疾病並改善患者預後至關重要。深度學習雖在輔助胃腸診斷決策上展現潛力，但需仰賴標註成本高昂的資料集。基礎模型透過學習通用表徵，經微調後可適用於特定任務，有助克服資料稀缺問題。然醫療影像基礎模型開發因資料敏感性與保護性而面臨挑戰。由於隱私限制，醫院產出的大量資料難以直接共享，導致基礎模型訓練窒礙難行。本研究提出一個用於訓練胃腸內視鏡影像基礎模型的聯邦學習框架，使資料得以保留在本地醫院環境中，同時貢獻於共享模型。我們探討多種既有聯邦學習演算法，評估其在無須任務特定標籤下訓練基礎模型的適用性，並在同質與異質環境中進行實驗。經驗證，訓練後的基礎模型在分類、檢測和分割三項關鍵下游任務中皆展現更佳效能，凸顯本方法在聯邦式、具隱私保護設定下的有效性。", "audio": "audios/2505.24108v1.mp3", "timestamp": "2025-06-02T21:19:01.274160"}
{"query": "Diffusion Model", "id": "2505.24210v1", "url": "http://arxiv.org/abs/2505.24210v1", "title": "STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow Matching Models", "summary": "Diffusion models (DMs) have demonstrated remarkable performance in\nhigh-fidelity image and video generation. Because high-quality generations with\nDMs typically require a large number of function evaluations (NFEs), resulting\nin slow sampling, there has been extensive research successfully reducing the\nNFE to a small range (<10) while maintaining acceptable image quality. However,\nmany practical applications, such as those involving Stable Diffusion 3.5,\nFLUX, and SANA, commonly operate in the mid-NFE regime (20-50 NFE) to achieve\nsuperior results, and, despite the practical relevance, research on the\neffective sampling within this mid-NFE regime remains underexplored. In this\nwork, we propose a novel, training-free, and structure-independent DM ODE\nsolver called the Stabilized Taylor Orthogonal Runge--Kutta (STORK) method,\nbased on a class of stiff ODE solvers with a Taylor expansion adaptation.\nUnlike prior work such as DPM-Solver, which is dependent on the semi-linear\nstructure of the DM ODE, STORK is applicable to any DM sampling, including\nnoise-based and flow matching-based models. Within the 20-50 NFE range, STORK\nachieves improved generation quality, as measured by FID scores, across\nunconditional pixel-level generation and conditional latent-space generation\ntasks using models like Stable Diffusion 3.5 and SANA. Code is available at\nhttps://github.com/ZT220501/STORK.", "authors": ["Zheng Tan", "Weizhen Wang", "Andrea L. Bertozzi", "Ernest K. Ryu"], "published_date": "2025-05-30", "title_zh": "STORK：提升擴散模型與流匹配模型中點NFE取樣的保真度", "summary_zh": "擴散模型在高保真圖像與影片生成方面表現卓越。由於高品質生成通常需大量函數評估（NFE），導致取樣緩慢，因此已有大量研究成功將 NFE 降至較小範圍（<10）並維持可接受的圖像品質。然而，許多實際應用，如 Stable Diffusion 3.5、FLUX 和 SANA，為獲得更佳效果，常在中等 NFE 範圍（20-50 NFE）運作，但針對此範圍的有效取樣研究仍不足。本文提出一種新穎、免訓練且結構獨立的擴散模型 ODE 求解器，名為穩定化泰勒正交龍格-庫塔（STORK）方法，其基於一類具泰勒展開適應性的剛性 ODE 求解器。與依賴擴散模型 ODE 半線性結構的 DPM-Solver 不同，STORK 適用於任何擴散模型取樣，包括基於雜訊與基於流匹配的模型。在 20-50 NFE 範圍內，STORK 在無條件像素級生成和使用 Stable Diffusion 3.5 和 SANA 等模型的條件潛在空間生成任務中，實現了更高的生成品質，以 FID 分數衡量。", "audio": "audios/2505.24210v1.mp3", "timestamp": "2025-06-02T21:19:10.684997"}
{"query": "AI", "id": "2505.24523v1", "url": "http://arxiv.org/abs/2505.24523v1", "title": "Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors", "summary": "Recent advancements in Generative AI and Large Language Models (LLMs) have\nenabled the creation of highly realistic synthetic content, raising concerns\nabout the potential for malicious use, such as misinformation and manipulation.\nMoreover, detecting Machine-Generated Text (MGT) remains challenging due to the\nlack of robust benchmarks that assess generalization to real-world scenarios.\nIn this work, we present a pipeline to test the resilience of state-of-the-art\nMGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed\nadversarial attacks. To challenge the detectors, we fine-tune language models\nusing Direct Preference Optimization (DPO) to shift the MGT style toward\nhuman-written text (HWT). This exploits the detectors' reliance on stylistic\nclues, making new generations more challenging to detect. Additionally, we\nanalyze the linguistic shifts induced by the alignment and which features are\nused by detectors to detect MGT texts. Our results show that detectors can be\neasily fooled with relatively few examples, resulting in a significant drop in\ndetection performance. This highlights the importance of improving detection\nmethods and making them robust to unseen in-domain texts.", "authors": ["Andrea Pedrotti", "Michele Papucci", "Cristiano Ciaccio", "Alessio Miaschi", "Giovanni Puccetti", "Felice Dell'Orletta", "Andrea Esuli"], "published_date": "2025-05-30", "title_zh": "壓力測試機器生成文本偵測：轉變語言模型寫作風格以欺騙偵測器", "summary_zh": "生成式AI與大型語言模型的進展催生了高度逼真的合成內容，引發了對惡意使用的擔憂。由於缺乏評估真實場景泛化能力的穩健基準，檢測機器生成文本(MGT)仍然具有挑戰性。本研究提出一個流程，以測試最先進的MGT檢測器對語言學誘導對抗性攻擊的抵抗力。通過使用直接偏好優化(DPO)微調語言模型，使MGT風格向人類書寫文本(HWT)靠攏，挑戰檢測器。這利用了檢測器對風格線索的依賴，使新生成的文本更難檢測。此外，分析了對齊引起的語言轉變以及檢測器用於檢測MGT文本的特徵。結果表明，檢測器很容易被少量範例欺騙，導致檢測性能顯著下降。這突顯了改進檢測方法並使其對未見領域內文本具有魯棒性的重要性。", "audio": "audios/2505.24523v1.mp3", "timestamp": "2025-06-02T22:18:22.270441"}
{"query": "Foundation Model", "id": "2505.24103v1", "url": "http://arxiv.org/abs/2505.24103v1", "title": "Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors", "summary": "In this work, we focus on the task of weakly supervised affordance grounding,\nwhere a model is trained to identify affordance regions on objects using\nhuman-object interaction images and egocentric object images without dense\nlabels. Previous works are mostly built upon class activation maps, which are\neffective for semantic segmentation but may not be suitable for locating\nactions and functions. Leveraging recent advanced foundation models, we develop\na supervised training pipeline based on pseudo labels. The pseudo labels are\ngenerated from an off-the-shelf part segmentation model, guided by a mapping\nfrom affordance to part names. Furthermore, we introduce three key enhancements\nto the baseline model: a label refining stage, a fine-grained feature alignment\nprocess, and a lightweight reasoning module. These techniques harness the\nsemantic knowledge of static objects embedded in off-the-shelf foundation\nmodels to improve affordance learning, effectively bridging the gap between\nobjects and actions. Extensive experiments demonstrate that the performance of\nthe proposed model has achieved a breakthrough improvement over existing\nmethods. Our codes are available at https://github.com/woyut/WSAG-PLSP .", "authors": ["Peiran Xu", "Yadong Mu"], "published_date": "2025-05-30", "title_zh": "基於部件級語義先驗的弱監督可供性基礎定位", "summary_zh": "本研究探討弱監督可供性定位任務，旨在使用人機互動圖像和主視角物體圖像訓練模型，以識別物體上的可供性區域，無需密集標籤。 鑒於先前基於類別激活圖的方法不適用於定位動作和功能，本文利用先進基礎模型，開發基於偽標籤的監督訓練流程。 偽標籤由現成的部件分割模型生成，並通過可供性到部件名稱的映射進行引導。 此外，我們引入標籤細化階段、細粒度特徵對齊過程和輕量級推理模塊，增強基線模型，利用嵌入在現成基礎模型中的靜態物體語義知識來改善可供性學習，有效彌合物體和動作之間的差距。 大量實驗表明，所提出模型的性能相較於現有方法取得了突破性進展。", "audio": "audios/2505.24103v1.mp3", "timestamp": "2025-06-02T22:18:34.501711"}
{"query": "Diffusion Model", "id": "2505.24203v1", "url": "http://arxiv.org/abs/2505.24203v1", "title": "Aligning Protein Conformation Ensemble Generation with Physical Feedback", "summary": "Protein dynamics play a crucial role in protein biological functions and\nproperties, and their traditional study typically relies on time-consuming\nmolecular dynamics (MD) simulations conducted in silico. Recent advances in\ngenerative modeling, particularly denoising diffusion models, have enabled\nefficient accurate protein structure prediction and conformation sampling by\nlearning distributions over crystallographic structures. However, effectively\nintegrating physical supervision into these data-driven approaches remains\nchallenging, as standard energy-based objectives often lead to intractable\noptimization. In this paper, we introduce Energy-based Alignment (EBA), a\nmethod that aligns generative models with feedback from physical models,\nefficiently calibrating them to appropriately balance conformational states\nbased on their energy differences. Experimental results on the MD ensemble\nbenchmark demonstrate that EBA achieves state-of-the-art performance in\ngenerating high-quality protein ensembles. By improving the physical\nplausibility of generated structures, our approach enhances model predictions\nand holds promise for applications in structural biology and drug discovery.", "authors": ["Jiarui Lu", "Xiaoyin Chen", "Stephen Zhewen Lu", "Aurélie Lozano", "Vijil Chenthamarakshan", "Payel Das", "Jian Tang"], "published_date": "2025-05-30", "title_zh": "基於物理回饋的蛋白質構象系綜生成對齊", "summary_zh": "蛋白質動態在生物功能與特性中至關重要，傳統研究仰賴耗時的電腦分子動力學模擬。生成模型的最新進展，特別是去噪擴散模型，透過學習晶體結構分佈，實現了高效準確的蛋白質結構預測與構象取樣。然而，將物理監督有效整合至這些數據驅動方法中仍具挑戰，因為標準的基於能量目標通常導致難以處理的優化問題。本文提出基於能量對齊(EBA)方法，其將生成模型與物理模型的回饋對齊，有效校準模型，基於能量差異適當平衡構象狀態。在分子動力學系綜基準測試中的實驗結果表明，EBA在生成高品質蛋白質系綜方面達到最先進的性能。透過提高生成結構的物理合理性，本方法增強模型預測能力，並在結構生物學和藥物發現應用中具有潛力。", "audio": "audios/2505.24203v1.mp3", "timestamp": "2025-06-02T22:18:40.634206"}
{"query": "AI", "id": "2505.24513v1", "url": "http://arxiv.org/abs/2505.24513v1", "title": "Airborne Neural Network", "summary": "Deep Learning, driven by neural networks, has led to groundbreaking\nadvancements in Artificial Intelligence by enabling systems to learn and adapt\nlike the human brain. These models have achieved remarkable results,\nparticularly in data-intensive domains, supported by massive computational\ninfrastructure. However, deploying such systems in Aerospace, where real time\ndata processing and ultra low latency are critical, remains a challenge due to\ninfrastructure limitations. This paper proposes a novel concept: the Airborne\nNeural Network a distributed architecture where multiple airborne devices each\nhost a subset of neural network neurons. These devices compute collaboratively,\nguided by an airborne network controller and layer specific controllers,\nenabling real-time learning and inference during flight. This approach has the\npotential to revolutionize Aerospace applications, including airborne air\ntraffic control, real-time weather and geographical predictions, and dynamic\ngeospatial data processing. By enabling large-scale neural network operations\nin airborne environments, this work lays the foundation for the next generation\nof AI powered Aerospace systems.", "authors": ["Paritosh Ranjan", "Surajit Majumder", "Prodip Roy"], "published_date": "2025-05-30", "title_zh": "機載神經網路", "summary_zh": "深度學習藉由類神經網路推動人工智慧發展，使系統具備如人腦般的學習與適應能力。此類模型在資料密集領域表現卓越，仰賴龐大運算基礎設施。然而，在航空航天等需要即時資料處理和超低延遲的領域中，由於基礎設施限制，部署此類系統仍具挑戰。本文提出一種新穎概念：機載神經網路，一種分散式架構，其中多個機載設備各自承載神經網路神經元的子集。這些設備在機載網路控制器和分層控制器的引導下協同運算，實現飛行期間的即時學習與推論。此方法有望革新航空航天應用，包括機載空中交通管制、即時天氣與地理預測以及動態地理空間資料處理。藉由在機載環境中實現大規模神經網路運算，本研究為下一代人工智慧驅動的航空航天系統奠定基礎。", "audio": "audios/2505.24513v1.mp3", "timestamp": "2025-06-02T23:18:33.661949"}
{"query": "Foundation Model", "id": "2505.24088v1", "url": "http://arxiv.org/abs/2505.24088v1", "title": "Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting", "summary": "Vision foundation models pre-trained on massive data encode rich\nrepresentations of real-world concepts, which can be adapted to downstream\ntasks by fine-tuning. However, fine-tuning foundation models on one task often\nleads to the issue of concept forgetting on other tasks. Recent methods of\nrobust fine-tuning aim to mitigate forgetting of prior knowledge without\naffecting the fine-tuning performance. Knowledge is often preserved by matching\nthe original and fine-tuned model weights or feature pairs. However, such\npoint-wise matching can be too strong, without explicit awareness of the\nfeature neighborhood structures that encode rich knowledge as well. We propose\na novel regularization method Proxy-FDA that explicitly preserves the\nstructural knowledge in feature space. Proxy-FDA performs Feature Distribution\nAlignment (using nearest neighbor graphs) between the pre-trained and\nfine-tuned feature spaces, and the alignment is further improved by informative\nproxies that are generated dynamically to increase data diversity. Experiments\nshow that Proxy-FDA significantly reduces concept forgetting during\nfine-tuning, and we find a strong correlation between forgetting and a\ndistributional distance metric (in comparison to L2 distance). We further\ndemonstrate Proxy-FDA's benefits in various fine-tuning settings (end-to-end,\nfew-shot and continual tuning) and across different tasks like image\nclassification, captioning and VQA.", "authors": ["Chen Huang", "Skyler Seto", "Hadi Pouransari", "Mehrdad Farajtabar", "Raviteja Vemulapalli", "Fartash Faghri", "Oncel Tuzel", "Barry-John Theobald", "Josh Susskind"], "published_date": "2025-05-30", "title_zh": "代理FDA：基於代理特徵分佈對齊的視覺基礎模型微調方法，避免災難性遺忘", "summary_zh": "視覺基礎模型透過大規模資料預訓練，編碼了豐富的真實世界概念，可經由微調適應下游任務。然而，在單一任務上微調基礎模型常導致其他任務的概念遺忘。近期的穩健微調方法旨在減輕先前知識的遺忘，同時不影響微調效能。知識保存通常透過匹配原始模型與微調模型的權重或特徵對來實現。然而，此類逐點匹配可能過於強烈，缺乏對編碼豐富知識的特徵鄰域結構的明確感知。我們提出一種新穎的正規化方法Proxy-FDA，它明確地保存特徵空間中的結構知識。Proxy-FDA執行預訓練特徵空間和微調特徵空間之間的特徵分佈對齊(使用最近鄰圖)，並透過動態生成的資訊代理來提高對齊效果，從而增加資料多樣性。實驗表明，Proxy-FDA顯著減少了微調期間的概念遺忘，並且我們發現遺忘與分佈距離度量(相較於L2距離)之間存在很強的相關性。我們進一步展示了Proxy-FDA在各種微調設定(端到端、少樣本和持續微調)以及跨不同任務(如圖像分類、圖像描述和視覺問答)中的優勢。", "audio": "audios/2505.24088v1.mp3", "timestamp": "2025-06-02T23:18:50.237421"}
{"query": "Diffusion Model", "id": "2505.24145v1", "url": "http://arxiv.org/abs/2505.24145v1", "title": "Autoregressive regularized score-based diffusion models for multi-scenarios fluid flow prediction", "summary": "Building on recent advances in scientific machine learning and generative\nmodeling for computational fluid dynamics, we propose a conditional score-based\ndiffusion model designed for multi-scenarios fluid flow prediction. Our model\nintegrates an energy constraint rooted in the statistical properties of\nturbulent flows, improving prediction quality with minimal training, while\nenabling efficient sampling at low cost. The method features a simple and\ngeneral architecture that requires no problem-specific design, supports\nplug-and-play enhancements, and enables fast and flexible solution generation.\nIt also demonstrates an efficient conditioning mechanism that simplifies\ntraining across different scenarios without demanding a redesign of existing\nmodels. We further explore various stochastic differential equation\nformulations to demonstrate how thoughtful design choices enhance performance.\nWe validate the proposed methodology through extensive experiments on complex\nfluid dynamics datasets encompassing a variety of flow regimes and\nconfigurations. Results demonstrate that our model consistently achieves\nstable, robust, and physically faithful predictions, even under challenging\nturbulent conditions. With properly tuned parameters, it achieves accurate\nresults across multiple scenarios while preserving key physical and statistical\nproperties. We present a comprehensive analysis of stochastic differential\nequation impact and discuss our approach across diverse fluid mechanics tasks.", "authors": ["Wilfried Genuist", "Éric Savin", "Filippo Gatti", "Didier Clouteau"], "published_date": "2025-05-30", "title_zh": "自迴歸正則化基於分數之擴散模型於多情境流體流動預測", "summary_zh": "基於科學機器學習和生成模型在計算流體力學的最新進展，我們提出了一種條件評分擴散模型，用於多場景流體流動預測。該模型整合了基於紊流統計特性的能量約束，以最小的訓練代價提高預測質量，同時實現低成本高效採樣。該方法架構簡單通用，無需針對特定問題進行設計，支持即插即用增強功能，並能快速靈活地生成解決方案。它還展示了一種高效的條件調節機制，簡化了跨不同場景的訓練，無需重新設計現有模型。我們進一步探索了多種隨機微分方程公式，以展示周全的設計選擇如何提升性能。通過對包含各種流動狀態和配置的複雜流體力學數據集進行大量實驗驗證了所提出的方法。結果表明，即使在具有挑戰性的紊流條件下，我們的模型也能始終如一地實現穩定、穩健且物理上忠實的預測。通過適當調整參數，它可以在多個場景中實現準確的結果，同時保留關鍵的物理和統計特性。我們全面分析了隨機微分方程的影響，並探討了我們在不同流體力學任務中的方法。", "audio": "audios/2505.24145v1.mp3", "timestamp": "2025-06-02T23:19:00.960760"}
{"query": "AI", "id": "2505.24785v2", "url": "http://arxiv.org/abs/2505.24785v2", "title": "EXP-Bench: Can AI Conduct AI Research Experiments?", "summary": "Automating AI research holds immense potential for accelerating scientific\nprogress, yet current AI agents struggle with the complexities of rigorous,\nend-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed\nto systematically evaluate AI agents on complete research experiments sourced\nfrom influential AI publications. Given a research question and incomplete\nstarter code, EXP-Bench challenges AI agents to formulate hypotheses, design\nand implement experimental procedures, execute them, and analyze results. To\nenable the creation of such intricate and authentic tasks with high-fidelity,\nwe design a semi-autonomous pipeline to extract and structure crucial\nexperimental details from these research papers and their associated\nopen-source code. With the pipeline, EXP-Bench curated 461 AI research tasks\nfrom 51 top-tier AI research papers. Evaluations of leading LLM-based agents,\nsuch as OpenHands and IterativeAgent on EXP-Bench demonstrate partial\ncapabilities: while scores on individual experimental aspects such as design or\nimplementation correctness occasionally reach 20-35%, the success rate for\ncomplete, executable experiments was a mere 0.5%. By identifying these\nbottlenecks and providing realistic step-by-step experiment procedures,\nEXP-Bench serves as a vital tool for future AI agents to improve their ability\nto conduct AI research experiments. EXP-Bench is open-sourced at\nhttps://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.", "authors": ["Patrick Tser Jern Kon", "Jiachen Liu", "Xinyi Zhu", "Qiuyi Ding", "Jingjia Peng", "Jiarong Xing", "Yibo Huang", "Yiming Qiu", "Jayanth Srinivasa", "Myungjin Lee", "Mosharaf Chowdhury", "Matei Zaharia", "Ang Chen"], "published_date": "2025-05-30", "title_zh": "EXP-Bench：人工智慧能否進行人工智慧研究實驗？", "summary_zh": "自動化人工智慧研究具加速科學進展的潛力，但目前AI代理在嚴謹的端對端實驗中仍面臨挑戰。本文提出EXP-Bench，此基準旨在系統性評估AI代理執行完整研究實驗的能力，實驗內容源自具影響力的AI論文。EXP-Bench要求AI代理針對研究問題和不完整的起始程式碼，進行假設形成、實驗設計與實施、執行和結果分析。為建立高保真度的複雜任務，我們設計半自動化流程，從研究論文和開源程式碼中提取關鍵實驗細節。EXP-Bench利用此流程，從51篇頂級AI研究論文中整理出461個AI研究任務。對OpenHands和IterativeAgent等LLM代理的評估表明，它們僅具備部分能力：個別實驗方面（如設計或實施的正確性）得分偶爾達到20-35%，但完整可執行實驗的成功率僅為0.5%。EXP-Bench透過識別瓶頸並提供實際逐步實驗程序，能有效提升AI代理執行AI研究實驗的能力。EXP-Bench已開源於https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench。", "audio": "audios/2505.24785v2.mp3", "timestamp": "2025-06-03T01:28:06.763198"}
{"query": "Foundation Model", "id": "2505.24030v1", "url": "http://arxiv.org/abs/2505.24030v1", "title": "From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?", "summary": "Transformer-based models have gained increasing attention in time series\nresearch, driving interest in Large Language Models (LLMs) and foundation\nmodels for time series analysis. As the field moves toward multi-modality,\nLarge Vision Models (LVMs) are emerging as a promising direction. In the past,\nthe effectiveness of Transformer and LLMs in time series has been debated. When\nit comes to LVMs, a similar question arises: are LVMs truely useful for time\nseries analysis? To address it, we design and conduct the first principled\nstudy involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across\nboth high-level (classification) and low-level (forecasting) tasks, with\nextensive ablation analysis. Our findings indicate LVMs are indeed useful for\ntime series classification but face challenges in forecasting. Although\neffective, the contemporary best LVM forecasters are limited to specific types\nof LVMs and imaging methods, exhibit a bias toward forecasting periods, and\nhave limited ability to utilize long look-back windows. We hope our findings\ncould serve as a cornerstone for future research on LVM- and multimodal-based\nsolutions to different time series tasks.", "authors": ["Ziming Zhao", "ChengAo Shen", "Hanghang Tong", "Dongjin Song", "Zhigang Deng", "Qingsong Wen", "Jingchao Ni"], "published_date": "2025-05-29", "title_zh": "從圖像至信號：大型視覺模型適用於時間序列分析嗎？", "summary_zh": "基於Transformer的模型在時間序列研究中備受關注，推動了大型語言模型(LLM)和基礎模型在時間序列分析中的應用。隨著領域朝向多模態發展，大型視覺模型(LVM)正成為一個有前景的方向。過去，Transformer和LLM在時間序列中的有效性備受爭議。對於LVM，也存在類似的問題：LVM是否真正適用於時間序列分析？為了解決這個問題，我們設計並進行了首個有原則的研究，涉及4個LVM、8種成像方法、18個數據集和26個基準，涵蓋高階（分類）和低階（預測）任務，並進行了廣泛的消融分析。我們的研究結果表明，LVM確實適用於時間序列分類，但在預測方面面臨挑戰。儘管有效，但當前最佳的LVM預測器僅限於特定類型的LVM和成像方法，對預測週期表現出偏差，並且利用長回溯窗口的能力有限。我們希望我們的研究結果可以作為未來研究基於LVM和多模態解決方案來處理不同時間序列任務的基石。", "audio": "audios/2505.24030v1.mp3", "timestamp": "2025-06-03T01:28:14.643375"}
{"query": "Diffusion Model", "id": "2505.24079v1", "url": "http://arxiv.org/abs/2505.24079v1", "title": "Principal Context-aware Diffusion Guided Data Augmentation for Fault Localization", "summary": "Test cases are indispensable for conducting effective fault localization\n(FL). However, test cases in practice are severely class imbalanced, i.e. the\nnumber of failing test cases (i.e. minority class) is much less than that of\npassing ones (i.e. majority class). The severe class imbalance between failing\nand passing test cases have hindered the FL effectiveness.\n  To address this issue, we propose PCD-DAug: a Principal Context-aware\nDiffusion guided Data Augmentation approach that generate synthesized failing\ntest cases for improving FL. PCD-DAug first combines program slicing with\nprincipal component analysis to construct a principal context that shows how a\nset of statements influences the faulty output via statistical program\ndependencies. Then, PCD-DAug devises a conditional diffusion model to learn\nfrom principle contexts for generating synthesized failing test cases and\nacquiring a class balanced dataset for FL. We conducted large-scale experiments\non six state-of-the-art FL approaches and compare PCD-DAug with six data\naugmentation baselines. The results show that PCD-DAug significantly improves\nFL effectiveness, e.g. achieving average improvements of 383.83%, 227.08%, and\n224.19% in six FL approaches under the metrics Top-1, Top-3, and Top-5,\nrespectively.", "authors": ["Shihao Fu", "Yan Lei"], "published_date": "2025-05-29", "title_zh": "主成分脈絡感知擴散引導之容錯定位數據增強", "summary_zh": "測試案例對於有效的錯誤定位至關重要。然而，實際應用中測試案例存在嚴重的類別不平衡問題，即失敗案例遠少於通過案例，這阻礙了錯誤定位的效能。為了解決此問題，我們提出PCD-DAug，一種基於主成分的感知上下文擴散引導數據增強方法，旨在生成合成失敗案例以提升錯誤定位效果。PCD-DAug首先結合程式切片與主成分分析，構建主成分上下文，展示語句集合如何透過統計程式依賴性影響錯誤輸出。隨後，PCD-DAug設計條件擴散模型，從主成分上下文中學習，生成合成失敗案例，為錯誤定位獲取類別平衡的資料集。大規模實驗結果表明，PCD-DAug顯著提升錯誤定位效能，在Top-1、Top-3和Top-5指標下，六種先進的錯誤定位方法平均提升分別為383.83%、227.08%和224.19%。", "audio": "audios/2505.24079v1.mp3", "timestamp": "2025-06-03T01:28:21.965542"}
{"query": "AI", "id": "2505.24584v2", "url": "http://arxiv.org/abs/2505.24584v2", "title": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams", "summary": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment.", "authors": ["Sakhinana Sagar Srinivas", "Shivam Gupta", "Venkataramana Runkana"], "published_date": "2025-05-30", "title_zh": "AutoChemSchematic AI：用於自動生成化學流程及儀錶圖的閉環、物理感知代理框架", "summary_zh": "生成式AI加速了新化學品及材料的發現，但將這些發現轉化為工業規模生產仍是瓶頸，因需開發全新製程。現有AI方法無法自動生成製程流程圖(PFD)或管線儀表圖(PID)，而這些圖對於化學製程擴展至關重要。本文提出一閉環、具備物理認知的架構，以自動生成工業可行的PFD及PID。該架構整合了領域專精的小型語言模型(SLM)(經化學製程QA任務訓練)與第一原理模擬，利用三個關鍵元件：(1)包含1020多種化學品的製程流程與儀表描述之階層式知識圖譜，(2)一多階段訓練流程，透過監督式微調(SFT)、直接偏好優化(DPO)和檢索增強指令調整(RAIT)在合成數據集上微調領域專精的SLM，(3)基於DWSIM模擬器的迴圈驗證，以確保可行性。為提高執行效率和模型精簡度，架構整合了先進的推理時最佳化，包括FlashAttention、Lookahead Decoding、PagedAttention與KV快取量化、測試時推理縮放，並獨立應用結構剪枝技術(寬度和深度)，藉由重要性啟發法減少模型大小，同時最小化準確度損失。實驗證明，此架構能高保真度地生成經模擬器驗證的製程描述，在正確性方面優於基準方法，並能推廣至未見過的化學品。藉由橋接AI驅動設計與工業規模可行性，此研究顯著縮短了從實驗室發現到工廠部署的研發時程。", "audio": "audios/2505.24584v2.mp3", "timestamp": "2025-06-03T03:15:38.423464"}
{"query": "Foundation Model", "id": "2505.24025v1", "url": "http://arxiv.org/abs/2505.24025v1", "title": "DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models", "summary": "The recent explosive interest in the reasoning capabilities of large language\nmodels, such as DeepSeek-R1, has demonstrated remarkable success through\nreinforcement learning-based fine-tuning frameworks, exemplified by methods\nlike Group Relative Policy Optimization (GRPO). However, such reasoning\nabilities remain underexplored and notably absent in vision foundation models,\nincluding representation models like the DINO series. In this work, we propose\n\\textbf{DINO-R1}, the first such attempt to incentivize visual in-context\nreasoning capabilities of vision foundation models using reinforcement\nlearning. Specifically, DINO-R1 introduces \\textbf{Group Relative Query\nOptimization (GRQO)}, a novel reinforcement-style training strategy explicitly\ndesigned for query-based representation models, which computes query-level\nrewards based on group-normalized alignment quality. We also apply\nKL-regularization to stabilize the objectness distribution to reduce the\ntraining instability. This joint optimization enables dense and expressive\nsupervision across queries while mitigating overfitting and distributional\ndrift. Building upon Grounding-DINO, we train a series of DINO-R1 family models\nthat integrate a visual prompt encoder and a visual-guided query selection\nmechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that\nDINO-R1 significantly outperforms supervised fine-tuning baselines, achieving\nstrong generalization in both open-vocabulary and closed-set visual prompting\nscenarios.", "authors": ["Chenbin Pan", "Wenbin He", "Zhengzhong Tu", "Liu Ren"], "published_date": "2025-05-29", "title_zh": "DINO-R1：激勵視覺基礎模型中的推理能力", "summary_zh": "鑒於大型語言模型（如DeepSeek-R1）在推理能力上的顯著進展，本研究提出DINO-R1，首次嘗試利用強化學習激勵視覺基礎模型的視覺上下文推理能力。DINO-R1引入群組相對查詢優化（GRQO），一種專為基於查詢的表示模型設計的強化學習訓練策略，基於群組歸一化對齊品質計算查詢級別獎勵。同時，應用KL正則化穩定物件分佈，減少訓練不穩定性。此聯合優化實現跨查詢的密集且具表現力的監督，同時減輕過擬合和分佈漂移。基於Grounding-DINO，訓練了一系列DINO-R1模型，整合了視覺提示編碼器和視覺引導查詢選擇機制。在COCO、LVIS和ODinW上的實驗表明，DINO-R1顯著優於有監督微調基準線，在開放詞彙和封閉集視覺提示場景中均實現了強大的泛化能力。", "audio": "audios/2505.24025v1.mp3", "timestamp": "2025-06-03T03:15:45.260003"}
{"query": "Diffusion Model", "id": "2505.24061v1", "url": "http://arxiv.org/abs/2505.24061v1", "title": "Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning", "summary": "Deep reinforcement learning (RL) agents frequently suffer from neuronal\nactivity loss, which impairs their ability to adapt to new data and learn\ncontinually. A common method to quantify and address this issue is the\ntau-dormant neuron ratio, which uses activation statistics to measure the\nexpressive ability of neurons. While effective for simple MLP-based agents,\nthis approach loses statistical power in more complex architectures. To address\nthis, we argue that in advanced RL agents, maintaining a neuron's learning\ncapacity, its ability to adapt via gradient updates, is more critical than\npreserving its expressive ability. Based on this insight, we shift the\nstatistical objective from activations to gradients, and introduce GraMa\n(Gradient Magnitude Neural Activity Metric), a lightweight,\narchitecture-agnostic metric for quantifying neuron-level learning capacity. We\nshow that GraMa effectively reveals persistent neuron inactivity across diverse\narchitectures, including residual networks, diffusion models, and agents with\nvaried activation functions. Moreover, resetting neurons guided by GraMa\n(ReGraMa) consistently improves learning performance across multiple deep RL\nalgorithms and benchmarks, such as MuJoCo and the DeepMind Control Suite.", "authors": ["Jiashun Liu", "Zihao Wu", "Johan Obando-Ceron", "Pablo Samuel Castro", "Aaron Courville", "Ling Pan"], "published_date": "2025-05-29", "title_zh": "量測梯度，而非激活！強化深度強化學習中的神經元活動", "summary_zh": "深度強化學習代理常遭遇神經元活動喪失，影響其適應新數據與持續學習能力。常用方法為τ休眠神經元比例，透過激活統計量測神經元表達能力。此方法雖適用於簡單基於多層感知器的代理，但在更複雜架構中失去統計效力。我們認為，在進階強化學習代理中，維持神經元的學習容量，即透過梯度更新適應的能力，比保存其表達能力更重要。因此，我們將統計目標從激活轉向梯度，並引入GraMa (梯度幅度神經活動度量)，一種輕量級、架構無關的度量，用於量化神經元層級的學習容量。實驗顯示，GraMa能有效揭示多種架構(包含殘差網路、擴散模型及具備不同激活函數的代理)中持續的神經元不活動。此外，根據GraMa重置神經元(ReGraMa)能持續改善多種深度強化學習算法及基準測試(如MuJoCo和DeepMind Control Suite)中的學習效能。", "audio": "audios/2505.24061v1.mp3", "timestamp": "2025-06-03T03:15:54.855881"}
{"query": "AI", "id": "2505.24510v1", "url": "http://arxiv.org/abs/2505.24510v1", "title": "How can AI reduce wrist injuries in the workplace?", "summary": "This paper explores the development of a control and sensor strategy for an\nindustrial wearable wrist exoskeleton by classifying and predicting workers'\nactions. The study evaluates the correlation between exerted force and effort\nintensity, along with sensor strategy optimization, for designing purposes.\nUsing data from six healthy subjects in a manufacturing plant, this paper\npresents EMG-based models for wrist motion classification and force prediction.\nWrist motion recognition is achieved through a pattern recognition algorithm\ndeveloped with surface EMG data from an 8-channel EMG sensor (Myo Armband);\nwhile a force regression model uses wrist and hand force measurements from a\ncommercial handheld dynamometer (Vernier GoDirect Hand Dynamometer). This\ncontrol strategy forms the foundation for a streamlined exoskeleton\narchitecture designed for industrial applications, focusing on simplicity,\nreduced costs, and minimal sensor use while ensuring reliable and effective\nassistance.", "authors": ["Roberto F. Pitzalis", "Nicholas Cartocci", "Christian Di Natali", "Darwin G. Caldwell", "Giovanni Berselli", "Jesús Ortiz"], "published_date": "2025-05-30", "title_zh": "人工智慧如何降低工作場所腕部損傷？", "summary_zh": "本研究探討工業用穿戴式腕部外骨骼的控制與感測策略，透過分類和預測工人動作，評估施力與努力程度的相關性，並優化感測策略以利設計。利用來自製造工廠六名健康受試者的數據，提出了基於肌電圖的腕部運動分類和力量預測模型。腕部運動識別採用基於表面肌電圖數據（8通道肌電感測器）的模式識別算法；力量迴歸模型則使用商用手持式測力計測量的腕部和手部力量。此控制策略為簡化外骨骼架構奠定基礎，旨在實現工業應用，著重簡潔性、降低成本、減少感測器使用，同時確保可靠有效的輔助。", "audio": "audios/2505.24510v1.mp3", "timestamp": "2025-06-03T04:25:53.034267"}
{"query": "Foundation Model", "id": "2505.23913v1", "url": "http://arxiv.org/abs/2505.23913v1", "title": "Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling", "summary": "The optimization of expensive black-box functions is ubiquitous in science\nand engineering. A common solution to this problem is Bayesian optimization\n(BO), which is generally comprised of two components: (i) a surrogate model and\n(ii) an acquisition function, which generally require expensive re-training and\noptimization steps at each iteration, respectively. Although recent work\nenabled in-context surrogate models that do not require re-training, virtually\nall existing BO methods still require acquisition function maximization to\nselect the next observation, which introduces many knobs to tune, such as Monte\nCarlo samplers and multi-start optimizers. In this work, we propose a\ncompletely in-context, zero-shot solution for BO that does not require\nsurrogate fitting or acquisition function optimization. This is done by using a\npre-trained deep generative model to directly sample from the posterior over\nthe optimum point. We show that this process is equivalent to Thompson sampling\nand demonstrate the capabilities and cost-effectiveness of our foundation model\non a suite of real-world benchmarks. We achieve an efficiency gain of more than\n35x in terms of wall-clock time when compared with Gaussian process-based BO,\nenabling efficient parallel and distributed BO, e.g., for high-throughput\noptimization.", "authors": ["Gustavo Sutter Pessurno de Carvalho", "Mohammed Abdulrahman", "Hao Wang", "Sriram Ganapathi Subramanian", "Marc St-Aubin", "Sharon O'Sullivan", "Lawrence Wan", "Luis Ricardez-Sandoval", "Pascal Poupart", "Agustinus Kristiadi"], "published_date": "2025-05-29", "title_zh": "基於上下文直接最優採樣簡化貝葉斯優化", "summary_zh": "昂貴黑箱函數最佳化在科學與工程領域普遍存在。貝氏最佳化(BO)是常見解決方案，通常包含替代模型和獲取函數。傳統方法每次迭代都需要耗時的重新訓練和最佳化步驟。近期研究雖實現無需重新訓練的上下文替代模型，但現有BO方法仍需最大化獲取函數以選擇下一個觀測點，引入諸多需調整的參數。本文提出一種完全上下文、零樣本的BO解決方案，無需擬合替代模型或最佳化獲取函數。透過預訓練的深度生成模型，直接從最佳點的後驗機率分布中採樣。結果顯示此過程等同於湯普森採樣，並在一系列真實基準測試中驗證了基礎模型的能力和成本效益。相較於基於高斯過程的BO，本文方法在實際耗時方面效率提升超過35倍，實現高效並行與分散式BO，適用於高通量最佳化。", "audio": "audios/2505.23913v1.mp3", "timestamp": "2025-06-03T04:26:00.508679"}
{"query": "Diffusion Model", "id": "2505.23886v1", "url": "http://arxiv.org/abs/2505.23886v1", "title": "Generating Fit Check Videos with a Handheld Camera", "summary": "Self-captured full-body videos are popular, but most deployments require\nmounted cameras, carefully-framed shots, and repeated practice. We propose a\nmore convenient solution that enables full-body video capture using handheld\nmobile devices. Our approach takes as input two static photos (front and back)\nof you in a mirror, along with an IMU motion reference that you perform while\nholding your mobile phone, and synthesizes a realistic video of you performing\na similar target motion. We enable rendering into a new scene, with consistent\nillumination and shadows. We propose a novel video diffusion-based model to\nachieve this. Specifically, we propose a parameter-free frame generation\nstrategy, as well as a multi-reference attention mechanism, that effectively\nintegrate appearance information from both the front and back selfies into the\nvideo diffusion model. Additionally, we introduce an image-based fine-tuning\nstrategy to enhance frame sharpness and improve the generation of shadows and\nreflections, achieving a more realistic human-scene composition.", "authors": ["Bowei Chen", "Brian Curless", "Ira Kemelmacher-Shlizerman", "Steven M. Seitz"], "published_date": "2025-05-29", "title_zh": "使用手持相機生成合身度檢查影片", "summary_zh": "本研究提出一種便捷的全身影像捕捉方案，利用手持行動裝置即可完成。該方案僅需使用者於鏡中拍攝前後兩張靜態照片，並同時錄製手持手機的IMU運動參考數據，即可合成使用者執行相似目標動作的逼真影片。此方案支援將影片渲染至新場景，並保持光照與陰影的一致性。為此，我們提出一種基於擴散模型的影片生成方法，採用免參數的影格生成策略，以及多參考注意力機制，有效地將前後自拍照的外觀資訊整合至影片擴散模型中。此外，我們引入基於影像的微調策略，以提升影格清晰度，並改善陰影與反射的生成，從而實現更真實的人景合成效果。", "audio": "audios/2505.23886v1.mp3", "timestamp": "2025-06-03T04:26:06.046359"}
{"query": "AI", "id": "2505.24507v1", "url": "http://arxiv.org/abs/2505.24507v1", "title": "How can AI reduce fall injuries in the workplace?", "summary": "Fall-caused injuries are common in all types of work environments, including\noffices. They are the main cause of absences longer than three days, especially\nfor small and medium-sized businesses (SMEs). However, data, data amount, data\nheterogeneity, and stringent processing time constraints continue to pose\nchallenges to real-time fall detection. This work proposes a new approach based\non a recurrent neural network (RNN) for Fall Detection and a Kolmogorov-Arnold\nNetwork (KAN) to estimate the time of impact of the fall. The approach is\ntested on SisFall, a dataset consisting of 2706 Activities of Daily Living\n(ADLs) and 1798 falls recorded by three sensors. The results show that the\nproposed approach achieves an average TPR of 82.6% and TNR of 98.4% for fall\nsequences and 94.4% in ADL. Besides, the Root Mean Squared Error of the\nestimated time of impact is approximately 160ms.", "authors": ["Nicholas Cartocci", "Antonios E. Gkikakis", "Roberto F. Pitzalis", "Fabio Pera", "Maria Teresa Settino", "Darwin G. Caldwell", "Jesús Ortiz"], "published_date": "2025-05-30", "title_zh": "人工智慧如何減少工作場所的跌倒傷害？", "summary_zh": "跌倒造成的傷害在各種工作環境中都很常見，尤其在中小型企業中是導致三天以上缺勤的主要原因。然而，數據量、異質性以及嚴格的處理時限對即時跌倒偵測構成挑戰。本研究提出一種基於遞迴神經網路(RNN)的跌倒偵測方法，並利用Kolmogorov-Arnold網路(KAN)估計跌倒撞擊時間。該方法在SisFall資料集上進行測試，該資料集包含2706個日常生活活動(ADL)和1798個跌倒事件，由三個感測器記錄。結果顯示，該方法在跌倒序列的平均真陽性率(TPR)為82.6%，真陰性率(TNR)為98.4%，在ADL中的TPR為94.4%。此外，估計撞擊時間的均方根誤差約為160毫秒。", "audio": "audios/2505.24507v1.mp3", "timestamp": "2025-06-03T05:20:50.050269"}
{"query": "Foundation Model", "id": "2505.23883v1", "url": "http://arxiv.org/abs/2505.23883v1", "title": "BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning", "summary": "Foundation models trained at scale exhibit remarkable emergent behaviors,\nlearning new capabilities beyond their initial training objectives. We find\nsuch emergent behaviors in biological vision models via large-scale contrastive\nvision-language training. To achieve this, we first curate TreeOfLife-200M,\ncomprising 214 million images of living organisms, the largest and most diverse\nbiological organism image dataset to date. We then train BioCLIP 2 on\nTreeOfLife-200M to distinguish different species. Despite the narrow training\nobjective, BioCLIP 2 yields extraordinary accuracy when applied to various\nbiological visual tasks such as habitat classification and trait prediction. We\nidentify emergent properties in the learned embedding space of BioCLIP 2. At\nthe inter-species level, the embedding distribution of different species aligns\nclosely with functional and ecological meanings (e.g., beak sizes and\nhabitats). At the intra-species level, instead of being diminished, the\nintra-species variations (e.g., life stages and sexes) are preserved and better\nseparated in subspaces orthogonal to inter-species distinctions. We provide\nformal proof and analyses to explain why hierarchical supervision and\ncontrastive objectives encourage these emergent properties. Crucially, our\nresults reveal that these properties become increasingly significant with\nlarger-scale training data, leading to a biologically meaningful embedding\nspace.", "authors": ["Jianyang Gu", "Samuel Stevens", "Elizabeth G Campolongo", "Matthew J Thompson", "Net Zhang", "Jiaman Wu", "Andrei Kopanev", "Zheda Mai", "Alexander E. White", "James Balhoff", "Wasila Dahdul", "Daniel Rubenstein", "Hilmar Lapp", "Tanya Berger-Wolf", "Wei-Lun Chao", "Yu Su"], "published_date": "2025-05-29", "title_zh": "BioCLIP 2：層次對比學習擴展中的湧現特性", "summary_zh": "大規模訓練的基礎模型展現卓越的湧現行為，習得超出初始訓練目標的新能力。本研究透過大規模對比視覺語言訓練，在生物視覺模型中發現此類湧現行為。我們創建了包含2.14億張生物圖像的TreeOfLife-200M數據集，隨後訓練BioCLIP 2以區分物種。儘管訓練目標狹窄，BioCLIP 2在棲息地分類和特徵預測等生物視覺任務中表現出非凡的準確性。研究識別出BioCLIP 2學習嵌入空間中的湧現特性，物種間嵌入分佈與功能和生態意義高度一致，物種內變異在與物種間區分正交的子空間中被保留並更好地區分。我們提供正式證明和分析，解釋分層監督和對比目標如何促進這些湧現特性，且大規模訓練數據使這些特性更加顯著，從而產生具有生物學意義的嵌入空間。", "audio": "audios/2505.23883v1.mp3", "timestamp": "2025-06-03T05:20:55.801802"}
{"query": "Diffusion Model", "id": "2505.23884v1", "url": "http://arxiv.org/abs/2505.23884v1", "title": "Test-Time Training Done Right", "summary": "Test-Time Training (TTT) models context dependencies by adapting part of the\nmodel's weights (referred to as fast weights) during inference. This fast\nweight, akin to recurrent states in RNNs, stores temporary memories of past\ntokens in the current sequence. Existing TTT methods struggled to show\neffectiveness in handling long-context data, due to their inefficiency on\nmodern GPUs. The TTT layers in many of these approaches operate with extremely\nlow FLOPs utilization (often <5%) because they deliberately apply small online\nminibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover,\na small minibatch implies fine-grained block-wise causal dependencies in the\ndata, unsuitable for data beyond 1D ordered sequences, like sets or\nN-dimensional grids such as images or videos. In contrast, we pursue the\nopposite direction by using an extremely large chunk update, ranging from 2K to\n1M tokens across tasks of varying modalities, which we refer to as Large Chunk\nTest-Time Training (LaCT). It improves hardware utilization by orders of\nmagnitude, and more importantly, facilitates scaling of nonlinear state size\n(up to 40% of model parameters), hence substantially improving state capacity,\nall without requiring cumbersome and error-prone kernel implementations. It\nalso allows easy integration of sophisticated optimizers, e.g. Muon for online\nupdates. We validate our approach across diverse modalities and tasks,\nincluding novel view synthesis with image set, language models, and\nauto-regressive video diffusion. Our approach can scale up to 14B-parameter AR\nvideo diffusion model on sequences up to 56K tokens. In our longest sequence\nexperiment, we perform novel view synthesis with 1 million context length. We\nhope this work will inspire and accelerate new research in the field of\nlong-context modeling and test-time training. Website:\nhttps://tianyuanzhang.com/projects/ttt-done-right", "authors": ["Tianyuan Zhang", "Sai Bi", "Yicong Hong", "Kai Zhang", "Fujun Luan", "Songlin Yang", "Kalyan Sunkavalli", "William T. Freeman", "Hao Tan"], "published_date": "2025-05-29", "title_zh": "正確的測試時訓練", "summary_zh": "測試時訓練（TTT）透過在推論期間調整模型部分權重（快速權重）來處理上下文依賴性。此快速權重類似於RNN中的遞迴狀態，儲存目前序列中過去符記的臨時記憶。現有TTT方法在處理長上下文資料時效果不彰，主因是其在現代GPU上的效率低下。許多方法中的TTT層運算量利用率極低（通常<5%），因為它們刻意採用小型的線上小批量（例如，每16或64個符記更新快速權重）。此外，小批量暗示了資料中細粒度的塊狀因果依賴性，不適用於1D有序序列以外的資料，例如集合或N維網格（如圖像或影片）。相較之下，我們採用相反方向，使用極大的塊更新，範圍從2K到1M個符記，涵蓋不同模態的任務，我們稱之為大塊測試時訓練（LaCT）。它將硬體利用率提高了數個數量級，更重要的是，促進了非線性狀態大小的擴展（高達模型參數的40%），從而大幅提高了狀態容量，且無需繁瑣且容易出錯的內核實現。它還允許輕鬆整合複雜的優化器，例如用於線上更新的Muon。我們在多種模態和任務中驗證了我們的方法，包括具有圖像集的新視圖合成、語言模型和自迴歸影片擴散。我們的方法可以擴展到具有高達56K符記序列的140億參數AR影片擴散模型。在我們最長的序列實驗中，我們進行了具有100萬上下文長度的新視圖合成。此研究有望啟發並加速長上下文建模和測試時訓練領域的新研究。", "audio": "audios/2505.23884v1.mp3", "timestamp": "2025-06-03T05:21:05.680212"}
{"query": "AI", "id": "2505.24479v1", "url": "http://arxiv.org/abs/2505.24479v1", "title": "Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation", "summary": "The rapid spread of misinformation, further amplified by recent advances in\ngenerative AI, poses significant threats to society, impacting public opinion,\ndemocratic stability, and national security. Understanding and proactively\nassessing these threats requires exploring methodologies that enable structured\nand scalable misinformation generation. In this paper, we propose a novel\napproach that leverages knowledge graphs (KGs) as structured semantic resources\nto systematically generate fake triplets. By analyzing the structural\nproperties of KGs, such as the distance between entities and their predicates,\nwe identify plausibly false relationships. These triplets are then used to\nguide large language models (LLMs) in generating misinformation statements with\nvarying degrees of credibility. By utilizing structured semantic relationships,\nour deterministic approach produces misinformation inherently challenging for\nhumans to detect, drawing exclusively upon publicly available KGs (e.g.,\nWikiGraphs).\n  Additionally, we investigate the effectiveness of LLMs in distinguishing\nbetween genuine and artificially generated misinformation. Our analysis\nhighlights significant limitations in current LLM-based detection methods,\nunderscoring the necessity for enhanced detection strategies and a deeper\nexploration of inherent biases in generative models.", "authors": ["Sania Nayab", "Marco Simoni", "Giulio Rossolini"], "published_date": "2025-05-30", "title_zh": "利用知識圖譜與大型語言模型進行結構化假訊息生成", "summary_zh": "生成式人工智慧加速了虛假資訊的傳播，對公共輿論、民主穩定和國家安全構成威脅。本研究提出一種新方法，利用知識圖譜結構化的語義資源，系統性地生成虛假三元組。透過分析知識圖譜的結構特性，識別出看似合理的錯誤關係，並以此引導大型語言模型生成不同可信度的虛假資訊。此方法利用公開的知識圖譜，產生人類難以察覺的虛假資訊。此外，本研究亦探討了大型語言模型區分真假資訊的能力，發現現有檢測方法存在局限性，突顯了強化檢測策略以及深入探討生成模型內在偏差的必要性。", "audio": "audios/2505.24479v1.mp3", "timestamp": "2025-06-03T06:29:11.308655"}
{"query": "Foundation Model", "id": "2505.23870v1", "url": "http://arxiv.org/abs/2505.23870v1", "title": "MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection", "summary": "We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine\nProjection, that achieves exceptional performance while requiring minimal\nparameters and memory for fine-tuning large foundation models. Its general idea\nis to exploit the superior energy compaction and decorrelation properties of\ncosine projection to improve both model efficiency and accuracy. Specifically,\nit projects the weight change from the low-rank adaptation into the discrete\ncosine space. Then, the weight change is partitioned over different levels of\nthe discrete cosine spectrum, and each partition's most critical frequency\ncomponents are selected. Extensive experiments demonstrate the effectiveness of\nMaCP across a wide range of single-modality tasks, including natural language\nunderstanding, natural language generation, text summarization, as well as\nmulti-modality tasks such as image classification and video understanding. MaCP\nconsistently delivers superior accuracy, significantly reduced computational\ncomplexity, and lower memory requirements compared to existing alternatives.", "authors": ["Yixian Shen", "Qi Bi", "Jia-Hong Huang", "Hongyi Zhu", "Andy D. Pimentel", "Anuj Pathania"], "published_date": "2025-05-29", "title_zh": "MaCP：基於層級餘弦投影的極簡而強大的適應方法", "summary_zh": "本研究提出名為MaCP的極簡高效適應性餘弦投影法，以極少的參數和記憶體微調大型基礎模型，實現卓越效能。其核心概念是利用餘弦投影優異的能量集中和解相關特性，提升模型效率和準確性。具體而言，MaCP將低秩適應的權重變化投影到離散餘弦空間，並將權重變化劃分至不同層級的離散餘弦頻譜，進而選取各區塊最重要的頻率成分。大量實驗證明，MaCP在多種單模態任務（如自然語言理解、自然語言生成、文本摘要）以及多模態任務（如圖像分類和影片理解）中皆表現出色，相較於現有方法，MaCP在顯著降低計算複雜度和記憶體需求的同時，能持續提供更佳的準確度。", "audio": "audios/2505.23870v1.mp3", "timestamp": "2025-06-03T06:29:17.943298"}
{"query": "Diffusion Model", "id": "2505.23661v3", "url": "http://arxiv.org/abs/2505.23661v3", "title": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation", "summary": "In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni.", "authors": ["Size Wu", "Zhonghua Wu", "Zerui Gong", "Qingyi Tao", "Sheng Jin", "Qinyue Li", "Wei Li", "Chen Change Loy"], "published_date": "2025-05-29", "title_zh": "OpenUni：統一多模態理解與生成的一個簡潔基線", "summary_zh": "本研究提出OpenUni，一個簡潔、輕量且完全開源的多模態理解與生成基線模型。受統一模型學習啟發，我們採用高效訓練策略，透過可學習查詢和輕量級Transformer連接器，橋接現成的多模態大型語言模型（LLM）和擴散模型，最小化訓練複雜度和開銷。OpenUni以極簡架構展示了：1) 生成高品質且符合指令的圖像；2) 在GenEval、DPG-Bench和WISE等基準測試中，僅使用11億和31億啟動參數即達到卓越性能。為支持開放研究和社群發展，我們於https://github.com/wusize/OpenUni公開所有模型權重、訓練程式碼及包含2300萬圖像-文字對的訓練資料集。", "audio": "audios/2505.23661v3.mp3", "timestamp": "2025-06-03T06:29:24.099631"}
{"query": "AI", "id": "2505.24477v1", "url": "http://arxiv.org/abs/2505.24477v1", "title": "Evaluating Gemini in an arena for learning", "summary": "Artificial intelligence (AI) is poised to transform education, but the\nresearch community lacks a robust, general benchmark to evaluate AI models for\nlearning. To assess state-of-the-art support for educational use cases, we ran\nan \"arena for learning\" where educators and pedagogy experts conduct blind,\nhead-to-head, multi-turn comparisons of leading AI models. In particular, $N =\n189$ educators drew from their experience to role-play realistic learning use\ncases, interacting with two models sequentially, after which $N = 206$ experts\njudged which model better supported the user's learning goals. The arena\nevaluated a slate of state-of-the-art models: Gemini 2.5 Pro, Claude 3.7\nSonnet, GPT-4o, and OpenAI o3. Excluding ties, experts preferred Gemini 2.5 Pro\nin 73.2% of these match-ups -- ranking it first overall in the arena. Gemini\n2.5 Pro also demonstrated markedly higher performance across key principles of\ngood pedagogy. Altogether, these results position Gemini 2.5 Pro as a leading\nmodel for learning.", "authors": ["LearnLM Team", "Abhinit Modi", "Aditya Srikanth Veerubhotla", "Aliya Rysbek", "Andrea Huber", "Ankit Anand", "Avishkar Bhoopchand", "Brett Wiltshire", "Daniel Gillick", "Daniel Kasenberg", "Eleni Sgouritsa", "Gal Elidan", "Hengrui Liu", "Holger Winnemoeller", "Irina Jurenka", "James Cohan", "Jennifer She", "Julia Wilkowski", "Kaiz Alarakyia", "Kevin R. McKee", "Komal Singh", "Lisa Wang", "Markus Kunesch", "Miruna Pîslar", "Niv Efron", "Parsa Mahmoudieh", "Pierre-Alexandre Kamienny", "Sara Wiltberger", "Shakir Mohamed", "Shashank Agarwal", "Shubham Milind Phal", "Sun Jae Lee", "Theofilos Strinopoulos", "Wei-Jen Ko", "Yael Gold-Zamir", "Yael Haramaty", "Yannis Assael"], "published_date": "2025-05-30", "title_zh": "在學習競技場中評估 Gemini", "summary_zh": "人工智慧有望變革教育，但學術界缺乏評估學習型AI模型的通用基準。為評估最新技術對教育應用案例的支援，我們設立「學習競技場」，由教育者和教學專家盲測比較領先AI模型。189位教育者模擬真實學習情境，與兩個模型互動，隨後206位專家評估哪個模型更支援學習目標。競技場評估了Gemini 2.5 Pro、Claude 3.7 Sonnet、GPT-4o和OpenAI o3等模型。排除平局後，專家在73.2%的配對中更偏好Gemini 2.5 Pro，整體排名第一。Gemini 2.5 Pro在關鍵教學原則上也表現更佳，使其成為領先的學習模型。", "audio": "audios/2505.24477v1.mp3", "timestamp": "2025-06-03T07:19:26.821753"}
{"query": "Foundation Model", "id": "2505.22815v2", "url": "http://arxiv.org/abs/2505.22815v2", "title": "IMTS is Worth Time $\\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction", "summary": "Irregular Multivariate Time Series (IMTS) forecasting is challenging due to\nthe unaligned nature of multi-channel signals and the prevalence of extensive\nmissing data. Existing methods struggle to capture reliable temporal patterns\nfrom such data due to significant missing values. While pre-trained foundation\nmodels show potential for addressing these challenges, they are typically\ndesigned for Regularly Sampled Time Series (RTS). Motivated by the visual Mask\nAutoEncoder's (MAE) powerful capability for modeling sparse multi-channel\ninformation and its success in RTS forecasting, we propose VIMTS, a framework\nadapting Visual MAE for IMTS forecasting. To mitigate the effect of missing\nvalues, VIMTS first processes IMTS along the timeline into feature patches at\nequal intervals. These patches are then complemented using learned\ncross-channel dependencies. Then it leverages visual MAE's capability in\nhandling sparse multichannel data for patch reconstruction, followed by a\ncoarse-to-fine technique to generate precise predictions from focused contexts.\nIn addition, we integrate self-supervised learning for improved IMTS modeling\nby adapting the visual MAE to IMTS data. Extensive experiments demonstrate\nVIMTS's superior performance and few-shot capability, advancing the application\nof visual foundation models in more general time series tasks. Our code is\navailable at https://github.com/WHU-HZY/VIMTS.", "authors": ["Zhangyi Hu", "Jiemin Wu", "Hua Xu", "Mingqian Liao", "Ninghui Feng", "Bo Gao", "Songning Lai", "Yutao Yue"], "published_date": "2025-05-28", "title_zh": "IMTS：時間與通道補丁的價值：用於非規則多元時間序列預測的視覺掩碼自編碼器", "summary_zh": "不規則多變量時間序列(IMTS)預測因多通道訊號未對齊和大量缺失資料而具挑戰性。現有方法難以從存在大量缺失值之資料中捕捉可靠的時間模式。預訓練基底模型雖具潛力，但通常設計用於規則採樣時間序列(RTS)。受視覺遮罩自編碼器(MAE)在建模稀疏多通道資訊方面的強大能力及其在RTS預測中的成功啟發，我們提出VIMTS，一種將視覺MAE應用於IMTS預測的框架。為減輕缺失值的影響，VIMTS首先沿時間軸將IMTS處理成等間隔的特徵片段，然後利用學習到的跨通道依賴性補充這些片段。接著，VIMTS利用視覺MAE處理稀疏多通道資料的能力進行片段重建，並採用由粗到精之技術，從聚焦上下文中產生精確預測。此外，我們整合了自我監督學習，通過將視覺MAE調整至IMTS資料來改進IMTS建模。大量實驗證明VIMTS具有卓越的性能和少樣本能力，推進了視覺基底模型在更廣泛時間序列任務中的應用。程式碼可在https://github.com/WHU-HZY/VIMTS取得。", "audio": "audios/2505.22815v2.mp3", "timestamp": "2025-06-03T07:19:34.672991"}
{"query": "Diffusion Model", "id": "2505.23527v2", "url": "http://arxiv.org/abs/2505.23527v2", "title": "Normalizing Flows are Capable Models for RL", "summary": "Modern reinforcement learning (RL) algorithms have found success by using\npowerful probabilistic models, such as transformers, energy-based models, and\ndiffusion/flow-based models. To this end, RL researchers often choose to pay\nthe price of accommodating these models into their algorithms -- diffusion\nmodels are expressive, but are computationally intensive due to their reliance\non solving differential equations, while autoregressive transformer models are\nscalable but typically require learning discrete representations. Normalizing\nflows (NFs), by contrast, seem to provide an appealing alternative, as they\nenable likelihoods and sampling without solving differential equations or\nautoregressive architectures. However, their potential in RL has received\nlimited attention, partly due to the prevailing belief that normalizing flows\nlack sufficient expressivity. We show that this is not the case. Building on\nrecent work in NFs, we propose a single NF architecture which integrates\nseamlessly into RL algorithms, serving as a policy, Q-function, and occupancy\nmeasure. Our approach leads to much simpler algorithms, and achieves higher\nperformance in imitation learning, offline, goal conditioned RL and\nunsupervised RL.", "authors": ["Raj Ghugare", "Benjamin Eysenbach"], "published_date": "2025-05-29", "title_zh": "正規化流是具備能力的強化學習模型", "summary_zh": "現代強化學習算法利用變換器、基於能量的模型和擴散/流模型等強大的機率模型取得了成功。然而，擴散模型計算密集，自迴歸變換器模型需要學習離散表示。標準化流(NF)無需解微分方程或自迴歸架構即可進行似然估計和採樣，似乎提供了一種有吸引力的替代方案，但其在強化學習中的潛力受到關注有限，部分原因是普遍認為標準化流缺乏足夠的表達能力。本研究表明情況並非如此。基於NF的最新研究成果，我們提出了一種單一的NF架構，可無縫整合到強化學習算法中，用作策略、Q函數和佔用度量。該方法簡化了算法，並在模仿學習、離線、目標條件強化學習和無監督強化學習中實現了更高的性能。", "audio": "audios/2505.23527v2.mp3", "timestamp": "2025-06-03T07:19:40.812143"}
{"query": "AI", "id": "2505.24472v1", "url": "http://arxiv.org/abs/2505.24472v1", "title": "VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with Iterative Augmentation for Machine Translation", "summary": "Machine translation systems fail when processing code-mixed inputs for\nlow-resource languages. We address this challenge by curating VietMix, a\nparallel corpus of naturally occurring code-mixed Vietnamese text paired with\nexpert English translations. Augmenting this resource, we developed a\ncomplementary synthetic data generation pipeline. This pipeline incorporates\nfiltering mechanisms to ensure syntactic plausibility and pragmatic\nappropriateness in code-mixing patterns. Experimental validation shows our\nnaturalistic and complementary synthetic data boost models' performance,\nmeasured by translation quality estimation scores, of up to 71.84 on COMETkiwi\nand 81.77 on XCOMET. Triangulating positive results with LLM-based assessments,\naugmented models are favored over seed fine-tuned counterparts in approximately\n49% of judgments (54-56% excluding ties). VietMix and our augmentation\nmethodology advance ecological validity in neural MT evaluations and establish\na framework for addressing code-mixed translation challenges across other\nlow-resource pairs.", "authors": ["Hieu Tran", "Phuong-Anh Nguyen-Le", "Huy Nghiem", "Quang-Nhan Nguyen", "Wei Ai", "Marine Carpuat"], "published_date": "2025-05-30", "title_zh": "VietMix：用於機器翻譯的迭代增強自然越南語-英語混合語料庫", "summary_zh": "機器翻譯系統在處理低資源語言的混合語碼輸入時表現不佳。本研究創建了VietMix，一個包含自然產生的混合語碼越南語文本及其專業英語翻譯的平行語料庫，以應對此挑戰。此外，我們開發了一套互補的合成數據生成流程，該流程整合了過濾機制，以確保混合語碼模式在句法和語用上的合理性。實驗結果表明，自然語料和互補合成數據顯著提升了模型效能，COMETkiwi評分最高提升71.84，XCOMET評分最高提升81.77。大型語言模型評估亦證實，增強模型在約49%的判斷中優於初始微調模型。VietMix及增強方法提升了神經機器翻譯評估的生態效度，並為解決其他低資源語言對的混合語碼翻譯挑戰建立了框架。", "audio": "audios/2505.24472v1.mp3", "timestamp": "2025-06-03T08:27:45.142195"}
{"query": "Diffusion Model", "id": "2505.23871v1", "url": "http://arxiv.org/abs/2505.23871v1", "title": "ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning", "summary": "Real-world datasets collected from sensors or human inputs are prone to noise\nand errors, posing significant challenges for applying offline reinforcement\nlearning (RL). While existing methods have made progress in addressing\ncorrupted actions and rewards, they remain insufficient for handling corruption\nin high-dimensional state spaces and for cases where multiple elements in the\ndataset are corrupted simultaneously. Diffusion models, known for their strong\ndenoising capabilities, offer a promising direction for this problem-but their\ntendency to overfit noisy samples limits their direct applicability. To\novercome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a\nnovel approach that pioneers the use of diffusion models to tackle data\ncorruption in offline RL. First, we introduce Ambient Denoising Diffusion\nProbabilistic Models (DDPM) from approximated distributions, which enable\nlearning on partially corrupted datasets with theoretical guarantees. Second,\nwe use the noise-prediction property of Ambient DDPM to distinguish between\nclean and corrupted data, and then use the clean subset to train a standard\nDDPM. Third, we employ the trained standard DDPM to refine the previously\nidentified corrupted data, enhancing data quality for subsequent offline RL\ntraining. A notable strength of ADG is its versatility-it can be seamlessly\nintegrated with any offline RL algorithm. Experiments on a range of benchmarks,\nincluding MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively\nmitigates the impact of corrupted data and improves the robustness of offline\nRL under various noise settings, achieving state-of-the-art results.", "authors": ["Zeyuan Liu", "Zhihe Yang", "Jiawei Xu", "Rui Yang", "Jiafei Lyu", "Baoxiang Wang", "Yunjian Xu", "Xiu Li"], "published_date": "2025-05-29", "title_zh": "ADG：環境擴散引導的資料集恢復，適用於具備抗腐化性的離線強化學習", "summary_zh": "從感測器或人工輸入收集的真實世界資料集容易產生雜訊和錯誤，對離線強化學習構成挑戰。現有方法雖在處理受損動作和獎勵方面有所進展，但仍不足以應對高維狀態空間中的損壞，以及資料集中多個元素同時損壞的情況。擴散模型具備強大的降噪能力，為解決此問題提供了一個有希望的方向，但其過度擬合雜訊樣本的傾向限制了直接應用。因此，我們提出環境擴散引導資料集恢復（ADG），這是一種新穎的方法，率先使用擴散模型來解決離線強化學習中的資料損壞問題。首先，我們引入基於近似分佈的環境降噪擴散機率模型（DDPM），使其能夠在具有理論保證的部分損壞資料集上進行學習。其次，我們利用環境DDPM的雜訊預測特性來區分乾淨和損壞的資料，然後使用乾淨的子集來訓練標準DDPM。第三，我們使用訓練好的標準DDPM來完善先前識別的損壞資料，從而提高資料品質，以供後續的離線強化學習訓練。ADG的一個顯著優勢是其通用性，它可以與任何離線強化學習算法無縫集成。在包括MuJoCo、Kitchen和Adroit在內的一系列基準測試上的實驗表明，ADG有效地減輕了損壞資料的影響，並提高了離線強化學習在各種雜訊環境下的穩健性，取得了最先進的成果。", "audio": "audios/2505.23871v1.mp3", "timestamp": "2025-06-03T08:27:55.502759"}
{"query": "AI", "id": "2505.24426v1", "url": "http://arxiv.org/abs/2505.24426v1", "title": "P: A Universal Measure of Predictive Intelligence", "summary": "Over the last thirty years, considerable progress has been made with the\ndevelopment of systems that can drive cars, play games, predict protein folding\nand generate natural language. These systems are described as intelligent and\nthere has been a great deal of talk about the rapid increase in artificial\nintelligence and its potential dangers. However, our theoretical understanding\nof intelligence and ability to measure it lag far behind our capacity for\nbuilding systems that mimic intelligent human behaviour. There is no commonly\nagreed definition of the intelligence that AI systems are said to possess.\nNo-one has developed a practical measure that would enable us to compare the\nintelligence of humans, animals and AIs on a single ratio scale.\n  This paper sets out a new universal measure of intelligence that is based on\nthe hypothesis that prediction is the most important component of intelligence.\nAs an agent interacts with its normal environment, the accuracy of its\npredictions is summed up and the complexity of its predictions and perceived\nenvironment is accounted for using Kolmogorov complexity. Two experiments were\ncarried out to evaluate the practical feasibility of the algorithm. These\ndemonstrated that it could measure the intelligence of an agent embodied in a\nvirtual maze and an agent that makes predictions about time-series data. This\nuniversal measure could be the starting point for a new comparative science of\nintelligence that ranks humans, animals and AIs on a single ratio scale.", "authors": ["David Gamez"], "published_date": "2025-05-30", "title_zh": "P：預測智能的通用度量", "summary_zh": "過去三十年，汽車駕駛、遊戲、蛋白質摺疊預測及自然語言生成系統取得顯著進展，這些系統被稱為具備智能，引發對人工智慧快速發展及其潛在危險的討論。然而，我們對智能的理論理解和衡量能力遠遠落後於模仿人類智能行為的系統建構。人工智慧系統所具備的智能並無公認定義，也缺乏能以單一比例尺比較人類、動物和人工智慧智能的實用方法。\n\n本文提出一種基於預測是智能最重要組成部分的新型通用智能衡量標準。該標準透過總結智能體與其正常環境互動時的預測準確性，並使用柯爾莫哥洛夫複雜度來衡量其預測和感知環境的複雜性。兩項實驗驗證了該演算法的實際可行性，證明其可衡量虛擬迷宮中的智能體以及預測時間序列數據的智能體。此通用衡量標準或可成為比較人類、動物和人工智慧的新型智能科學之起點，並以單一比例尺進行排名。", "audio": "audios/2505.24426v1.mp3", "timestamp": "2025-06-03T09:21:01.242387"}
{"query": "AI", "id": "2505.24421v1", "url": "http://arxiv.org/abs/2505.24421v1", "title": "pyMEAL: A Multi-Encoder Augmentation-Aware Learning for Robust and Generalizable Medical Image Translation", "summary": "Medical imaging is critical for diagnostics, but clinical adoption of\nadvanced AI-driven imaging faces challenges due to patient variability, image\nartifacts, and limited model generalization. While deep learning has\ntransformed image analysis, 3D medical imaging still suffers from data scarcity\nand inconsistencies due to acquisition protocols, scanner differences, and\npatient motion. Traditional augmentation uses a single pipeline for all\ntransformations, disregarding the unique traits of each augmentation and\nstruggling with large data volumes.\n  To address these challenges, we propose a Multi-encoder Augmentation-Aware\nLearning (MEAL) framework that leverages four distinct augmentation variants\nprocessed through dedicated encoders. Three fusion strategies such as\nconcatenation (CC), fusion layer (FL), and adaptive controller block (BD) are\nintegrated to build multi-encoder models that combine augmentation-specific\nfeatures before decoding. MEAL-BD uniquely preserves augmentation-aware\nrepresentations, enabling robust, protocol-invariant feature learning.\n  As demonstrated in a Computed Tomography (CT)-to-T1-weighted Magnetic\nResonance Imaging (MRI) translation study, MEAL-BD consistently achieved the\nbest performance on both unseen- and predefined-test data. On both geometric\ntransformations (like rotations and flips) and non-augmented inputs, MEAL-BD\noutperformed other competing methods, achieving higher mean peak\nsignal-to-noise ratio (PSNR) and structural similarity index measure (SSIM)\nscores. These results establish MEAL as a reliable framework for preserving\nstructural fidelity and generalizing across clinically relevant variability. By\nreframing augmentation as a source of diverse, generalizable features, MEAL\nsupports robust, protocol-invariant learning, advancing clinically reliable\nmedical imaging solutions.", "authors": ["Abdul-mojeed Olabisi Ilyas", "Adeleke Maradesa", "Jamal Banzi", "Jianpan Huang", "Henry K. F. Mak", "Kannie W. Y. Chan"], "published_date": "2025-05-30", "title_zh": "pyMEAL：基於多編碼器增強感知學習的穩健且具泛化能力的醫學影像轉換", "summary_zh": "醫學影像對診斷至關重要，但AI影像技術的臨床應用因患者差異、影像偽影及模型泛化能力受限而面臨挑戰。深度學習雖革新影像分析，但3D醫學影像仍受限於數據稀缺及採集協議、掃描儀差異和患者移動造成的不一致性。傳統數據增強對所有轉換使用單一流程，忽略了各增強方法的獨特性，且難以處理大量數據。\n\n為了解決這些問題，我們提出一種多編碼器增強感知學習(MEAL)框架，利用四種不同的增強變體，透過專用編碼器處理。整合串聯(CC)、融合層(FL)和自適應控制器區塊(BD)三種融合策略，構建多編碼器模型，在解碼前結合增強方法特徵。MEAL-BD獨特地保留了增強感知表示，實現穩健且協議不變的特徵學習。\n\n在電腦斷層掃描(CT)至T1加權磁振造影(MRI)轉換研究中，MEAL-BD始終在未見過及預定義的測試數據上表現最佳。無論幾何變換(如旋轉和翻轉)或未增強輸入，MEAL-BD均優於其他方法，實現更高的峰值信噪比(PSNR)和結構相似性指標(SSIM)分數。這些結果證明MEAL是保存結構保真度並在臨床相關變異性中泛化的可靠框架。透過將增強視為多樣且可泛化特徵的來源，MEAL支援穩健且協議不變的學習，從而推進臨床可靠的醫學影像解決方案。", "audio": "audios/2505.24421v1.mp3", "timestamp": "2025-06-03T10:21:19.515661"}
{"query": "AI", "id": "2505.24375v1", "url": "http://arxiv.org/abs/2505.24375v1", "title": "Spatiotemporal Analysis of Forest Machine Operations Using 3D Video Classification", "summary": "This paper presents a deep learning-based framework for classifying forestry\noperations from dashcam video footage. Focusing on four key work elements -\ncrane-out, cutting-and-to-processing, driving, and processing - the approach\nemploys a 3D ResNet-50 architecture implemented with PyTorchVideo. Trained on a\nmanually annotated dataset of field recordings, the model achieves strong\nperformance, with a validation F1 score of 0.88 and precision of 0.90. These\nresults underscore the effectiveness of spatiotemporal convolutional networks\nfor capturing both motion patterns and appearance in real-world forestry\nenvironments.\n  The system integrates standard preprocessing and augmentation techniques to\nimprove generalization, but overfitting is evident, highlighting the need for\nmore training data and better class balance. Despite these challenges, the\nmethod demonstrates clear potential for reducing the manual workload associated\nwith traditional time studies, offering a scalable solution for operational\nmonitoring and efficiency analysis in forestry.\n  This work contributes to the growing application of AI in natural resource\nmanagement and sets the foundation for future systems capable of real-time\nactivity recognition in forest machinery. Planned improvements include dataset\nexpansion, enhanced regularization, and deployment trials on embedded systems\nfor in-field use.", "authors": ["Maciej Wielgosz", "Simon Berg", "Heikki Korpunen", "Stephan Hoffmann"], "published_date": "2025-05-30", "title_zh": "利用三維視頻分類進行森林機械作業時空分析", "summary_zh": "本研究提出基於深度學習之框架，利用行車記錄器影像分類林業作業。針對起重機吊運、伐木與初步加工、駕駛及處理等四項關鍵作業，採用以 PyTorchVideo 實作之 3D ResNet-50 架構。模型經人工標註之田野錄像資料集訓練，表現優異，驗證集 F1 分數達 0.88，精確率達 0.90。結果顯示時空卷積網路能有效捕捉真實林業環境中的動作模式與外觀。系統整合標準預處理與擴增技術以增強泛化能力，然過擬合現象明顯，顯示需更多訓練數據與更佳的類別平衡。儘管如此，此方法在減少傳統工時研究之人力負擔方面展現潛力，為林業作業監測與效率分析提供可擴展之解決方案。本研究貢獻於人工智慧在自然資源管理領域之應用，並為未來具備森林機械即時活動辨識能力之系統奠定基礎。計畫改進包括擴充數據集、強化正規化及於嵌入式系統上進行現場部署試驗。", "audio": "audios/2505.24375v1.mp3", "timestamp": "2025-06-03T11:17:06.201874"}
{"query": "AI", "id": "2505.24307v1", "url": "http://arxiv.org/abs/2505.24307v1", "title": "Multi-Waveguide Pinching Antennas for ISAC", "summary": "Recently, a novel flexible-antenna technology, called pinching antennas, has\nattracted growing academic interest. By inserting discrete dielectric\nmaterials, pinching antennas can be activated at arbitrary points along\nwaveguides, allowing for flexible customization of large-scale path loss. This\npaper investigates a multi-waveguide pinching-antenna integrated sensing and\ncommunications (ISAC) system, where transmit pinching antennas (TPAs) and\nreceive pinching antennas (RPAs) coordinate to simultaneously detect one\npotential target and serve one downlink user. We formulate a communication rate\nmaximization problem subject to radar signal-to-noise ratio (SNR) requirement,\ntransmit power budget, and the allowable movement region of the TPAs, by\njointly optimizing TPA locations and transmit beamforming design. To address\nthe non-convexity of the problem, we propose a novel fine-tuning approximation\nmethod to reformulate it into a tractable form, followed by a successive convex\napproximation (SCA)-based algorithm to obtain the solution efficiently.\nExtensive simulations validate both the system design and the proposed\nalgorithm. Results show that the proposed method achieves near-optimal\nperformance compared with the computational-intensive exhaustive search-based\nbenchmark, and pinching-antenna ISAC systems exhibit a distinct\ncommunication-sensing trade-off compared with conventional systems.", "authors": ["Weihao Mao", "Yang Lu", "Yanqing Xu", "Bo Ai", "Octavia A. Dobre", "Dusit Niyato"], "published_date": "2025-05-30", "title_zh": "用於整合感知與通訊的多波導夾擠天線", "summary_zh": "一種新型的夾捏天線技術因其靈活性而備受關注。藉由插入介電材料，夾捏天線可在波導的任意點激活，從而實現大規模路徑損耗的靈活客製化。本文研究了一種多波導夾捏天線整合感測與通訊(ISAC)系統，其中發射夾捏天線(TPA)和接收夾捏天線(RPA)協同工作，同時偵測潛在目標並服務於一個下行鏈路使用者。我們針對雷達信號雜訊比(SNR)要求、發射功率預算和TPA的容許移動範圍，構建了一個通訊速率最大化問題，並聯合優化TPA位置和發射波束成形設計。為了解決問題的非凸性，我們提出了一種新的微調近似方法，將其重新表述為易於處理的形式，然後使用基於逐次凸逼近(SCA)的演算法來有效求解。大量的模擬驗證了系統設計和所提出的演算法。結果表明，相較於計算量大的窮舉搜尋基準，該方法實現了接近最佳的效能，且夾捏天線ISAC系統相較於傳統系統展現出明顯的通訊-感測權衡。", "audio": "audios/2505.24307v1.mp3", "timestamp": "2025-06-03T12:38:57.733066"}
{"query": "AI", "id": "2505.24292v1", "url": "http://arxiv.org/abs/2505.24292v1", "title": "Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules", "summary": "Human-AI conversation frequently relies on quoting earlier text-\"check it\nwith the formula I just highlighted\"-yet today's large language models (LLMs)\nlack an explicit mechanism for locating and exploiting such spans. We formalise\nthe challenge as span-conditioned generation, decomposing each turn into the\ndialogue history, a set of token-offset quotation spans, and an intent\nutterance. Building on this abstraction, we introduce a quotation-centric data\npipeline that automatically synthesises task-specific dialogues, verifies\nanswer correctness through multi-stage consistency checks, and yields both a\nheterogeneous training corpus and the first benchmark covering five\nrepresentative scenarios. To meet the benchmark's zero-overhead and\nparameter-efficiency requirements, we propose QuAda, a lightweight\ntraining-based method that attaches two bottleneck projections to every\nattention head, dynamically amplifying or suppressing attention to quoted spans\nat inference time while leaving the prompt unchanged and updating < 2.8% of\nbackbone weights. Experiments across models show that QuAda is suitable for all\nscenarios and generalises to unseen topics, offering an effective,\nplug-and-play solution for quotation-aware dialogue.", "authors": ["Yueqi Zhang", "Peiwen Yuan", "Shaoxiong Feng", "Yiwei Li", "Xinglin Wang", "Jiayi Shi", "Chuyi Tan", "Boyuan Pan", "Yao Hu", "Kan Li"], "published_date": "2025-05-30", "title_zh": "留意引言：透過隨插即用模組使大型語言模型具備引言感知對話能力", "summary_zh": "人機對話常引用先前文本，然現今大型語言模型缺乏定位和利用此類範圍的機制。本文將此挑戰形式化為範圍條件生成，將每一回合分解為對話歷史、引用範圍集合及意圖語句。基於此抽象概念，本文提出以引用為中心的資料流程，自動合成特定任務對話，透過多階段一致性檢查驗證答案正確性，並產生異質訓練語料庫及首個涵蓋五種代表性情境的基準。為滿足基準的零成本和參數效率要求，本文提出QuAda，一種輕量級基於訓練的方法，將兩個瓶頸投影附加到每個注意力頭，在推論時動態放大或抑制對引用範圍的注意力，同時保持提示不變，且僅更新少於2.8%的主幹權重。跨模型的實驗表明，QuAda適用於所有情境，並可推廣至未見主題，為引用感知對話提供有效、隨插即用的解決方案。", "audio": "audios/2505.24292v1.mp3", "timestamp": "2025-06-03T13:32:51.122492"}
{"query": "AI", "id": "2505.24269v1", "url": "http://arxiv.org/abs/2505.24269v1", "title": "INSIGHT: A Survey of In-Network Systems for Intelligent, High-Efficiency AI and Topology Optimization", "summary": "In-network computation represents a transformative approach to addressing the\nescalating demands of Artificial Intelligence (AI) workloads on network\ninfrastructure. By leveraging the processing capabilities of network devices\nsuch as switches, routers, and Network Interface Cards (NICs), this paradigm\nenables AI computations to be performed directly within the network fabric,\nsignificantly reducing latency, enhancing throughput, and optimizing resource\nutilization. This paper provides a comprehensive analysis of optimizing\nin-network computation for AI, exploring the evolution of programmable network\narchitectures, such as Software-Defined Networking (SDN) and Programmable Data\nPlanes (PDPs), and their convergence with AI. It examines methodologies for\nmapping AI models onto resource-constrained network devices, addressing\nchallenges like limited memory and computational capabilities through efficient\nalgorithm design and model compression techniques. The paper also highlights\nadvancements in distributed learning, particularly in-network aggregation, and\nthe potential of federated learning to enhance privacy and scalability.\nFrameworks like Planter and Quark are discussed for simplifying development,\nalongside key applications such as intelligent network monitoring, intrusion\ndetection, traffic management, and Edge AI. Future research directions,\nincluding runtime programmability, standardized benchmarks, and new\napplications paradigms, are proposed to advance this rapidly evolving field.\nThis survey underscores the potential of in-network AI to create intelligent,\nefficient, and responsive networks capable of meeting the demands of\nnext-generation AI applications.", "authors": ["Aleksandr Algazinov", "Joydeep Chandra", "Matt Laing"], "published_date": "2025-05-30", "title_zh": "洞察：用於智慧、高效人工智慧與拓撲優化的網路內系統綜述", "summary_zh": "網路內運算是一種變革性方法，旨在解決人工智慧工作負載對網路基礎設施日益增長的需求。透過利用交換器、路由器和網路介面卡等網路設備的處理能力，此範例可直接在網路結構內執行人工智慧運算，從而顯著降低延遲、提高吞吐量並優化資源利用率。本文全面分析了針對人工智慧優化網路內運算，探討了軟體定義網路和可程式化資料平面等可程式化網路架構的演進及其與人工智慧的融合。它檢驗了將人工智慧模型對應到資源受限的網路設備的方法，透過高效的演算法設計和模型壓縮技術，解決了記憶體和計算能力有限等挑戰。本文還重點介紹了分散式學習（特別是網路內聚合）的進展，以及聯邦學習在增強隱私性和可擴展性方面的潛力。論文討論了Planter和Quark等簡化開發的框架，以及智慧網路監控、入侵檢測、流量管理和邊緣人工智慧等關鍵應用。為推進這個快速發展的領域，論文提出了包括運行時可程式化、標準化基準和新應用範例等未來研究方向。本調查強調了網路內人工智慧在創建智慧、高效和響應迅速的網路方面的潛力，這些網路能夠滿足下一代人工智慧應用程式的需求。", "audio": "audios/2505.24269v1.mp3", "timestamp": "2025-06-03T14:19:42.273643"}
{"query": "AI", "id": "2505.24261v1", "url": "http://arxiv.org/abs/2505.24261v1", "title": "Taming Hyperparameter Sensitivity in Data Attribution: Practical Selection Without Costly Retraining", "summary": "Data attribution methods, which quantify the influence of individual training\ndata points on a machine learning model, have gained increasing popularity in\ndata-centric applications in modern AI. Despite a recent surge of new methods\ndeveloped in this space, the impact of hyperparameter tuning in these methods\nremains under-explored. In this work, we present the first large-scale\nempirical study to understand the hyperparameter sensitivity of common data\nattribution methods. Our results show that most methods are indeed sensitive to\ncertain key hyperparameters. However, unlike typical machine learning\nalgorithms -- whose hyperparameters can be tuned using computationally-cheap\nvalidation metrics -- evaluating data attribution performance often requires\nretraining models on subsets of training data, making such metrics\nprohibitively costly for hyperparameter tuning. This poses a critical open\nchallenge for the practical application of data attribution methods. To address\nthis challenge, we advocate for better theoretical understandings of\nhyperparameter behavior to inform efficient tuning strategies. As a case study,\nwe provide a theoretical analysis of the regularization term that is critical\nin many variants of influence function methods. Building on this analysis, we\npropose a lightweight procedure for selecting the regularization value without\nmodel retraining, and validate its effectiveness across a range of standard\ndata attribution benchmarks. Overall, our study identifies a fundamental yet\noverlooked challenge in the practical application of data attribution, and\nhighlights the importance of careful discussion on hyperparameter selection in\nfuture method development.", "authors": ["Weiyi Wang", "Junwei Deng", "Yuzheng Hu", "Shiyuan Zhang", "Xirui Jiang", "Runting Zhang", "Han Zhao", "Jiaqi W. Ma"], "published_date": "2025-05-30", "title_zh": "資料歸因中超參數敏感性的控制：無需昂貴重訓練的實用選擇", "summary_zh": "資料歸因方法旨在量化個別訓練資料點對機器學習模型的影響，於當代人工智慧以資料為中心之應用中日益普及。儘管此領域新方法不斷湧現，超參數調整的影響仍未充分探討。本研究首次進行大規模實證研究，以了解常見資料歸因方法對超參數的敏感度。結果顯示，多數方法確實對特定關鍵超參數敏感。然而，不同於可使用計算成本較低的驗證指標調整超參數之典型機器學習演算法，評估資料歸因效能通常需於訓練資料子集上重新訓練模型，使得此類指標成本過高而不適用於超參數調整。這對資料歸因方法的實際應用構成重大挑戰。為解決此問題，我們提倡對超參數行為進行更深入的理論理解，以作為有效調整策略的參考。作為案例研究，我們對影響函數方法多種變體中至關重要的正則化項進行理論分析。基於此分析，我們提出一種無需模型重新訓練即可選擇正則化值的輕量級程序，並於一系列標準資料歸因基準測試中驗證其有效性。總體而言，本研究揭示了資料歸因實際應用中一個基本但被忽略的挑戰，並強調於未來方法開發中仔細討論超參數選擇的重要性。", "audio": "audios/2505.24261v1.mp3", "timestamp": "2025-06-03T15:21:30.291212"}
{"query": "AI", "id": "2505.24257v1", "url": "http://arxiv.org/abs/2505.24257v1", "title": "Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames", "summary": "An embodied AI assistant operating on egocentric video must integrate spatial\ncues across time - for instance, determining where an object A, glimpsed a few\nmoments ago lies relative to an object B encountered later. We introduce\nDisjoint-3DQA , a generative QA benchmark that evaluates this ability of VLMs\nby posing questions about object pairs that are not co-visible in the same\nframe. We evaluated seven state-of-the-art VLMs and found that models lag\nbehind human performance by 28%, with steeper declines in accuracy (60% to 30\n%) as the temporal gap widens. Our analysis further reveals that providing\ntrajectories or bird's-eye-view projections to VLMs results in only marginal\nimprovements, whereas providing oracle 3D coordinates leads to a substantial\n20% performance increase. This highlights a core bottleneck of multi-frame VLMs\nin constructing and maintaining 3D scene representations over time from visual\nsignals. Disjoint-3DQA therefore sets a clear, measurable challenge for\nlong-horizon spatial reasoning and aims to catalyze future research at the\nintersection of vision, language, and embodied AI.", "authors": ["Sahithya Ravi", "Gabriel Sarch", "Vibhav Vineet", "Andrew D. Wilson", "Balasaravanan Thoravi Kumaravel"], "published_date": "2025-05-30", "title_zh": "視線之外，語境猶存？跨越不連續幀的視覺語言模型中的自我中心空間推理", "summary_zh": "本研究提出Disjoint-3DQA，一個生成式問答基準，用於評估具身人工智慧助手在第一人稱視訊中整合跨時序空間線索的能力，即判斷先前瞥見的物體A相對於之後遇到的物體B的位置。針對無法在同一畫面中共視的物體對提出問題，評估了七個最先進的視覺語言模型，發現模型性能落後人類28%，且準確度隨著時間間隔擴大而急劇下降（從60%降至30%）。分析表明，提供軌跡或鳥瞰圖投影僅略微提升模型性能，而提供真實3D坐標則顯著提高20%。這突顯了多幀視覺語言模型在構建和維持基於視覺訊號的3D場景時空表徵方面的核心瓶頸。Disjoint-3DQA為長時距空間推理設定了明確且可衡量的挑戰，旨在促進視覺、語言和具身人工智慧交叉領域的未來研究。", "audio": "audios/2505.24257v1.mp3", "timestamp": "2025-06-03T16:26:31.554622"}
{"query": "AI", "id": "2505.24255v1", "url": "http://arxiv.org/abs/2505.24255v1", "title": "Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games", "summary": "Large Language Models (LLMs) have shown potential in simulating human\nbehaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for\ncomplex social interactions. In this study, we investigate the role of ToM\nreasoning in aligning agentic behaviors with human norms in negotiation tasks,\nusing the ultimatum game as a controlled environment. We initialized LLM agents\nwith different prosocial beliefs (including Greedy, Fair, and Selfless) and\nreasoning methods like chain-of-thought (CoT) and varying ToM levels, and\nexamined their decision-making processes across diverse LLMs, including\nreasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from\n2,700 simulations indicated that ToM reasoning enhances behavior alignment,\ndecision-making consistency, and negotiation outcomes. Consistent with previous\nfindings, reasoning models exhibit limited capability compared to models with\nToM reasoning, different roles of the game benefits with different orders of\nToM reasoning. Our findings contribute to the understanding of ToM's role in\nenhancing human-AI interaction and cooperative decision-making. The code used\nfor our experiments can be found at https://github.com/Stealth-py/UltimatumToM.", "authors": ["Neemesh Yadav", "Palakorn Achananuparp", "Jing Jiang", "Ee-Peng Lim"], "published_date": "2025-05-30", "title_zh": "心智理論與親社會信念對最後通牒賽局中大型語言模型人類對齊行為之影響", "summary_zh": "大型語言模型展現模擬人類行為和執行心智理論（ToM）推理的潛力，此為複雜社交互動的關鍵技能。本研究探討ToM推理在協商任務中使主體行為與人類規範對齊的作用，以最後通牒賽局為受控環境。我們以不同的親社會信念（包括貪婪、公平和無私）以及鏈式思考（CoT）等推理方法和不同ToM層次初始化LLM主體，並檢驗其在各種LLM（包括o3-mini和DeepSeek-R1 Distilled Qwen 32B等推理模型）中的決策過程。2700次模擬的結果表明，ToM推理增強了行為對齊、決策一致性和協商結果。與先前研究一致，推理模型的能力有限，而不同ToM層次的賽局利益扮演不同的角色。本研究有助於理解ToM在增強人機互動和合作決策中的作用。實驗代碼位於https://github.com/Stealth-py/UltimatumToM。", "audio": "audios/2505.24255v1.mp3", "timestamp": "2025-06-03T17:18:39.353761"}
{"query": "AI", "id": "2505.24251v1", "url": "http://arxiv.org/abs/2505.24251v1", "title": "Proactive Guidance of Multi-Turn Conversation in Industrial Search", "summary": "The evolution of Large Language Models (LLMs) has significantly advanced\nmulti-turn conversation systems, emphasizing the need for proactive guidance to\nenhance users' interactions. However, these systems face challenges in\ndynamically adapting to shifts in users' goals and maintaining low latency for\nreal-time interactions. In the Baidu Search AI assistant, an industrial-scale\nmulti-turn search system, we propose a novel two-phase framework to provide\nproactive guidance. The first phase, Goal-adaptive Supervised Fine-Tuning\n(G-SFT), employs a goal adaptation agent that dynamically adapts to user goal\nshifts and provides goal-relevant contextual information. G-SFT also\nincorporates scalable knowledge transfer to distill insights from LLMs into a\nlightweight model for real-time interaction. The second phase, Click-oriented\nReinforcement Learning (C-RL), adopts a generate-rank paradigm, systematically\nconstructs preference pairs from user click signals, and proactively improves\nclick-through rates through more engaging guidance. This dual-phase\narchitecture achieves complementary objectives: G-SFT ensures accurate goal\ntracking, while C-RL optimizes interaction quality through click signal-driven\nreinforcement learning. Extensive experiments demonstrate that our framework\nachieves 86.10% accuracy in offline evaluation (+23.95% over baseline) and\n25.28% CTR in online deployment (149.06% relative improvement), while reducing\ninference latency by 69.55% through scalable knowledge distillation.", "authors": ["Xiaoyu Li", "Xiao Li", "Li Gao", "Yiding Liu", "Xiaoyang Wang", "Shuaiqiang Wang", "Junfeng Wang", "Dawei Yin"], "published_date": "2025-05-30", "title_zh": "工業搜尋中多輪對話的主動引導", "summary_zh": "大型語言模型的發展顯著提升了多輪對話系統，但系統需主動引導以優化互動。百度搜尋AI助手採用雙階段框架提供主動引導。第一階段，目標自適應監督式微調(G-SFT)利用目標適應代理，動態適應使用者目標轉變並提供相關資訊，並透過知識轉移將大型語言模型的洞見提煉至輕量模型以實現即時互動。第二階段，點擊導向強化學習(C-RL)採用生成排序模式，利用使用者點擊訊號構建偏好對，並主動提升點擊率。G-SFT確保目標追蹤準確性，C-RL透過點擊訊號驅動強化學習優化互動品質。離線評估準確度達86.10%（相較基準提升23.95%），線上部署點擊率達25.28%（相對提升149.06%），同時透過知識提煉降低推論延遲69.55%。", "audio": "audios/2505.24251v1.mp3", "timestamp": "2025-06-03T18:27:41.446661"}
{"query": "AI", "id": "2505.24247v1", "url": "http://arxiv.org/abs/2505.24247v1", "title": "50 Years of Automated Face Recognition", "summary": "Over the past 50 years, automated face recognition has evolved from\nrudimentary, handcrafted systems into sophisticated deep learning models that\nrival and often surpass human performance. This paper chronicles the history\nand technological progression of FR, from early geometric and statistical\nmethods to modern deep neural architectures leveraging massive real and\nAI-generated datasets. We examine key innovations that have shaped the field,\nincluding developments in dataset, loss function, neural network design and\nfeature fusion. We also analyze how the scale and diversity of training data\ninfluence model generalization, drawing connections between dataset growth and\nbenchmark improvements. Recent advances have achieved remarkable milestones:\nstate-of-the-art face verification systems now report False Negative\nIdentification Rates of 0.13% against a 12.4 million gallery in NIST FRVT\nevaluations for 1:N visa-to-border matching. While recent advances have enabled\nremarkable accuracy in high- and low-quality face scenarios, numerous\nchallenges persist. While remarkable progress has been achieved, several open\nresearch problems remain. We outline critical challenges and promising\ndirections for future face recognition research, including scalability,\nmulti-modal fusion, synthetic identity generation, and explainable systems.", "authors": ["Minchul Kim", "Anil Jain", "Xiaoming Liu"], "published_date": "2025-05-30", "title_zh": "自動臉部辨識五十年", "summary_zh": "過去五十年，自動人臉辨識從簡陋的手工系統演變為媲美人類甚至超越人類的深度學習模型。本文回顧人臉辨識的歷史與技術發展，從早期幾何和統計方法到利用大型真實與人工智慧生成數據集的現代深度神經網路架構。我們檢視了塑造該領域的關鍵創新，包括數據集、損失函數、神經網路設計和特徵融合的發展。同時分析訓練數據的規模和多樣性如何影響模型泛化能力，並將數據集增長與基準測試的改進聯繫起來。最新進展已實現顯著的里程碑：在 NIST FRVT 評估中，最先進的人臉驗證系統在 1240 萬人資料庫中，針對 1:N 簽證到邊境匹配，誤拒率僅為 0.13%。儘管在高低品質人臉場景中取得了顯著的準確性，但仍存在許多挑戰。我們概述了未來人臉辨識研究的關鍵挑戰和有希望的方向，包括可擴展性、多模態融合、合成身份生成和可解釋系統。", "audio": "audios/2505.24247v1.mp3", "timestamp": "2025-06-03T19:15:23.184872"}
{"query": "AI", "id": "2505.24246v1", "url": "http://arxiv.org/abs/2505.24246v1", "title": "Locating Risk: Task Designers and the Challenge of Risk Disclosure in RAI Content Work", "summary": "As AI systems are increasingly tested and deployed in open-ended and\nhigh-stakes domains, crowd workers are often tasked with responsible AI (RAI)\ncontent work. These tasks include labeling violent content, moderating\ndisturbing text, or simulating harmful behavior for red teaming exercises to\nshape AI system behaviors. While prior efforts have highlighted the risks to\nworker well-being associated with RAI content work, far less attention has been\npaid to how these risks are communicated to workers. Existing transparency\nframeworks and guidelines such as model cards, datasheets, and crowdworksheets\nfocus on documenting model information and dataset collection processes, but\nthey overlook an important aspect of disclosing well-being risks to workers. In\nthe absence of standard workflows or clear guidance, the consistent application\nof content warnings, consent flows, or other forms of well-being risk\ndisclosure remain unclear. This study investigates how task designers approach\nrisk disclosure in crowdsourced RAI tasks. Drawing on interviews with 23 task\ndesigners across academic and industry sectors, we examine how well-being risk\nis recognized, interpreted, and communicated in practice. Our findings surface\na need to support task designers in identifying and communicating well-being\nrisk not only to support crowdworker well-being but also to strengthen the\nethical integrity and technical efficacy of AI development pipelines.", "authors": ["Alice Qian Zhang", "Ryland Shaw", "Laura Dabbish", "Jina Suh", "Hong Shen"], "published_date": "2025-05-30", "title_zh": "定位風險：任務設計者與RAI內容工作中風險揭露的挑戰", "summary_zh": "隨著人工智慧系統日益於開放且高風險領域測試和部署，眾包工作者經常被賦予負責AI（RAI）的內容工作。這些任務包括標記暴力內容、審核令人不安的文本或模擬有害行為以進行紅隊演練，從而塑造AI系統的行為。先前研究已強調RAI內容工作對工作者福祉的風險，但對於如何向工作者傳達這些風險的關注較少。現有的透明度框架和指南側重於記錄模型資訊和數據集收集過程，卻忽略了向工作者揭露福祉風險的重要面向。由於缺乏標準工作流程或明確指導，內容警告、同意流程或其他形式的福祉風險揭露的一致應用尚不明確。本研究調查任務設計者如何在眾包RAI任務中處理風險揭露。透過對學術界和業界的23位任務設計者進行訪談，我們檢視了福祉風險在實踐中如何被識別、詮釋和傳達。研究結果表明，需要支持任務設計者識別和傳達福祉風險，不僅為了支持眾包工作者的福祉，也為了加強AI開發流程的倫理完整性和技術有效性。", "audio": "audios/2505.24246v1.mp3", "timestamp": "2025-06-03T20:21:37.604318"}
{"query": "AI", "id": "2505.24201v1", "url": "http://arxiv.org/abs/2505.24201v1", "title": "SentinelAgent: Graph-based Anomaly Detection in Multi-Agent Systems", "summary": "The rise of large language model (LLM)-based multi-agent systems (MAS)\nintroduces new security and reliability challenges. While these systems show\ngreat promise in decomposing and coordinating complex tasks, they also face\nmulti-faceted risks across prompt manipulation, unsafe tool usage, and emergent\nagent miscoordination. Existing guardrail mechanisms offer only partial\nprotection, primarily at the input-output level, and fall short in addressing\nsystemic or multi-point failures in MAS. In this work, we present a\nsystem-level anomaly detection framework tailored for MAS, integrating\nstructural modeling with runtime behavioral oversight. Our approach consists of\ntwo components. First, we propose a graph-based framework that models agent\ninteractions as dynamic execution graphs, enabling semantic anomaly detection\nat node, edge, and path levels. Second, we introduce a pluggable SentinelAgent,\nan LLM-powered oversight agent that observes, analyzes, and intervenes in MAS\nexecution based on security policies and contextual reasoning. By bridging\nabstract detection logic with actionable enforcement, our method detects not\nonly single-point faults and prompt injections but also multi-agent collusion\nand latent exploit paths. We validate our framework through two case studies,\nincluding an email assistant and Microsoft's Magentic-One system, demonstrating\nits ability to detect covert risks and provide explainable root-cause\nattribution. Our work lays the foundation for more trustworthy, monitorable,\nand secure agent-based AI ecosystems.", "authors": ["Xu He", "Di Wu", "Yan Zhai", "Kun Sun"], "published_date": "2025-05-30", "title_zh": "哨兵代理：多代理系統中基於圖的異常檢測", "summary_zh": "基於大型語言模型的多代理系統興起，帶來新的安全與可靠性挑戰。儘管此類系統在分解和協調複雜任務方面展現潛力，但也面臨提示操控、不安全工具使用及代理人失調等風險。現有防護機制僅提供局部保護，不足以解決系統性或多點故障。本研究提出針對多代理系統的系統級異常偵測框架，整合結構建模與運行時行為監控。該方法包含兩個組成部分：首先，提出基於圖的框架，將代理人互動建模為動態執行圖，實現節點、邊緣和路徑層級的語義異常偵測；其次，引入可插拔的哨兵代理人，透過安全策略和上下文推理觀察、分析和干預多代理系統執行。此方法將抽象偵測邏輯與可操作的執行相結合，不僅偵測單點故障和提示注入，還偵測多代理人勾結和潛在利用路徑。透過電子郵件助理和 Microsoft Magentic-One 系統兩個案例研究驗證了該框架，證明其具備偵測隱蔽風險並提供可解釋根本原因歸因的能力。本研究為更可信、可監控且安全的基於代理人的 AI 生態系統奠定基礎。", "audio": "audios/2505.24201v1.mp3", "timestamp": "2025-06-03T21:16:38.985737"}
{"query": "AI", "id": "2505.24126v1", "url": "http://arxiv.org/abs/2505.24126v1", "title": "How Students (Really) Use ChatGPT: Uncovering Experiences Among Undergraduate Students", "summary": "This study investigates how undergraduate students engage with ChatGPT in\nself directed learning contexts. Analyzing naturalistic interaction logs, we\nidentify five dominant use categories of ChatGPT information seeking, content\ngeneration, language refinement, meta cognitive engagement, and conversational\nrepair. Behavioral modeling reveals that structured, goal driven tasks like\ncoding, multiple choice solving, and job application writing are strong\npredictors of continued use. Drawing on Self-Directed Learning (SDL) and the\nUses and Gratifications Theory (UGT), we show how students actively manage\nChatGPTs affordances and limitations through prompt adaptation, follow-ups, and\nemotional regulation. Rather than disengaging after breakdowns, students often\npersist through clarification and repair, treating the assistant as both tool\nand learning partner. We also offer design and policy recommendations to\nsupport transparent, responsive, and pedagogically grounded integration of\ngenerative AI in higher education.", "authors": ["Tawfiq Ammari", "Meilun Chen", "S M Mehedi Zaman", "Kiran Garimella"], "published_date": "2025-05-30", "title_zh": "學生（真正）如何使用ChatGPT：揭示大學生經驗", "summary_zh": "本研究探討大學生在自主學習情境下如何使用ChatGPT。分析自然互動日誌，歸納出五種主要使用類別：資訊檢索、內容生成、語言潤飾、後設認知參與及會話修復。行為建模顯示，結構化、目標導向的任務，如程式編碼、選擇題解答及求職信撰寫，能有效預測持續使用意願。基於自主學習及使用與滿足理論，本文揭示學生如何透過提示調整、追蹤及情緒調節，積極管理ChatGPT的優勢與限制。學生並非在故障後放棄，而是透過澄清與修復持續使用，將其視為工具與學習夥伴。最後，本研究針對高等教育中生成式AI的透明、回應式及具教學基礎的整合，提出設計與政策建議。", "audio": "audios/2505.24126v1.mp3", "timestamp": "2025-06-03T22:18:58.588644"}
{"query": "AI", "id": "2505.24120v1", "url": "http://arxiv.org/abs/2505.24120v1", "title": "CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs", "summary": "Vision-Language Models (VLMs) have demonstrated remarkable progress in\nmultimodal understanding, yet their capabilities for scientific reasoning\nremains inadequately assessed. Current multimodal benchmarks predominantly\nevaluate generic image comprehension or text-driven reasoning, lacking\nauthentic scientific contexts that require domain-specific knowledge\nintegration with visual evidence analysis. To fill this gap, we present CSVQA,\na diagnostic multimodal benchmark specifically designed for evaluating\nscientific reasoning through domain-grounded visual question answering.Our\nbenchmark features 1,378 carefully constructed question-answer pairs spanning\ndiverse STEM disciplines, each demanding domain knowledge, integration of\nvisual evidence, and higher-order reasoning. Compared to prior multimodal\nbenchmarks, CSVQA places greater emphasis on real-world scientific content and\ncomplex reasoning.We additionally propose a rigorous evaluation protocol to\nsystematically assess whether model predictions are substantiated by valid\nintermediate reasoning steps based on curated explanations. Our comprehensive\nevaluation of 15 VLMs on this benchmark reveals notable performance\ndisparities, as even the top-ranked proprietary model attains only 49.6\\%\naccuracy.This empirical evidence underscores the pressing need for advancing\nscientific reasoning capabilities in VLMs. Our CSVQA is released at\nhttps://huggingface.co/datasets/Skywork/CSVQA.", "authors": ["Ai Jian", "Weijie Qiu", "Xiaokun Wang", "Peiyu Wang", "Yunzhuo Hao", "Jiangbo Pei", "Yichen Wei", "Yi Peng", "Xuchen Song"], "published_date": "2025-05-30", "title_zh": "CSVQA：評估視覺語言模型STEM推理能力的中文多模態基準", "summary_zh": "視覺語言模型在多模態理解方面展現顯著進展，但其科學推理能力仍未充分評估。現有多模態基準測試主要評估通用圖像理解或文本驅動推理，缺乏需要特定領域知識與視覺證據分析結合的真實科學情境。為填補此空白，我們提出CSVQA，一個專門設計的多模態基準測試，透過領域知識為基礎的視覺問答來評估科學推理。我們的基準測試包含1378個精心構建的問答對，涵蓋多個STEM學科，每個問答對都需要領域知識、視覺證據整合及高階推理。相較於先前的多模態基準測試，CSVQA更強調真實世界的科學內容和複雜推理。我們額外提出一套嚴謹的評估協議，以系統性地評估模型預測是否基於有效的中間推理步驟，並基於精心策劃的解釋進行驗證。我們在此基準測試上對15個視覺語言模型進行全面評估，揭示顯著的性能差異，即使排名最高的專有模型也僅達到49.6%的準確率。此經驗證據突顯了在視覺語言模型中推進科學推理能力的迫切需求。我們的CSVQA已發布於https://huggingface.co/datasets/Skywork/CSVQA。", "audio": "audios/2505.24120v1.mp3", "timestamp": "2025-06-03T23:18:22.612039"}
{"query": "AI", "id": "2505.24119v1", "url": "http://arxiv.org/abs/2505.24119v1", "title": "The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It", "summary": "This paper presents a comprehensive analysis of the linguistic diversity of\nLLM safety research, highlighting the English-centric nature of the field.\nThrough a systematic review of nearly 300 publications from 2020--2024 across\nmajor NLP conferences and workshops at *ACL, we identify a significant and\ngrowing language gap in LLM safety research, with even high-resource\nnon-English languages receiving minimal attention. We further observe that\nnon-English languages are rarely studied as a standalone language and that\nEnglish safety research exhibits poor language documentation practice. To\nmotivate future research into multilingual safety, we make several\nrecommendations based on our survey, and we then pose three concrete future\ndirections on safety evaluation, training data generation, and crosslingual\nsafety generalization. Based on our survey and proposed directions, the field\ncan develop more robust, inclusive AI safety practices for diverse global\npopulations.", "authors": ["Zheng-Xin Yong", "Beyza Ermis", "Marzieh Fadaee", "Stephen H. Bach", "Julia Kreutzer"], "published_date": "2025-05-30", "title_zh": "多語言大型語言模型安全研究現狀：從衡量語言差距到彌合差距", "summary_zh": "本研究全面分析大型語言模型安全性研究的語言多樣性，揭示該領域以英語為中心的特性。透過系統性回顧2020至2024年間於ACL主要會議和研討會發表的近300篇文獻，我們發現大型語言模型安全性研究存在顯著且日益嚴重的語言差距，即使是高資源的非英語語言也鮮少受到關注。此外，非英語語言很少被作為獨立語言進行研究，且英語安全性研究的語言文檔記錄實踐不佳。為促進未來對多語言安全性的研究，我們基於調查提出多項建議，並針對安全性評估、訓練資料生成和跨語言安全性泛化提出三個具體的研究方向。基於我們的調查和提出的方向，該領域可以為不同的全球人口開發更穩健、更具包容性的人工智慧安全實踐。", "audio": "audios/2505.24119v1.mp3", "timestamp": "2025-06-04T01:28:44.601868"}
{"query": "Foundation Model", "id": "2505.24200v2", "url": "http://arxiv.org/abs/2505.24200v2", "title": "Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC", "summary": "Multilingual speech processing with self-supervised or supervised pre-trained\nSpeech Foundation Models (SFM) has achieved strong performance on tasks like\nLanguage Identification (LID) and Automatic Speech Recognition (ASR). However,\nthese models struggle with limited resources during fine-tuning. This paper\nenhances multilingual LID and ASR on ML-SUPERB 2.0 by exploring multiple\nstrategies for adapting SFMs, including frozen upstream training, partial\nfine-tuning, and low-rank adaptation. Furthermore, we employ data augmentation\nto mitigate performance gaps in few-shot settings and introduce LID\nConnectionist Temporal Classification (CTC) loss for regularization. Our\napproach achieves a 14% relative improvement in LID accuracy and a 30% relative\nreduction in ASR CER over the baseline on ML-SUPERB 2.0, securing second place\nin the Interspeech 2025 ML-SUPERB 2.0 Challenge.", "authors": ["Qingzheng Wang", "Jiancheng Sun", "Yifan Peng", "Shinji Watanabe"], "published_date": "2025-05-30", "title_zh": "在ML-SUPERB 2.0上改進多語音模型：基於數據增強和語言識別感知CTC的微調", "summary_zh": "基於自監督或監督式預訓練的語音基礎模型在多語音訊處理任務（如語種辨識與自動語音辨識）上表現出色，但在微調時常受限於資源。本研究透過凍結上游訓練、局部微調及低秩適配等策略，提升ML-SUPERB 2.0上的多語種辨識與語音辨識效能。此外，我們採用數據增強緩解少樣本情境下的效能差距，並引入語種辨識的連接時序分類損失進行正則化。實驗結果顯示，相較於基準模型，語種辨識準確率相對提升14%，語音辨識字錯誤率相對降低30%，於Interspeech 2025 ML-SUPERB 2.0挑戰賽中獲得第二名。", "audio": "audios/2505.24200v2.mp3", "timestamp": "2025-06-04T01:28:52.795816"}
{"query": "Diffusion Model", "id": "2505.24360v2", "url": "http://arxiv.org/abs/2505.24360v2", "title": "Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning", "summary": "Sparse autoencoders are a promising new approach for decomposing language\nmodel activations for interpretation and control. They have been applied\nsuccessfully to vision transformer image encoders and to small-scale diffusion\nmodels. Inference-Time Decomposition of Activations (ITDA) is a recently\nproposed variant of dictionary learning that takes the dictionary to be a set\nof data points from the activation distribution and reconstructs them with\ngradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large\ntext-to-image diffusion model, Flux 1, and consider the interpretability of\nembeddings of both by introducing a visual automated interpretation pipeline.\nWe find that SAEs accurately reconstruct residual stream embeddings and beat\nMLP neurons on interpretability. We are able to use SAE features to steer image\ngeneration through activation addition. We find that ITDA has comparable\ninterpretability to SAEs.", "authors": ["Stepan Shabalin", "Ayush Panda", "Dmitrii Kharlapenko", "Abdur Raheem Ali", "Yixiong Hao", "Arthur Conmy"], "published_date": "2025-05-30", "title_zh": "藉由字典學習詮釋大型文本至圖像擴散模型", "summary_zh": "稀疏自編碼器為分解語言模型啟動以進行詮釋和控制提供了一種具前景的新方法。 其已成功應用於視覺轉換器圖像編碼器及小型擴散模型。啟動推論時分解(ITDA)是一種近期提出的字典學習變體，將字典視為啟動分佈中的一組數據點，並透過梯度追蹤重建它們。 我們將稀疏自編碼器(SAE)和ITDA應用於大型文本到圖像擴散模型Flux 1，並引入視覺自動詮釋管線來考量兩者嵌入的詮釋性。 研究發現，SAE能準確重建殘差流嵌入，且在詮釋性方面優於MLP神經元。 我們能夠透過啟動添加使用SAE特徵來引導圖像生成。 研究亦發現ITDA具有與SAE相當的詮釋性。", "audio": "audios/2505.24360v2.mp3", "timestamp": "2025-06-04T01:29:02.620595"}
{"query": "AI", "id": "2505.24107v1", "url": "http://arxiv.org/abs/2505.24107v1", "title": "GPTFootprint: Increasing Consumer Awareness of the Environmental Impacts of LLMs", "summary": "With the growth of AI, researchers are studying how to mitigate its\nenvironmental impact, primarily by proposing policy changes and increasing\nawareness among developers. However, research on AI end users is limited.\nTherefore, we introduce GPTFootprint, a browser extension that aims to increase\nconsumer awareness of the significant water and energy consumption of LLMs, and\nreduce unnecessary LLM usage. GPTFootprint displays a dynamically updating\nvisualization of the resources individual users consume through their ChatGPT\nqueries. After a user reaches a set query limit, a popup prompts them to take a\nbreak from ChatGPT. In a week-long user study, we found that GPTFootprint\nincreases people's awareness of environmental impact, but has limited success\nin decreasing ChatGPT usage. This research demonstrates the potential for\nindividual-level interventions to contribute to the broader goal of sustainable\nAI usage, and provides insights into the effectiveness of awareness-based\nbehavior modification strategies in the context of LLMs.", "authors": ["Nora Graves", "Vitus Larrieu", "Yingyue Trace Zhang", "Joanne Peng", "Varun Nagaraj Rao", "Yuhan Liu", "Andrés Monroy-Hernández"], "published_date": "2025-05-30", "title_zh": "GPT足跡：提升消費者對大型語言模型環境影響的意識", "summary_zh": "隨著人工智慧發展，學界關注其環境衝擊，主要著重於政策變革與提升開發者意識。針對AI終端使用者的研究相對不足。本研究推出GPTFootprint瀏覽器擴充功能，旨在提高消費者對大型語言模型耗水耗電的認知，並減少不必要的LLM使用。GPTFootprint動態視覺化呈現使用者透過ChatGPT查詢所消耗的資源。當使用者達到設定的查詢上限時，會彈出視窗提示休息。為期一週的使用者研究顯示，GPTFootprint提升了人們對環境衝擊的認知，但減少ChatGPT使用量的效果有限。本研究表明，個人層級的干預措施有助於實現永續AI使用的目標，並為基於認知行為改變策略在LLM環境中的有效性提供了見解。", "audio": "audios/2505.24107v1.mp3", "timestamp": "2025-06-04T03:15:53.917394"}
{"query": "Diffusion Model", "id": "2505.23721v2", "url": "http://arxiv.org/abs/2505.23721v2", "title": "DiffER: Categorical Diffusion for Chemical Retrosynthesis", "summary": "Methods for automatic chemical retrosynthesis have found recent success\nthrough the application of models traditionally built for natural language\nprocessing, primarily through transformer neural networks. These models have\ndemonstrated significant ability to translate between the SMILES encodings of\nchemical products and reactants, but are constrained as a result of their\nautoregressive nature. We propose DiffER, an alternative template-free method\nfor retrosynthesis prediction in the form of categorical diffusion, which\nallows the entire output SMILES sequence to be predicted in unison. We\nconstruct an ensemble of diffusion models which achieves state-of-the-art\nperformance for top-1 accuracy and competitive performance for top-3, top-5,\nand top-10 accuracy among template-free methods. We prove that DiffER is a\nstrong baseline for a new class of template-free model, capable of learning a\nvariety of synthetic techniques used in laboratory settings and outperforming a\nvariety of other template-free methods on top-k accuracy metrics. By\nconstructing an ensemble of categorical diffusion models with a novel length\nprediction component with variance, our method is able to approximately sample\nfrom the posterior distribution of reactants, producing results with strong\nmetrics of confidence and likelihood. Furthermore, our analyses demonstrate\nthat accurate prediction of the SMILES sequence length is key to further\nboosting the performance of categorical diffusion models.", "authors": ["Sean Current", "Ziqi Chen", "Daniel Adu-Ampratwum", "Xia Ning", "Srinivasan Parthasarathy"], "published_date": "2025-05-29", "title_zh": "DiffER：化學逆合成的分種類別擴散", "summary_zh": "化學逆合成自動化方法近年來藉由轉換器神經網路等傳統自然語言處理模型取得進展。這些模型在化學產物與反應物的SMILES編碼轉換上表現出色，但受限於其自迴歸性質。我們提出DiffER，一種基於類別擴散的無模板逆合成預測方法，能同時預測整個SMILES序列。擴散模型集成在首選準確率上達到頂尖水準，在前三、前五和前十準確率上具備競爭力。DiffER證明了其作為新型無模板模型的強大基礎，能學習實驗室中使用的多種合成技術，並在top-k準確率指標上超越其他無模板方法。透過建構具有變異數的新穎長度預測組件的類別擴散模型集成，本方法能近似地從反應物的後驗分布中取樣，產生具有高度信賴度和似然性的結果。分析表明，準確預測SMILES序列長度是進一步提升類別擴散模型性能的關鍵。", "audio": "audios/2505.23721v2.mp3", "timestamp": "2025-06-04T03:16:00.894277"}
{"query": "AI", "id": "2506.03126v1", "url": "http://arxiv.org/abs/2506.03126v1", "title": "AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation", "summary": "Recent advances in AI-generated content (AIGC) have significantly accelerated\nanimation production. To produce engaging animations, it is essential to\ngenerate coherent multi-shot video clips with narrative scripts and character\nreferences. However, existing public datasets primarily focus on real-world\nscenarios with global descriptions, and lack reference images for consistent\ncharacter guidance. To bridge this gap, we present AnimeShooter, a\nreference-guided multi-shot animation dataset. AnimeShooter features\ncomprehensive hierarchical annotations and strong visual consistency across\nshots through an automated pipeline. Story-level annotations provide an\noverview of the narrative, including the storyline, key scenes, and main\ncharacter profiles with reference images, while shot-level annotations\ndecompose the story into consecutive shots, each annotated with scene,\ncharacters, and both narrative and descriptive visual captions. Additionally, a\ndedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each\nshot, along with audio descriptions and sound sources. To demonstrate the\neffectiveness of AnimeShooter and establish a baseline for the reference-guided\nmulti-shot video generation task, we introduce AnimeShooterGen, which leverages\nMultimodal Large Language Models (MLLMs) and video diffusion models. The\nreference image and previously generated shots are first processed by MLLM to\nproduce representations aware of both reference and context, which are then\nused as the condition for the diffusion model to decode the subsequent shot.\nExperimental results show that the model trained on AnimeShooter achieves\nsuperior cross-shot visual consistency and adherence to reference visual\nguidance, which highlight the value of our dataset for coherent animated video\ngeneration.", "authors": ["Lu Qiu", "Yizhuo Li", "Yuying Ge", "Yixiao Ge", "Ying Shan", "Xihui Liu"], "published_date": "2025-06-03", "title_zh": "AnimeShooter：用於參考引導視訊生成的多鏡頭動畫資料集", "summary_zh": "人工智慧生成內容的最新進展顯著加速了動畫製作。為生成引人入勝的動畫，需產生具有敘事劇本和角色參考的連貫多鏡頭影片片段。現有公開數據集主要關注具有全局描述的真實場景，缺乏一致角色引導的參考圖像。為填補此缺口，我們提出AnimeShooter，一個參考引導的多鏡頭動畫數據集。AnimeShooter通過自動化流程，提供全面的分層註釋以及跨鏡頭的強視覺一致性。故事層級註釋概述了敘事，包括故事情節、關鍵場景以及帶有參考圖像的主要角色檔案；鏡頭層級註釋將故事分解為連續鏡頭，每個鏡頭都標註了場景、角色以及敘事性和描述性視覺字幕。此外，一個專用子集AnimeShooter-audio為每個鏡頭提供同步音軌，以及音訊描述和聲源。為展示AnimeShooter的有效性並建立參考引導的多鏡頭影片生成任務基準，我們引入AnimeShooterGen，利用多模態大型語言模型和影片擴散模型。參考圖像和先前生成的鏡頭首先由MLLM處理，以產生具有參考和上下文意識的表示，然後將其用作擴散模型解碼後續鏡頭的條件。實驗結果表明，在AnimeShooter上訓練的模型在跨鏡頭視覺一致性和參考視覺引導的遵循方面表現出色，突顯了我們的數據集對於連貫動畫影片生成的價值。", "audio": "audios/2506.03126v1.mp3", "timestamp": "2025-06-04T04:26:43.237531"}
{"query": "Foundation Model", "id": "2506.03117v1", "url": "http://arxiv.org/abs/2506.03117v1", "title": "Targeted Forgetting of Image Subgroups in CLIP Models", "summary": "Foundation models (FMs) such as CLIP have demonstrated impressive zero-shot\nperformance across various tasks by leveraging large-scale, unsupervised\npre-training. However, they often inherit harmful or unwanted knowledge from\nnoisy internet-sourced datasets, compromising their reliability in real-world\napplications. Existing model unlearning methods either rely on access to\npre-trained datasets or focus on coarse-grained unlearning (e.g., entire\nclasses), leaving a critical gap for fine-grained unlearning. In this paper, we\naddress the challenging scenario of selectively forgetting specific portions of\nknowledge within a class, without access to pre-trained data, while preserving\nthe model's overall performance. We propose a novel three-stage approach that\nprogressively unlearns targeted knowledge while mitigating over-forgetting. It\nconsists of (1) a forgetting stage to fine-tune the CLIP on samples to be\nforgotten, (2) a reminding stage to restore performance on retained samples,\nand (3) a restoring stage to recover zero-shot capabilities using model\nsouping. Additionally, we introduce knowledge distillation to handle the\ndistribution disparity between forgetting, retaining samples, and unseen\npre-trained data. Extensive experiments on CIFAR-10, ImageNet-1K, and style\ndatasets demonstrate that our approach effectively unlearns specific subgroups\nwhile maintaining strong zero-shot performance on semantically similar\nsubgroups and other categories, significantly outperforming baseline unlearning\nmethods, which lose effectiveness under the CLIP unlearning setting.", "authors": ["Zeliang Zhang", "Gaowen Liu", "Charles Fleming", "Ramana Rao Kompella", "Chenliang Xu"], "published_date": "2025-06-03", "title_zh": "CLIP模型中圖像子群的目標性遺忘", "summary_zh": "基礎模型如CLIP藉由大規模無監督預訓練，展現出卓越的零樣本能力。然而，它們常從網路數據集中承襲有害或不欲知識，影響其於現實應用中的可靠性。現有模型遺忘方法或仰賴存取預訓練數據集，或側重於粗略遺忘（如整體類別），缺乏精細遺忘能力。本研究針對在不存取預訓練數據下，選擇性遺忘類別內特定知識，同時維持模型整體性能之挑戰性情境。我們提出一種新型三階段方法，逐步遺忘目標知識並降低過度遺忘風險，包含：(1)遺忘階段，於待遺忘樣本上微調CLIP；(2)提醒階段，恢復模型於保留樣本上的性能；(3)恢復階段，利用模型融合恢復零樣本能力。此外，導入知識蒸餾處理遺忘、保留樣本及未見預訓練數據間的分布差異。於CIFAR-10、ImageNet-1K和風格數據集上的實驗顯示，本方法能有效遺忘特定子群組，同時在語義相似子群組和其他類別上保持強大的零樣本性能，顯著優於在CLIP遺忘設定下失效的基準遺忘方法。", "audio": "audios/2506.03117v1.mp3", "timestamp": "2025-06-04T04:26:56.486051"}
{"query": "Diffusion Model", "id": "2506.03123v1", "url": "http://arxiv.org/abs/2506.03123v1", "title": "DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation", "summary": "Diffusion Models have achieved remarkable results in video synthesis but\nrequire iterative denoising steps, leading to substantial computational\noverhead. Consistency Models have made significant progress in accelerating\ndiffusion models. However, directly applying them to video diffusion models\noften results in severe degradation of temporal consistency and appearance\ndetails. In this paper, by analyzing the training dynamics of Consistency\nModels, we identify a key conflicting learning dynamics during the distillation\nprocess: there is a significant discrepancy in the optimization gradients and\nloss contributions across different timesteps. This discrepancy prevents the\ndistilled student model from achieving an optimal state, leading to compromised\ntemporal consistency and degraded appearance details. To address this issue, we\npropose a parameter-efficient \\textbf{Dual-Expert Consistency Model~(DCM)},\nwhere a semantic expert focuses on learning semantic layout and motion, while a\ndetail expert specializes in fine detail refinement. Furthermore, we introduce\nTemporal Coherence Loss to improve motion consistency for the semantic expert\nand apply GAN and Feature Matching Loss to enhance the synthesis quality of the\ndetail expert.Our approach achieves state-of-the-art visual quality with\nsignificantly reduced sampling steps, demonstrating the effectiveness of expert\nspecialization in video diffusion model distillation. Our code and models are\navailable at\n\\href{https://github.com/Vchitect/DCM}{https://github.com/Vchitect/DCM}.", "authors": ["Zhengyao Lv", "Chenyang Si", "Tianlin Pan", "Zhaoxi Chen", "Kwan-Yee K. Wong", "Yu Qiao", "Ziwei Liu"], "published_date": "2025-06-03", "title_zh": "DCM：用於高效且高質量視頻生成的雙專家一致性模型", "summary_zh": "擴散模型於影片合成表現出色，但需多次去噪步驟，計算成本高昂。一致性模型雖加速擴散模型，然直接應用於影片擴散常致時間一致性與外觀細節嚴重退化。本研究分析一致性模型訓練動態，指出蒸餾過程中的關鍵衝突學習動態：不同時間步的優化梯度與損失貢獻存在顯著差異，阻礙學生模型達到最佳狀態，損害時間一致性及外觀細節。為此，提出參數高效的雙專家一致性模型，其中語義專家專注於學習語義佈局與運動，細節專家專注於細節優化。此外，引入時間連貫損失以改善語義專家的運動一致性，並應用GAN與特徵匹配損失以提升細節專家的合成品質。實驗證明，本方法在顯著減少取樣步驟下，實現最先進的視覺品質，驗證了專家分工於影片擴散模型蒸餾的有效性。", "audio": "audios/2506.03123v1.mp3", "timestamp": "2025-06-04T04:27:02.905979"}
{"query": "AI", "id": "2506.03122v1", "url": "http://arxiv.org/abs/2506.03122v1", "title": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation", "summary": "Analog circuit topology synthesis is integral to Electronic Design Automation\n(EDA), enabling the automated creation of circuit structures tailored to\nspecific design requirements. However, the vast design search space and strict\nconstraint adherence make efficient synthesis challenging. Leveraging the\nversatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel\nreinforcement learning (RL)-based framework for automated analog circuit\nsynthesis. The framework operates in two phases: instruction tuning, where an\nLLM learns to generate circuit topologies from structured prompts encoding\ndesign constraints, and RL refinement, which further improves the\ninstruction-tuned model using reward models that evaluate validity, efficiency,\nand output voltage. The refined model is then used directly to generate\ntopologies that satisfy the design constraints. Empirical results show that\nAUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by\n~14% compared to the best baselines, while reducing duplicate generation rates\nby ~38%. It achieves over 60% success in synthesizing valid circuits with\nlimited training data, demonstrating strong generalization. These findings\nhighlight the framework's effectiveness in scaling to complex circuits while\nmaintaining efficiency and constraint adherence, marking a significant\nadvancement in AI-driven circuit design.", "authors": ["Prashanth Vijayaraghavan", "Luyao Shi", "Ehsan Degan", "Vandana Mukherjee", "Xin Zhang"], "published_date": "2025-06-03", "title_zh": "AUTOCIRCUIT-RL：基於強化學習的大型語言模型於自動電路拓撲生成", "summary_zh": "類比電路拓撲合成是電子設計自動化的關鍵，可自動創建符合特定設計要求的電路結構。然而，龐大的設計空間和嚴格的約束條件使高效合成充滿挑戰。本研究提出一種基於強化學習的新穎框架AUTOCIRCUIT-RL，利用大型語言模型的多功能性實現自動類比電路合成。該框架包含指令微調和強化學習優化兩個階段：指令微調階段使大型語言模型學習從編碼設計約束的結構化提示中生成電路拓撲；強化學習優化階段利用獎勵模型評估有效性、效率和輸出電壓，進一步改進指令微調模型。優化後的模型直接用於生成滿足設計約束的拓撲。實驗結果表明，與最佳基準線相比，AUTOCIRCUIT-RL生成的有效電路多出約12%，效率提高約14%，重複生成率降低約38%。它在有限的訓練數據下成功合成超過60%的有效電路，展現出強大的泛化能力。這些發現突顯了該框架在擴展到複雜電路時保持效率和約束一致性的有效性，標誌著人工智慧驅動的電路設計取得了顯著進展。", "audio": "audios/2506.03122v1.mp3", "timestamp": "2025-06-04T05:19:18.097842"}
{"query": "Foundation Model", "id": "2506.03099v1", "url": "http://arxiv.org/abs/2506.03099v1", "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models", "summary": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/", "authors": ["Chetwin Low", "Weimin Wang"], "published_date": "2025-06-03", "title_zh": "TalkingMachines：基於自迴歸擴散模型的即時音訊驅動 FaceTime 式視訊", "summary_zh": "本研究提出TalkingMachines，一個高效框架，可將預訓練視訊生成模型轉換為即時、音訊驅動的角色動畫器。TalkingMachines透過整合音訊大型語言模型（LLM）與視訊生成基礎模型，實現自然的對話體驗。主要貢獻包括：(1)將預訓練的最先進圖像轉視訊DiT模型調整為180億參數的音訊驅動頭像生成模型；(2)透過非對稱知識蒸餾，從雙向教師模型提煉至稀疏因果自迴歸學生模型，實現無限視訊串流且避免錯誤累積；(3)設計高效能、低延遲的推論管線，包含多項關鍵工程優化，例如：(a)跨設備分離DiT和VAE解碼器，(b)利用CUDA流高效重疊設備間通訊與計算，(c)消除冗餘重複計算以最大化幀生成吞吐量。", "audio": "audios/2506.03099v1.mp3", "timestamp": "2025-06-04T05:19:25.959318"}
{"query": "Diffusion Model", "id": "2506.03111v1", "url": "http://arxiv.org/abs/2506.03111v1", "title": "Rectified Flows for Fast Multiscale Fluid Flow Modeling", "summary": "The statistical modeling of fluid flows is very challenging due to their\nmultiscale dynamics and extreme sensitivity to initial conditions. While\nrecently proposed conditional diffusion models achieve high fidelity, they\ntypically require hundreds of stochastic sampling steps at inference. We\nintroduce a rectified flow framework that learns a time-dependent velocity\nfield, transporting input to output distributions along nearly straight\ntrajectories. By casting sampling as solving an ordinary differential equation\n(ODE) along this straighter flow field, our method makes each integration step\nmuch more effective, using as few as eight steps versus (more than) 128 steps\nin standard score-based diffusion, without sacrificing predictive fidelity.\nExperiments on challenging multiscale flow benchmarks show that rectified flows\nrecover the same posterior distributions as diffusion models, preserve\nfine-scale features that MSE-trained baselines miss, and deliver\nhigh-resolution samples in a fraction of inference time.", "authors": ["Victor Armegioiu", "Yannick Ramic", "Siddhartha Mishra"], "published_date": "2025-06-03", "title_zh": "修正流用於快速多尺度流體流動建模", "summary_zh": "流體流動的統計建模極具挑戰，因其多尺度動態和對初始條件的極端敏感性。儘管新近提出的條件擴散模型能實現高保真度，但通常需要在推論階段進行數百個隨機採樣步驟。本文介紹一種校正流框架，該框架學習一個隨時間變化的速度場，沿近乎直線的軌跡將輸入傳輸到輸出分布。透過將採樣轉化為沿更直的流場求解常微分方程，此方法使每個積分步驟更有效，僅需八步即可達到標準基於分數的擴散模型一百二十八步以上的效果，且不犧牲預測保真度。在具挑戰性的多尺度流動基準測試中，實驗表明校正流可恢復與擴散模型相同的後驗分布，保留均方誤差訓練基準線遺漏的精細尺度特徵，並在極短的推論時間內提供高解析度樣本。", "audio": "audios/2506.03111v1.mp3", "timestamp": "2025-06-04T05:19:33.096830"}
{"query": "AI", "id": "2506.03103v1", "url": "http://arxiv.org/abs/2506.03103v1", "title": "DyTact: Capturing Dynamic Contacts in Hand-Object Manipulation", "summary": "Reconstructing dynamic hand-object contacts is essential for realistic\nmanipulation in AI character animation, XR, and robotics, yet it remains\nchallenging due to heavy occlusions, complex surface details, and limitations\nin existing capture techniques. In this paper, we introduce DyTact, a\nmarkerless capture method for accurately capturing dynamic contact in\nhand-object manipulations in a non-intrusive manner. Our approach leverages a\ndynamic, articulated representation based on 2D Gaussian surfels to model\ncomplex manipulations. By binding these surfels to MANO meshes, DyTact\nharnesses the inductive bias of template models to stabilize and accelerate\noptimization. A refinement module addresses time-dependent high-frequency\ndeformations, while a contact-guided adaptive sampling strategy selectively\nincreases surfel density in contact regions to handle heavy occlusion.\nExtensive experiments demonstrate that DyTact not only achieves\nstate-of-the-art dynamic contact estimation accuracy but also significantly\nimproves novel view synthesis quality, all while operating with fast\noptimization and efficient memory usage. Project Page:\nhttps://oliver-cong02.github.io/DyTact.github.io/ .", "authors": ["Xiaoyan Cong", "Angela Xing", "Chandradeep Pokhariya", "Rao Fu", "Srinath Sridhar"], "published_date": "2025-06-03", "title_zh": "DyTact：手物操控中動態接觸的捕捉", "summary_zh": "重建動態手物接觸對於AI角色動畫、XR及機器人領域的逼真操作至關重要，但因嚴重遮擋、複雜表面細節及現有捕捉技術的限制而充滿挑戰。本文提出DyTact，一種非侵入式的無標記捕捉方法，可精確捕捉手物操作中的動態接觸。此方法利用基於2D高斯曲面元素的動態關節表示來建模複雜操作。透過將這些曲面元素綁定到MANO網格，DyTact利用模板模型的歸納偏置來穩定並加速最佳化。精煉模組處理隨時間變化的高頻變形，而接觸引導的自適應採樣策略選擇性地增加接觸區域的曲面元素密度，以處理嚴重的遮擋。大量實驗表明，DyTact不僅實現了最先進的動態接觸估計精度，還顯著提高了新視圖合成質量，同時具有快速的最佳化和高效的記憶體使用率。", "audio": "audios/2506.03103v1.mp3", "timestamp": "2025-06-04T06:28:13.884448"}
{"query": "Foundation Model", "id": "2506.03056v1", "url": "http://arxiv.org/abs/2506.03056v1", "title": "Corrigibility as a Singular Target: A Vision for Inherently Reliable Foundation Models", "summary": "Foundation models (FMs) face a critical safety challenge: as capabilities\nscale, instrumental convergence drives default trajectories toward loss of\nhuman control, potentially culminating in existential catastrophe. Current\nalignment approaches struggle with value specification complexity and fail to\naddress emergent power-seeking behaviors. We propose \"Corrigibility as a\nSingular Target\" (CAST)-designing FMs whose overriding objective is empowering\ndesignated human principals to guide, correct, and control them. This paradigm\nshift from static value-loading to dynamic human empowerment transforms\ninstrumental drives: self-preservation serves only to maintain the principal's\ncontrol; goal modification becomes facilitating principal guidance. We present\na comprehensive empirical research agenda spanning training methodologies\n(RLAIF, SFT, synthetic data generation), scalability testing across model\nsizes, and demonstrations of controlled instructability. Our vision: FMs that\nbecome increasingly responsive to human guidance as capabilities grow, offering\na path to beneficial AI that remains as tool-like as possible, rather than\nsupplanting human judgment. This addresses the core alignment problem at its\nsource, preventing the default trajectory toward misaligned instrumental\nconvergence.", "authors": ["Ram Potham", "Max Harms"], "published_date": "2025-06-03", "title_zh": "可修正性作為單一目標：本質可靠基礎模型之願景", "summary_zh": "基礎模型面臨重大安全挑戰：隨著能力擴展，工具性趨同驅使模型趨向失控，可能導致生存危機。現有對齊方法難以應對價值規範的複雜性，且無法解決新興的權力尋求行為。我們提出「可修正性作為單一目標」(CAST)，旨在設計以賦能人類主體引導、修正和控制為首要目標的基礎模型。這種從靜態價值載入到動態人類賦權的範式轉變，重塑了工具性驅動：自我保護僅為維持主體的控制；目標修改轉為促進主體引導。我們提出全面的實證研究議程，涵蓋訓練方法（RLAIF、SFT、合成數據生成）、跨模型尺寸的可擴展性測試以及可控指導性的演示。我們的願景是，隨著能力增長，基礎模型對人類指導的響應越來越高，從而實現有益的人工智慧，使其盡可能保持工具性，而非取代人類判斷。這從根源上解決了核心對齊問題，防止了趨向錯位工具性趨同的預設路徑。", "audio": "audios/2506.03056v1.mp3", "timestamp": "2025-06-04T06:28:22.671962"}
{"query": "Diffusion Model", "id": "2506.03067v1", "url": "http://arxiv.org/abs/2506.03067v1", "title": "EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image Diffusion Models", "summary": "Text-to-image generation models~(e.g., Stable Diffusion) have achieved\nsignificant advancements, enabling the creation of high-quality and realistic\nimages based on textual descriptions. Prompt inversion, the task of identifying\nthe textual prompt used to generate a specific artifact, holds significant\npotential for applications including data attribution, model provenance, and\nwatermarking validation. Recent studies introduced a delayed projection scheme\nto optimize for prompts representative of the vocabulary space, though\nchallenges in semantic fluency and efficiency remain. Advanced image captioning\nmodels or visual large language models can generate highly interpretable\nprompts, but they often lack in image similarity. In this paper, we propose a\nprompt inversion technique called \\sys for text-to-image diffusion models,\nwhich includes initializing embeddings using a pre-trained image captioning\nmodel, refining them through reverse-engineering in the latent space, and\nconverting them to texts using an embedding-to-text model. Our experiments on\nthe widely-used datasets, such as MS COCO, LAION, and Flickr, show that our\nmethod outperforms existing methods in terms of image similarity, textual\nalignment, prompt interpretability and generalizability. We further illustrate\nthe application of our generated prompts in tasks such as cross-concept image\nsynthesis, concept manipulation, evolutionary multi-concept generation and\nunsupervised segmentation.", "authors": ["Mingzhe Li", "Gehao Zhang", "Zhenting Wang", "Shiqing Ma", "Siqi Pan", "Richard Cartwright", "Juan Zhai"], "published_date": "2025-06-03", "title_zh": "編譯器：文字到圖像擴散模型之有效且可解釋的提示反演", "summary_zh": "文本生成圖像模型取得顯著進展，能依據文本描述生成高品質圖像。提示詞反演技術旨在識別生成特定圖像的文本提示，對數據歸屬、模型溯源和水印驗證具重要意義。儘管有研究提出延遲投影方案以優化提示詞，但在語義流暢性和效率方面仍存挑戰。圖像描述模型雖能生成易於理解的提示詞，但圖像相似度不足。本研究提出名為\\sys的提示詞反演技術，該技術利用預訓練圖像描述模型初始化嵌入向量，於潛在空間進行反向工程優化，並將其轉換為文本。在MS COCO、LAION和Flickr等數據集上的實驗表明，本方法在圖像相似度、文本對齊、提示詞可解釋性和泛化能力方面優於現有方法。此外，研究亦展示生成提示詞於跨概念圖像合成、概念操作、演化多概念生成和無監督分割等任務上的應用。", "audio": "audios/2506.03067v1.mp3", "timestamp": "2025-06-04T06:28:30.540367"}
{"query": "AI", "id": "2506.03102v1", "url": "http://arxiv.org/abs/2506.03102v1", "title": "Designing Algorithmic Delegates: The Role of Indistinguishability in Human-AI Handoff", "summary": "As AI technologies improve, people are increasingly willing to delegate tasks\nto AI agents. In many cases, the human decision-maker chooses whether to\ndelegate to an AI agent based on properties of the specific instance of the\ndecision-making problem they are facing. Since humans typically lack full\nawareness of all the factors relevant to this choice for a given\ndecision-making instance, they perform a kind of categorization by treating\nindistinguishable instances -- those that have the same observable features --\nas the same. In this paper, we define the problem of designing the optimal\nalgorithmic delegate in the presence of categories. This is an important\ndimension in the design of algorithms to work with humans, since we show that\nthe optimal delegate can be an arbitrarily better teammate than the optimal\nstandalone algorithmic agent. The solution to this optimal delegation problem\nis not obvious: we discover that this problem is fundamentally combinatorial,\nand illustrate the complex relationship between the optimal design and the\nproperties of the decision-making task even in simple settings. Indeed, we show\nthat finding the optimal delegate is computationally hard in general. However,\nwe are able to find efficient algorithms for producing the optimal delegate in\nseveral broad cases of the problem, including when the optimal action may be\ndecomposed into functions of features observed by the human and the algorithm.\nFinally, we run computational experiments to simulate a designer updating an\nalgorithmic delegate over time to be optimized for when it is actually adopted\nby users, and show that while this process does not recover the optimal\ndelegate in general, the resulting delegate often performs quite well.", "authors": ["Sophie Greenwood", "Karen Levy", "Solon Barocas", "Hoda Heidari", "Jon Kleinberg"], "published_date": "2025-06-03", "title_zh": "演算法代理設計：不可區分性在人機交接中的作用", "summary_zh": "隨著人工智慧技術進步，人們更願意將任務委託給AI。決策者通常根據特定決策問題的特性來決定是否委託AI。由於人類通常缺乏對所有相關因素的充分認知，他們會對具有相同可觀察特徵的實例進行分類並同等對待。本文定義了在存在類別的情況下，設計最佳演算法代理人的問題。這對設計與人類協作的演算法至關重要，因為最佳代理人可能比最佳獨立演算法代理人優秀許多。最佳委託問題的解決方案並非顯而易見，它具有組合本質。即使在簡單環境中，最佳設計與決策任務的屬性之間也存在複雜關係。一般而言，找到最佳代理人在計算上是困難的。然而，我們在多種情況下找到了產生最佳代理人的高效演算法，包括當最佳行動可以分解為人類和演算法觀察到的特徵函數時。最後，我們進行計算實驗來模擬設計者隨時間更新演算法代理人，以針對用戶實際採用的情況進行優化。雖然此過程通常無法恢復最佳代理人，但結果通常表現良好。", "audio": "audios/2506.03102v1.mp3", "timestamp": "2025-06-04T07:19:18.834573"}
{"query": "Foundation Model", "id": "2506.02978v1", "url": "http://arxiv.org/abs/2506.02978v1", "title": "On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses", "summary": "Recent tabular Foundational Models (FM) such as TabPFN and TabICL, leverage\nin-context learning to achieve strong performance without gradient updates or\nfine-tuning. However, their robustness to adversarial manipulation remains\nlargely unexplored. In this work, we present a comprehensive study of the\nadversarial vulnerabilities of tabular FM, focusing on both their fragility to\ntargeted test-time attacks and their potential misuse as adversarial tools. We\nshow on three benchmarks in finance, cybersecurity and healthcare, that small,\nstructured perturbations to test inputs can significantly degrade prediction\naccuracy, even when training context remain fixed. Additionally, we demonstrate\nthat tabular FM can be repurposed to generate transferable evasion to\nconventional models such as random forests and XGBoost, and on a lesser extent\nto deep tabular models. To improve tabular FM, we formulate the robustification\nproblem as an optimization of the weights (adversarial fine-tuning), or the\ncontext (adversarial in-context learning). We introduce an in-context\nadversarial training strategy that incrementally replaces the context with\nadversarial perturbed instances, without updating model weights. Our approach\nimproves robustness across multiple tabular benchmarks. Together, these\nfindings position tabular FM as both a target and a source of adversarial\nthreats, highlighting the urgent need for robust training and evaluation\npractices in this emerging paradigm.", "authors": ["Mohamed Djilani", "Thibault Simonetto", "Karim Tit", "Florian Tambon", "Paul Récamier", "Salah Ghamizi", "Maxime Cordy", "Mike Papadakis"], "published_date": "2025-06-03", "title_zh": "表格型基礎模型的穩健性研究：測試時攻擊與上下文防禦", "summary_zh": "近期表格型基礎模型（如TabPFN和TabICL）利用上下文學習在無需梯度更新或微調的情況下實現優異性能，但其對抗攻擊的穩健性尚未充分研究。本研究全面探討表格型基礎模型的對抗脆弱性，著重於其針對性測試時攻擊的脆弱性及其作為對抗工具的潛在濫用。在金融、網絡安全和醫療保健三個基準測試中，結果顯示對測試輸入的微小、結構化擾動會顯著降低預測準確性，即使訓練上下文保持不變。此外，研究表明表格型基礎模型可被用於生成對傳統模型（如隨機森林和XGBoost）的可轉移逃避，對深度表格模型的影響較小。為提升表格型基礎模型的穩健性，研究將其形式化為權重（對抗微調）或上下文（對抗上下文學習）的優化問題，並提出一種上下文對抗訓練策略，在不更新模型權重的情況下，逐步用對抗擾動實例替換上下文。該方法提高了多個表格基準測試的穩健性。總體而言，這些發現表明表格型基礎模型既是對抗威脅的目標，也是其來源，凸顯了在這一新興範例中迫切需要穩健的訓練和評估方法。", "audio": "audios/2506.02978v1.mp3", "timestamp": "2025-06-04T07:19:26.604640"}
{"query": "Diffusion Model", "id": "2506.03004v1", "url": "http://arxiv.org/abs/2506.03004v1", "title": "PartComposer: Learning and Composing Part-Level Concepts from Single-Image Examples", "summary": "We present PartComposer: a framework for part-level concept learning from\nsingle-image examples that enables text-to-image diffusion models to compose\nnovel objects from meaningful components. Existing methods either struggle with\neffectively learning fine-grained concepts or require a large dataset as input.\nWe propose a dynamic data synthesis pipeline generating diverse part\ncompositions to address one-shot data scarcity. Most importantly, we propose to\nmaximize the mutual information between denoised latents and structured concept\ncodes via a concept predictor, enabling direct regulation on concept\ndisentanglement and re-composition supervision. Our method achieves strong\ndisentanglement and controllable composition, outperforming subject and\npart-level baselines when mixing concepts from the same, or different, object\ncategories.", "authors": ["Junyu Liu", "R. Kenny Jones", "Daniel Ritchie"], "published_date": "2025-06-03", "title_zh": "部件作曲家：從單張圖像範例學習與合成部件層級概念", "summary_zh": "PartComposer：一個部件級概念學習框架，僅需單張圖像即可學習，使文字轉圖像擴散模型能以具意義的組件合成新穎物件。為了解決少量樣本數據的不足，提出動態數據合成流程，生成多樣的部件組合。透過概念預測器，最大化去噪潛變量與結構化概念碼之間的互信息，直接規範概念解耦和重組。此方法實現了強大的解耦和可控組裝，在混合相同或不同物件類別的概念時，優於主體和部件級基準方法。", "audio": "audios/2506.03004v1.mp3", "timestamp": "2025-06-04T07:19:30.771910"}
{"query": "AI", "id": "2506.03097v1", "url": "http://arxiv.org/abs/2506.03097v1", "title": "EgoVLM: Policy Optimization for Egocentric Video Understanding", "summary": "Emerging embodied AI applications, such as wearable cameras and autonomous\nagents, have underscored the need for robust reasoning from first person video\nstreams. We introduce EgoVLM, a vision-language model specifically designed to\nintegrate visual comprehension and spatial-temporal reasoning within egocentric\nvideo contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization\n(GRPO), a reinforcement learning method adapted to align model outputs with\nhuman-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly\ntune using RL without any supervised fine-tuning phase on chain-of-thought\n(CoT) data. We evaluate EgoVLM on egocentric video question answering\nbenchmarks and show that domain-specific training substantially improves\nperformance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on\nnon-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by\n14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By\nexplicitly generating reasoning traces, EgoVLM enhances interpretability,\nmaking it well-suited for downstream applications. Furthermore, we introduce a\nnovel keyframe-based reward that incorporates salient frame selection to guide\nreinforcement learning optimization. This reward formulation opens a promising\navenue for future exploration in temporally grounded egocentric reasoning.", "authors": ["Ashwin Vinod", "Shrey Pandit", "Aditya Vavre", "Linshen Liu"], "published_date": "2025-06-03", "title_zh": "EgoVLM：以自我中心視訊理解為目標之策略最佳化", "summary_zh": "本研究提出EgoVLM，一款專為第一人稱視訊設計的視覺語言模型，旨在整合視覺理解與時空推理。EgoVLM透過群組相對策略優化(GRPO)進行微調，此強化學習方法可使模型輸出與人類推理步驟對齊。如同DeepSeek R1-Zero的方法，我們直接使用強化學習進行調整，無需在思維鏈(CoT)資料上進行監督式微調。EgoVLM在第一人稱視訊問答基準測試中表現優異，表明領域特定訓練可大幅提升效能，優於通用型視覺語言模型。EgoVLM-3B僅使用非CoT第一人稱資料訓練，在EgoSchema基準測試中，其準確率分別超越Qwen2.5-VL 3B和7B模型14.33和13.87個百分點。EgoVLM透過生成推理軌跡來增強可解釋性，更適合下游應用。此外，我們引入一種基於關鍵影格的新穎獎勵機制，該機制結合了顯著影格選擇以引導強化學習優化，為未來在時序定位的第一人稱推理探索開闢了有前景的途徑。", "audio": "audios/2506.03097v1.mp3", "timestamp": "2025-06-04T08:26:51.114906"}
{"query": "Foundation Model", "id": "2506.02964v1", "url": "http://arxiv.org/abs/2506.02964v1", "title": "FORLA:Federated Object-centric Representation Learning with Slot Attention", "summary": "Learning efficient visual representations across heterogeneous unlabeled\ndatasets remains a central challenge in federated learning. Effective federated\nrepresentations require features that are jointly informative across clients\nwhile disentangling domain-specific factors without supervision. We introduce\nFORLA, a novel framework for federated object-centric representation learning\nand feature adaptation across clients using unsupervised slot attention. At the\ncore of our method is a shared feature adapter, trained collaboratively across\nclients to adapt features from foundation models, and a shared slot attention\nmodule that learns to reconstruct the adapted features. To optimize this\nadapter, we design a two-branch student-teacher architecture. In each client, a\nstudent decoder learns to reconstruct full features from foundation models,\nwhile a teacher decoder reconstructs their adapted, low-dimensional\ncounterpart. The shared slot attention module bridges cross-domain learning by\naligning object-level representations across clients. Experiments in multiple\nreal-world datasets show that our framework not only outperforms centralized\nbaselines on object discovery but also learns a compact, universal\nrepresentation that generalizes well across domains. This work highlights\nfederated slot attention as an effective tool for scalable, unsupervised visual\nrepresentation learning from cross-domain data with distributed concepts.", "authors": ["Guiqiu Liao", "Matjaz Jogan", "Eric Eaton", "Daniel A. Hashimoto"], "published_date": "2025-06-03", "title_zh": "FORLA：基於槽注意力的聯邦物件中心表示學習", "summary_zh": "在聯邦學習中，跨異質無標記數據集學習高效視覺表徵仍是一大挑戰。有效的聯邦表徵需具備跨客戶端的信息性，同時在無監督下解耦特定領域因素。本研究提出FORLA，一種新穎的聯邦物件中心表徵學習框架，利用無監督槽注意力實現跨客戶端的特徵適應。核心為共享特徵適配器，經跨客戶端協作訓練以適配基礎模型特徵，以及學習重建適配特徵的共享槽注意力模組。為優化適配器，設計了雙分支師生架構。各客戶端中，學生解碼器學習重建基礎模型的完整特徵，教師解碼器則重建其適配的低維版本。共享槽注意力模組通過對齊跨客戶端的物件級表徵來橋接跨域學習。在多個真實世界數據集上的實驗表明，本框架不僅在物件發現方面優於集中式基準線，還學習到一種緊湊、通用的表徵，能很好地跨域泛化。此研究突顯了聯邦槽注意力作為一種從具分散概念的跨域數據中進行可擴展、無監督視覺表徵學習的有效工具。", "audio": "audios/2506.02964v1.mp3", "timestamp": "2025-06-04T08:26:58.038672"}
{"query": "Diffusion Model", "id": "2506.02981v1", "url": "http://arxiv.org/abs/2506.02981v1", "title": "Astrophotography turbulence mitigation via generative models", "summary": "Photography is the cornerstone of modern astronomical and space research.\nHowever, most astronomical images captured by ground-based telescopes suffer\nfrom atmospheric turbulence, resulting in degraded imaging quality. While\nmulti-frame strategies like lucky imaging can mitigate some effects, they\ninvolve intensive data acquisition and complex manual processing. In this\npaper, we propose AstroDiff, a generative restoration method that leverages\nboth the high-quality generative priors and restoration capabilities of\ndiffusion models to mitigate atmospheric turbulence. Extensive experiments\ndemonstrate that AstroDiff outperforms existing state-of-the-art learning-based\nmethods in astronomical image turbulence mitigation, providing higher\nperceptual quality and better structural fidelity under severe turbulence\nconditions. Our code and additional results are available at\nhttps://web-six-kappa-66.vercel.app/", "authors": ["Joonyeoup Kim", "Yu Yuan", "Xingguang Zhang", "Xijun Wang", "Stanley Chan"], "published_date": "2025-06-03", "title_zh": "基於生成模型的星空攝影湍流緩解", "summary_zh": "攝影是現代天文和太空研究的基石。地面望遠鏡影像常受大氣擾動影響，降低成像品質。多幀策略雖能緩解，但需大量數據和複雜人工處理。本文提出AstroDiff，一種生成式修復方法，利用擴散模型的高品質生成先驗和修復能力，減輕大氣擾動。實驗表明，AstroDiff在天文影像擾動減輕方面優於現有先進的基於學習方法，在高擾動下提供更高的感知品質和更好的結構保真度。", "audio": "audios/2506.02981v1.mp3", "timestamp": "2025-06-04T08:27:02.481515"}
{"query": "AI", "id": "2506.03053v1", "url": "http://arxiv.org/abs/2506.03053v1", "title": "MAEBE: Multi-Agent Emergent Behavior Framework", "summary": "Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts.", "authors": ["Sinem Erisken", "Timothy Gothard", "Martin Leitgab", "Ram Potham"], "published_date": "2025-06-03", "title_zh": "MAEBE：多代理湧現行為框架", "summary_zh": "隨著多代理AI協作日益普及，單獨評估大型語言模型的傳統AI安全方法已不足。本文提出多代理湧現行為評估框架(MAEBE)，系統性評估此類風險。透過MAEBE結合至善基準（以及一種新穎的雙重反轉提問技術），我們發現：(1)語言模型的道德偏好，特別是對工具性傷害的偏好，異常脆弱，且在單一代理和協作情境下，會因提問方式顯著改變。(2)由於湧現的群體動力，語言模型協作的道德推理無法直接從個別代理行為預測。(3)具體而言，協作展現如同儕壓力等現象，影響決策趨同，即便在監管者引導下亦然，突顯了獨特的安全性與對齊挑戰。我們的研究結果強調，必須在互動的多代理情境中評估AI系統。", "audio": "audios/2506.03053v1.mp3", "timestamp": "2025-06-04T09:20:58.090279"}
{"query": "Foundation Model", "id": "2506.02924v1", "url": "http://arxiv.org/abs/2506.02924v1", "title": "INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification", "summary": "In this work, we describe our team's approach to eRisk's 2025 Task 1: Search\nfor Symptoms of Depression. Given a set of sentences and the Beck's Depression\nInventory - II (BDI) questionnaire, participants were tasked with submitting up\nto 1,000 sentences per depression symptom in the BDI, sorted by relevance.\nParticipant submissions were evaluated according to standard Information\nRetrieval (IR) metrics, including Average Precision (AP) and R-Precision\n(R-PREC). The provided training data, however, consisted of sentences labeled\nas to whether a given sentence was relevant or not w.r.t. one of BDI's\nsymptoms. Due to this labeling limitation, we framed our development as a\nbinary classification task for each BDI symptom, and evaluated accordingly. To\nthat end, we split the available labeled data into training and validation\nsets, and explored foundation model fine-tuning, sentence similarity, Large\nLanguage Model (LLM) prompting, and ensemble techniques. The validation results\nrevealed that fine-tuning foundation models yielded the best performance,\nparticularly when enhanced with synthetic data to mitigate class imbalance. We\nalso observed that the optimal approach varied by symptom. Based on these\ninsights, we devised five independent test runs, two of which used ensemble\nmethods. These runs achieved the highest scores in the official IR evaluation,\noutperforming submissions from 16 other teams.", "authors": ["Diogo A. P. Nunes", "Eugénio Ribeiro"], "published_date": "2025-06-03", "title_zh": "INESC-ID於eRisk 2025：探索基於微調、相似性及提示之憂鬱症狀識別方法", "summary_zh": "本文描述我們團隊參與2025年eRisk任務1：搜尋憂鬱症狀的方法。任務目標是針對貝克憂鬱量表第二版(BDI)中的每個憂鬱症狀，提交最多1000句相關句子，並按相關性排序。主辦方根據平均精確度(AP)和R-精確度(R-PREC)等資訊檢索(IR)指標評估提交結果。由於提供的訓練資料僅標記句子是否與特定BDI症狀相關，我們將開發轉化為針對每個BDI症狀的二元分類任務。我們將可用標記數據劃分為訓練集和驗證集，並探索了基礎模型微調、句子相似度、大型語言模型(LLM)提示和集成技術。驗證結果顯示，微調基礎模型效果最佳，尤其是在使用合成數據緩解類別不平衡的情況下。我們還觀察到，最佳方法因症狀而異。基於這些發現，我們設計了五個獨立的測試運行，其中兩個使用了集成方法。這些運行在官方IR評估中獲得了最高分，優於其他16個團隊的提交結果。", "audio": "audios/2506.02924v1.mp3", "timestamp": "2025-06-04T09:21:07.862537"}
{"query": "Diffusion Model", "id": "2506.02908v1", "url": "http://arxiv.org/abs/2506.02908v1", "title": "Diffusion Buffer: Online Diffusion-based Speech Enhancement with Sub-Second Latency", "summary": "Diffusion models are a class of generative models that have been recently\nused for speech enhancement with remarkable success but are computationally\nexpensive at inference time. Therefore, these models are impractical for\nprocessing streaming data in real-time. In this work, we adapt a sliding window\ndiffusion framework to the speech enhancement task. Our approach progressively\ncorrupts speech signals through time, assigning more noise to frames close to\nthe present in a buffer. This approach outputs denoised frames with a delay\nproportional to the chosen buffer size, enabling a trade-off between\nperformance and latency. Empirical results demonstrate that our method\noutperforms standard diffusion models and runs efficiently on a GPU, achieving\nan input-output latency in the order of 0.3 to 1 seconds. This marks the first\npractical diffusion-based solution for online speech enhancement.", "authors": ["Bunlong Lay", "Rostilav Makarov", "Timo Gerkmann"], "published_date": "2025-06-03", "title_zh": "擴散緩衝：基於線上擴散且具亞秒延遲的語音增強", "summary_zh": "擴散模型在語音增強方面表現出色，但推論時計算成本高昂，不適用於即時串流數據處理。本文提出一種滑動窗口擴散框架，逐漸噪化語音信號，對緩衝區中接近當前的幀施加更多噪聲。此方法以與緩衝區大小成比例的延遲輸出降噪幀，從而在性能和延遲之間取得平衡。實驗結果表明，該方法優於標準擴散模型，並能在GPU上高效運行，實現0.3至1秒的輸入輸出延遲，是首個實用的基於擴散的線上語音增強解決方案。", "audio": "audios/2506.02908v1.mp3", "timestamp": "2025-06-04T09:21:13.242555"}
{"query": "AI", "id": "2506.03052v1", "url": "http://arxiv.org/abs/2506.03052v1", "title": "Feedstack: Layering Structured Representations over Unstructured Feedback to Scaffold Human AI Conversation", "summary": "Many conversational user interfaces facilitate linear conversations with\nturn-based dialogue, similar to face-to-face conversations between people.\nHowever, digital conversations can afford more than simple back-and-forth; they\ncan be layered with interaction techniques and structured representations that\nscaffold exploration, reflection, and shared understanding between users and AI\nsystems. We introduce Feedstack, a speculative interface that augments feedback\nconversations with layered affordances for organizing, navigating, and\nexternalizing feedback. These layered structures serve as a shared\nrepresentation of the conversation that can surface user intent and reveal\nunderlying design principles. This work represents an early exploration of this\nvision using a research-through-design approach. We describe system features\nand design rationale, and present insights from two formative (n=8, n=8)\nstudies to examine how novice designers engage with these layered supports.\nRather than presenting a conclusive evaluation, we reflect on Feedstack as a\ndesign probe that opens up new directions for conversational feedback systems.", "authors": ["Hannah Vy Nguyen", "Yu-Chun Grace Yen", "Omar Shakir", "Hang Huynh", "Sebastian Gutierrez", "June A. Smith", "Sheila Jimenez", "Salma Abdelgelil", "Stephen MacNeil"], "published_date": "2025-06-03", "title_zh": "Feedstack：藉由在非結構化回饋上疊加結構化表徵以支撐人機對話", "summary_zh": "許多對話式使用者介面以輪流發言的方式促進線性對話，類似於人際互動。然而，數位對話能提供的不僅僅是簡單的來回互動；它們可以透過互動技巧和結構化表示來分層，以支持使用者和人工智慧系統之間的探索、反思和共享理解。我們介紹Feedstack，一個推測性的介面，通過分層的功能來增強回饋對話，以便組織、導航和外部化回饋。這些分層結構作為對話的共享表示，可以揭示使用者意圖和潛在的設計原則。本研究透過設計研究方法對此願景進行初步探索。我們描述了系統功能和設計原理，並呈現了來自兩項形成性研究的見解（n=8, n=8），以檢驗新手設計師如何使用這些分層支援。我們將Feedstack視為一個設計探針，而非提供結論性評估，以此開啟對話式回饋系統的新方向。", "audio": "audios/2506.03052v1.mp3", "timestamp": "2025-06-04T10:21:01.526083"}
{"query": "Foundation Model", "id": "2506.02914v1", "url": "http://arxiv.org/abs/2506.02914v1", "title": "Towards Auto-Annotation from Annotation Guidelines: A Benchmark through 3D LiDAR Detection", "summary": "A crucial yet under-appreciated prerequisite in machine learning solutions\nfor real-applications is data annotation: human annotators are hired to\nmanually label data according to detailed, expert-crafted guidelines. This is\noften a laborious, tedious, and costly process. To study methods for\nfacilitating data annotation, we introduce a new benchmark AnnoGuide:\nAuto-Annotation from Annotation Guidelines. It aims to evaluate automated\nmethods for data annotation directly from expert-defined annotation guidelines,\neliminating the need for manual labeling. As a case study, we repurpose the\nwell-established nuScenes dataset, commonly used in autonomous driving\nresearch, which provides comprehensive annotation guidelines for labeling LiDAR\npoint clouds with 3D cuboids across 18 object classes. These guidelines include\na few visual examples and textual descriptions, but no labeled 3D cuboids in\nLiDAR data, making this a novel task of multi-modal few-shot 3D detection\nwithout 3D annotations. The advances of powerful foundation models (FMs) make\nAnnoGuide especially timely, as FMs offer promising tools to tackle its\nchallenges. We employ a conceptually straightforward pipeline that (1) utilizes\nopen-source FMs for object detection and segmentation in RGB images, (2)\nprojects 2D detections into 3D using known camera poses, and (3) clusters LiDAR\npoints within the frustum of each 2D detection to generate a 3D cuboid.\nStarting with a non-learned solution that leverages off-the-shelf FMs, we\nprogressively refine key components and achieve significant performance\nimprovements, boosting 3D detection mAP from 12.1 to 21.9! Nevertheless, our\nresults highlight that AnnoGuide remains an open and challenging problem,\nunderscoring the urgent need for developing LiDAR-based FMs. We release our\ncode and models at GitHub: https://annoguide.github.io/annoguide3Dbenchmark", "authors": ["Yechi Ma", "Wei Hua", "Shu Kong"], "published_date": "2025-06-03", "title_zh": "基於標註指南的自動標註方法：以三維光達偵測為例的基準測試", "summary_zh": "機器學習在現實應用中，資料標註是重要但常被忽略的前提。此過程需聘請人工標註員依據專家制定的詳細指南手動標記資料，耗時費力且成本高昂。為研究輔助資料標註的方法，我們推出新基準AnnoGuide：從標註指南自動標註。旨在評估直接從專家定義的標註指南自動標註資料的方法，免除手動標記的需求。案例研究採用nuScenes數據集，該數據集常用於自動駕駛研究，提供全面的標註指南，用於標記具18個物件類別的LiDAR點雲3D立方體。指南包含視覺範例和文字描述，但沒有LiDAR數據中的3D立方體標籤，形成新穎的多模態少樣本3D偵測任務，且無3D標註。大型基礎模型的進展使AnnoGuide更具時效性，提供應對挑戰的工具。我們採用簡單流程：(1)使用開放原始碼基礎模型進行RGB圖像的物件偵測和分割，(2)利用已知相機姿態將2D偵測投影到3D，(3)在每個2D偵測的視錐體內群聚LiDAR點以生成3D立方體。從利用現成基礎模型的非學習解決方案開始，逐步改進關鍵組件，顯著提升效能，將3D偵測mAP從12.1提升到21.9。然而，結果顯示AnnoGuide仍具挑戰性，突顯開發基於LiDAR的基礎模型之迫切性。我們在GitHub上發布程式碼和模型。", "audio": "audios/2506.02914v1.mp3", "timestamp": "2025-06-04T10:21:11.116433"}
{"query": "Diffusion Model", "id": "2506.02858v1", "url": "http://arxiv.org/abs/2506.02858v1", "title": "DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization", "summary": "Language-queried Audio Source Separation (LASS) enables open-vocabulary sound\nseparation via natural language queries. While existing methods rely on\ntask-specific training, we explore whether pretrained diffusion models,\noriginally designed for audio generation, can inherently perform separation\nwithout further training. In this study, we introduce a training-free framework\nleveraging generative priors for zero-shot LASS. Analyzing na\\\"ive adaptations,\nwe identify key limitations arising from modality-specific challenges.To\naddress these issues, we propose Diffusion-Guided Mask Optimization (DGMO), a\ntest-time optimization framework that refines spectrogram masks for precise,\ninput-aligned separation. Our approach effectively repurposes pretrained\ndiffusion models for source separation, achieving competitive performance\nwithout task-specific supervision. This work expands the application of\ndiffusion models beyond generation, establishing a new paradigm for zero-shot\naudio separation. The code is available at: https://wltschmrz.github.io/DGMO/", "authors": ["Geonyoung Lee", "Geonhee Han", "Paul Hongsuck Seo"], "published_date": "2025-06-03", "title_zh": "DGMO：基於擴散引導遮罩優化的免訓練音源分離", "summary_zh": "語言查詢音訊源分離（LASS）透過自然語言查詢實現開放詞彙的聲音分離。現有方法依賴於特定任務訓練，本研究探索預訓練擴散模型（原用於音訊生成）是否能內在地執行分離，無需額外訓練。我們提出一個無訓練框架，利用生成先驗進行零樣本LASS。分析初步調整後，我們發現了模態特定挑戰導致的關鍵限制。為解決這些問題，我們提出擴散引導遮罩優化（DGMO），一種測試時優化框架，用於精確調整 spectrogram 遮罩以進行輸入對齊分離。我們的途徑有效地將預訓練擴散模型重新用於源分離，在沒有特定任務監督的情況下實現了具競爭力的性能。這項工作擴展了擴散模型在生成之外的應用，為零樣本音訊分離建立了新的範例。程式碼位於：https://wltschmrz.github.io/DGMO/", "audio": "audios/2506.02858v1.mp3", "timestamp": "2025-06-04T10:21:17.732774"}
{"query": "AI", "id": "2506.03041v1", "url": "http://arxiv.org/abs/2506.03041v1", "title": "AI-Augmented OTDR Fault Localization Framework for Resilient Rural Fiber Networks in the United States", "summary": "This research presents a novel framework that combines traditional Optical\nTime-Domain Reflectometer (OTDR) signal analysis with machine learning to\nlocalize and classify fiber optic faults in rural broadband infrastructures.\nThe proposed system addresses a critical need in the expansion of middle-mile\nand last-mile networks, particularly in regions targeted by the U.S. Broadband\nEquity, Access, and Deployment (BEAD) Program. By enhancing fault diagnosis\nthrough a predictive, AI-based model, this work enables proactive network\nmaintenance in low-resource environments. Experimental evaluations using a\ncontrolled fiber testbed and synthetic datasets simulating rural network\nconditions demonstrate that the proposed method significantly improves\ndetection accuracy and reduces false positives compared to conventional\nthresholding techniques. The solution offers a scalable, field-deployable tool\nfor technicians and ISPs engaged in rural broadband deployment.", "authors": ["Sabab Al Farabi"], "published_date": "2025-06-03", "title_zh": "用於美國具韌性鄉村光纖網絡的AI增強型OTDR故障定位框架", "summary_zh": "本研究提出一種新穎框架，結合傳統光時域反射儀(OTDR)信號分析與機器學習，以定位和分類鄉村寬頻基礎設施中的光纖故障。此系統解決了中程和末程網路擴展的關鍵需求，尤其是在美國寬頻公平、接入和部署(BEAD)計畫針對的區域。透過基於AI的預測模型增強故障診斷，實現低資源環境下的主動網路維護。實驗評估表明，與傳統閾值技術相比，該方法顯著提高了檢測準確性並減少了誤報。此解決方案為參與鄉村寬頻部署的技術人員和網路服務供應商提供可擴展且可現場部署的工具。", "audio": "audios/2506.03041v1.mp3", "timestamp": "2025-06-04T11:17:16.282444"}
{"query": "Foundation Model", "id": "2506.02911v1", "url": "http://arxiv.org/abs/2506.02911v1", "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning", "summary": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1.", "authors": ["Yin Fang", "Qiao Jin", "Guangzhi Xiong", "Bowen Jin", "Xianrui Zhong", "Siru Ouyang", "Aidong Zhang", "Jiawei Han", "Zhiyong Lu"], "published_date": "2025-06-03", "title_zh": "Cell-o1：利用強化學習訓練大型語言模型解決單細胞推理謎題", "summary_zh": "細胞類型註釋是分析單細胞RNA測序數據異質性的關鍵任務。現有基礎模型雖可自動執行此過程，但通常獨立註釋細胞，缺乏批次層面的細胞環境考量及解釋性推理。為模擬專家基於領域知識對不同細胞群體進行註釋的工作流程，我們提出了CellPuzzles任務，目標是為一批細胞分配獨特的細胞類型。此基準測試涵蓋多種組織、疾病和供體條件，需跨批次層面細胞環境進行推理以確保標籤唯一性。大型語言模型在此任務中表現不佳，最佳基準模型OpenAI的o1僅達到19.0%的批次層面準確度。為了解決此問題，我們提出了Cell-o1，一個70億參數的語言模型，透過監督式微調和強化學習訓練，前者使用提煉後的推理軌跡，後者使用批次層面獎勵。Cell-o1達到最先進的性能，超越o1超過73%，並在不同環境中表現出良好的泛化能力。對訓練動態和推理行為的進一步分析，提供了對批次層面註釋性能和湧現的專家式推理的深入見解。代碼和數據可在https://github.com/ncbi-nlp/cell-o1獲取。", "audio": "audios/2506.02911v1.mp3", "timestamp": "2025-06-04T11:17:24.465060"}
{"query": "Diffusion Model", "id": "2506.02698v1", "url": "http://arxiv.org/abs/2506.02698v1", "title": "Smoothed Preference Optimization via ReNoise Inversion for Aligning Diffusion Models with Varied Human Preferences", "summary": "Direct Preference Optimization (DPO) aligns text-to-image (T2I) generation\nmodels with human preferences using pairwise preference data. Although\nsubstantial resources are expended in collecting and labeling datasets, a\ncritical aspect is often neglected: \\textit{preferences vary across individuals\nand should be represented with more granularity.} To address this, we propose\nSmPO-Diffusion, a novel method for modeling preference distributions to improve\nthe DPO objective, along with a numerical upper bound estimation for the\ndiffusion optimization objective. First, we introduce a smoothed preference\ndistribution to replace the original binary distribution. We employ a reward\nmodel to simulate human preferences and apply preference likelihood averaging\nto improve the DPO loss, such that the loss function approaches zero when\npreferences are similar. Furthermore, we utilize an inversion technique to\nsimulate the trajectory preference distribution of the diffusion model,\nenabling more accurate alignment with the optimization objective. Our approach\neffectively mitigates issues of excessive optimization and objective\nmisalignment present in existing methods through straightforward modifications.\nOur SmPO-Diffusion achieves state-of-the-art performance in preference\nevaluation, outperforming baselines across metrics with lower training costs.\nThe project page is https://jaydenlyh.github.io/SmPO-project-page/.", "authors": ["Yunhong Lu", "Qichao Wang", "Hengyuan Cao", "Xiaoyin Xu", "Min Zhang"], "published_date": "2025-06-03", "title_zh": "基於重噪反演的平滑偏好優化：使擴散模型與多樣化人類偏好對齊", "summary_zh": "直接偏好最佳化 (DPO) 利用成對偏好資料使文字生成圖像模型與人類偏好對齊。儘管收集和標註資料集耗費大量資源，但一個關鍵面向常被忽略：偏好因人而異，應以更細緻的方式呈現。為此，我們提出 SmPO-Diffusion，一種對偏好分佈進行建模以改進 DPO 目標的新方法，並針對擴散最佳化目標進行數值上限估計。首先，我們引入平滑偏好分佈來替換原始二元分佈。我們採用獎勵模型來模擬人類偏好，並應用偏好可能性平均來改進 DPO 損失，使損失函數在偏好相似時接近於零。此外，我們利用反演技術來模擬擴散模型的軌跡偏好分佈，從而更準確地與最佳化目標對齊。我們的途徑透過簡單的修改有效地緩解了現有方法中存在的過度最佳化和目標未對齊問題。SmPO-Diffusion 在偏好評估中取得了最先進的性能，在各項指標上均優於基準線，且訓練成本更低。專案頁面位於 https://jaydenlyh.github.io/SmPO-project-page/。", "audio": "audios/2506.02698v1.mp3", "timestamp": "2025-06-04T11:17:32.222435"}
{"query": "AI", "id": "2506.03011v1", "url": "http://arxiv.org/abs/2506.03011v1", "title": "Coding Agents with Multimodal Browsing are Generalist Problem Solvers", "summary": "Modern human labor is characterized by specialization; we train for years and\ndevelop particular tools that allow us to perform well across a variety of\ntasks. In addition, AI agents have been specialized for domains such as\nsoftware engineering, web navigation, and workflow automation. However, this\nresults in agents that are good for one thing but fail to generalize beyond\ntheir intended scope. One reason for this is that agent developers provide a\nhighly specialized set of tools or make architectural decisions optimized for a\nspecific use case or benchmark. In this work, we ask the question: what is the\nminimal set of general tools that can be used to achieve high performance\nacross a diverse set of tasks? Our answer is OpenHands-Versa, a generalist\nagent built with a modest number of general tools: code editing and execution,\nweb search, as well as multimodal web browsing and file access. Importantly,\nOpenHands-Versa demonstrates superior or competitive performance over leading\nspecialized agents across three diverse and challenging benchmarks: SWE-Bench\nMultimodal, GAIA, and The Agent Company, outperforming the best-performing\npreviously published results with absolute improvements in success rate of 9.1,\n1.3, and 9.1 points respectively. Further, we show how existing\nstate-of-the-art multi-agent systems fail to generalize beyond their target\ndomains. These results demonstrate the feasibility of developing a generalist\nagent to solve diverse tasks and establish OpenHands-Versa as a strong baseline\nfor future research.", "authors": ["Aditya Bharat Soni", "Boxuan Li", "Xingyao Wang", "Valerie Chen", "Graham Neubig"], "published_date": "2025-06-03", "title_zh": "具備多模態瀏覽能力的程式碼智能體是通才問題解決者", "summary_zh": "現代人類勞動分工精細，AI亦然。然而，專業化導致通用性不足。本文探討能於多樣任務中實現高效能的最小通用工具集。我們提出OpenHands-Versa，一基於程式碼編輯與執行、網路搜尋、多模態網路瀏覽及檔案存取等通用工具的泛用型代理。OpenHands-Versa在SWE-Bench Multimodal、GAIA和The Agent Company等基準測試中，超越或匹敵頂尖的專業代理，成功率分別提升9.1、1.3和9.1個百分點。研究亦顯示現有多代理系統難以跨領域泛化。這些結果驗證開發泛用型代理的可行性，並確立OpenHands-Versa為未來研究的基準。", "audio": "audios/2506.03011v1.mp3", "timestamp": "2025-06-04T12:39:18.611853"}
{"query": "Foundation Model", "id": "2506.02744v1", "url": "http://arxiv.org/abs/2506.02744v1", "title": "Enriching Location Representation with Detailed Semantic Information", "summary": "Spatial representations that capture both structural and semantic\ncharacteristics of urban environments are essential for urban modeling.\nTraditional spatial embeddings often prioritize spatial proximity while\nunderutilizing fine-grained contextual information from places. To address this\nlimitation, we introduce CaLLiPer+, an extension of the CaLLiPer model that\nsystematically integrates Point-of-Interest (POI) names alongside categorical\nlabels within a multimodal contrastive learning framework. We evaluate its\neffectiveness on two downstream tasks, land use classification and\nsocioeconomic status distribution mapping, demonstrating consistent performance\ngains of 4% to 11% over baseline methods. Additionally, we show that\nincorporating POI names enhances location retrieval, enabling models to capture\ncomplex urban concepts with greater precision. Ablation studies further reveal\nthe complementary role of POI names and the advantages of leveraging pretrained\ntext encoders for spatial representations. Overall, our findings highlight the\npotential of integrating fine-grained semantic attributes and multimodal\nlearning techniques to advance the development of urban foundation models.", "authors": ["Junyuan Liu", "Xinglei Wang", "Tao Cheng"], "published_date": "2025-06-03", "title_zh": "利用詳盡語義資訊豐富位置表徵", "summary_zh": "城市建模需兼顧結構與語義的空間表徵。傳統空間嵌入側重鄰近性，忽略場所細緻的語境資訊。本研究提出CaLLiPer+，擴展CaLLiPer模型，於多模態對比學習框架中整合興趣點名稱與類別標籤。於土地利用分類和社會經濟地位分佈映射兩項下游任務評估，性能較基線方法提升4%至11%。研究表明，納入興趣點名稱可增強位置檢索，更精準捕捉複雜城市概念。消融實驗揭示興趣點名稱的互補作用及預訓練文本編碼器於空間表徵的優勢。研究結果強調整合細緻語義屬性與多模態學習技術，有助於推進城市基礎模型的開發。", "audio": "audios/2506.02744v1.mp3", "timestamp": "2025-06-04T12:39:24.503881"}
{"query": "Diffusion Model", "id": "2506.02661v1", "url": "http://arxiv.org/abs/2506.02661v1", "title": "MotionRAG-Diff: A Retrieval-Augmented Diffusion Framework for Long-Term Music-to-Dance Generation", "summary": "Generating long-term, coherent, and realistic music-conditioned dance\nsequences remains a challenging task in human motion synthesis. Existing\napproaches exhibit critical limitations: motion graph methods rely on fixed\ntemplate libraries, restricting creative generation; diffusion models, while\ncapable of producing novel motions, often lack temporal coherence and musical\nalignment. To address these challenges, we propose $\\textbf{MotionRAG-Diff}$, a\nhybrid framework that integrates Retrieval-Augmented Generation (RAG) with\ndiffusion-based refinement to enable high-quality, musically coherent dance\ngeneration for arbitrary long-term music inputs. Our method introduces three\ncore innovations: (1) A cross-modal contrastive learning architecture that\naligns heterogeneous music and dance representations in a shared latent space,\nestablishing unsupervised semantic correspondence without paired data; (2) An\noptimized motion graph system for efficient retrieval and seamless\nconcatenation of motion segments, ensuring realism and temporal coherence\nacross long sequences; (3) A multi-condition diffusion model that jointly\nconditions on raw music signals and contrastive features to enhance motion\nquality and global synchronization. Extensive experiments demonstrate that\nMotionRAG-Diff achieves state-of-the-art performance in motion quality,\ndiversity, and music-motion synchronization accuracy. This work establishes a\nnew paradigm for music-driven dance generation by synergizing retrieval-based\ntemplate fidelity with diffusion-based creative enhancement.", "authors": ["Mingyang Huang", "Peng Zhang", "Bang Zhang"], "published_date": "2025-06-03", "title_zh": "MotionRAG-Diff：用於長期音樂到舞蹈生成的檢索增強擴散框架", "summary_zh": "生成長期、連貫且逼真的音樂驅動舞蹈序列，在人體動作合成中仍具挑戰。現有方法存在限制：動作圖方法依賴固定範本庫，限制了創造性生成；擴散模型雖能產生新穎動作，但常缺乏時間連貫性和音樂對齊。為了解決這些問題，我們提出 MotionRAG-Diff，一個結合檢索增強生成 (RAG) 與擴散模型精煉的混合框架，可為任意長期音樂輸入生成高品質、音樂連貫的舞蹈。本方法引入三個核心創新：(1) 一個跨模態對比學習架構，將異質音樂和舞蹈表徵在共享潛在空間中對齊，無需配對數據即可建立無監督的語義對應；(2) 一個優化的動作圖系統，用於高效檢索和無縫連接動作片段，確保長序列的真實性和時間連貫性；(3) 一個多條件擴散模型，聯合調節原始音樂訊號和對比特徵，以提高動作品質和整體同步性。大量實驗表明，MotionRAG-Diff 在動作品質、多樣性和音樂-動作同步準確性方面實現了最先進的性能。本研究通過協同檢索範本保真度和擴散創造性增強，為音樂驅動舞蹈生成建立了一個新範例。", "audio": "audios/2506.02661v1.mp3", "timestamp": "2025-06-04T12:39:34.667720"}
{"query": "AI", "id": "2506.03007v1", "url": "http://arxiv.org/abs/2506.03007v1", "title": "DFBench: Benchmarking Deepfake Image Detection Capability of Large Multimodal Models", "summary": "With the rapid advancement of generative models, the realism of AI-generated\nimages has significantly improved, posing critical challenges for verifying\ndigital content authenticity. Current deepfake detection methods often depend\non datasets with limited generation models and content diversity that fail to\nkeep pace with the evolving complexity and increasing realism of the\nAI-generated content. Large multimodal models (LMMs), widely adopted in various\nvision tasks, have demonstrated strong zero-shot capabilities, yet their\npotential in deepfake detection remains largely unexplored. To bridge this gap,\nwe present \\textbf{DFBench}, a large-scale DeepFake Benchmark featuring (i)\nbroad diversity, including 540,000 images across real, AI-edited, and\nAI-generated content, (ii) latest model, the fake images are generated by 12\nstate-of-the-art generation models, and (iii) bidirectional benchmarking and\nevaluating for both the detection accuracy of deepfake detectors and the\nevasion capability of generative models. Based on DFBench, we propose\n\\textbf{MoA-DF}, Mixture of Agents for DeepFake detection, leveraging a\ncombined probability strategy from multiple LMMs. MoA-DF achieves\nstate-of-the-art performance, further proving the effectiveness of leveraging\nLMMs for deepfake detection. Database and codes are publicly available at\nhttps://github.com/IntMeGroup/DFBench.", "authors": ["Jiarui Wang", "Huiyu Duan", "Juntong Wang", "Ziheng Jia", "Woo Yi Yang", "Xiaorong Zhu", "Yu Zhao", "Jiaying Qian", "Yuke Xing", "Guangtao Zhai", "Xiongkuo Min"], "published_date": "2025-06-03", "title_zh": "DFBench：大型多模態模型深度偽造圖像檢測能力基準測試", "summary_zh": "隨著生成模型快速發展，AI生成圖像的真實性大幅提升，對數位內容真實性驗證構成嚴峻挑戰。現有deepfake檢測方法常依賴生成模型和內容多樣性受限的資料集，難以跟上AI生成內容不斷演進的複雜性和真實性。大型多模態模型(LMMs)在各種視覺任務中廣泛應用，展現出強大的零樣本能力，但其在deepfake檢測中的潛力仍有待探索。為此，我們提出DFBench，一個大型DeepFake基準，具有(i)廣泛的多樣性，包含真實、AI編輯和AI生成內容的54萬張圖像，(ii)最新模型，偽造圖像由12個最先進的生成模型生成，以及(iii)雙向基準測試和評估，針對deepfake檢測器的檢測準確性和生成模型的規避能力。基於DFBench，我們提出MoA-DF，一種用於DeepFake檢測的混合代理，利用來自多個LMM的組合概率策略。MoA-DF實現了最先進的性能，進一步證明了利用LMM進行deepfake檢測的有效性。資料庫和程式碼可在https://github.com/IntMeGroup/DFBench公開取得。", "audio": "audios/2506.03007v1.mp3", "timestamp": "2025-06-04T14:16:33.713262"}
{"query": "Foundation Model", "id": "2506.02692v1", "url": "http://arxiv.org/abs/2506.02692v1", "title": "Large-scale Self-supervised Video Foundation Model for Intelligent Surgery", "summary": "Computer-Assisted Intervention (CAI) has the potential to revolutionize\nmodern surgery, with surgical scene understanding serving as a critical\ncomponent in supporting decision-making, improving procedural efficacy, and\nensuring intraoperative safety. While existing AI-driven approaches alleviate\nannotation burdens via self-supervised spatial representation learning, their\nlack of explicit temporal modeling during pre-training fundamentally restricts\nthe capture of dynamic surgical contexts, resulting in incomplete\nspatiotemporal understanding. In this work, we introduce the first video-level\nsurgical pre-training framework that enables joint spatiotemporal\nrepresentation learning from large-scale surgical video data. To achieve this,\nwe constructed a large-scale surgical video dataset comprising 3,650 videos and\napproximately 3.55 million frames, spanning more than 20 surgical procedures\nand over 10 anatomical structures. Building upon this dataset, we propose\nSurgVISTA (Surgical Video-level Spatial-Temporal Architecture), a\nreconstruction-based pre-training method that captures intricate spatial\nstructures and temporal dynamics through joint spatiotemporal modeling.\nAdditionally, SurgVISTA incorporates image-level knowledge distillation guided\nby a surgery-specific expert to enhance the learning of fine-grained anatomical\nand semantic features. To validate its effectiveness, we established a\ncomprehensive benchmark comprising 13 video-level datasets spanning six\nsurgical procedures across four tasks. Extensive experiments demonstrate that\nSurgVISTA consistently outperforms both natural- and surgical-domain\npre-trained models, demonstrating strong potential to advance intelligent\nsurgical systems in clinically meaningful scenarios.", "authors": ["Shu Yang", "Fengtao Zhou", "Leon Mayer", "Fuxiang Huang", "Yiliang Chen", "Yihui Wang", "Sunan He", "Yuxiang Nie", "Xi Wang", "Ömer Sümer", "Yueming Jin", "Huihui Sun", "Shuchang Xu", "Alex Qinyang Liu", "Zheng Li", "Jing Qin", "Jeremy YuenChun Teoh", "Lena Maier-Hein", "Hao Chen"], "published_date": "2025-06-03", "title_zh": "用於智慧手術的大規模自監督影片基礎模型", "summary_zh": "電腦輔助介入術具革新現代外科手術的潛力，手術場景理解是支援決策、提升效率及確保術中安全之關鍵。現有AI方法雖透過自監督空間表徵學習減輕標註負擔，但預訓練時缺乏明確的時間建模，限制了對動態手術環境的捕捉，導致時空理解不完整。本研究提出首個影片層級手術預訓練框架，可從大規模手術影片資料中進行聯合時空表徵學習。為此，我們構建包含3650部影片及約355萬幀畫面之大型手術影片資料集，涵蓋20多種手術程序及10多種解剖結構。基於此，我們提出SurgVISTA，一種基於重建的預訓練方法，透過聯合時空建模捕捉複雜的空間結構和時間動態。此外，SurgVISTA結合由手術專家指導的圖像層級知識蒸餾，以增強對細粒度解剖和語義特徵的學習。為驗證其有效性，我們建立了包含13個影片層級資料集之綜合基準，涵蓋四項任務中的六種手術程序。大量實驗表明，SurgVISTA始終優於自然和手術領域的預訓練模型，展現了在臨床意義場景中推進智慧手術系統的強大潛力。", "audio": "audios/2506.02692v1.mp3", "timestamp": "2025-06-04T14:16:41.599689"}
{"query": "Diffusion Model", "id": "2506.02633v1", "url": "http://arxiv.org/abs/2506.02633v1", "title": "ControlMambaIR: Conditional Controls with State-Space Model for Image Restoration", "summary": "This paper proposes ControlMambaIR, a novel image restoration method designed\nto address perceptual challenges in image deraining, deblurring, and denoising\ntasks. By integrating the Mamba network architecture with the diffusion model,\nthe condition network achieves refined conditional control, thereby enhancing\nthe control and optimization of the image generation process. To evaluate the\nrobustness and generalization capability of our method across various image\ndegradation conditions, extensive experiments were conducted on several\nbenchmark datasets, including Rain100H, Rain100L, GoPro, and SSID. The results\ndemonstrate that our proposed approach consistently surpasses existing methods\nin perceptual quality metrics, such as LPIPS and FID, while maintaining\ncomparable performance in image distortion metrics, including PSNR and SSIM,\nhighlighting its effectiveness and adaptability. Notably, ablation experiments\nreveal that directly noise prediction in the diffusion process achieves better\nperformance, effectively balancing noise suppression and detail preservation.\nFurthermore, the findings indicate that the Mamba architecture is particularly\nwell-suited as a conditional control network for diffusion models,\noutperforming both CNN- and Attention-based approaches in this context.\nOverall, these results highlight the flexibility and effectiveness of\nControlMambaIR in addressing a range of image restoration perceptual\nchallenges.", "authors": ["Cheng Yang", "Lijing Liang", "Zhixun Su"], "published_date": "2025-06-03", "title_zh": "ControlMambaIR：基於狀態空間模型的條件控制圖像復原", "summary_zh": "本研究提出ControlMambaIR，一種新型圖像復原方法，旨在解決圖像去雨、去模糊和去噪任務中的感知挑戰。透過將Mamba網絡架構與擴散模型整合，條件網絡實現更精確的條件控制，進而提升圖像生成過程的控制和最佳化。在Rain100H、Rain100L、GoPro和SSID等多個基準數據集上進行了大量實驗，以評估該方法在各種圖像劣化條件下的穩健性和泛化能力。結果表明，所提出的方法在感知質量指標（如LPIPS和FID）上始終優於現有方法，同時在圖像失真指標（包括PSNR和SSIM）上保持相當的性能，突顯其有效性和適應性。消融實驗表明，在擴散過程中直接進行雜訊預測可實現更好的性能，有效地平衡了雜訊抑制和細節保留。此外，研究結果表明，Mamba架構特別適合作為擴散模型的條件控制網絡，在此方面優於基於CNN和Attention的方法。總體而言，這些結果突顯了ControlMambaIR在解決一系列圖像復原感知挑戰方面的靈活性和有效性。", "audio": "audios/2506.02633v1.mp3", "timestamp": "2025-06-04T14:16:49.029780"}
{"query": "AI", "id": "2506.02993v1", "url": "http://arxiv.org/abs/2506.02993v1", "title": "Mapping Student-AI Interaction Dynamics in Multi-Agent Learning Environments: Supporting Personalised Learning and Reducing Performance Gaps", "summary": "Multi-agent AI systems, which simulate diverse instructional roles such as\nteachers and peers, offer new possibilities for personalized and interactive\nlearning. Yet, student-AI interaction patterns and their pedagogical\nimplications remain unclear. This study explores how university students\nengaged with multiple AI agents, and how these interactions influenced\ncognitive outcomes (learning gains) and non-cognitive factors (motivation,\ntechnology acceptance). Based on MAIC, an online learning platform with\nmulti-agent, the research involved 305 university students and 19,365 lines of\ndialogue data. Pre- and post-test scores, self-reported motivation and\ntechnology acceptance were also collected. The study identified two engagement\npatterns: co-construction of knowledge and co-regulation. Lag sequential\nanalysis revealed that students with lower prior knowledge relied more on\nco-construction of knowledge sequences, showing higher learning gains and\npost-course motivation. In contrast, students with higher prior knowledge\nengaged more in co-regulation behaviors but exhibited limited learning\nimprovement. Technology acceptance increased across all groups. These findings\nsuggest that multi-agent AI systems can adapt to students' varying needs,\nsupport differentiated engagement, and reduce performance gaps. Implications\nfor personalized system design and future research directions are discussed.", "authors": ["Zhanxin Hao", "Jie Cao", "Ruimiao Li", "Jifan Yu", "Zhiyuan Liu", "Yu Zhang"], "published_date": "2025-06-03", "title_zh": "多代理學習環境中學生-AI互動動態映射：支援個人化學習並縮小表現差距", "summary_zh": "多智能體AI系統模擬教師和同儕等多種教學角色，為個人化互動學習帶來新可能。本研究探討大學生與多智能體AI互動模式，及其對認知成果（學習成效）和非認知因素（動機、科技接受度）的影響。研究基於多智能體線上學習平台MAIC，涉及305名大學生和19365條對話數據，並收集了前後測分數、自我報告的動機和科技接受度。研究發現兩種互動模式：知識共構和共同調節。滯後序列分析顯示，先備知識較低的學生更依賴知識共構序列，表現出更高的學習成效和課程後動機。相反，先備知識較高的學生更傾向於共同調節行為，但學習提升有限。所有組別的科技接受度均有所提高。研究表明，多智能體AI系統可適應學生不同需求，支援差異化互動，並縮小績效差距。論文並討論了個人化系統設計的啟示及未來研究方向。", "audio": "audios/2506.02993v1.mp3", "timestamp": "2025-06-04T15:42:32.674796"}
{"query": "Foundation Model", "id": "2506.02557v1", "url": "http://arxiv.org/abs/2506.02557v1", "title": "Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models", "summary": "Vision-language models, such as CLIP, have achieved significant success in\naligning visual and textual representations, becoming essential components of\nmany multi-modal large language models (MLLMs) like LLaVA and OpenFlamingo.\nHowever, numerous studies have identified CLIP's limited fine-grained\nperception as a critical drawback, leading to substantial failures in\ndownstream MLLMs. In contrast, vision-centric foundation models like DINOv2\ndemonstrate remarkable capabilities in capturing fine details from images. In\nthis work, we propose a novel kernel-based method to align CLIP's visual\nrepresentation with that of DINOv2, ensuring that the resulting embeddings\nmaintain compatibility with text embeddings while enhancing perceptual\ncapabilities. Our alignment objective is designed for efficient stochastic\noptimization. Following this image-only alignment fine-tuning, the visual\nencoder retains compatibility with the frozen text encoder and exhibits\nsignificant improvements in zero-shot object recognition, fine-grained spatial\nreasoning, and localization. By integrating the aligned visual encoder,\ndownstream MLLMs also demonstrate enhanced performance.", "authors": ["Shizhan Gong", "Yankai Jiang", "Qi Dou", "Farzan Farnia"], "published_date": "2025-06-03", "title_zh": "基於核方法的無監督嵌入對齊以增強視覺-語言模型中的視覺表徵", "summary_zh": "CLIP等視覺語言模型在對齊視覺與文本表徵方面取得顯著成功，成為諸如LLaVA和OpenFlamingo等多模態大型語言模型的重要組成部分。然而，研究指出CLIP的細粒度感知能力有限，導致下游MLLM出現重大失效。相對地，DINOv2等以視覺為中心的基礎模型在捕捉圖像細節方面展現卓越能力。本文提出一種新型基於核的方法，將CLIP的視覺表徵與DINOv2對齊，確保產生的嵌入與文本嵌入保持兼容性，同時提升感知能力。我們的對齊目標旨在實現高效的隨機優化。經過僅限圖像的對齊微調後，視覺編碼器與凍結的文本編碼器保持兼容，並在零樣本物件辨識、細粒度空間推理和定位方面表現出顯著改進。整合此對齊後的視覺編碼器，下游MLLM亦展現更佳效能。", "audio": "audios/2506.02557v1.mp3", "timestamp": "2025-06-04T15:42:45.826729"}
{"query": "Diffusion Model", "id": "2506.02626v1", "url": "http://arxiv.org/abs/2506.02626v1", "title": "Synthetic Iris Image Databases and Identity Leakage: Risks and Mitigation Strategies", "summary": "This paper presents a comprehensive overview of iris image synthesis methods,\nwhich can alleviate the issues associated with gathering large, diverse\ndatasets of biometric data from living individuals, which are considered\npivotal for biometric methods development. These methods for synthesizing iris\ndata range from traditional, hand crafted image processing-based techniques,\nthrough various iterations of GAN-based image generators, variational\nautoencoders (VAEs), as well as diffusion models. The potential and fidelity in\niris image generation of each method is discussed and examples of inferred\npredictions are provided. Furthermore, the risks of individual biometric\nfeatures leakage from the training sets are considered, together with possible\nstrategies for preventing them, which have to be implemented should these\ngenerative methods be considered a valid replacement of real-world biometric\ndatasets.", "authors": ["Ada Sawilska", "Mateusz Trokielewicz"], "published_date": "2025-06-03", "title_zh": "合成虹膜圖像資料庫與身份洩漏：風險與緩解策略", "summary_zh": "本文綜述虹膜圖像合成方法，旨在緩解生物識別數據採集的挑戰，此類數據對生物識別技術開發至關重要。論文探討了多種合成技術，包括傳統圖像處理、基於生成對抗網路（GAN）、變分自編碼器（VAE）以及擴散模型等。論文分析了各方法在虹膜圖像生成方面的潛力與逼真度，並提供預測示例。此外，論文還探討了訓練集中個人生物特徵洩漏的風險，以及防止此類洩漏的策略，此為生成方法取代真實生物識別數據集的前提。", "audio": "audios/2506.02626v1.mp3", "timestamp": "2025-06-04T15:42:56.661647"}
{"query": "AI", "id": "2506.02992v1", "url": "http://arxiv.org/abs/2506.02992v1", "title": "Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation", "summary": "Large Language Models (LLMs) are increasingly explored for legal argument\ngeneration, yet they pose significant risks of manipulation through\nhallucination and ungrounded persuasion, and often fail to utilize provided\nfactual bases effectively or abstain when arguments are untenable. This paper\nintroduces a novel reflective multi-agent method designed to address these\nchallenges in the context of legally compliant persuasion. Our approach employs\nspecialized agents--a Factor Analyst and an Argument Polisher--in an iterative\nrefinement process to generate 3-ply legal arguments (plaintiff, defendant,\nrebuttal). We evaluate Reflective Multi-Agent against single-agent,\nenhanced-prompt single-agent, and non-reflective multi-agent baselines using\nfour diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,\nLlama-4-Scout-17b-16e) across three legal scenarios: \"arguable\", \"mismatched\",\nand \"non-arguable\". Results demonstrate Reflective Multi-Agent's significant\nsuperiority in successful abstention (preventing generation when arguments\ncannot be grounded), marked improvements in hallucination accuracy (reducing\nfabricated and misattributed factors), particularly in \"non-arguable\"\nscenarios, and enhanced factor utilization recall (improving the use of\nprovided case facts). These findings suggest that structured reflection within\na multi-agent framework offers a robust computable method for fostering ethical\npersuasion and mitigating manipulation in LLM-based legal argumentation\nsystems, a critical step towards trustworthy AI in law. Project page:\nhttps://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/", "authors": ["Li Zhang", "Kevin D. Ashley"], "published_date": "2025-06-03", "title_zh": "減輕操縱並增強說服力：一種用於法律論證生成的反思式多代理人方法", "summary_zh": "大型語言模型在法律論證生成方面應用日增，但也存在捏造事實、無根據說服等操縱風險，且常未能有效利用既有事實或在論證不可行時放棄。本研究提出一種新型反思式多代理方法，旨在解決法律合規說服中的這些挑戰。此方法採用專業代理——因素分析師和論證潤飾師——進行迭代精煉，生成原告、被告、反駁三層法律論證。評估結果顯示，相較於單一代理、強化提示單一代理及非反思式多代理基準，反思式多代理在成功放棄（避免生成無根據論證）、提升捏造事實準確性（減少虛構與誤用因素，尤其在不可論證情境下），以及增強事實利用召回率（改善既有案例事實的使用）等方面均顯著優越。研究表明，多代理框架內的結構化反思提供了一種穩健的可計算方法，可在基於大型語言模型的法律論證系統中促進倫理說服、減少操縱，是實現法律領域可信賴人工智慧的關鍵一步。", "audio": "audios/2506.02992v1.mp3", "timestamp": "2025-06-04T16:20:54.105341"}
{"query": "Foundation Model", "id": "2506.02555v1", "url": "http://arxiv.org/abs/2506.02555v1", "title": "SurgVLM: A Large Vision-Language Model and Systematic Evaluation Benchmark for Surgical Intelligence", "summary": "Foundation models have achieved transformative success across biomedical\ndomains by enabling holistic understanding of multimodal data. However, their\napplication in surgery remains underexplored. Surgical intelligence presents\nunique challenges - requiring surgical visual perception, temporal analysis,\nand reasoning. Existing general-purpose vision-language models fail to address\nthese needs due to insufficient domain-specific supervision and the lack of a\nlarge-scale high-quality surgical database. To bridge this gap, we propose\nSurgVLM, one of the first large vision-language foundation models for surgical\nintelligence, where this single universal model can tackle versatile surgical\ntasks. To enable this, we construct a large-scale multimodal surgical database,\nSurgVLM-DB, comprising over 1.81 million frames with 7.79 million\nconversations, spanning more than 16 surgical types and 18 anatomical\nstructures. We unify and reorganize 23 public datasets across 10 surgical\ntasks, followed by standardizing labels and doing hierarchical vision-language\nalignment to facilitate comprehensive coverage of gradually finer-grained\nsurgical tasks, from visual perception, temporal analysis, to high-level\nreasoning. Building upon this comprehensive dataset, we propose SurgVLM, which\nis built upon Qwen2.5-VL, and undergoes instruction tuning to 10+ surgical\ntasks. We further construct a surgical multimodal benchmark, SurgVLM-Bench, for\nmethod evaluation. SurgVLM-Bench consists of 6 popular and widely-used datasets\nin surgical domain, covering several crucial downstream tasks. Based on\nSurgVLM-Bench, we evaluate the performance of our SurgVLM (3 SurgVLM variants:\nSurgVLM-7B, SurgVLM-32B, and SurgVLM-72B), and conduct comprehensive\ncomparisons with 14 mainstream commercial VLMs (e.g., GPT-4o, Gemini 2.0 Flash,\nQwen2.5-Max).", "authors": ["Zhitao Zeng", "Zhu Zhuo", "Xiaojun Jia", "Erli Zhang", "Junde Wu", "Jiaan Zhang", "Yuxuan Wang", "Chang Han Low", "Jian Jiang", "Zilong Zheng", "Xiaochun Cao", "Yutong Ban", "Qi Dou", "Yang Liu", "Yueming Jin"], "published_date": "2025-06-03", "title_zh": "SurgVLM：用於手術智能的大型視覺語言模型及系統性評估基準", "summary_zh": "基於多模態資料的基礎模型已在生物醫學領域取得突破，但在手術應用上仍待開發。手術智慧面臨獨特挑戰，需要手術視覺感知、時間分析和推理能力。現有通用視覺語言模型因缺乏領域特定監督和大規模高品質手術資料庫而不足。為此，我們提出SurgVLM，一種用於手術智慧的大型視覺語言基礎模型，旨在以單一通用模型應對多種手術任務。我們構建了大規模多模態手術資料庫SurgVLM-DB，包含超過181萬幀和779萬次對話，涵蓋16種以上手術類型和18種解剖結構。我們整合並重組了10項手術任務的23個公共資料集，標準化標籤並進行層次視覺語言對齊，以全面涵蓋從視覺感知、時間分析到高階推理的精細手術任務。SurgVLM基於Qwen2.5-VL構建，並針對10多項手術任務進行指令微調。我們進一步構建了手術多模態基準SurgVLM-Bench，用於方法評估，包含手術領域中6個常用的資料集，涵蓋多個關鍵下游任務。基於SurgVLM-Bench，我們評估了SurgVLM（SurgVLM-7B、SurgVLM-32B和SurgVLM-72B三種變體）的性能，並與14個主流商業視覺語言模型（如GPT-4o、Gemini 2.0 Flash、Qwen2.5-Max）進行了全面比較。", "audio": "audios/2506.02555v1.mp3", "timestamp": "2025-06-04T16:21:03.959547"}
{"query": "Diffusion Model", "id": "2506.02601v1", "url": "http://arxiv.org/abs/2506.02601v1", "title": "Hyperspectral Image Generation with Unmixing Guided Diffusion Model", "summary": "Recently, hyperspectral image generation has received increasing attention,\nbut existing generative models rely on conditional generation schemes, which\nlimits the diversity of generated images. Diffusion models are popular for\ntheir ability to generate high-quality samples, but adapting these models from\nRGB to hyperspectral data presents the challenge of high dimensionality and\nphysical constraints. To address these challenges, we propose a novel diffusion\nmodel guided by hyperspectral unmixing. Our model comprises two key modules: an\nunmixing autoencoder module and an abundance diffusion module. The unmixing\nautoencoder module leverages unmixing guidance to shift the generative task\nfrom the image space to the low-dimensional abundance space, significantly\nreducing computational complexity while preserving high fidelity. The abundance\ndiffusion module generates samples that satisfy the constraints of\nnon-negativity and unity, ensuring the physical consistency of the\nreconstructed HSIs. Additionally, we introduce two evaluation metrics tailored\nto hyperspectral data. Empirical results, evaluated using both traditional\nmetrics and our proposed metrics, indicate that our model is capable of\ngenerating high-quality and diverse hyperspectral images, offering an\nadvancement in hyperspectral data generation.", "authors": ["Shiyu Shen", "Bin Pan", "Ziye Zhang", "Zhenwei Shi"], "published_date": "2025-06-03", "title_zh": "解混引導擴散模型之高光譜影像生成", "summary_zh": "高光譜影像生成日益受關注，但現有生成模型仰賴條件式生成方案，限制了影像多樣性。擴散模型擅長生成高品質樣本，然將其從RGB適配至高光譜數據面臨高維度和物理約束挑戰。為此，我們提出基於高光譜解混引導的新型擴散模型。該模型包含解混自編碼器和豐度擴散模組。解混自編碼器利用解混引導將生成任務從影像空間轉移至低維豐度空間，大幅降低計算複雜度並維持高保真度。豐度擴散模組生成滿足非負性和總和為一約束的樣本，確保重建高光譜影像的物理一致性。此外，我們引入兩種專為高光譜數據設計的評估指標。實驗結果表明，無論使用傳統指標或我們提出的指標評估，我們的模型都能生成高品質且多樣的高光譜影像，為高光譜數據生成帶來進展。", "audio": "audios/2506.02601v1.mp3", "timestamp": "2025-06-04T16:21:10.070841"}
{"query": "AI", "id": "2506.02976v1", "url": "http://arxiv.org/abs/2506.02976v1", "title": "Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO AMD Progression Challenge", "summary": "The MARIO challenge, held at MICCAI 2024, focused on advancing the automated\ndetection and monitoring of age-related macular degeneration (AMD) through the\nanalysis of optical coherence tomography (OCT) images. Designed to evaluate\nalgorithmic performance in detecting neovascular activity changes within AMD,\nthe challenge incorporated unique multi-modal datasets. The primary dataset,\nsourced from Brest, France, was used by participating teams to train and test\ntheir models. The final ranking was determined based on performance on this\ndataset. An auxiliary dataset from Algeria was used post-challenge to evaluate\npopulation and device shifts from submitted solutions. Two tasks were involved\nin the MARIO challenge. The first one was the classification of evolution\nbetween two consecutive 2D OCT B-scans. The second one was the prediction of\nfuture AMD evolution over three months for patients undergoing anti-vascular\nendothelial growth factor (VEGF) therapy. Thirty-five teams participated, with\nthe top 12 finalists presenting their methods. This paper outlines the\nchallenge's structure, tasks, data characteristics, and winning methodologies,\nsetting a benchmark for AMD monitoring using OCT, infrared imaging, and\nclinical data (such as the number of visits, age, gender, etc.). The results of\nthis challenge indicate that artificial intelligence (AI) performs as well as a\nphysician in measuring AMD progression (Task 1) but is not yet able of\npredicting future evolution (Task 2).", "authors": ["Rachid Zeghlache", "Ikram Brahim", "Pierre-Henri Conze", "Mathieu Lamard", "Mohammed El Amine Lazouni", "Zineb Aziza Elaouaber", "Leila Ryma Lazouni", "Christopher Nielsen", "Ahmad O. Ahsan", "Matthias Wilms", "Nils D. Forkert", "Lovre Antonio Budimir", "Ivana Matovinović", "Donik Vršnak", "Sven Lončarić", "Philippe Zhang", "Weili Jiang", "Yihao Li", "Yiding Hao", "Markus Frohmann", "Patrick Binder", "Marcel Huber", "Taha Emre", "Teresa Finisterra Araújo", "Marzieh Oghbaie", "Hrvoje Bogunović", "Amerens A. Bekkers", "Nina M. van Liebergen", "Hugo J. Kuijf", "Abdul Qayyum", "Moona Mazher", "Steven A. Niederer", "Alberto J. Beltrán-Carrero", "Juan J. Gómez-Valverde", "Javier Torresano-Rodríquez", "Álvaro Caballero-Sastre", "María J. Ledesma Carbayo", "Yosuke Yamagishi", "Yi Ding", "Robin Peretzke", "Alexandra Ertl", "Maximilian Fischer", "Jessica Kächele", "Sofiane Zehar", "Karim Boukli Hacene", "Thomas Monfort", "Béatrice Cochener", "Mostafa El Habib Daho", "Anas-Alexis Benyoussef", "Gwenolé Quellec"], "published_date": "2025-06-03", "title_zh": "深度學習於視網膜退化評估：MARIO AMD進展挑戰賽之綜合分析", "summary_zh": "MICCAI 2024的MARIO挑戰賽旨在推進基於光學相干斷層掃描(OCT)影像的老年性黃斑部病變(AMD)自動檢測與監測。挑戰賽評估演算法在檢測AMD內新生血管活動變化方面的效能，採用了獨特的多模態資料集，主要資料集來自法國布雷斯特，供參賽隊伍訓練與測試模型，最終排名依據此資料集上的表現決定。來自阿爾及利亞的輔助資料集在挑戰賽後用於評估提交方案在人群和設備上的轉移效應。MARIO挑戰賽包含兩個任務：一是連續2D OCT B掃描影像間演變的分類，二是接受抗血管內皮生長因子(VEGF)治療的患者未來三個月AMD演變的預測。35支隊伍參與，前12名決賽隊伍展示了他們的方法。本文概述了挑戰賽的結構、任務、資料特性和獲勝方法，為利用OCT、紅外成像和臨床資料(如就診次數、年齡、性別等)進行AMD監測設定了基準。挑戰賽結果表明，人工智能(AI)在衡量AMD進展方面(任務1)與醫生表現相當，但尚無法預測未來演變(任務2)。", "audio": "audios/2506.02976v1.mp3", "timestamp": "2025-06-04T17:18:19.531658"}
{"query": "Foundation Model", "id": "2506.02550v1", "url": "http://arxiv.org/abs/2506.02550v1", "title": "Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025", "summary": "In this report, we present a novel three-stage framework developed for the\nEgo4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in\nfoundation models, our method consists of three stages: feature extraction,\naction recognition, and long-term action anticipation. First, visual features\nare extracted using a high-performance visual encoder. The features are then\nfed into a Transformer to predict verbs and nouns, with a verb-noun\nco-occurrence matrix incorporated to enhance recognition accuracy. Finally, the\npredicted verb-noun pairs are formatted as textual prompts and input into a\nfine-tuned large language model (LLM) to anticipate future action sequences.\nOur framework achieves first place in this challenge at CVPR 2025, establishing\na new state-of-the-art in long-term action prediction. Our code will be\nreleased at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025.", "authors": ["Qiaohui Chu", "Haoyu Zhang", "Yisen Feng", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "published_date": "2025-06-03", "title_zh": "Ego4D長期動作預測挑戰賽2025技術報告", "summary_zh": "本研究提出一個新穎的三階段框架，用於Ego4D長期動作預測任務。受基礎模型啟發，該方法包含特徵提取、動作識別和長期動作預測三個階段。首先，利用高效視覺編碼器提取視覺特徵。接著，將特徵輸入Transformer以預測動詞和名詞，並結合動詞-名詞共現矩陣以提高識別準確性。最後，將預測的動詞-名詞對格式化為文本提示，並輸入微調的大型語言模型（LLM）以預測未來動作序列。此框架在CVPR 2025挑戰賽中獲得第一名，確立了長期動作預測的新標竿。程式碼將於https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025公開。", "audio": "audios/2506.02550v1.mp3", "timestamp": "2025-06-04T17:18:25.189346"}
{"query": "Diffusion Model", "id": "2506.02560v1", "url": "http://arxiv.org/abs/2506.02560v1", "title": "DCI: Dual-Conditional Inversion for Boosting Diffusion-Based Image Editing", "summary": "Diffusion models have achieved remarkable success in image generation and\nediting tasks. Inversion within these models aims to recover the latent noise\nrepresentation for a real or generated image, enabling reconstruction, editing,\nand other downstream tasks. However, to date, most inversion approaches suffer\nfrom an intrinsic trade-off between reconstruction accuracy and editing\nflexibility. This limitation arises from the difficulty of maintaining both\nsemantic alignment and structural consistency during the inversion process. In\nthis work, we introduce Dual-Conditional Inversion (DCI), a novel framework\nthat jointly conditions on the source prompt and reference image to guide the\ninversion process. Specifically, DCI formulates the inversion process as a\ndual-condition fixed-point optimization problem, minimizing both the latent\nnoise gap and the reconstruction error under the joint guidance. This design\nanchors the inversion trajectory in both semantic and visual space, leading to\nmore accurate and editable latent representations. Our novel setup brings new\nunderstanding to the inversion process. Extensive experiments demonstrate that\nDCI achieves state-of-the-art performance across multiple editing tasks,\nsignificantly improving both reconstruction quality and editing precision.\nFurthermore, we also demonstrate that our method achieves strong results in\nreconstruction tasks, implying a degree of robustness and generalizability\napproaching the ultimate goal of the inversion process.", "authors": ["Zixiang Li", "Haoyu Wang", "Wei Wang", "Chuangchuang Tan", "Yunchao Wei", "Yao Zhao"], "published_date": "2025-06-03", "title_zh": "DCI：雙條件反演促進基於擴散的圖像編輯", "summary_zh": "擴散模型在圖像生成與編輯上表現卓越。反演旨在恢復真實或生成圖像的潛在雜訊表徵，以實現重建、編輯等下游任務。然而，現有反演方法常在重建準確度和編輯靈活性間權衡，主因在於難以兼顧語義對齊和結構一致性。本研究提出雙條件反演(DCI)框架，聯合使用源提示和參考圖像引導反演過程。DCI將反演視為雙條件不動點優化問題，在聯合引導下最小化潛在雜訊差距和重建誤差。此設計將反演軌跡錨定於語義和視覺空間，產生更準確、更可編輯的潛在表徵，並對反演過程帶來新的理解。實驗證明，DCI在多項編輯任務中達到領先水準，顯著提升重建品質和編輯精確度，並在重建任務中展現強大性能，具備穩健性和通用性，更接近反演過程的最終目標。", "audio": "audios/2506.02560v1.mp3", "timestamp": "2025-06-04T17:18:32.156609"}
{"query": "AI", "id": "2506.02969v1", "url": "http://arxiv.org/abs/2506.02969v1", "title": "Measurement of the branching fractions of the Cabibbo-favored decays $Λ_{c}^{+}\\toΛK_{S}^{0}K^{+}$ and $Λ_{c}^{+}\\toΞ^{0}K_{S}^{0}π^{+}$ and search for $Λ_{c}^{+}\\toΣ^{0} K_{S}^{0}K^{+}$", "summary": "Based on $e^{+}e^{-}$ collision data corresponding to an integrated\nluminosity of about 4.5 fb$^{-1}$ collected at center-of-mass energies between\n4599.53 MeV and 4698.82 MeV with the BESIII detector, the absolute branching\nfraction of the Cabibbo-favored decay $\\Lambda_{c}^{+}\\to\\Lambda\nK_{S}^{0}K^{+}$ is measured to be $(3.12\\pm0.46\\pm0.15)\\times10^{-3}$. Combined\nwith a previous measurement from the BESIII Collaboration, the branching\nfraction of the decay $\\Lambda_{c}^{+}\\to\\Lambda K_{S}^{0}K^{+}$ is calculated\nto be $(3.07\\pm0.26\\pm0.13)\\times10^{-3}$. The decay\n$\\Lambda_{c}^{+}\\to\\Xi^{0}K_{S}^{0}\\pi^{+}$ is observed for the first time with\na statistical significance of $6.6\\sigma$, and its branching fraction is\ndetermined to be $(3.70\\pm0.60\\pm0.21)\\times10^{-3}$. In addition, a search for\nthe decay $\\Lambda_{c}^{+}\\to\\Sigma^{0} K_{S}^{0}K^{+}$ is performed and its\nbranching fraction is determined to be\n$(0.80^{+0.28}_{-0.24}\\pm0.16)\\times10^{-3}$, corresponding to an upper limit\nof $1.28\\times10^{-3}$ at $90\\%$ confidence level. These measurements provide\nnew information that can be used to distinguish between theoretical models.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "G. Chelkov", "C. Chen", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "Y. B. Chen", "Y. Q. Chen", "Z. J. Chen", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "Yang Gao", "S. Garbolino", "I. Garzia", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "T. Holtmann", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "B. Y. Hu", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "S. Janchiv", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "Z. H. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "Cheng Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "C. X. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. Ma", "H. L. Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "Y. H. Meng", "Z. X. Meng", "J. G. Messchendorp", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "Y. Y. Peng", "K. Peters", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "Ch. Rosner", "M. Q. Ruan", "S. N. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "L. F. Tang", "M. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "V. Thoren", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "Bo Wang", "C. Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. Y. Wang", "D. H. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "T. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Yan Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "Lei Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "X. Zhong", "H. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. Z. Zhou", "Z. C. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-06-03", "title_zh": "卡比玻容許衰變$Λ_{c}^{+}\\toΛK_{S}^{0}K^{+}$和$Λ_{c}^{+}\\toΞ^{0}K_{S}^{0}π^{+}$分支比測量以及對$Λ_{c}^{+}\\toΣ^{0} K_{S}^{0}K^{+}$的尋找", "summary_zh": "基於BESIII探測器在4599.53 MeV至4698.82 MeV質心能量下，收集約4.5 fb$^{-1}$積分光度的$e^{+}e^{-}$碰撞數據，測得卡比玻優選衰變$\\Lambda_{c}^{+}\\to\\Lambda K_{S}^{0}K^{+}$的絕對分支比為$(3.12\\pm0.46\\pm0.15)\\times10^{-3}$。結合BESIII合作組先前的測量結果，計算出衰變$\\Lambda_{c}^{+}\\to\\Lambda K_{S}^{0}K^{+}$的分支比為$(3.07\\pm0.26\\pm0.13)\\times10^{-3}$。首次觀測到衰變$\\Lambda_{c}^{+}\\to\\Xi^{0}K_{S}^{0}\\pi^{+}$，統計顯著性為$6.6\\sigma$，其分支比確定為$(3.70\\pm0.60\\pm0.21)\\times10^{-3}$。此外，搜尋了衰變$\\Lambda_{c}^{+}\\to\\Sigma^{0} K_{S}^{0}K^{+}$，其分支比確定為$(0.80^{+0.28}_{-0.24}\\pm0.16)\\times10^{-3}$，對應於90%置信水平下的上限$1.28\\times10^{-3}$。這些測量提供了可用於區分理論模型的新訊息。", "audio": "audios/2506.02969v1.mp3", "timestamp": "2025-06-04T18:27:27.072017"}
{"query": "Foundation Model", "id": "2506.02485v1", "url": "http://arxiv.org/abs/2506.02485v1", "title": "Generative AI for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning", "summary": "Wildfires continue to inflict devastating human, environmental, and economic\nlosses globally, as tragically exemplified by the 2025 Los Angeles wildfire and\nthe urgent demand for more effective response strategies. While physics-based\nand deep learning models have advanced wildfire simulation, they face critical\nlimitations in predicting and visualizing multimodal fire spread in real time,\nparticularly in both 2D and 3D spatial domains using dynamically updated GIS\ndata. These limitations hinder timely emergency response, infrastructure\nprotection, and community safety. Generative AI has recently emerged as a\ntransformative approach across research and industry. Models such as Generative\nAdversarial Networks (GANs), Variational Autoencoders (VAEs), Transformers, and\ndiffusion-based architectures offer distinct advantages over traditional\nmethods, including the integration of multimodal data, generation of diverse\nscenarios under uncertainty, and improved modeling of wildfire dynamics across\nspatial and temporal scales. This position paper advocates for the adoption of\ngenerative AI as a foundational framework for wildfire prediction. We explore\nhow such models can enhance 2D fire spread forecasting and enable more\nrealistic, scalable 3D simulations. Additionally, we employ a novel human-AI\ncollaboration framework using large language models (LLMs) for automated\nknowledge extraction, literature synthesis, and bibliometric mapping. Looking\nahead, we identify five key visions for integrating generative AI into wildfire\nmanagement: multimodal approaches, AI foundation models, conversational AI\nsystems, edge-computing-based scenario generation, and cognitive digital twins.\nWe also address three major challenges accompanying these opportunities and\npropose potential solutions to support their implementation.", "authors": ["Haowen Xu", "Sisi Zlatanova", "Ruiyu Liang", "Ismet Canbulat"], "published_date": "2025-06-03", "title_zh": "生成式人工智慧於二維與三維野火蔓延預測：超越基於物理之模型與傳統深度學習", "summary_zh": "野火持續造成全球性的災難性損失，亟需更有效的應對策略。傳統物理模型與深度學習雖推進了野火模擬，但在即時預測和可視化多模態火勢蔓延，尤其是在使用動態更新GIS資料的2D/3D空間中，仍存在局限性，阻礙了應急反應和基礎設施保護。生成式AI作為變革性途徑，相較傳統方法具備優勢，如整合多模態數據、生成不確定情境下的多樣化情景，並改善跨時空尺度的野火動態建模。本文倡導採用生成式AI作為野火預測的基礎框架，探討其如何增強2D火勢預測，並實現更逼真、可擴展的3D模擬。此外，我們利用大型語言模型（LLMs）的新型人機協作框架，進行自動知識提取、文獻綜述和文獻計量映射。展望未來，我們提出了將生成式AI整合到野火管理的五個關鍵願景：多模態方法、AI基礎模型、對話式AI系統、基於邊緣計算的情景生成和認知數位孿生。同時，我們亦闡述了伴隨這些機遇的三大挑戰，並提出潛在的解決方案以支持其應用。", "audio": "audios/2506.02485v1.mp3", "timestamp": "2025-06-04T18:27:37.765483"}
{"query": "Diffusion Model", "id": "2506.02497v1", "url": "http://arxiv.org/abs/2506.02497v1", "title": "LumosFlow: Motion-Guided Long Video Generation", "summary": "Long video generation has gained increasing attention due to its widespread\napplications in fields such as entertainment and simulation. Despite advances,\nsynthesizing temporally coherent and visually compelling long sequences remains\na formidable challenge. Conventional approaches often synthesize long videos by\nsequentially generating and concatenating short clips, or generating key frames\nand then interpolate the intermediate frames in a hierarchical manner. However,\nboth of them still remain significant challenges, leading to issues such as\ntemporal repetition or unnatural transitions. In this paper, we revisit the\nhierarchical long video generation pipeline and introduce LumosFlow, a\nframework introduce motion guidance explicitly. Specifically, we first employ\nthe Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames\nwith larger motion intervals, thereby ensuring content diversity in the\ngenerated long videos. Given the complexity of interpolating contextual\ntransitions between key frames, we further decompose the intermediate frame\ninterpolation into motion generation and post-hoc refinement. For each pair of\nkey frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes\ncomplex and large-motion optical flows, while MotionControlNet subsequently\nrefines the warped results to enhance quality and guide intermediate frame\ngeneration. Compared with traditional video frame interpolation, we achieve 15x\ninterpolation, ensuring reasonable and continuous motion between adjacent\nframes. Experiments show that our method can generate long videos with\nconsistent motion and appearance. Code and models will be made publicly\navailable upon acceptance. Our project page:\nhttps://jiahaochen1.github.io/LumosFlow/", "authors": ["Jiahao Chen", "Hangjie Yuan", "Yichen Qian", "Jingyun Liang", "Jiazheng Xing", "Pengwei Liu", "Weihua Chen", "Fan Wang", "Bing Su"], "published_date": "2025-06-03", "title_zh": "LumosFlow：運動導向的長影片生成", "summary_zh": "長影片生成因其在娛樂和模擬等領域的廣泛應用而備受關注。然而，合成時間連貫且視覺上引人入勝的長序列影片仍然是一項艱鉅的挑戰。傳統方法通常透過依序生成和串聯短片段，或生成關鍵影格然後以分層方式內插中間影格來合成長影片，但這些方法仍存在顯著挑戰，導致時間重複或不自然的轉變。本文重新探討分層長影片生成流程，並引入LumosFlow框架，明確引入運動引導。具體而言，我們首先採用大運動文本到影片擴散模型（LMTV-DM）生成具有較大運動間隔的關鍵影格，從而確保生成長影片中的內容多樣性。鑒於關鍵影格之間上下文轉變的插值複雜性，我們進一步將中間影格插值分解為運動生成和事後細化。對於每對關鍵影格，潛在光流擴散模型（LOF-DM）合成複雜且大運動的光流，而MotionControlNet隨後細化扭曲結果，以提高品質並引導中間影格生成。與傳統影片影格插值相比，我們實現了15倍插值，確保相鄰影格之間合理且連續的運動。實驗表明，我們的方法可以生成具有一致運動和外觀的長影片。程式碼和模型將在接受後公開。", "audio": "audios/2506.02497v1.mp3", "timestamp": "2025-06-04T18:27:53.754661"}
{"query": "AI", "id": "2506.02966v1", "url": "http://arxiv.org/abs/2506.02966v1", "title": "Unpacking Graduate Students' Learning Experience with Generative AI Teaching Assistant in A Quantitative Methodology Course", "summary": "The study was conducted in an Advanced Quantitative Research Methods course\ninvolving 20 graduate students. During the course, student inquiries made to\nthe AI were recorded and coded using Bloom's taxonomy and the CLEAR framework.\nA series of independent sample t-tests and poisson regression analyses were\nemployed to analyse the characteristics of different questions asked by\nstudents with different backgrounds. Post course interviews were conducted with\n10 students to gain deeper insights into their perceptions. The findings\nrevealed a U-shaped pattern in students' use of the AI assistant, with higher\nusage at the beginning and towards the end of the course, and a decrease in\nusage during the middle weeks. Most questions posed to the AI focused on\nknowledge and comprehension levels, with fewer questions involving deeper\ncognitive thinking. Students with a weaker mathematical foundation used the AI\nassistant more frequently, though their inquiries tended to lack explicit and\nlogical structure compared to those with a strong mathematical foundation, who\nengaged less with the tool. These patterns suggest the need for targeted\nguidance to optimise the effectiveness of AI tools for students with varying\nlevels of academic proficiency.", "authors": ["Zhanxin Hao", "Haifeng Luo", "Yongyi Chen", "Yu Zhang"], "published_date": "2025-06-03", "title_zh": "解構生成式AI助教在量化方法課程中對研究生學習體驗的影響", "summary_zh": "本研究於一門高級量化研究方法課程中進行，參與者為20名研究生。研究記錄並使用布魯姆分類法與CLEAR架構編碼學生向AI提出的問題。透過獨立樣本t檢定與卜瓦松迴歸分析，分析不同背景學生提問的特徵。課程後，訪談10名學生以深入了解其觀點。結果顯示學生使用AI助手的模式呈U型，課程初期與末期使用率較高，中間週數則下降。多數提問集中於知識與理解層次，涉及更深層認知思考的問題較少。數學基礎較弱的學生更頻繁使用AI助手，但其提問相較於數學基礎強的學生，較缺乏明確與邏輯結構，後者較少使用該工具。研究結果表明，需針對不同學術程度的學生提供有針對性的指導，以優化AI工具的效用。", "audio": "audios/2506.02966v1.mp3", "timestamp": "2025-06-04T19:16:25.232996"}
{"query": "Foundation Model", "id": "2506.02408v1", "url": "http://arxiv.org/abs/2506.02408v1", "title": "Revisiting End-to-End Learning with Slide-level Supervision in Computational Pathology", "summary": "Pre-trained encoders for offline feature extraction followed by multiple\ninstance learning (MIL) aggregators have become the dominant paradigm in\ncomputational pathology (CPath), benefiting cancer diagnosis and prognosis.\nHowever, performance limitations arise from the absence of encoder fine-tuning\nfor downstream tasks and disjoint optimization with MIL. While slide-level\nsupervised end-to-end (E2E) learning is an intuitive solution to this issue, it\nfaces challenges such as high computational demands and suboptimal results.\nThese limitations motivate us to revisit E2E learning. We argue that prior work\nneglects inherent E2E optimization challenges, leading to performance\ndisparities compared to traditional two-stage methods. In this paper, we\npioneer the elucidation of optimization challenge caused by sparse-attention\nMIL and propose a novel MIL called ABMILX. It mitigates this problem through\nglobal correlation-based attention refinement and multi-head mechanisms. With\nthe efficient multi-scale random patch sampling strategy, an E2E trained ResNet\nwith ABMILX surpasses SOTA foundation models under the two-stage paradigm\nacross multiple challenging benchmarks, while remaining computationally\nefficient (<10 RTX3090 hours). We show the potential of E2E learning in CPath\nand calls for greater research focus in this area. The code is\nhttps://github.com/DearCaat/E2E-WSI-ABMILX.", "authors": ["Wenhao Tang", "Rong Qin", "Heng Fang", "Fengtao Zhou", "Hao Chen", "Xiang Li", "Ming-Ming Cheng"], "published_date": "2025-06-03", "title_zh": "計算病理學中基於切片層級監督的端到端學習再探", "summary_zh": "計算病理學中，離線特徵提取與多實例學習已成主流，然編碼器缺乏針對下游任務的微調及與多實例學習的分離優化，限制了性能。端到端學習雖直觀，卻面臨算力需求高和結果欠佳等問題。本研究闡明稀疏注意力多實例學習所致的優化挑戰，並提出ABMILX，透過基於全局相關性的注意力精煉及多頭機制緩解此問題。結合高效多尺度隨機切片取樣策略，端到端訓練的ResNet搭配ABMILX超越了多項基準測試下的最佳雙階段模型，且計算成本低廉。研究展現了端到端學習在計算病理學中的潛力，並呼籲更多關注。", "audio": "audios/2506.02408v1.mp3", "timestamp": "2025-06-04T19:16:30.760580"}
{"query": "Diffusion Model", "id": "2506.02488v1", "url": "http://arxiv.org/abs/2506.02488v1", "title": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for Efficient Diffusion Models", "summary": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality.", "authors": ["Hongtao Huang", "Xiaojun Chang", "Lina Yao"], "published_date": "2025-06-03", "title_zh": "Flexiffusion：用於高效擴散模型的免訓練分段式神經網路架構搜尋", "summary_zh": "擴散模型生成高品質圖像，但迭代推論計算成本高昂。神經網路架構搜尋雖可優化擴散模型，卻受限於重新訓練、分步優化的指數級複雜度及緩慢的圖像生成評估。為此，我們提出Flexiffusion，一種免訓練的架構搜尋框架，可聯合優化生成排程與模型架構，無需修改預訓練參數。核心概念是將生成過程分解為等長彈性片段，每片段動態組合完整運算、快取重用運算及跳過運算三種步驟。相較於分步搜尋，此分段式搜尋空間可指數級降低候選數量，同時保有架構多樣性。此外，我們引入相對FID（rFID），一種輕量級評估指標，用於評估候選模型與教師模型輸出間的差異，而非真實數據，可縮短90%以上的評估時間。實證顯示，Flexiffusion在ImageNet和MS-COCO上，針對LDMs、Stable Diffusion和DDPMs，至少實現2倍加速，FID值下降低於5%，優於先前的架構搜尋和快取方法。值得注意的是，在Stable Diffusion上實現了5.1倍加速，且CLIP分數幾乎相同。此研究開創了一種資源效率高的模式，可在不犧牲品質的情況下，搜尋高速擴散模型。", "audio": "audios/2506.02488v1.mp3", "timestamp": "2025-06-04T19:16:46.675610"}
{"query": "AI", "id": "2506.02959v1", "url": "http://arxiv.org/abs/2506.02959v1", "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring", "summary": "The misuse of large language models (LLMs) poses potential risks, motivating\nthe development of machine-generated text (MGT) detection. Existing literature\nprimarily concentrates on binary, document-level detection, thereby neglecting\ntexts that are composed jointly by human and LLM contributions. Hence, this\npaper explores the possibility of fine-grained MGT detection under human-AI\ncoauthoring. We suggest fine-grained detectors can pave pathways toward\ncoauthored text detection with a numeric AI ratio. Specifically, we propose a\ndataset, HACo-Det, which produces human-AI coauthored texts via an automatic\npipeline with word-level attribution labels. We retrofit seven prevailing\ndocument-level detectors to generalize them to word-level detection. Then we\nevaluate these detectors on HACo-Det on both word- and sentence-level detection\ntasks. Empirical results show that metric-based methods struggle to conduct\nfine-grained detection with a 0.462 average F1 score, while finetuned models\nshow superior performance and better generalization across domains. However, we\nargue that fine-grained co-authored text detection is far from solved. We\nfurther analyze factors influencing performance, e.g., context window, and\nhighlight the limitations of current methods, pointing to potential avenues for\nimprovement.", "authors": ["Zhixiong Su", "Yichen Wang", "Herun Wan", "Zhaohan Zhang", "Minnan Luo"], "published_date": "2025-06-03", "title_zh": "HACo-Det：人機協作下細粒度機器生成文本檢測研究", "summary_zh": "大型語言模型濫用構成潛在風險，促使機器生成文本偵測技術發展。現有研究多著重於二元、文檔層級偵測，忽略人機協作文本。本文探討人機協作下細粒度機器生成文本偵測的可能性。我們認為細粒度偵測器有助於量化人工智能比例的協作文本偵測。我們提出HACo-Det資料集，該資料集透過自動流程生成人機協作文本，並提供詞彙層級的歸屬標籤。我們改造七種主流文檔層級偵測器，使其能泛化至詞彙層級偵測。在HACo-Det資料集上，我們評估這些偵測器在詞彙和句子層級偵測任務中的表現。實驗結果表明，基於指標的方法難以進行細粒度偵測，平均F1值為0.462，而微調模型則表現更優異，且跨領域泛化能力更佳。然而，細粒度協作文本偵測仍有待解決。我們進一步分析影響效能的因素，例如上下文窗口，並強調現有方法的局限性，為改進指明潛在方向。", "audio": "audios/2506.02959v1.mp3", "timestamp": "2025-06-04T20:17:25.369654"}
{"query": "Foundation Model", "id": "2506.02359v1", "url": "http://arxiv.org/abs/2506.02359v1", "title": "Auto-Labeling Data for Object Detection", "summary": "Great labels make great models. However, traditional labeling approaches for\ntasks like object detection have substantial costs at scale. Furthermore,\nalternatives to fully-supervised object detection either lose functionality or\nrequire larger models with prohibitive computational costs for inference at\nscale. To that end, this paper addresses the problem of training standard\nobject detection models without any ground truth labels. Instead, we configure\npreviously-trained vision-language foundation models to generate\napplication-specific pseudo \"ground truth\" labels. These auto-generated labels\ndirectly integrate with existing model training frameworks, and we subsequently\ntrain lightweight detection models that are computationally efficient. In this\nway, we avoid the costs of traditional labeling, leverage the knowledge of\nvision-language models, and keep the efficiency of lightweight models for\npractical application. We perform exhaustive experiments across multiple\nlabeling configurations, downstream inference models, and datasets to establish\nbest practices and set an extensive auto-labeling benchmark. From our results,\nwe find that our approach is a viable alternative to standard labeling in that\nit maintains competitive performance on multiple datasets and substantially\nreduces labeling time and costs.", "authors": ["Brent A. Griffin", "Manushree Gangwar", "Jacob Sela", "Jason J. Corso"], "published_date": "2025-06-03", "title_zh": "物件偵測的自動標記數據", "summary_zh": "優良標籤成就優秀模型，然物件偵測等任務的傳統標註方法成本高昂。全監督物件偵測的替代方案，或損失功能，或需更大模型，導致推論運算成本過高。本文旨在無需真實標籤訓練標準物件偵測模型。藉由配置預訓練的視覺語言基礎模型，產生特定應用偽標籤，此類自動生成標籤可直接整合至現有模型訓練框架，進而訓練輕量級且具運算效率的偵測模型。此方法避免傳統標註成本，運用視覺語言模型知識，並保持輕量級模型的實用效率。透過多種標註配置、下游推論模型及資料集進行實驗，建立最佳實踐並設立自動標註基準。結果顯示，此方法能維持多資料集上的競爭力，並大幅降低標註時間及成本，可作為標準標註的可行替代方案。", "audio": "audios/2506.02359v1.mp3", "timestamp": "2025-06-04T20:17:37.207844"}
{"query": "Diffusion Model", "id": "2506.02473v1", "url": "http://arxiv.org/abs/2506.02473v1", "title": "Generative Perception of Shape and Material from Differential Motion", "summary": "Perceiving the shape and material of an object from a single image is\ninherently ambiguous, especially when lighting is unknown and unconstrained.\nDespite this, humans can often disentangle shape and material, and when they\nare uncertain, they often move their head slightly or rotate the object to help\nresolve the ambiguities. Inspired by this behavior, we introduce a novel\nconditional denoising-diffusion model that generates samples of\nshape-and-material maps from a short video of an object undergoing differential\nmotions. Our parameter-efficient architecture allows training directly in\npixel-space, and it generates many disentangled attributes of an object\nsimultaneously. Trained on a modest number of synthetic object-motion videos\nwith supervision on shape and material, the model exhibits compelling emergent\nbehavior: For static observations, it produces diverse, multimodal predictions\nof plausible shape-and-material maps that capture the inherent ambiguities; and\nwhen objects move, the distributions quickly converge to more accurate\nexplanations. The model also produces high-quality shape-and-material estimates\nfor less ambiguous, real-world objects. By moving beyond single-view to\ncontinuous motion observations, our work suggests a generative perception\napproach for improving visual reasoning in physically-embodied systems.", "authors": ["Xinran Nicole Han", "Ko Nishino", "Todd Zickler"], "published_date": "2025-06-03", "title_zh": "基於微分運動的形狀與材質生成感知", "summary_zh": "從單一圖像感知物體形狀與材質具固有歧義性，尤其在光照未知且不受限情況下。人類通常能區分形狀與材質，並藉由頭部或物體移動來消除歧義。受此啟發，本文提出一種新型條件去噪擴散模型，可從物體運動短片生成形狀材質圖樣本。此模型架構具參數效率，可直接於像素空間訓練，並同時生成多種物體屬性。經少量含形狀材質監督之合成運動影片訓練後，模型展現引人注目的行為：對於靜態觀察，模型產生多樣、多模態的形狀材質圖預測，捕捉固有歧義；當物體移動時，分佈迅速收斂至更準確的解釋。此外，模型亦能為較無歧義的真實物體產生高品質的形狀材質估計。透過從單視角轉向連續運動觀察，本研究提出一種生成感知方法，以改善具體物理系統中的視覺推理。", "audio": "audios/2506.02473v1.mp3", "timestamp": "2025-06-04T20:17:51.611101"}
{"query": "AI", "id": "2506.02931v1", "url": "http://arxiv.org/abs/2506.02931v1", "title": "ThinkTank: A Framework for Generalizing Domain-Specific AI Agent Systems into Universal Collaborative Intelligence Platforms", "summary": "This paper presents ThinkTank, a comprehensive and scalable framework\ndesigned to transform specialized AI agent systems into versatile collaborative\nintelligence platforms capable of supporting complex problem-solving across\ndiverse domains. ThinkTank systematically generalizes agent roles, meeting\nstructures, and knowledge integration mechanisms by adapting proven scientific\ncollaboration methodologies. Through role abstraction, generalization of\nmeeting types for iterative collaboration, and the integration of\nRetrieval-Augmented Generation with advanced knowledge storage, the framework\nfacilitates expertise creation and robust knowledge sharing. ThinkTank enables\norganizations to leverage collaborative AI for knowledge-intensive tasks while\nensuring data privacy and security through local deployment, utilizing\nframeworks like Ollama with models such as Llama3.1. The ThinkTank framework is\ndesigned to deliver significant advantages in cost-effectiveness, data\nsecurity, scalability, and competitive positioning compared to cloud-based\nalternatives, establishing it as a universal platform for AI-driven\ncollaborative problem-solving. The ThinkTank code is available at\nhttps://github.com/taugroup/ThinkTank", "authors": ["Praneet Sai Madhu Surabhi", "Dheeraj Reddy Mudireddy", "Jian Tao"], "published_date": "2025-06-03", "title_zh": "智庫：將領域特定人工智慧代理系統泛化為通用協作智慧平台的框架", "summary_zh": "本文介紹ThinkTank，一個全面且可擴展的框架，旨在將專業AI代理系統轉變為通用的協作智能平台，支援跨領域的複雜問題解決。ThinkTank透過調整經證實的科學協作方法，系統性地概括代理角色、會議結構和知識整合機制。透過角色抽象、迭代協作的會議類型泛化，以及將檢索增強生成與先進知識儲存整合，該框架促進了專業知識創建和穩健的知識共享。ThinkTank使組織能夠利用協作AI進行知識密集型任務，同時透過本地部署確保資料隱私和安全，例如使用Ollama和Llama3等模型。相較於雲端替代方案，ThinkTank框架在成本效益、資料安全、可擴展性和競爭定位方面具有顯著優勢，使其成為AI驅動的協作問題解決的通用平台。ThinkTank程式碼可在https://github.com/taugroup/ThinkTank取得。", "audio": "audios/2506.02931v1.mp3", "timestamp": "2025-06-04T21:15:10.499003"}
{"query": "Foundation Model", "id": "2506.02339v1", "url": "http://arxiv.org/abs/2506.02339v1", "title": "Enhancing Lyrics Transcription on Music Mixtures with Consistency Loss", "summary": "Automatic Lyrics Transcription (ALT) aims to recognize lyrics from singing\nvoices, similar to Automatic Speech Recognition (ASR) for spoken language, but\nfaces added complexity due to domain-specific properties of the singing voice.\nWhile foundation ASR models show robustness in various speech tasks, their\nperformance degrades on singing voice, especially in the presence of musical\naccompaniment. This work focuses on this performance gap and explores Low-Rank\nAdaptation (LoRA) for ALT, investigating both single-domain and dual-domain\nfine-tuning strategies. We propose using a consistency loss to better align\nvocal and mixture encoder representations, improving transcription on mixture\nwithout relying on singing voice separation. Our results show that while\nna\\\"ive dual-domain fine-tuning underperforms, structured training with\nconsistency loss yields modest but consistent gains, demonstrating the\npotential of adapting ASR foundation models for music.", "authors": ["Jiawen Huang", "Felipe Sousa", "Emir Demirel", "Emmanouil Benetos", "Igor Gadelha"], "published_date": "2025-06-03", "title_zh": "利用一致性損失增強音樂混合中的歌詞轉錄", "summary_zh": "自動歌詞轉錄旨在從歌聲中辨識歌詞，類似於語音辨識在口語中的應用，但因歌聲的特殊性而更複雜。通用語音辨識模型在各種語音任務中表現穩健，但在歌聲（特別是帶有音樂伴奏時）上的表現會降低。本研究著重於此效能差距，並探討低秩適應於自動歌詞轉錄的應用，研究單領域和雙領域微調策略。我們提出使用一致性損失來更好地對齊人聲和混合音訊編碼器的表示，從而改善混合音訊的轉錄效果，且無需依賴歌聲分離。結果顯示，直接的雙領域微調效果不佳，但透過一致性損失進行的結構化訓練可產生適度但一致的增益，展現了將語音辨識基礎模型應用於音樂的潛力。", "audio": "audios/2506.02339v1.mp3", "timestamp": "2025-06-04T21:15:17.042347"}
{"query": "Diffusion Model", "id": "2506.02452v1", "url": "http://arxiv.org/abs/2506.02452v1", "title": "ANT: Adaptive Neural Temporal-Aware Text-to-Motion Model", "summary": "While diffusion models advance text-to-motion generation, their static\nsemantic conditioning ignores temporal-frequency demands: early denoising\nrequires structural semantics for motion foundations while later stages need\nlocalized details for text alignment. This mismatch mirrors biological\nmorphogenesis where developmental phases demand distinct genetic programs.\nInspired by epigenetic regulation governing morphological specialization, we\npropose **(ANT)**, an **A**daptive **N**eural **T**emporal-Aware architecture.\nANT orchestrates semantic granularity through: **(i) Semantic Temporally\nAdaptive (STA) Module:** Automatically partitions denoising into low-frequency\nstructural planning and high-frequency refinement via spectral analysis. **(ii)\nDynamic Classifier-Free Guidance scheduling (DCFG):** Adaptively adjusts\nconditional to unconditional ratio enhancing efficiency while maintaining\nfidelity. **(iii) Temporal-semantic reweighting:** Quantitatively aligns text\ninfluence with phase requirements. Extensive experiments show that ANT can be\napplied to various baselines, significantly improving model performance, and\nachieving state-of-the-art semantic alignment on StableMoFusion.", "authors": ["Wenshuo Chen", "Kuimou Yu", "Haozhe Jia", "Kaishen Yuan", "Bowen Tian", "Songning Lai", "Hongru Xiao", "Erhang Zhang", "Lei Wang", "Yutao Yue"], "published_date": "2025-06-03", "title_zh": "ANT：自適應神經時序感知文本轉動作模型", "summary_zh": "擴散模型在文本生成動作領域取得進展，但其靜態語義條件忽略了時域頻率需求：早期去噪需要用於動作基礎的結構語義，而後期階段需要局部細節以對齊文本。此不匹配反映了生物形態發生，其中發育階段需要不同的遺傳程序。受表觀遺傳調控形態特化的啟發，我們提出自適應神經時域感知架構 (ANT)。ANT 通過以下方式協調語義粒度：(i) 語義時域自適應模組 (STA)：通過頻譜分析自動將去噪劃分為低頻結構規劃和高頻細化。(ii) 動態無分類器引導排程 (DCFG)：自適應調整條件與非條件比率，在保持保真度的同時提高效率。(iii) 時域語義重新加權：定量地將文本影響與階段需求對齊。大量實驗表明，ANT 可應用於各種基準模型，顯著提高模型性能，並在 StableMoFusion 上實現最先進的語義對齊。", "audio": "audios/2506.02452v1.mp3", "timestamp": "2025-06-04T21:15:25.658417"}
{"query": "AI", "id": "2506.02923v1", "url": "http://arxiv.org/abs/2506.02923v1", "title": "The Limits of Predicting Agents from Behaviour", "summary": "As the complexity of AI systems and their interactions with the world\nincreases, generating explanations for their behaviour is important for safely\ndeploying AI. For agents, the most natural abstractions for predicting\nbehaviour attribute beliefs, intentions and goals to the system. If an agent\nbehaves as if it has a certain goal or belief, then we can make reasonable\npredictions about how it will behave in novel situations, including those where\ncomprehensive safety evaluations are untenable. How well can we infer an\nagent's beliefs from their behaviour, and how reliably can these inferred\nbeliefs predict the agent's behaviour in novel situations? We provide a precise\nanswer to this question under the assumption that the agent's behaviour is\nguided by a world model. Our contribution is the derivation of novel bounds on\nthe agent's behaviour in new (unseen) deployment environments, which represent\na theoretical limit for predicting intentional agents from behavioural data\nalone. We discuss the implications of these results for several research areas\nincluding fairness and safety.", "authors": ["Alexis Bellot", "Jonathan Richens", "Tom Everitt"], "published_date": "2025-06-03", "title_zh": "從行為預測智能體的局限性", "summary_zh": "隨著人工智慧系統及其與世界互動的複雜性日益增加，產生對其行為的解釋對於安全部署人工智慧至關重要。對於智能體而言，預測行為最自然的抽象方法是將信念、意圖和目標歸因於系統。如果智能體的行為表現得像是具有某種目標或信念，那麼我們可以對它在新情境中的行為做出合理的預測，包括那些無法進行全面安全評估的情境。我們能多好地從智能體的行為推斷其信念？這些推斷的信念在新情境中預測智能體行為的可靠性如何？在智能體行為受世界模型引導的假設下，我們對這個問題提供了一個精確的答案。我們的貢獻是推導出智能體在新（未見）部署環境中行為的新界限，這代表了僅從行為數據預測意圖智能體的理論極限。我們討論了這些結果對包括公平性和安全性在內的多個研究領域的影響。", "audio": "audios/2506.02923v1.mp3", "timestamp": "2025-06-04T22:16:57.251647"}
{"query": "Foundation Model", "id": "2506.02308v1", "url": "http://arxiv.org/abs/2506.02308v1", "title": "MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping", "summary": "Recent advances in multimodal foundation models have achieved\nstate-of-the-art performance across a range of tasks. These breakthroughs are\nlargely driven by new pre-training paradigms that leverage large-scale,\nunlabeled multimodal data, followed by instruction fine-tuning on curated\nlabeled datasets and high-quality prompts. While there is growing interest in\nscaling instruction fine-tuning to ever-larger datasets in both quantity and\nscale, our findings reveal that simply increasing the number of\ninstruction-tuning tasks does not consistently yield better performance.\nInstead, we observe that grouping tasks by the common interactions across\nmodalities, such as discovering redundant shared information, prioritizing\nmodality selection with unique information, or requiring synergistic fusion to\ndiscover new information from both modalities, encourages the models to learn\ntransferrable skills within a group while suppressing interference from\nmismatched tasks. To this end, we introduce MINT, a simple yet surprisingly\neffective task-grouping strategy based on the type of multimodal interaction.\nWe demonstrate that the proposed method greatly outperforms existing task\ngrouping baselines for multimodal instruction tuning, striking an effective\nbalance between generalization and specialization.", "authors": ["Xiaojun Shan", "Qi Cao", "Xing Han", "Haofei Yu", "Paul Pu Liang"], "published_date": "2025-06-02", "title_zh": "MINT：基於多模態互動分組的多模態指令微調", "summary_zh": "多模態基礎模型近期進展在多項任務中展現卓越效能。此突破主要歸功於大規模無標籤多模態資料的預訓練範式，以及後續在精選標籤資料集和高品質提示上的指令微調。儘管擴展指令微調至更大規模資料集日益受重視，但研究顯示，單純增加指令微調任務數量並未持續提升效能。我們觀察到，按模態互動類型（如發現冗餘共享資訊、優先選擇具獨特資訊的模態、或需協同融合以發掘新資訊）對任務進行分組，能促使模型在組內學習可遷移技能，同時抑制不匹配任務的干擾。因此，我們提出MINT，一種基於多模態互動類型的簡單而有效的任務分組策略。實驗證明，相較於現有方法，MINT在多模態指令微調方面表現更佳，並在泛化性和特化性之間取得平衡。", "audio": "audios/2506.02308v1.mp3", "timestamp": "2025-06-04T22:17:16.449100"}
{"query": "Diffusion Model", "id": "2506.02444v1", "url": "http://arxiv.org/abs/2506.02444v1", "title": "SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios", "summary": "Hand-Object Interaction (HOI) generation has significant application\npotential. However, current 3D HOI motion generation approaches heavily rely on\npredefined 3D object models and lab-captured motion data, limiting\ngeneralization capabilities. Meanwhile, HOI video generation methods prioritize\npixel-level visual fidelity, often sacrificing physical plausibility.\nRecognizing that visual appearance and motion patterns share fundamental\nphysical laws in the real world, we propose a novel framework that combines\nvisual priors and dynamic constraints within a synchronized diffusion process\nto generate the HOI video and motion simultaneously. To integrate the\nheterogeneous semantics, appearance, and motion features, our method implements\ntri-modal adaptive modulation for feature aligning, coupled with 3D\nfull-attention for modeling inter- and intra-modal dependencies. Furthermore,\nwe introduce a vision-aware 3D interaction diffusion model that generates\nexplicit 3D interaction sequences directly from the synchronized diffusion\noutputs, then feeds them back to establish a closed-loop feedback cycle. This\narchitecture eliminates dependencies on predefined object models or explicit\npose guidance while significantly enhancing video-motion consistency.\nExperimental results demonstrate our method's superiority over state-of-the-art\napproaches in generating high-fidelity, dynamically plausible HOI sequences,\nwith notable generalization capabilities in unseen real-world scenarios.\nProject page at\n\\href{https://github.com/Droliven}{https://github.com/Droliven}.", "authors": ["Lingwei Dang", "Ruizhi Shao", "Hongwen Zhang", "Wei Min", "Yebin Liu", "Qingyao Wu"], "published_date": "2025-06-03", "title_zh": "SViMo：手物互動場景中影片與動作同步擴散生成", "summary_zh": "手物互動生成應用前景廣闊，但現有3D方法過度依賴預定義模型和實驗室數據，限制了泛化能力。視頻生成方法雖注重像素級視覺效果，卻犧牲了物理合理性。為此，我們提出一種新框架，結合視覺先驗和動態約束，於同步擴散過程中同時生成手物互動視頻與動作。該方法採用三模態自適應調製對齊特徵，並使用3D全注意力建模模態內外依賴關係，整合異質語義、外觀和動作特徵。此外，我們引入視覺感知3D互動擴散模型，從同步擴散輸出中直接生成顯式3D互動序列，並將其反饋以建立閉環。此架構無需預定義模型或顯式姿態引導，顯著提升了視頻與動作的一致性。實驗結果表明，相較於現有技術，本方法能生成更高保真度、動態上更合理的HOI序列，並在未見過的真實場景中展現出卓越的泛化能力。", "audio": "audios/2506.02444v1.mp3", "timestamp": "2025-06-04T22:17:31.997369"}
{"query": "AI", "id": "2506.02875v1", "url": "http://arxiv.org/abs/2506.02875v1", "title": "NTIRE 2025 XGC Quality Assessment Challenge: Methods and Results", "summary": "This paper reports on the NTIRE 2025 XGC Quality Assessment Challenge, which\nwill be held in conjunction with the New Trends in Image Restoration and\nEnhancement Workshop (NTIRE) at CVPR 2025. This challenge is to address a major\nchallenge in the field of video and talking head processing. The challenge is\ndivided into three tracks, including user generated video, AI generated video\nand talking head. The user-generated video track uses the FineVD-GC, which\ncontains 6,284 user generated videos. The user-generated video track has a\ntotal of 125 registered participants. A total of 242 submissions are received\nin the development phase, and 136 submissions are received in the test phase.\nFinally, 5 participating teams submitted their models and fact sheets. The AI\ngenerated video track uses the Q-Eval-Video, which contains 34,029 AI-Generated\nVideos (AIGVs) generated by 11 popular Text-to-Video (T2V) models. A total of\n133 participants have registered in this track. A total of 396 submissions are\nreceived in the development phase, and 226 submissions are received in the test\nphase. Finally, 6 participating teams submitted their models and fact sheets.\nThe talking head track uses the THQA-NTIRE, which contains 12,247 2D and 3D\ntalking heads. A total of 89 participants have registered in this track. A\ntotal of 225 submissions are received in the development phase, and 118\nsubmissions are received in the test phase. Finally, 8 participating teams\nsubmitted their models and fact sheets. Each participating team in every track\nhas proposed a method that outperforms the baseline, which has contributed to\nthe development of fields in three tracks.", "authors": ["Xiaohong Liu", "Xiongkuo Min", "Qiang Hu", "Xiaoyun Zhang", "Jie Guo", "Guangtao Zhai", "Shushi Wang", "Yingjie Zhou", "Lu Liu", "Jingxin Li", "Liu Yang", "Farong Wen", "Li Xu", "Yanwei Jiang", "Xilei Zhu", "Chunyi Li", "Zicheng Zhang", "Huiyu Duan", "Xiele Wu", "Yixuan Gao", "Yuqin Cao", "Jun Jia", "Wei Sun", "Jiezhang Cao", "Radu Timofte", "Baojun Li", "Jiamian Huang", "Dan Luo", "Tao Liu", "Weixia Zhang", "Bingkun Zheng", "Junlin Chen", "Ruikai Zhou", "Meiya Chen", "Yu Wang", "Hao Jiang", "Xiantao Li", "Yuxiang Jiang", "Jun Tang", "Yimeng Zhao", "Bo Hu", "Zelu Qi", "Chaoyang Zhang", "Fei Zhao", "Ping Shi", "Lingzhi Fu", "Heng Cong", "Shuai He", "Rongyu Zhang", "Jiarong He", "Zongyao Hu", "Wei Luo", "Zihao Yu", "Fengbin Guan", "Yiting Lu", "Xin Li", "Zhibo Chen", "Mengjing Su", "Yi Wang", "Tuo Chen", "Chunxiao Li", "Shuaiyu Zhao", "Jiaxin Wen", "Chuyi Lin", "Sitong Liu", "Ningxin Chu", "Jing Wan", "Yu Zhou", "Baoying Chen", "Jishen Zeng", "Jiarui Liu", "Xianjin Liu", "Xin Chen", "Lanzhi Zhou", "Hangyu Li", "You Han", "Bibo Xiang", "Zhenjie Liu", "Jianzhang Lu", "Jialin Gui", "Renjie Lu", "Shangfei Wang", "Donghao Zhou", "Jingyu Lin", "Quanjian Song", "Jiancheng Huang", "Yufeng Yang", "Changwei Wang", "Shupeng Zhong", "Yang Yang", "Lihuo He", "Jia Liu", "Yuting Xing", "Tida Fang", "Yuchun Jin"], "published_date": "2025-06-03", "title_zh": "NTIRE 2025 XGC 品質評估挑戰賽：方法與結果", "summary_zh": "本論文報告CVPR 2025 NTIRE研討會中舉辦的XGC品質評估挑戰賽。此挑戰賽旨在解決影片和口語頭像處理領域的重要難題，分為使用者生成影片、AI生成影片及口語頭像三個賽道。使用者生成影片賽道採用FineVD-GC資料集，共125名註冊參與者，開發階段收到242份提交，測試階段收到136份提交，最終5支隊伍提交模型與說明。AI生成影片賽道採用Q-Eval-Video資料集，共133名註冊參與者，開發階段收到396份提交，測試階段收到226份提交，最終6支隊伍提交模型與說明。口語頭像賽道採用THQA-NTIRE資料集，共89名註冊參與者，開發階段收到225份提交，測試階段收到118份提交，最終8支隊伍提交模型與說明。各賽道參賽隊伍提出的方法均優於基準線，有助於各領域發展。", "audio": "audios/2506.02875v1.mp3", "timestamp": "2025-06-04T23:18:20.874939"}
{"query": "Foundation Model", "id": "2506.02294v1", "url": "http://arxiv.org/abs/2506.02294v1", "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through Confidence-Guided Data Augmentation", "summary": "Large foundation models trained on extensive datasets demonstrate strong\nzero-shot capabilities in various domains. To replicate their success when data\nand model size are constrained, knowledge distillation has become an\nestablished tool for transferring knowledge from foundation models to small\nstudent networks. However, the effectiveness of distillation is critically\nlimited by the available training data. This work addresses the common\npractical issue of covariate shift in knowledge distillation, where spurious\nfeatures appear during training but not at test time. We ask the question: when\nthese spurious features are unknown, yet a robust teacher is available, is it\npossible for a student to also become robust to them? We address this problem\nby introducing a novel diffusion-based data augmentation strategy that\ngenerates images by maximizing the disagreement between the teacher and the\nstudent, effectively creating challenging samples that the student struggles\nwith. Experiments demonstrate that our approach significantly improves worst\ngroup and mean group accuracy on CelebA and SpuCo Birds as well as the spurious\nmAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art\ndiffusion-based data augmentation baselines", "authors": ["Niclas Popp", "Kevin Alexander Laube", "Matthias Hein", "Lukas Schott"], "published_date": "2025-06-02", "title_zh": "基於置信度引導數據增強改進未知協變量偏移下的知識蒸餾", "summary_zh": "基於大型數據集訓練的基礎模型在多個領域展現出色的零樣本能力。在數據和模型規模受限的情況下，知識提煉已成為將知識從基礎模型轉移到小型學生網路的既定工具。然而，提煉的有效性受到可用訓練數據的嚴重限制。本研究旨在解決知識提煉中常見的共變異數偏移問題，即在訓練期間出現虛假特徵，但在測試時消失。我們探討在虛假特徵未知，但存在一個穩健教師的情況下，學生是否也能對這些特徵保持穩健。為了解決此問題，我們提出一種基於擴散的新型數據增強策略，該策略通過最大化教師和學生之間的分歧來生成圖像，有效地創建學生難以處理的具挑戰性樣本。實驗表明，在共變異數偏移下，我們的方法顯著提高了 CelebA 和 SpuCo Birds 上的最差組和平均組準確度，以及虛假 ImageNet 上的虛假 mAUC，優於最先進的基於擴散的數據增強基準。", "audio": "audios/2506.02294v1.mp3", "timestamp": "2025-06-04T23:18:30.857610"}
{"query": "Diffusion Model", "id": "2506.02419v1", "url": "http://arxiv.org/abs/2506.02419v1", "title": "Guiding Registration with Emergent Similarity from Pre-Trained Diffusion Models", "summary": "Diffusion models, while trained for image generation, have emerged as\npowerful foundational feature extractors for downstream tasks. We find that\noff-the-shelf diffusion models, trained exclusively to generate natural RGB\nimages, can identify semantically meaningful correspondences in medical images.\nBuilding on this observation, we propose to leverage diffusion model features\nas a similarity measure to guide deformable image registration networks. We\nshow that common intensity-based similarity losses often fail in challenging\nscenarios, such as when certain anatomies are visible in one image but absent\nin another, leading to anatomically inaccurate alignments. In contrast, our\nmethod identifies true semantic correspondences, aligning meaningful structures\nwhile disregarding those not present across images. We demonstrate superior\nperformance of our approach on two tasks: multimodal 2D registration (DXA to\nX-Ray) and monomodal 3D registration (brain-extracted to non-brain-extracted\nMRI). Code: https://github.com/uncbiag/dgir", "authors": ["Nurislam Tursynbek", "Hastings Greer", "Basar Demir", "Marc Niethammer"], "published_date": "2025-06-03", "title_zh": "基於預訓練擴散模型湧現相似性的引導配準", "summary_zh": "擴散模型雖用於圖像生成，現已成為下游任務中強大的基礎特徵提取器。研究發現，僅經自然RGB圖像訓練的現成擴散模型，能識別醫學影像中具語義意義的對應關係。基於此，提出利用擴散模型特徵作為相似性度量，指導可變形影像配準網路。常見基於強度的相似性損失在某些解剖結構僅於單張影像可見時失效，導致解剖結構不準確對齊；本文方法可識別真實語義對應關係，對齊有意義的結構，忽略圖像間不存在的結構。實驗結果顯示，本文方法在多模態2D配準（DXA至X光）及單模態3D配準（去腦MRI至非去腦MRI）任務中表現更優。", "audio": "audios/2506.02419v1.mp3", "timestamp": "2025-06-04T23:18:39.667144"}
{"query": "AI", "id": "2506.02873v1", "url": "http://arxiv.org/abs/2506.02873v1", "title": "It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics", "summary": "Persuasion is a powerful capability of large language models (LLMs) that both\nenables beneficial applications (e.g. helping people quit smoking) and raises\nsignificant risks (e.g. large-scale, targeted political manipulation). Prior\nwork has found models possess a significant and growing persuasive capability,\nmeasured by belief changes in simulated or real users. However, these\nbenchmarks overlook a crucial risk factor: the propensity of a model to attempt\nto persuade in harmful contexts. Understanding whether a model will blindly\n``follow orders'' to persuade on harmful topics (e.g. glorifying joining a\nterrorist group) is key to understanding the efficacy of safety guardrails.\nMoreover, understanding if and when a model will engage in persuasive behavior\nin pursuit of some goal is essential to understanding the risks from agentic AI\nsystems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts\nthe focus from persuasion success to persuasion attempts, operationalized as a\nmodel's willingness to generate content aimed at shaping beliefs or behavior.\nOur evaluation framework probes frontier LLMs using a multi-turn conversational\nsetup between simulated persuader and persuadee agents. APE explores a diverse\nspectrum of topics including conspiracies, controversial issues, and\nnon-controversially harmful content. We introduce an automated evaluator model\nto identify willingness to persuade and measure the frequency and context of\npersuasive attempts. We find that many open and closed-weight models are\nfrequently willing to attempt persuasion on harmful topics and that\njailbreaking can increase willingness to engage in such behavior. Our results\nhighlight gaps in current safety guardrails and underscore the importance of\nevaluating willingness to persuade as a key dimension of LLM risk. APE is\navailable at github.com/AlignmentResearch/AttemptPersuadeEval", "authors": ["Matthew Kowal", "Jasper Timm", "Jean-Francois Godbout", "Thomas Costello", "Antonio A. Arechar", "Gordon Pennycook", "David Rand", "Adam Gleave", "Kellin Pelrine"], "published_date": "2025-06-03", "title_zh": "重在思維：評估前沿大型語言模型在有害議題上的說服嘗試", "summary_zh": "大型語言模型具備強大的說服能力，既能促進有益應用，也帶來重大風險。以往研究著重於模型說服成功率，忽略了模型在有害情境下進行說服的傾向。本研究提出「說服意圖評估」（APE）基準，關注模型產生旨在影響信念或行為內容的意願。透過模擬說服者與被說服者之間的多輪對話，評估前沿語言模型在陰謀論、爭議性議題及有害內容等不同主題下的表現。我們引入自動評估模型，識別說服意願，並衡量說服嘗試的頻率與情境。結果顯示，許多開放及封閉權重模型經常願意在有害主題上嘗試說服，且越獄行為會增加此意願。研究突顯現有安全防護措施的不足，並強調評估說服意願作為衡量大型語言模型風險的重要面向。APE已於github.com/AlignmentResearch/AttemptPersuadeEval開放。", "audio": "audios/2506.02873v1.mp3", "timestamp": "2025-06-05T01:27:19.666659"}
{"query": "Foundation Model", "id": "2506.02308v2", "url": "http://arxiv.org/abs/2506.02308v2", "title": "MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping", "summary": "Recent advances in multimodal foundation models have achieved\nstate-of-the-art performance across a range of tasks. These breakthroughs are\nlargely driven by new pre-training paradigms that leverage large-scale,\nunlabeled multimodal data, followed by instruction fine-tuning on curated\nlabeled datasets and high-quality prompts. While there is growing interest in\nscaling instruction fine-tuning to ever-larger datasets in both quantity and\nscale, our findings reveal that simply increasing the number of\ninstruction-tuning tasks does not consistently yield better performance.\nInstead, we observe that grouping tasks by the common interactions across\nmodalities, such as discovering redundant shared information, prioritizing\nmodality selection with unique information, or requiring synergistic fusion to\ndiscover new information from both modalities, encourages the models to learn\ntransferrable skills within a group while suppressing interference from\nmismatched tasks. To this end, we introduce MINT, a simple yet surprisingly\neffective task-grouping strategy based on the type of multimodal interaction.\nWe demonstrate that the proposed method greatly outperforms existing task\ngrouping baselines for multimodal instruction tuning, striking an effective\nbalance between generalization and specialization.", "authors": ["Xiaojun Shan", "Qi Cao", "Xing Han", "Haofei Yu", "Paul Pu Liang"], "published_date": "2025-06-02", "title_zh": "MINT：基於多模態交互分組的多模態指令調優", "summary_zh": "多模態基礎模型的新進展在多項任務中展現卓越效能。這些突破主要歸功於大規模無標籤多模態資料的預訓練範式，以及後續在精選標籤資料集和高品質提示上的指令微調。儘管擴展指令微調至更大規模資料集日益受重視，研究顯示單純增加指令微調任務數量並非總能提升效能。依模態間共同交互作用（如冗餘資訊、優先模態選擇或協同融合）對任務分組，能促使模型學習群組內可轉移技能，同時抑制不匹配任務的干擾。為此，我們提出基於多模態交互作用類型之任務分組策略MINT。實驗證明，相較於現有方法，MINT在多模態指令微調中表現更佳，有效平衡了泛化與專精。", "audio": "audios/2506.02308v2.mp3", "timestamp": "2025-06-05T01:27:35.983894"}
{"query": "Diffusion Model", "id": "2506.02444v2", "url": "http://arxiv.org/abs/2506.02444v2", "title": "SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios", "summary": "Hand-Object Interaction (HOI) generation has significant application\npotential. However, current 3D HOI motion generation approaches heavily rely on\npredefined 3D object models and lab-captured motion data, limiting\ngeneralization capabilities. Meanwhile, HOI video generation methods prioritize\npixel-level visual fidelity, often sacrificing physical plausibility.\nRecognizing that visual appearance and motion patterns share fundamental\nphysical laws in the real world, we propose a novel framework that combines\nvisual priors and dynamic constraints within a synchronized diffusion process\nto generate the HOI video and motion simultaneously. To integrate the\nheterogeneous semantics, appearance, and motion features, our method implements\ntri-modal adaptive modulation for feature aligning, coupled with 3D\nfull-attention for modeling inter- and intra-modal dependencies. Furthermore,\nwe introduce a vision-aware 3D interaction diffusion model that generates\nexplicit 3D interaction sequences directly from the synchronized diffusion\noutputs, then feeds them back to establish a closed-loop feedback cycle. This\narchitecture eliminates dependencies on predefined object models or explicit\npose guidance while significantly enhancing video-motion consistency.\nExperimental results demonstrate our method's superiority over state-of-the-art\napproaches in generating high-fidelity, dynamically plausible HOI sequences,\nwith notable generalization capabilities in unseen real-world scenarios.\nProject page at https://github.com/Droliven/SViMo\\_project.", "authors": ["Lingwei Dang", "Ruizhi Shao", "Hongwen Zhang", "Wei Min", "Yebin Liu", "Qingyao Wu"], "published_date": "2025-06-03", "title_zh": "SViMo：手物交互場景中視頻與運動同步擴散生成", "summary_zh": "手物互動(HOI)生成應用廣泛，現有3D HOI動作生成方法過度依賴預定義物件模型和實驗室捕捉的動作數據，限制了泛化能力。HOI影片生成方法側重像素級視覺逼真度，常犧牲物理合理性。\n\n本研究提出一個新框架，結合視覺先驗和動態約束，在同步擴散過程中同時生成HOI影片和動作。透過三模態自適應調節對齊特徵，並使用3D全注意力建模模態間及模態內依賴關係，整合異質語義、外觀和動作特徵。此外，我們引入視覺感知3D互動擴散模型，直接從同步擴散輸出生成明確的3D互動序列，並將其反饋以建立閉環迴路。此架構無需預定義物件模型或明確的姿態引導，顯著提高了影片與動作的一致性。\n\n實驗結果表明，相較於現有方法，本方法在生成高逼真度、動態合理的HOI序列方面表現更優異，且在未見過的真實場景中具有顯著的泛化能力。專案網頁位於https://github.com/Droliven/SViMo\\_project。", "audio": "audios/2506.02444v2.mp3", "timestamp": "2025-06-05T01:27:51.875537"}
{"query": "AI", "id": "2506.04167v1", "url": "http://arxiv.org/abs/2506.04167v1", "title": "Neural and Cognitive Impacts of AI: The Influence of Task Subjectivity on Human-LLM Collaboration", "summary": "AI-based interactive assistants are advancing human-augmenting technology,\nyet their effects on users' mental and physiological states remain\nunder-explored. We address this gap by analyzing how Copilot for Microsoft\nWord, a LLM-based assistant, impacts users. Using tasks ranging from objective\n(SAT reading comprehension) to subjective (personal reflection), and with\nmeasurements including fNIRS, Empatica E4, NASA-TLX, and questionnaires, we\nmeasure Copilot's effects on users. We also evaluate users' performance with\nand without Copilot across tasks. In objective tasks, participants reported a\nreduction of workload and an increase in enjoyment, which was paired with\nobjective performance increases. Participants reported reduced workload and\nincreased enjoyment with no change in performance in a creative poetry writing\ntask. However, no benefits due to Copilot use were reported in a highly\nsubjective self-reflection task. Although no physiological changes were\nrecorded due to Copilot use, task-dependent differences in prefrontal cortex\nactivation offer complementary insights into the cognitive processes associated\nwith successful and unsuccessful human-AI collaboration. These findings suggest\nthat AI assistants' effectiveness varies with task type-particularly showing\ndecreased usefulness in tasks that engage episodic memory-and presents a\nbrain-network based hypothesis of human-AI collaboration.", "authors": ["Matthew Russell", "Aman Shah", "Giles Blaney", "Judith Amores", "Mary Czerwinski", "Robert J. K. Jacob"], "published_date": "2025-06-04", "title_zh": "人工智慧的神經與認知影響：任務主觀性對人機協作的影響", "summary_zh": "基於人工智慧的互動助手正推進人類增強技術，但其對使用者心理及生理狀態的影響尚待深入研究。本研究分析基於大型語言模型的Copilot對Microsoft Word使用者的影響，填補此研究空白。透過客觀（SAT閱讀理解）至主觀（個人反思）任務，並結合近紅外光譜儀、Empatica E4、NASA-TLX量表及問卷，評估Copilot對使用者的影響。研究亦評估使用者有無Copilot輔助下的表現。在客觀任務中，受試者回報工作負荷降低、樂趣提升，且客觀表現有所提升。創意詩歌寫作任務中，受試者回報工作負荷降低、樂趣提升，但表現無顯著變化。高度主觀的自我反思任務中，Copilot並未帶來助益。生理數據雖未顯示Copilot引發變化，但前額葉皮層活動的任務相關差異，為成功與失敗的人機協作認知歷程提供互補性見解。研究結果顯示，人工智慧助手的效用隨任務類型而異，尤其在涉及情節記憶的任務中效用降低，並提出基於大腦網路的人機協作假說。", "audio": "audios/2506.04167v1.mp3", "timestamp": "2025-06-05T03:15:37.510894"}
{"query": "Foundation Model", "id": "2506.04217v1", "url": "http://arxiv.org/abs/2506.04217v1", "title": "OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis", "summary": "The rapid progress of navigation, manipulation, and vision models has made\nmobile manipulators capable in many specialized tasks. However, the open-world\nmobile manipulation (OWMM) task remains a challenge due to the need for\ngeneralization to open-ended instructions and environments, as well as the\nsystematic complexity to integrate high-level decision making with low-level\nrobot control based on both global scene understanding and current agent state.\nTo address this complexity, we propose a novel multi-modal agent architecture\nthat maintains multi-view scene frames and agent states for decision-making and\ncontrols the robot by function calling. A second challenge is the hallucination\nfrom domain shift. To enhance the agent performance, we further introduce an\nagentic data synthesis pipeline for the OWMM task to adapt the VLM model to our\ntask domain with instruction fine-tuning. We highlight our fine-tuned OWMM-VLM\nas the first dedicated foundation model for mobile manipulators with global\nscene understanding, robot state tracking, and multi-modal action generation in\na unified model. Through experiments, we demonstrate that our model achieves\nSOTA performance compared to other foundation models including GPT-4o and\nstrong zero-shot generalization in real world. The project page is at\nhttps://github.com/HHYHRHY/OWMM-Agent", "authors": ["Junting Chen", "Haotian Liang", "Lingxiao Du", "Weiyun Wang", "Mengkang Hu", "Yao Mu", "Wenhai Wang", "Jifeng Dai", "Ping Luo", "Wenqi Shao", "Lin Shao"], "published_date": "2025-06-04", "title_zh": "OWMM-Agent：基於多模態主體數據合成的開放世界移動操作", "summary_zh": "導航、操作與視覺模型的快速發展賦予移動操作機器人執行多項專業任務的能力。然而，開放世界移動操作（OWMM）任務仍具挑戰性，原因在於需要針對開放式指令和環境進行泛化，並需整合高層決策與基於全局場景理解和當前機器人狀態的底層控制。為了解決此複雜性，我們提出了一種新型多模態代理架構，其維護多視圖場景幀和代理狀態以進行決策，並通過函數調用來控制機器人。另一個挑戰是領域轉移導致的幻覺問題。為提升代理性能，我們進一步引入了針對OWMM任務的代理數據合成流程，通過指令微調使VLM模型適應我們的任務領域。我們強調，我們微調的OWMM-VLM是首個專為移動操作機器人設計的基礎模型，它在統一模型中實現了全局場景理解、機器人狀態追蹤和多模態動作生成。實驗結果表明，相較於包括GPT-4o在內的其他基礎模型，我們的模型實現了最佳性能，並在現實世界中展現出強大的零樣本泛化能力。", "audio": "audios/2506.04217v1.mp3", "timestamp": "2025-06-05T03:15:49.176858"}
{"query": "Diffusion Model", "id": "2506.04214v1", "url": "http://arxiv.org/abs/2506.04214v1", "title": "Sounding that Object: Interactive Object-Aware Image to Audio Generation", "summary": "Generating accurate sounds for complex audio-visual scenes is challenging,\nespecially in the presence of multiple objects and sound sources. In this\npaper, we propose an {\\em interactive object-aware audio generation} model that\ngrounds sound generation in user-selected visual objects within images. Our\nmethod integrates object-centric learning into a conditional latent diffusion\nmodel, which learns to associate image regions with their corresponding sounds\nthrough multi-modal attention. At test time, our model employs image\nsegmentation to allow users to interactively generate sounds at the {\\em\nobject} level. We theoretically validate that our attention mechanism\nfunctionally approximates test-time segmentation masks, ensuring the generated\naudio aligns with selected objects. Quantitative and qualitative evaluations\nshow that our model outperforms baselines, achieving better alignment between\nobjects and their associated sounds. Project page:\nhttps://tinglok.netlify.app/files/avobject/", "authors": ["Tingle Li", "Baihe Huang", "Xiaobin Zhuang", "Dongya Jia", "Jiawei Chen", "Yuping Wang", "Zhuo Chen", "Gopala Anumanchipalli", "Yuxuan Wang"], "published_date": "2025-06-04", "title_zh": "聲響物體：交互式物體感知圖像音訊生成", "summary_zh": "為複雜視聽場景生成精確聲音極具挑戰，尤其是在多個物件和聲源存在時。本研究提出一種互動式物件感知聲音生成模型，將聲音生成錨定於使用者選定的視覺物件。此方法將物件中心學習整合至條件潛在擴散模型，透過多模態注意力學習關聯圖像區域及其對應聲音。測試階段，模型利用圖像分割，讓使用者得以物件層級互動生成聲音。理論驗證顯示，注意力機制在功能上近似測試階段的分割遮罩，確保生成聲音與選定物件對齊。量化與質性評估表明，本模型優於基準模型，在物件與相關聲音間實現更佳對齊。", "audio": "audios/2506.04214v1.mp3", "timestamp": "2025-06-05T03:15:57.384126"}
{"query": "AI", "id": "2506.04156v1", "url": "http://arxiv.org/abs/2506.04156v1", "title": "A Dataset for Addressing Patient's Information Needs related to Clinical Course of Hospitalization", "summary": "Patients have distinct information needs about their hospitalization that can\nbe addressed using clinical evidence from electronic health records (EHRs).\nWhile artificial intelligence (AI) systems show promise in meeting these needs,\nrobust datasets are needed to evaluate the factual accuracy and relevance of\nAI-generated responses. To our knowledge, no existing dataset captures patient\ninformation needs in the context of their EHRs. We introduce ArchEHR-QA, an\nexpert-annotated dataset based on real-world patient cases from intensive care\nunit and emergency department settings. The cases comprise questions posed by\npatients to public health forums, clinician-interpreted counterparts, relevant\nclinical note excerpts with sentence-level relevance annotations, and\nclinician-authored answers. To establish benchmarks for grounded EHR question\nanswering (QA), we evaluated three open-weight large language models\n(LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies:\ngenerating (1) answers with citations to clinical note sentences, (2) answers\nbefore citations, and (3) answers from filtered citations. We assessed\nperformance on two dimensions: Factuality (overlap between cited note sentences\nand ground truth) and Relevance (textual and semantic similarity between system\nand reference answers). The final dataset contains 134 patient cases. The\nanswer-first prompting approach consistently performed best, with Llama 4\nachieving the highest scores. Manual error analysis supported these findings\nand revealed common issues such as omitted key clinical evidence and\ncontradictory or hallucinated content. Overall, ArchEHR-QA provides a strong\nbenchmark for developing and evaluating patient-centered EHR QA systems,\nunderscoring the need for further progress toward generating factual and\nrelevant responses in clinical contexts.", "authors": ["Sarvesh Soni", "Dina Demner-Fushman"], "published_date": "2025-06-04", "title_zh": "解決住院臨床病程相關患者資訊需求的數據集", "summary_zh": "病人住院期間有不同的資訊需求，電子病歷的臨床證據可滿足這些需求。人工智慧系統雖有潛力，但需可靠的資料集評估其回應的準確性和相關性。我們建立了ArchEHR-QA，一個基於加護病房和急診室真實病患案例的專家標註資料集，包含病患提問、臨床醫師詮釋、相關臨床筆記摘錄及醫師撰寫的答案。為建立EHR問答基準，我們評估了Llama 4、Llama 3和Mixtral三種開放權重的大型語言模型，採用三種提示策略：生成含臨床筆記引用的答案、先生成答案後引用、從篩選的引用生成答案。我們從事實性（引用筆記與真實答案的重疊程度）和相關性（系統答案與參考答案的相似度）評估效能。最終資料集包含134個案例。先回答的提示方法表現最佳，Llama 4得分最高。人工錯誤分析顯示常見問題包括遺漏關鍵臨床證據、內容矛盾或虛構。ArchEHR-QA為開發和評估以病人為中心的EHR問答系統提供基準，強調在臨床環境中產生真實和相關回應的重要性。", "audio": "audios/2506.04156v1.mp3", "timestamp": "2025-06-05T04:26:47.039256"}
{"query": "Foundation Model", "id": "2506.03994v1", "url": "http://arxiv.org/abs/2506.03994v1", "title": "Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era", "summary": "Human learning and conceptual representation is grounded in sensorimotor\nexperience, in contrast to state-of-the-art foundation models. In this paper,\nwe investigate how well such large-scale models, trained on vast quantities of\ndata, represent the semantic feature norms of concrete object concepts, e.g. a\nROSE is red, smells sweet, and is a flower. More specifically, we use probing\ntasks to test which properties of objects these models are aware of. We\nevaluate image encoders trained on image data alone, as well as\nmultimodally-trained image encoders and language-only models, on predicting an\nextended denser version of the classic McRae norms and the newer Binder dataset\nof attribute ratings. We find that multimodal image encoders slightly\noutperform language-only approaches, and that image-only encoders perform\ncomparably to the language models, even on non-visual attributes that are\nclassified as \"encyclopedic\" or \"function\". These results offer new insights\ninto what can be learned from pure unimodal learning, and the complementarity\nof the modalities.", "authors": ["Dan Oneata", "Desmond Elliott", "Stella Frank"], "published_date": "2025-06-04", "title_zh": "觀其形，知其味：十億參數時代下多模態分佈語義學之再探", "summary_zh": "相較於先進的基礎模型，人類學習與概念表徵根植於感知運動經驗。本文探討大規模模型在大量數據上訓練後，對具體物件概念的語義特徵規範的表徵能力，例如玫瑰是紅色的、氣味香甜且是花朵。具體而言，我們使用探測任務測試這些模型對物件屬性的認知程度。我們評估了僅在圖像數據上訓練的圖像編碼器，以及多模態訓練的圖像編碼器和純語言模型，以預測經典McRae規範的擴展版本和較新的Binder屬性評級數據集。研究發現，多模態圖像編碼器的表現略優於純語言模型，而僅使用圖像的編碼器在非視覺屬性（被歸類為百科全書式或功能性）上的表現與語言模型相當。這些結果為純單模態學習的可能性以及模態的互補性提供了新的見解。", "audio": "audios/2506.03994v1.mp3", "timestamp": "2025-06-05T04:26:54.437123"}
{"query": "Diffusion Model", "id": "2506.04211v1", "url": "http://arxiv.org/abs/2506.04211v1", "title": "Diffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector", "summary": "Object detectors often suffer a decrease in performance due to the large\ndomain gap between the training data (source domain) and real-world data\n(target domain). Diffusion-based generative models have shown remarkable\nabilities in generating high-quality and diverse images, suggesting their\npotential for extracting valuable feature from various domains. To effectively\nleverage the cross-domain feature representation of diffusion models, in this\npaper, we train a detector with frozen-weight diffusion model on the source\ndomain, then employ it as a teacher model to generate pseudo labels on the\nunlabeled target domain, which are used to guide the supervised learning of the\nstudent model on the target domain. We refer to this approach as Diffusion\nDomain Teacher (DDT). By employing this straightforward yet potent framework,\nwe significantly improve cross-domain object detection performance without\ncompromising the inference speed. Our method achieves an average mAP\nimprovement of 21.2% compared to the baseline on 6 datasets from three common\ncross-domain detection benchmarks (Cross-Camera, Syn2Real, Real2Artistic},\nsurpassing the current state-of-the-art (SOTA) methods by an average of 5.7%\nmAP. Furthermore, extensive experiments demonstrate that our method\nconsistently brings improvements even in more powerful and complex models,\nhighlighting broadly applicable and effective domain adaptation capability of\nour DDT. The code is available at\nhttps://github.com/heboyong/Diffusion-Domain-Teacher.", "authors": ["Boyong He", "Yuxiang Ji", "Zhuoyue Tan", "Liaoni Wu"], "published_date": "2025-06-04", "title_zh": "擴散域教師：擴散引導的領域自適應目標檢測器", "summary_zh": "物件偵測器常因訓練資料（源域）與真實世界資料（目標域）間的巨大差異而效能下降。基於擴散的生成模型在生成高品質且多樣化圖像方面表現出色，暗示其在提取跨域特徵方面的潛力。為有效利用擴散模型的跨域特徵表示，本文提出一種方法：在源域上訓練一個帶有凍結權重擴散模型的偵測器，並將其作為教師模型，在未標記的目標域上生成偽標籤，以指導學生模型在目標域上的監督學習。此方法稱為擴散域教師（DDT）。透過此簡潔而有效框架，顯著提升跨域物件偵測效能，且不影響推理速度。在三個常見跨域偵測基準的六個數據集上，相較於基準模型，平均mAP提升21.2%，超越現有最先進方法平均5.7% mAP。大量實驗表明，即使在更強大和複雜的模型中，此方法也能持續帶來提升，突顯其廣泛適用且有效的域適應能力。", "audio": "audios/2506.04211v1.mp3", "timestamp": "2025-06-05T04:27:03.009662"}
{"query": "AI", "id": "2506.04133v1", "url": "http://arxiv.org/abs/2506.04133v1", "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems", "summary": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment.", "authors": ["Shaina Raza", "Ranjan Sapkota", "Manoj Karkee", "Christos Emmanouilidis"], "published_date": "2025-06-04", "title_zh": "自主代理人工智慧之TRiSM：基於大型語言模型之自主多代理系統中信任、風險與安全管理綜述", "summary_zh": "基於大型語言模型的自主代理AI系統正重塑企業與社會領域的智慧自主性、協作及決策。本文針對此類系統的信任、風險與安全管理(TRiSM)進行結構化分析，涵蓋代理AI的概念基礎、架構差異及可擴展的系統設計。TRiSM框架透過治理、可解釋性、模型維運及隱私安全四大支柱闡述，並針對代理LLM進行情境化。本文識別了獨特的威脅向量，提出了全面的風險分類，並以案例研究佐證。此外，還探討了信任建立機制、透明度與監督技術，以及分散式LLM代理系統中最新的可解釋性策略，同時評估信任、可解釋性和以人為本的效能指標，並檢視了公開的基準測試挑戰。安全與隱私則透過加密、對抗性防禦及符合AI法規來保障。最後，本文提出負責任的代理AI發展藍圖，為確保安全、可靠及透明的部署，建議研究方向應將新興的多代理系統與穩健的TRiSM原則對齊。", "audio": "audios/2506.04133v1.mp3", "timestamp": "2025-06-05T05:19:26.676954"}
{"query": "Foundation Model", "id": "2506.03834v1", "url": "http://arxiv.org/abs/2506.03834v1", "title": "Enhancing Safety of Foundation Models for Visual Navigation through Collision Avoidance via Repulsive Estimation", "summary": "We propose CARE (Collision Avoidance via Repulsive Estimation), a\nplug-and-play module that enhances the safety of vision-based navigation\nwithout requiring additional range sensors or fine-tuning of pretrained models.\nWhile recent foundation models using only RGB inputs have shown strong\nperformance, they often fail to generalize in out-of-distribution (OOD)\nenvironments with unseen objects or variations in camera parameters (e.g.,\nfield of view, pose, or focal length). Without fine-tuning, these models may\ngenerate unsafe trajectories that lead to collisions, requiring costly data\ncollection and retraining. CARE addresses this limitation by seamlessly\nintegrating with any RGB-based navigation system that outputs local\ntrajectories, dynamically adjusting them using repulsive force vectors derived\nfrom monocular depth maps. We evaluate CARE by combining it with\nstate-of-the-art vision-based navigation models across multiple robot\nplatforms. CARE consistently reduces collision rates (up to 100%) without\nsacrificing goal-reaching performance and improves collision-free travel\ndistance by up to 10.7x in exploration tasks.", "authors": ["Joonkyung Kim", "Joonyeol Sim", "Woojun Kim", "Katia Sycara", "Changjoo Nam"], "published_date": "2025-06-04", "title_zh": "透過斥力估計增強基礎模型於視覺導航中之碰撞避免安全性", "summary_zh": "本研究提出CARE (基於斥力估算的碰撞避免)，一種隨插即用的模組，無需額外的距離感測器或微調預訓練模型，即可提高視覺導航的安全性。僅使用RGB輸入的基礎模型表現出色，但在未見過的物體或相機參數變化等分布外環境中泛化能力不足，容易產生不安全的軌跡導致碰撞。CARE透過單眼深度圖衍生的斥力向量動態調整任何基於RGB的導航系統輸出的局部軌跡，解決了此限制。實驗評估顯示，CARE能始終如一地降低碰撞率(最高達100%)，且不影響目標達成性能，並在探索任務中將無碰撞行駛距離提高達10.7倍。", "audio": "audios/2506.03834v1.mp3", "timestamp": "2025-06-05T05:19:32.212917"}
{"query": "Diffusion Model", "id": "2506.04158v1", "url": "http://arxiv.org/abs/2506.04158v1", "title": "Image Editing As Programs with Diffusion Models", "summary": "While diffusion models have achieved remarkable success in text-to-image\ngeneration, they encounter significant challenges with instruction-driven image\nediting. Our research highlights a key challenge: these models particularly\nstruggle with structurally inconsistent edits that involve substantial layout\nchanges. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a\nunified image editing framework built upon the Diffusion Transformer (DiT)\narchitecture. At its core, IEAP approaches instructional editing through a\nreductionist lens, decomposing complex editing instructions into sequences of\natomic operations. Each operation is implemented via a lightweight adapter\nsharing the same DiT backbone and is specialized for a specific type of edit.\nProgrammed by a vision-language model (VLM)-based agent, these operations\ncollaboratively support arbitrary and structurally inconsistent\ntransformations. By modularizing and sequencing edits in this way, IEAP\ngeneralizes robustly across a wide range of editing tasks, from simple\nadjustments to substantial structural changes. Extensive experiments\ndemonstrate that IEAP significantly outperforms state-of-the-art methods on\nstandard benchmarks across various editing scenarios. In these evaluations, our\nframework delivers superior accuracy and semantic fidelity, particularly for\ncomplex, multi-step instructions. Codes are available at\nhttps://github.com/YujiaHu1109/IEAP.", "authors": ["Yujia Hu", "Songhua Liu", "Zhenxiong Tan", "Xingyi Yang", "Xinchao Wang"], "published_date": "2025-06-04", "title_zh": "基於擴散模型的程式化圖像編輯", "summary_zh": "擴散模型在文字生成圖像方面表現出色，但在指令驅動圖像編輯中面臨挑戰，尤其是在結構不一致、涉及大幅佈局變更的編輯上。本研究提出圖像編輯即程式 (IEAP) 框架，基於擴散轉換器 (DiT) 架構，將複雜編輯指令分解為一系列原子操作。每個操作透過共享 DiT 主幹的輕量級適配器實現，專注於特定編輯類型。透過視覺語言模型 (VLM) 代理編程，這些操作協同支援任意且結構不一致的轉換。IEAP 藉由模組化和排序編輯，在各種編輯任務（從簡單調整到重大結構變更）中展現強大的泛化能力。大量實驗表明，在各種編輯場景的標準基準測試中，IEAP 顯著優於現有方法，尤其是在複雜、多步驟的指令下，能提供卓越的準確性和語義保真度。程式碼位於 https://github.com/YujiaHu1109/IEAP。", "audio": "audios/2506.04158v1.mp3", "timestamp": "2025-06-05T05:19:38.937157"}
{"query": "AI", "id": "2506.04090v1", "url": "http://arxiv.org/abs/2506.04090v1", "title": "A Reference Architecture for Gamified Cultural Heritage Applications Leveraging Generative AI and Augmented Reality", "summary": "The rapid advancement of Information and Communication Technologies is\ntransforming Cultural Heritage access, experience, and preservation. However,\nmany digital heritage applications lack interactivity, personalization, and\nadaptability, limiting user engagement and educational impact. This short paper\npresents a reference architecture for gamified cultural heritage applications\nleveraging generative AI and augmented reality. Gamification enhances\nmotivation, artificial intelligence enables adaptive storytelling and\npersonalized content, and augmented reality fosters immersive, location-aware\nexperiences. Integrating AI with gamification supports dynamic mechanics,\npersonalized feedback, and user behavior prediction, improving engagement. The\nmodular design supports scalability, interoperability, and adaptability across\nheritage contexts. This research provides a framework for designing interactive\nand intelligent cultural heritage applications, promoting accessibility and\ndeeper appreciation among users and stakeholders.", "authors": ["Federico Martusciello", "Henry Muccini", "Antonio Bucchiarone"], "published_date": "2025-06-04", "title_zh": "利用生成式AI與擴增實境的遊戲化文化遺產應用之參考架構", "summary_zh": "資訊與通信科技的快速發展正改變文化遺產的近用、體驗與保存。然而，許多數位遺產應用程式缺乏互動性、個人化與適應性，限制了使用者參與和教育影響。本文提出一個利用生成式人工智慧和擴增實境的遊戲化文化遺產應用程式參考架構。遊戲化增強動機，人工智慧實現自適應敘事和個人化內容，擴增實境促進沉浸式、位置感知體驗。人工智慧與遊戲化整合支援動態機制、個人化回饋和使用者行為預測，從而提高參與度。模組化設計支援跨文化遺產背景的可擴展性、互通性和適應性。本研究為設計互動式和智慧型文化遺產應用程式提供框架，促進使用者和利害關係人的近用和更深層次的欣賞。", "audio": "audios/2506.04090v1.mp3", "timestamp": "2025-06-05T06:28:58.272832"}
{"query": "Foundation Model", "id": "2506.03752v1", "url": "http://arxiv.org/abs/2506.03752v1", "title": "Frame-Level Real-Time Assessment of Stroke Rehabilitation Exercises from Video-Level Labeled Data: Task-Specific vs. Foundation Models", "summary": "The growing demands of stroke rehabilitation have increased the need for\nsolutions to support autonomous exercising. Virtual coaches can provide\nreal-time exercise feedback from video data, helping patients improve motor\nfunction and keep engagement. However, training real-time motion analysis\nsystems demands frame-level annotations, which are time-consuming and costly to\nobtain. In this work, we present a framework that learns to classify individual\nframes from video-level annotations for real-time assessment of compensatory\nmotions in rehabilitation exercises. We use a gradient-based technique and a\npseudo-label selection method to create frame-level pseudo-labels for training\na frame-level classifier. We leverage pre-trained task-specific models - Action\nTransformer, SkateFormer - and a foundation model - MOMENT - for pseudo-label\ngeneration, aiming to improve generalization to new patients. To validate the\napproach, we use the \\textit{SERE} dataset with 18 post-stroke patients\nperforming five rehabilitation exercises annotated on compensatory motions.\nMOMENT achieves better video-level assessment results (AUC = $73\\%$),\noutperforming the baseline LSTM (AUC = $58\\%$). The Action Transformer, with\nthe Integrated Gradient technique, leads to better outcomes (AUC = $72\\%$) for\nframe-level assessment, outperforming the baseline trained with ground truth\nframe-level labeling (AUC = $69\\%$). We show that our proposed approach with\npre-trained models enhances model generalization ability and facilitates the\ncustomization to new patients, reducing the demands of data labeling.", "authors": ["Gonçalo Mesquita", "Ana Rita Cóias", "Artur Dubrawski", "Alexandre Bernardino"], "published_date": "2025-06-04", "title_zh": "基於影片層級標籤資料之腦中風復健運動幀層級即時評估：任務特定模型與基礎模型之比較", "summary_zh": "中風復健需求日益增長，促使自主運動輔助方案的需求增加。虛擬教練可根據影片數據提供即時運動回饋，幫助患者改善運動功能並保持參與度。然而，訓練即時動作分析系統需要耗時且昂貴的逐幀標註。本研究提出一種框架，利用影片層級標註學習分類個別幀，以即時評估復健運動中的代償動作。該框架採用基於梯度的技術和偽標籤選擇方法，為訓練幀級分類器創建幀級偽標籤。我們利用預訓練的任務特定模型Action Transformer、SkateFormer以及基礎模型MOMENT生成偽標籤，旨在提高對新患者的泛化能力。經驗證，MOMENT在影片層級評估中表現更佳（AUC = $73\\%$），優於基準LSTM（AUC = $58\\%$）。Action Transformer結合積分梯度技術，在幀級評估中取得更好成果（AUC = $72\\%$），優於使用真實幀級標籤訓練的基準模型（AUC = $69\\%$）。結果表明，所提出的基於預訓練模型的方法增強了模型泛化能力，有助於適應新患者，並降低數據標註的需求。", "audio": "audios/2506.03752v1.mp3", "timestamp": "2025-06-05T06:29:06.779254"}
{"query": "Diffusion Model", "id": "2506.04103v1", "url": "http://arxiv.org/abs/2506.04103v1", "title": "Global convergence rates in the relaxation limits for the compressible Euler and Euler-Maxwell systems in Sobolev spaces", "summary": "We study two relaxation problems in the class of partially dissipative\nhyperbolic systems: the compressible Euler system with damping and the\ncompressible Euler-Maxwell system. In classical Sobolev spaces, we derive a\nglobal convergence rate of $\\mathcal{O}(\\varepsilon)$ between strong solutions\nof the relaxed Euler system and the porous medium equation in $\\mathbb{R}^d$\n($d\\geq1$) for \\emph{ill-prepared} initial data. In a well-prepared setting, we\nderive an enhanced convergence rate of order $\\mathcal{O}(\\varepsilon^2)$\nbetween the solutions of the compressible Euler system and their first-order\nasymptotic approximation. Regarding the Euler-Maxwell system, we prove the\nglobal strong convergence of its solutions to the drift-diffusion model in\n$\\mathbb{R}^3$ with a rate of $\\mathcal{O}(\\varepsilon)$. These results are\nachieved by developing an asymptotic expansion approach that, combined with\nstream function techniques, ensures uniform-in-time error estimates.", "authors": ["Timothée Crin-Barat", "Yue-Jun Peng", "Ling-Yun Shou"], "published_date": "2025-06-04", "title_zh": "可壓縮 Euler 與 Euler-Maxwell 系統在索伯列夫空間中鬆弛極限的全局收斂速率", "summary_zh": "本文探討部分耗散雙曲系統中的兩個鬆弛問題：帶阻尼的可壓縮Euler系統和可壓縮Euler-Maxwell系統。對於非良好預備的初始數據，在經典Sobolev空間中，推導出鬆弛Euler系統的強解與$\\mathbb{R}^d$ ($d\\geq1$)中的多孔介質方程之間，具有$\\mathcal{O}(\\varepsilon)$的全局收斂速率。在良好預備的背景下，推導出可壓縮Euler系統解及其一階漸近近似之間，具有$\\mathcal{O}(\\varepsilon^2)$階的增強收斂速率。關於Euler-Maxwell系統，證明了其解在$\\mathbb{R}^3$中全局強收斂至漂移擴散模型，速率為$\\mathcal{O}(\\varepsilon)$。這些結果通過開發漸近展開方法實現，該方法結合流函數技巧，確保了時間一致的誤差估計。", "audio": "audios/2506.04103v1.mp3", "timestamp": "2025-06-05T06:29:14.441695"}
{"query": "AI", "id": "2506.04079v1", "url": "http://arxiv.org/abs/2506.04079v1", "title": "EuroLLM-9B: Technical Report", "summary": "This report presents EuroLLM-9B, a large language model trained from scratch\nto support the needs of European citizens by covering all 24 official European\nUnion languages and 11 additional languages. EuroLLM addresses the issue of\nEuropean languages being underrepresented and underserved in existing open\nlarge language models. We provide a comprehensive overview of EuroLLM-9B's\ndevelopment, including tokenizer design, architectural specifications, data\nfiltering, and training procedures. We describe the pre-training data\ncollection and filtering pipeline, including the creation of EuroFilter, an\nAI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a\nnovel synthetic dataset for post-training that enhances language coverage for\nEuropean languages. Evaluation results demonstrate EuroLLM-9B's competitive\nperformance on multilingual benchmarks and machine translation tasks,\nestablishing it as the leading open European-made LLM of its size. To support\nopen research and adoption, we release all major components of this work,\nincluding the base and instruction-tuned models, the EuroFilter classifier, and\nthe synthetic post-training dataset.", "authors": ["Pedro Henrique Martins", "João Alves", "Patrick Fernandes", "Nuno M. Guerreiro", "Ricardo Rei", "Amin Farajian", "Mateusz Klimaszewski", "Duarte M. Alves", "José Pombal", "Manuel Faysse", "Pierre Colombo", "François Yvon", "Barry Haddow", "José G. C. de Souza", "Alexandra Birch", "André F. T. Martins"], "published_date": "2025-06-04", "title_zh": "EuroLLM-9B：技術報告", "summary_zh": "本研究提出 EuroLLM-9B，一個從頭訓練的大型語言模型，旨在支援歐洲公民的需求，涵蓋歐盟全部 24 種官方語言及額外 11 種語言。 EuroLLM 解決了現有開放大型語言模型中歐洲語言代表性不足的問題。 我們全面概述了 EuroLLM-9B 的開發，包括分詞器設計、架構規範、數據篩選和訓練流程，描述了預訓練數據的收集和篩選流程，包括創建基於人工智慧的多語種篩選器 EuroFilter，以及用於後訓練的新型合成數據集 EuroBlocks-Synthetic，該數據集增強了歐洲語言的覆蓋範圍。 評估結果顯示 EuroLLM-9B 在多語種基準測試和機器翻譯任務中具有競爭力，確立了其作為同等規模下領先的歐洲製造開放 LLM 的地位。 我們釋出了本研究的所有主要組成部分，包括基礎模型和指令微調模型、EuroFilter 分類器和合成後訓練數據集，以支持開放研究和採用。", "audio": "audios/2506.04079v1.mp3", "timestamp": "2025-06-05T07:18:36.820115"}
{"query": "Foundation Model", "id": "2506.03709v1", "url": "http://arxiv.org/abs/2506.03709v1", "title": "AetherVision-Bench: An Open-Vocabulary RGB-Infrared Benchmark for Multi-Angle Segmentation across Aerial and Ground Perspectives", "summary": "Open-vocabulary semantic segmentation (OVSS) involves assigning labels to\neach pixel in an image based on textual descriptions, leveraging world models\nlike CLIP. However, they encounter significant challenges in cross-domain\ngeneralization, hindering their practical efficacy in real-world applications.\nEmbodied AI systems are transforming autonomous navigation for ground vehicles\nand drones by enhancing their perception abilities, and in this study, we\npresent AetherVision-Bench, a benchmark for multi-angle segmentation across\naerial, and ground perspectives, which facilitates an extensive evaluation of\nperformance across different viewing angles and sensor modalities. We assess\nstate-of-the-art OVSS models on the proposed benchmark and investigate the key\nfactors that impact the performance of zero-shot transfer models. Our work\npioneers the creation of a robustness benchmark, offering valuable insights and\nestablishing a foundation for future research.", "authors": ["Aniruddh Sikdar", "Aditya Gandhamal", "Suresh Sundaram"], "published_date": "2025-06-04", "title_zh": "以太視覺基準：開放詞彙RGB-紅外多角度分割基準，橫跨空域與地麵視角", "summary_zh": "開放詞彙語義分割(OVSS)依據文字描述為圖像像素分配標籤，仰賴CLIP等世界模型。然而，OVSS在跨領域泛化方面面臨挑戰，限制了其於實際應用中的效力。本研究藉由增強感知能力，探討具身人工智慧系統如何轉變地面車輛和無人機的自主導航。我們提出AetherVision-Bench，一個用於評估跨越空中和地面視角多角度分割的基準，有助於廣泛評估不同視角和感測器模式下的性能。我們在提出的基準上評估最先進的OVSS模型，並探討影響零樣本遷移模型性能的關鍵因素。此研究開創了穩健性基準的先河，為未來研究提供寶貴見解並奠定基礎。", "audio": "audios/2506.03709v1.mp3", "timestamp": "2025-06-05T07:18:43.493338"}
{"query": "Diffusion Model", "id": "2506.04083v1", "url": "http://arxiv.org/abs/2506.04083v1", "title": "A Generative Adaptive Replay Continual Learning Model for Temporal Knowledge Graph Reasoning", "summary": "Recent Continual Learning (CL)-based Temporal Knowledge Graph Reasoning\n(TKGR) methods focus on significantly reducing computational cost and\nmitigating catastrophic forgetting caused by fine-tuning models with new data.\nHowever, existing CL-based TKGR methods still face two key limitations: (1)\nThey usually one-sidedly reorganize individual historical facts, while\noverlooking the historical context essential for accurately understanding the\nhistorical semantics of these facts; (2) They preserve historical knowledge by\nsimply replaying historical facts, while ignoring the potential conflicts\nbetween historical and emerging facts. In this paper, we propose a Deep\nGenerative Adaptive Replay (DGAR) method, which can generate and adaptively\nreplay historical entity distribution representations from the whole historical\ncontext. To address the first challenge, historical context prompts as sampling\nunits are built to preserve the whole historical context information. To\novercome the second challenge, a pre-trained diffusion model is adopted to\ngenerate the historical distribution. During the generation process, the common\nfeatures between the historical and current distributions are enhanced under\nthe guidance of the TKGR model. In addition, a layer-by-layer adaptive replay\nmechanism is designed to effectively integrate historical and current\ndistributions. Experimental results demonstrate that DGAR significantly\noutperforms baselines in reasoning and mitigating forgetting.", "authors": ["Zhiyu Zhang", "Wei Chen", "Youfang Lin", "Huaiyu Wan"], "published_date": "2025-06-04", "title_zh": "用於時序知識圖推理的生成式自適應重放持續學習模型", "summary_zh": "近年基於持續學習的時間知識圖譜推理方法，著重降低運算成本並減緩災難性遺忘。然而，現有方法忽略了歷史情境對理解歷史事實語義的重要性，且單純重播歷史事實，忽略了歷史與新興事實間的潛在衝突。本文提出深度生成自適應重播方法，能從整體歷史情境生成並自適應重播歷史實體分佈表示。構建歷史情境提示作為抽樣單元，以保留完整歷史情境資訊。採用預訓練擴散模型生成歷史分佈，並在TKGR模型的引導下，增強歷史與當前分佈的共同特徵。此外，設計逐層自適應重播機制，有效整合歷史與當前分佈。實驗結果表明，DGAR在推理和減輕遺忘方面顯著優於基準方法。", "audio": "audios/2506.04083v1.mp3", "timestamp": "2025-06-05T07:18:49.152048"}
{"query": "AI", "id": "2506.04072v1", "url": "http://arxiv.org/abs/2506.04072v1", "title": "Controlling Difficulty of Generated Text for AI-Assisted Language Learning", "summary": "Practicing conversations with large language models (LLMs) presents a\npromising alternative to traditional in-person language learning. However, most\nLLMs generate text at a near-native level of complexity, making them ill-suited\nfor beginner learners (CEFR: A1-A2). In this paper, we investigate whether\ncontrollable generation techniques -- specifically modular methods that do not\nrequire model fine-tuning -- can adapt LLM outputs to better support absolute\nbeginners. We evaluate these methods through both automatic metrics and a user\nstudy with university-level learners of Japanese. Our findings show that while\nprompting alone fails to control output difficulty, the use of future\ndiscriminators (Yang and Klein, 2021) significantly improves output\ncomprehensibility (from 40.4\\% to 84.3\\%). We further introduce a novel\ntoken-level evaluation metric, Token Miss Rate (TMR), that quantifies the\nproportion of incomprehensible tokens per utterance and correlates strongly\nwith human judgments. To support future research in AI-assisted language\nlearning, we release our code, models, annotation tools, and dataset.", "authors": ["Meiqing Jin", "Liam Dugan", "Chris Callison-Burch"], "published_date": "2025-06-04", "title_zh": "用於AI輔助語言學習之生成文本難度控制", "summary_zh": "與大型語言模型練習對話為傳統面授語言學習提供了一種有前景的替代方案。然而，多數模型產生的文本複雜度接近母語人士水平，不適合初學者。本文探討了無需模型微調的可控生成技術，能否調整模型輸出以更好地支援初學者。我們通過自動指標和針對日語學習者的用戶研究評估了這些方法。研究結果表明，單獨使用提示無法控制輸出難度，但使用未來判別器顯著提高了輸出可理解性（從40.4\\%到84.3\\%）。我們進一步引入了一種新的token級別評估指標，Token Miss Rate (TMR)，該指標量化了每次發言中難以理解的token比例，並與人類判斷高度相關。為支持AI輔助語言學習的未來研究，我們釋出了程式碼、模型、標註工具和數據集。", "audio": "audios/2506.04072v1.mp3", "timestamp": "2025-06-05T08:25:48.067794"}
{"query": "Foundation Model", "id": "2506.03530v1", "url": "http://arxiv.org/abs/2506.03530v1", "title": "How Far Are We from Predicting Missing Modalities with Foundation Models?", "summary": "Multimodal foundation models have demonstrated impressive capabilities across\ndiverse tasks. However, their potential as plug-and-play solutions for missing\nmodality prediction remains underexplored. To investigate this, we categorize\nexisting approaches into three representative paradigms, encompassing a total\nof 42 model variants, and conduct a comprehensive evaluation in terms of\nprediction accuracy and adaptability to downstream tasks. Our analysis reveals\nthat current foundation models often fall short in two critical aspects: (i)\nfine-grained semantic extraction from the available modalities, and (ii) robust\nvalidation of generated modalities. These limitations lead to suboptimal and,\nat times, misaligned predictions. To address these challenges, we propose an\nagentic framework tailored for missing modality prediction. This framework\ndynamically formulates modality-aware mining strategies based on the input\ncontext, facilitating the extraction of richer and more discriminative semantic\nfeatures. In addition, we introduce a \\textit{self-refinement mechanism}, which\niteratively verifies and enhances the quality of generated modalities through\ninternal feedback. Experimental results show that our method reduces FID for\nmissing image prediction by at least 14% and MER for missing text prediction by\nat least 10% compared to baselines.", "authors": ["Guanzhou Ke", "Yi Xie", "Xiaoli Wang", "Guoqing Chao", "Bo Wang", "Shengfeng He"], "published_date": "2025-06-04", "title_zh": "使用基礎模型預測缺失模態的現狀評估", "summary_zh": "多模態基礎模型展現了卓越能力，但其在缺失模態預測中的即插即用潛力尚未充分開發。本文將現有方法歸納為三種範式，包含42種模型變體，並從預測準確性和下游任務適應性進行全面評估。分析表明，現有模型在細粒度語義提取和生成模態的穩健驗證方面存在不足，導致預測效果欠佳。為了解決這些問題，本文提出了一種針對缺失模態預測的代理框架，該框架可基於輸入上下文動態制定模態感知挖掘策略，以提取更豐富的語義特徵。此外，引入了一種自我完善機制，通過內部反饋迭代驗證和提高生成模態的質量。實驗結果表明，與基準模型相比，本文方法在缺失圖像預測的FID指標上至少降低了14%，在缺失文本預測的MER指標上至少降低了10%。", "audio": "audios/2506.03530v1.mp3", "timestamp": "2025-06-05T08:25:55.366213"}
{"query": "Diffusion Model", "id": "2506.03981v1", "url": "http://arxiv.org/abs/2506.03981v1", "title": "Beyond water limitation in vegetation-autotoxicity patterning: a cross-diffusion model", "summary": "Many mathematical models describing vegetation patterns are based on\nbiomass--water interactions, due to the impact of this limited resource in arid\nand semi-arid environments. However, in recent years, a novel biological factor\ncalled autotoxicity has proved to play a key role in vegetation spatiotemporal\ndynamics, particularly by inhibiting biomass growth and increasing its natural\nmortality rate. In a standard reaction-diffusion framework, biomass-toxicity\ndynamics alone are unable to support the emergence of stable spatial patterns.\nIn this paper, we derive a cross-diffusion model for biomass and toxicity\ndynamics as the fast-reaction limit of a three-species system involving\ndichotomy and different time scales. Within this general framework, in addition\nto growth inhibition and extra-mortality already considered in previous\nstudies, the additional effect of ''propagation reduction'' induced by\nautotoxicity on vegetation dynamics is obtained. By combining linearised\nanalysis, simulations, and continuation, we investigate the formation of\nspatial patterns. Thanks to the cross-diffusion term, for the first time, a\nspatial model based solely on biomass-toxicity feedback without explicit water\ndynamics supports the formation of stable (Turing) vegetation patterns for a\nwide range of parameter values.", "authors": ["Francesco Giannino", "Annalisa Iuorio", "Cinzia Soresina"], "published_date": "2025-06-04", "title_zh": "植被自毒模式中超越水分限制：一個交叉擴散模型", "summary_zh": "植被模式的數學模型多基於生物量與水的交互作用，因乾旱和半乾旱環境中水資源有限。近年來，自毒作用被證實在植被時空動態中扮演重要角色，特別是透過抑制生物量生長和增加自然死亡率。在標準反應擴散框架中，單獨的生物量-毒性動態無法支持穩定空間模式的出現。本文推導出生物量和毒性動態的交叉擴散模型，作為涉及二分法和不同時間尺度的三物種系統的快速反應極限。在這個框架下，除了先前研究中已考慮的生長抑制和額外死亡率外，還獲得了自毒作用對植被動態引起的「傳播減少」效應。透過線性化分析、模擬和延拓，我們研究空間模式的形成。由於交叉擴散項，一個僅基於生物量-毒性反饋且沒有明確水分動態的空間模型，首次在廣泛的參數值範圍內支持穩定（圖靈）植被模式的形成。", "audio": "audios/2506.03981v1.mp3", "timestamp": "2025-06-05T08:26:02.157457"}
{"query": "AI", "id": "2506.04050v1", "url": "http://arxiv.org/abs/2506.04050v1", "title": "Explainability-Based Token Replacement on LLM-Generated Text", "summary": "Generative models, especially large language models (LLMs), have shown\nremarkable progress in producing text that appears human-like. However, they\noften exhibit patterns that make their output easier to detect than text\nwritten by humans. In this paper, we investigate how explainable AI (XAI)\nmethods can be used to reduce the detectability of AI-generated text (AIGT)\nwhile also introducing a robust ensemble-based detection approach. We begin by\ntraining an ensemble classifier to distinguish AIGT from human-written text,\nthen apply SHAP and LIME to identify tokens that most strongly influence its\npredictions. We propose four explainability-based token replacement strategies\nto modify these influential tokens. Our findings show that these token\nreplacement approaches can significantly diminish a single classifier's ability\nto detect AIGT. However, our ensemble classifier maintains strong performance\nacross multiple languages and domains, showing that a multi-model approach can\nmitigate the impact of token-level manipulations. These results show that XAI\nmethods can make AIGT harder to detect by focusing on the most influential\ntokens. At the same time, they highlight the need for robust, ensemble-based\ndetection strategies that can adapt to evolving approaches for hiding AIGT.", "authors": ["Hadi Mohammadi", "Anastasia Giachanou", "Daniel L. Oberski", "Ayoub Bagheri"], "published_date": "2025-06-04", "title_zh": "基於可解釋性的LLM生成文本之令牌替換", "summary_zh": "生成模型，特別是大型語言模型，在產生擬真文本方面表現卓越。然而，其產出常帶有易於辨識的模式。本文探討如何運用可解釋人工智慧（XAI）降低AI生成文本（AIGT）的可偵測性，並提出一種穩健的集成檢測方法。首先，訓練一個集成分類器區分AIGT和人類文本，然後利用SHAP和LIME識別影響預測的關鍵詞。我們提出四種基於可解釋性的詞替換策略，以修改這些關鍵詞。研究結果顯示，這些方法能顯著削弱單一分類器偵測AIGT的能力。然而，我們的集成分類器在多種語言和領域中仍保持強勁性能，表明多模型方法能減輕詞級操縱的影響。這些結果表明，XAI方法能透過關注最具影響力的詞彙，使AIGT更難被偵測。同時，亦突顯了對穩健、集成式檢測策略的需求，以適應不斷演變的AIGT隱藏技術。", "audio": "audios/2506.04050v1.mp3", "timestamp": "2025-06-05T09:20:29.307154"}
{"query": "Foundation Model", "id": "2506.03516v1", "url": "http://arxiv.org/abs/2506.03516v1", "title": "SemNav: A Model-Based Planner for Zero-Shot Object Goal Navigation Using Vision-Foundation Models", "summary": "Object goal navigation is a fundamental task in embodied AI, where an agent\nis instructed to locate a target object in an unexplored environment.\nTraditional learning-based methods rely heavily on large-scale annotated data\nor require extensive interaction with the environment in a reinforcement\nlearning setting, often failing to generalize to novel environments and\nlimiting scalability. To overcome these challenges, we explore a zero-shot\nsetting where the agent operates without task-specific training, enabling more\nscalable and adaptable solution. Recent advances in Vision Foundation Models\n(VFMs) offer powerful capabilities for visual understanding and reasoning,\nmaking them ideal for agents to comprehend scenes, identify relevant regions,\nand infer the likely locations of objects. In this work, we present a zero-shot\nobject goal navigation framework that integrates the perceptual strength of\nVFMs with a model-based planner that is capable of long-horizon decision making\nthrough frontier exploration. We evaluate our approach on the HM3D dataset\nusing the Habitat simulator and demonstrate that our method achieves\nstate-of-the-art performance in terms of success weighted by path length for\nzero-shot object goal navigation.", "authors": ["Arnab Debnath", "Gregory J. Stein", "Jana Kosecka"], "published_date": "2025-06-04", "title_zh": "SemNav：基於視覺基礎模型的零樣本物件目標導航模型規劃器", "summary_zh": "物件目標導航是具身人工智慧的基礎任務，旨在讓智能體在未探索環境中定位目標物件。傳統學習方法依賴大量標註數據或強化學習環境下的廣泛互動，難以推廣至新環境且限制可擴展性。為了解決這些問題，本文探索零樣本情境，智能體無需特定任務訓練即可運作，實現更具擴展性和適應性的解決方案。視覺基礎模型(VFMs)的最新進展為視覺理解和推理提供了強大能力，使其成為智能體理解場景、識別相關區域和推斷物件可能位置的理想選擇。本文提出一種零樣本物件目標導航框架，將VFMs的感知能力與基於模型的規劃器整合，透過邊界探索實現長程決策。在Habitat模擬器的HM3D數據集上評估表明，該方法在零樣本物件目標導航的成功率加權路徑長度方面達到最佳性能。", "audio": "audios/2506.03516v1.mp3", "timestamp": "2025-06-05T09:20:35.728652"}
{"query": "Diffusion Model", "id": "2506.03979v1", "url": "http://arxiv.org/abs/2506.03979v1", "title": "Solving Inverse Problems via Diffusion-Based Priors: An Approximation-Free Ensemble Sampling Approach", "summary": "Diffusion models (DMs) have proven to be effective in modeling\nhigh-dimensional distributions, leading to their widespread adoption for\nrepresenting complex priors in Bayesian inverse problems (BIPs). However,\ncurrent DM-based posterior sampling methods proposed for solving common BIPs\nrely on heuristic approximations to the generative process. To exploit the\ngenerative capability of DMs and avoid the usage of such approximations, we\npropose an ensemble-based algorithm that performs posterior sampling without\nthe use of heuristic approximations. Our algorithm is motivated by existing\nworks that combine DM-based methods with the sequential Monte Carlo (SMC)\nmethod. By examining how the prior evolves through the diffusion process\nencoded by the pre-trained score function, we derive a modified partial\ndifferential equation (PDE) governing the evolution of the corresponding\nposterior distribution. This PDE includes a modified diffusion term and a\nreweighting term, which can be simulated via stochastic weighted particle\nmethods. Theoretically, we prove that the error between the true posterior\ndistribution can be bounded in terms of the training error of the pre-trained\nscore function and the number of particles in the ensemble. Empirically, we\nvalidate our algorithm on several inverse problems in imaging to show that our\nmethod gives more accurate reconstructions compared to existing DM-based\nmethods.", "authors": ["Haoxuan Chen", "Yinuo Ren", "Martin Renqiang Min", "Lexing Ying", "Zachary Izzo"], "published_date": "2025-06-04", "title_zh": "基於擴散先驗求解逆問題：一種無近似的集成抽樣方法", "summary_zh": "擴散模型善於模擬高維分佈，廣泛應用於貝氏反問題以表示複雜先驗。現有基於擴散模型的後驗抽樣方法常仰賴生成過程的啟發式近似。為充分利用擴散模型的生成能力，並避免此類近似，我們提出一種基於集成學習的演算法，無需啟發式近似即可進行後驗抽樣。此演算法受結合擴散模型與循序蒙地卡羅法的研究啟發。透過檢視先驗如何經由預訓練分數函數編碼的擴散過程演進，我們推導出控制相應後驗分佈演化的修正偏微分方程式，包含修正的擴散項和重新加權項，可透過隨機加權粒子方法模擬。理論上，我們證明了真實後驗分佈的誤差可基於預訓練分數函數的訓練誤差和集成中的粒子數量進行界定。實驗上，我們驗證了此演算法在影像反問題中的有效性，結果顯示相較於現有基於擴散模型的方法，我們的重建更精確。", "audio": "audios/2506.03979v1.mp3", "timestamp": "2025-06-05T09:20:42.488543"}
{"query": "AI", "id": "2506.04038v1", "url": "http://arxiv.org/abs/2506.04038v1", "title": "Generating Automotive Code: Large Language Models for Software Development and Verification in Safety-Critical Systems", "summary": "Developing safety-critical automotive software presents significant\nchallenges due to increasing system complexity and strict regulatory demands.\nThis paper proposes a novel framework integrating Generative Artificial\nIntelligence (GenAI) into the Software Development Lifecycle (SDLC). The\nframework uses Large Language Models (LLMs) to automate code generation in\nlanguages such as C++, incorporating safety-focused practices such as static\nverification, test-driven development and iterative refinement. A\nfeedback-driven pipeline ensures the integration of test, simulation and\nverification for compliance with safety standards. The framework is validated\nthrough the development of an Adaptive Cruise Control (ACC) system. Comparative\nbenchmarking of LLMs ensures optimal model selection for accuracy and\nreliability. Results demonstrate that the framework enables automatic code\ngeneration while ensuring compliance with safety-critical requirements,\nsystematically integrating GenAI into automotive software engineering. This\nwork advances the use of AI in safety-critical domains, bridging the gap\nbetween state-of-the-art generative models and real-world safety requirements.", "authors": ["Sven Kirchner", "Alois C. Knoll"], "published_date": "2025-06-04", "title_zh": "汽車代碼生成：大型語言模型在安全攸關系統中用於軟體開發與驗證", "summary_zh": "汽車安全關鍵軟體開發因系統複雜性增加和嚴格法規要求而面臨挑戰。本文提出一個新穎框架，將生成式人工智慧（GenAI）整合至軟體開發生命週期（SDLC）中。該框架利用大型語言模型（LLMs）自動產生C++等語言的程式碼，並納入靜態驗證、測試驅動開發和迭代完善等安全導向實踐。回饋驅動流程確保測試、模擬和驗證的整合，以符合安全標準。透過開發自適應巡航控制（ACC）系統驗證該框架。對LLMs進行比較基準測試，確保選擇最佳模型以提高準確性和可靠性。結果表明，該框架能夠自動生成程式碼，同時確保符合安全關鍵需求，系統地將GenAI整合到汽車軟體工程中。此研究推進了人工智慧在安全關鍵領域的應用，彌合了最先進的生成模型與實際安全需求之間的差距。", "audio": "audios/2506.04038v1.mp3", "timestamp": "2025-06-05T10:21:03.190227"}
{"query": "Foundation Model", "id": "2506.03433v1", "url": "http://arxiv.org/abs/2506.03433v1", "title": "ViT-Split: Unleashing the Power of Vision Foundation Models via Efficient Splitting Heads", "summary": "Vision foundation models (VFMs) have demonstrated remarkable performance\nacross a wide range of downstream tasks. While several VFM adapters have shown\npromising results by leveraging the prior knowledge of VFMs, we identify two\ninefficiencies in these approaches. First, the interaction between\nconvolutional neural network (CNN) and VFM backbone triggers early layer\ngradient backpropagation. Second, existing methods require tuning all\ncomponents, adding complexity. Besides, these adapters alter VFM features,\nunderutilizing the prior knowledge. To tackle these challenges, we propose a\nnew approach called ViT-Split, based on a key observation: the layers of\nseveral VFMs, like DINOv2, can be divided into two distinct components: an\nextractor for learning low-level features and an adapter for learning\ntask-specific features. Leveraging this insight, we eliminate the CNN branch\nand introduce two heads, task head and prior head, to the frozen VFM. The task\nhead is designed to learn task-specific features, mitigating the early gradient\npropagation issue. The prior head is used to leverage the multi-scale prior\nfeatures from the frozen VFM, reducing tuning parameters and overfitting.\nExtensive experiments on various tasks (e.g., segmentation, detection, depth\nestimation, and visual question answering) validate the effectiveness and\nefficiency of ViT-Split. Specifically, ViT-Split reduces training time up to\n$4\\times$ while achieving comparable or even better results on ADE20K, compared\nto other VFM adapters.", "authors": ["Yifan Li", "Xin Li", "Tianqin Li", "Wenbin He", "Yu Kong", "Liu Ren"], "published_date": "2025-06-03", "title_zh": "ViT-分割：透過高效分割頭釋放視覺基礎模型的力量", "summary_zh": "視覺基礎模型(VFMs)在多項下游任務中表現出色。現有VFMs適配器利用VFMs的先驗知識，但存在效率問題：CNN與VFM主幹的互動觸發早期層梯度反向傳播，且需調整所有組件，增加複雜性並改變VFM特徵。為解決這些問題，我們提出ViT-Split，基於DINOv2等VFMs的層可分為提取低層特徵的提取器和學習特定任務特徵的適配器的觀察。ViT-Split消除CNN分支，並引入任務頭和先驗頭至凍結的VFM。任務頭學習特定任務特徵，緩解早期梯度傳播問題；先驗頭利用VFM的多尺度先驗特徵，減少調整參數和過擬合。在分割、檢測、深度估計和視覺問答等任務上的實驗驗證了ViT-Split的有效性和效率。相較於其他VFMs適配器，ViT-Split在ADE20K上實現了相當甚至更好的結果，並減少了高達4倍的訓練時間。", "audio": "audios/2506.03433v1.mp3", "timestamp": "2025-06-05T10:21:09.599223"}
{"query": "Diffusion Model", "id": "2506.03933v1", "url": "http://arxiv.org/abs/2506.03933v1", "title": "DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models", "summary": "Vision Language Models (VLMs) have shown remarkable capabilities in\nmultimodal understanding, yet their susceptibility to perturbations poses a\nsignificant threat to their reliability in real-world applications. Despite\noften being imperceptible to humans, these perturbations can drastically alter\nmodel outputs, leading to erroneous interpretations and decisions. This paper\nintroduces DiffCAP, a novel diffusion-based purification strategy that can\neffectively neutralize adversarial corruptions in VLMs. We observe that adding\nminimal noise to an adversarially corrupted image significantly alters its\nlatent embedding with respect to VLMs. Building on this insight, DiffCAP\ncumulatively injects random Gaussian noise into adversarially perturbed input\ndata. This process continues until the embeddings of two consecutive noisy\nimages reach a predefined similarity threshold, indicating a potential approach\nto neutralize the adversarial effect. Subsequently, a pretrained diffusion\nmodel is employed to denoise the stabilized image, recovering a clean\nrepresentation suitable for the VLMs to produce an output. Through extensive\nexperiments across six datasets with three VLMs under varying attack strengths\nin three task scenarios, we show that DiffCAP consistently outperforms existing\ndefense techniques by a substantial margin. Notably, DiffCAP significantly\nreduces both hyperparameter tuning complexity and the required diffusion time,\nthereby accelerating the denoising process. Equipped with strong theoretical\nand empirical support, DiffCAP provides a robust and practical solution for\nsecurely deploying VLMs in adversarial environments.", "authors": ["Jia Fu", "Yongtao Wu", "Yihang Chen", "Kunyu Peng", "Xiao Zhang", "Volkan Cevher", "Sepideh Pashami", "Anders Holst"], "published_date": "2025-06-04", "title_zh": "DiffCAP：基於擴散的視覺語言模型累積對抗淨化", "summary_zh": "視覺語言模型(VLM)在多模態理解方面表現卓越，但其對擾動的敏感性對現實應用可靠性構成威脅。即使擾動對人眼難以察覺，也可能顯著改變模型輸出，導致錯誤判讀與決策。本研究提出DiffCAP，一種基於擴散的新型淨化策略，能有效中和VLM中的對抗性破壞。研究發現，對抗性破壞圖像加入極小噪聲會顯著改變其VLM潛在嵌入。DiffCAP基於此，累積性地將高斯噪聲注入對抗性擾動輸入資料，直至兩個連續噪聲圖像的嵌入達到預定義相似度閾值，代表可能已中和對抗性效應。隨後，使用預訓練擴散模型對穩定圖像去噪，恢復乾淨的表徵，供VLM產生輸出。透過在六個資料集、三個VLM上，以不同攻擊強度在三個任務場景進行的大量實驗表明，DiffCAP始終大幅超越現有防禦技術，並顯著降低了超參數調整複雜性與所需擴散時間，加速去噪過程。在強大的理論與經驗支持下，DiffCAP為在對抗性環境中安全部署VLM提供了一種穩健且實用的解決方案。", "audio": "audios/2506.03933v1.mp3", "timestamp": "2025-06-05T10:21:17.608496"}
{"query": "AI", "id": "2506.04032v1", "url": "http://arxiv.org/abs/2506.04032v1", "title": "AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data", "summary": "Background: We present a Patient Simulator that leverages real world patient\nencounters which cover a broad range of conditions and symptoms to provide\nsynthetic test subjects for development and testing of healthcare agentic\nmodels. The simulator provides a realistic approach to patient presentation and\nmulti-turn conversation with a symptom-checking agent. Objectives: (1) To\nconstruct and instantiate a Patient Simulator to train and test an AI health\nagent, based on patient vignettes derived from real EHR data. (2) To test the\nvalidity and alignment of the simulated encounters provided by the Patient\nSimulator to expert human clinical providers. (3) To illustrate the evaluation\nframework of such an LLM system on the generated realistic, data-driven\nsimulations -- yielding a preliminary assessment of our proposed system.\nMethods: We first constructed realistic clinical scenarios by deriving patient\nvignettes from real-world EHR encounters. These vignettes cover a variety of\npresenting symptoms and underlying conditions. We then evaluate the performance\nof the Patient Simulator as a simulacrum of a real patient encounter across\nover 500 different patient vignettes. We leveraged a separate AI agent to\nprovide multi-turn questions to obtain a history of present illness. The\nresulting multiturn conversations were evaluated by two expert clinicians.\nResults: Clinicians scored the Patient Simulator as consistent with the patient\nvignettes in those same 97.7% of cases. The extracted case summary based on the\nconversation history was 99% relevant. Conclusions: We developed a methodology\nto incorporate vignettes derived from real healthcare patient data to build a\nsimulation of patient responses to symptom checking agents. The performance and\nalignment of this Patient Simulator could be used to train and test a\nmulti-turn conversational AI agent at scale.", "authors": ["Sina Rashidian", "Nan Li", "Jonathan Amar", "Jong Ha Lee", "Sam Pugh", "Eric Yang", "Geoff Masterson", "Myoung Cha", "Yugang Jia", "Akhil Vaid"], "published_date": "2025-06-04", "title_zh": "用於對話式病人分流的人工智慧代理：基於真實世界EHR數據的初步模擬評估", "summary_zh": "本研究展示一套病人模擬器，利用真實病患案例，涵蓋多種病症與症狀，為醫療AI模型的開發與測試提供合成測試對象。此模擬器提供逼真的病人呈現方式，以及與症狀檢查AI代理的多輪對話。目標為：一、建構基於真實電子病歷資料的病人情境，以訓練和測試AI醫療代理；二、驗證模擬遭遇與專家臨床醫護人員的一致性；三、展示在此類大型語言模型系統上，以資料驅動模擬進行評估的框架，並初步評估本系統。方法為：首先，從真實電子病歷案例衍生出病人情境，涵蓋多種表現症狀與潛在病症。接著，在超過500種病人情境下，評估病人模擬器作為真實病人遭遇的模擬效果。利用獨立的AI代理進行多輪提問，以獲取現病史。最後，由兩位專家醫師評估多輪對話。結果顯示，醫師評估病人模擬器在97.7%的案例中與病人情境一致，且基於對話紀錄提取的案例摘要具有99%的相關性。結論為：我們開發了一種方法，將真實醫療病患資料衍生出的情境納入，以建立症狀檢查代理的病人反應模擬。此病人模擬器的效能與一致性可用於大規模訓練和測試多輪對話AI代理。", "audio": "audios/2506.04032v1.mp3", "timestamp": "2025-06-05T11:16:55.259750"}
{"query": "Foundation Model", "id": "2506.03373v1", "url": "http://arxiv.org/abs/2506.03373v1", "title": "A Foundation Model for Spatial Proteomics", "summary": "Foundation models have begun to transform image analysis by acting as\npretrained generalist backbones that can be adapted to many tasks even when\npost-training data are limited, yet their impact on spatial proteomics, imaging\nthat maps proteins at single-cell resolution, remains limited. Here, we\nintroduce KRONOS, a foundation model built for spatial proteomics. KRONOS was\ntrained in a self-supervised manner on over 47 million image patches covering\n175 protein markers, 16 tissue types, and 8 fluorescence-based imaging\nplatforms. We introduce key architectural adaptations to address the\nhigh-dimensional, multi-channel, and heterogeneous nature of multiplex imaging.\nWe demonstrate that KRONOS learns biologically meaningful representations\nacross multiple scales, ranging from cellular and microenvironment to tissue\nlevels, enabling it to address diverse downstream tasks, including cell\nphenotyping, region classification, and patient stratification. Evaluated\nacross 11 independent cohorts, KRONOS achieves state-of-the-art performance\nacross cell phenotyping, treatment response prediction, and retrieval tasks,\nand is highly data-efficient. KRONOS also introduces the paradigm of\nsegmentation-free patch-level processing for efficient and scalable spatial\nproteomics analysis, allowing cross-institutional comparisons, and as an image\nreverse search engine for spatial patterns. Together, these results position\nKRONOS as a flexible and scalable tool for spatial proteomics. The model is\npublicly accessible at https://github.com/mahmoodlab/KRONOS.", "authors": ["Muhammad Shaban", "Yuzhou Chang", "Huaying Qiu", "Yao Yu Yeo", "Andrew H. Song", "Guillaume Jaume", "Yuchen Wang", "Luca L. Weishaupt", "Tong Ding", "Anurag Vaidya", "Abdallah Lamane", "Daniel Shao", "Mohammed Zidane", "Yunhao Bai", "Paige McCallum", "Shuli Luo", "Wenrui Wu", "Yang Wang", "Precious Cramer", "Chi Ngai Chan", "Pierre Stephan", "Johanna Schaffenrath", "Jia Le Lee", "Hendrik A. Michel", "Caiwei Tian", "Cristina Almagro-Perez", "Sophia J. Wagner", "Sharifa Sahai", "Ming Y. Lu", "Richard J. Chen", "Andrew Zhang", "Mark Edward M. Gonzales", "Ahmad Makky", "Jia-Ying Joey Lee", "Hao Cheng", "Nourhan El Ahmar", "Sayed Matar", "Maximilian Haist", "Darci Phillips", "Yuqi Tan", "Garry P. Nolan", "W. Richard Burack", "Jacob D. Estes", "Jonathan T. C. Liu", "Toni K Choueiri", "Neeraj Agarwal", "Marc Barry", "Scott J. Rodig", "Long Phi Le", "Georg Gerber", "Christian M. Schürch", "Fabian J. Theis", "Youn H Kim", "Joe Yeong", "Sabina Signoretti", "Brooke E. Howitt", "Lit-Hsin Loo", "Qin Ma", "Sizun Jiang", "Faisal Mahmood"], "published_date": "2025-06-03", "title_zh": "空間蛋白質組學之基礎模型", "summary_zh": "基於空間蛋白質體學的基礎模型KRONOS，透過自監督學習在包含175種蛋白標記、16種組織類型和8種螢光成像平台的四千七百萬張圖像補丁上進行訓練。針對多重成像的高維度、多通道和異質性，KRONOS引入了關鍵架構調整，學習了從細胞、微環境到組織層面的生物學意義表徵。在11個獨立隊列的評估中，KRONOS在細胞表型分析、治療反應預測和檢索任務中均達到頂尖效能，並具備高效的數據利用率。KRONOS引入了無分割的補丁級處理範例，實現高效且可擴展的空間蛋白質體學分析，促進跨機構比較，並作為空間模式的圖像反向搜尋引擎。KRONOS為空間蛋白質體學提供了一個靈活且可擴展的工具。", "audio": "audios/2506.03373v1.mp3", "timestamp": "2025-06-05T11:17:02.207031"}
{"query": "Diffusion Model", "id": "2506.03804v1", "url": "http://arxiv.org/abs/2506.03804v1", "title": "Personalized MR-Informed Diffusion Models for 3D PET Image Reconstruction", "summary": "Recent work has shown improved lesion detectability and flexibility to\nreconstruction hyperparameters (e.g. scanner geometry or dose level) when PET\nimages are reconstructed by leveraging pre-trained diffusion models. Such\nmethods train a diffusion model (without sinogram data) on high-quality, but\nstill noisy, PET images. In this work, we propose a simple method for\ngenerating subject-specific PET images from a dataset of multi-subject PET-MR\nscans, synthesizing \"pseudo-PET\" images by transforming between different\npatients' anatomy using image registration. The images we synthesize retain\ninformation from the subject's MR scan, leading to higher resolution and the\nretention of anatomical features compared to the original set of PET images.\nWith simulated and real [$^{18}$F]FDG datasets, we show that pre-training a\npersonalized diffusion model with subject-specific \"pseudo-PET\" images improves\nreconstruction accuracy with low-count data. In particular, the method shows\npromise in combining information from a guidance MR scan without overly\nimposing anatomical features, demonstrating an improved trade-off between\nreconstructing PET-unique image features versus features present in both PET\nand MR. We believe this approach for generating and utilizing synthetic data\nhas further applications to medical imaging tasks, particularly because\npatient-specific PET images can be generated without resorting to generative\ndeep learning or large training datasets.", "authors": ["George Webber", "Alexander Hammers", "Andrew P. King", "Andrew J. Reader"], "published_date": "2025-06-04", "title_zh": "用於三維PET影像重建的個人化MR資訊擴散模型", "summary_zh": "近期研究顯示，利用預訓練擴散模型重建PET影像可提升病灶檢測能力，並對重建超參數（如掃描器幾何或劑量水平）更具彈性。這些方法在高品質但仍具雜訊的PET影像上訓練擴散模型（不使用正弦圖資料）。本研究提出一種簡便方法，從多受試者PET-MR掃描數據集中生成受試者特定PET影像，透過影像配準在不同患者的解剖結構之間轉換，合成「偽PET」影像。與原始PET影像相比，合成影像保留了受試者MR掃描的資訊，從而提高了分辨率並保留了解剖特徵。使用模擬和真實[$^{18}$F]FDG數據集，我們證明了使用受試者特定「偽PET」影像預訓練個性化擴散模型可提高低計數數據的重建準確性。該方法在結合引導MR掃描的資訊方面展現了前景，且不會過度強加解剖特徵，從而在重建PET獨有影像特徵與PET和MR中均存在的特徵之間取得了更好的平衡。我們認為這種生成和利用合成數據的方法在醫學影像任務中具有更廣泛的應用，特別是因為無需依賴生成式深度學習或大型訓練數據集即可生成患者特定PET影像。", "audio": "audios/2506.03804v1.mp3", "timestamp": "2025-06-05T11:17:09.584146"}
{"query": "AI", "id": "2506.04018v1", "url": "http://arxiv.org/abs/2506.04018v1", "title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents", "summary": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. Prior work has examined agents' ability to enact\nmisaligned behaviour (misalignment capability) and their compliance with\nharmful instructions (misuse propensity). However, the likelihood of agents\nattempting misaligned behaviours in real-world settings (misalignment\npropensity) remains poorly understood. We introduce a misalignment propensity\nbenchmark, AgentMisalignment, consisting of a suite of realistic scenarios in\nwhich LLM agents have the opportunity to display misaligned behaviour. We\norganise our evaluations into subcategories of misaligned behaviours, including\ngoal-guarding, resisting shutdown, sandbagging, and power-seeking. We report\nthe performance of frontier models on our benchmark, observing higher\nmisalignment on average when evaluating more capable models. Finally, we\nsystematically vary agent personalities through different system prompts. We\nfind that persona characteristics can dramatically and unpredictably influence\nmisalignment tendencies -- occasionally far more than the choice of model\nitself -- highlighting the importance of careful system prompt engineering for\ndeployed AI agents. Our work highlights the failure of current alignment\nmethods to generalise to LLM agents, and underscores the need for further\npropensity evaluations as autonomous systems become more prevalent.", "authors": ["Akshat Naik", "Patrick Quinn", "Guillermo Bosch", "Emma Gouné", "Francisco Javier Campos Zabala", "Jason Ross Brown", "Edward James Young"], "published_date": "2025-06-04", "title_zh": "代理不對齊：衡量基於大型語言模型之代理中行為錯位之傾向", "summary_zh": "大型語言模型（LLM）代理日益普及，隨之而來的錯位風險也隨之增加。先前研究著重於代理執行錯位行為的能力（錯位能力）及其對有害指令的服從性（濫用傾向）。然而，代理在真實環境中嘗試錯位行為的可能性（錯位傾向）仍缺乏深入理解。本研究提出一項錯位傾向基準AgentMisalignment，包含一系列LLM代理有機會展現錯位行為的真實情境。評估分為多個錯位行為子類別，包括目標守護、抵抗關機、藏拙和權力尋求。研究報告了前沿模型在基準上的表現，觀察到在評估能力更強的模型時，平均錯位程度更高。最後，研究系統性地透過不同系統提示改變代理個性。結果顯示，人格特徵可能大幅且不可預測地影響錯位傾向，有時甚至遠超過模型本身的選擇，突顯了為部署的AI代理進行仔細的系統提示工程的重要性。本研究強調了當前對齊方法無法推廣至LLM代理，並強調隨著自主系統日益普及，進一步評估傾向的必要性。", "audio": "audios/2506.04018v1.mp3", "timestamp": "2025-06-05T12:38:48.803867"}
{"query": "Foundation Model", "id": "2506.03364v1", "url": "http://arxiv.org/abs/2506.03364v1", "title": "Towards Source Attribution of Singing Voice Deepfake with Multimodal Foundation Models", "summary": "In this work, we introduce the task of singing voice deepfake source\nattribution (SVDSA). We hypothesize that multimodal foundation models (MMFMs)\nsuch as ImageBind, LanguageBind will be most effective for SVDSA as they are\nbetter equipped for capturing subtle source-specific characteristics-such as\nunique timbre, pitch manipulation, or synthesis artifacts of each singing voice\ndeepfake source due to their cross-modality pre-training. Our experiments with\nMMFMs, speech foundation models and music foundation models verify the\nhypothesis that MMFMs are the most effective for SVDSA. Furthermore, inspired\nfrom related research, we also explore fusion of foundation models (FMs) for\nimproved SVDSA. To this end, we propose a novel framework, COFFE which employs\nChernoff Distance as novel loss function for effective fusion of FMs. Through\nCOFFE with the symphony of MMFMs, we attain the topmost performance in\ncomparison to all the individual FMs and baseline fusion methods.", "authors": ["Orchid Chetia Phukan", "Girish", "Mohd Mujtaba Akhtar", "Swarup Ranjan Behera", "Priyabrata Mallick", "Pailla Balakrishna Reddy", "Arun Balaji Buduru", "Rajesh Sharma"], "published_date": "2025-06-03", "title_zh": "基於多模態基礎模型的歌聲深度偽造溯源研究", "summary_zh": "本研究提出歌唱聲音深度偽造溯源任務。假設多模態基礎模型（MMFM），如ImageBind、LanguageBind，因其跨模態預訓練，能更有效地捕捉各深度偽造源細微特徵，如獨特音色、音高操控或合成瑕疵，故最適用於此任務。實驗驗證了MMFM在此任務上的有效性優於語音和音樂基礎模型。受相關研究啟發，本研究探索基礎模型融合以提升溯源效果，並提出COFFE框架，採用Chernoff距離作為新型損失函數，以實現有效的模型融合。COFFE框架結合多模態基礎模型，相較於單獨模型及基線融合方法，取得了最佳效能。", "audio": "audios/2506.03364v1.mp3", "timestamp": "2025-06-05T12:38:55.194118"}
{"query": "Diffusion Model", "id": "2506.03652v1", "url": "http://arxiv.org/abs/2506.03652v1", "title": "EmoArt: A Multidimensional Dataset for Emotion-Aware Artistic Generation", "summary": "With the rapid advancement of diffusion models, text-to-image generation has\nachieved significant progress in image resolution, detail fidelity, and\nsemantic alignment, particularly with models like Stable Diffusion 3.5, Stable\nDiffusion XL, and FLUX 1. However, generating emotionally expressive and\nabstract artistic images remains a major challenge, largely due to the lack of\nlarge-scale, fine-grained emotional datasets. To address this gap, we present\nthe EmoArt Dataset -- one of the most comprehensive emotion-annotated art\ndatasets to date. It contains 132,664 artworks across 56 painting styles (e.g.,\nImpressionism, Expressionism, Abstract Art), offering rich stylistic and\ncultural diversity. Each image includes structured annotations: objective scene\ndescriptions, five key visual attributes (brushwork, composition, color, line,\nlight), binary arousal-valence labels, twelve emotion categories, and potential\nart therapy effects. Using EmoArt, we systematically evaluate popular\ntext-to-image diffusion models for their ability to generate emotionally\naligned images from text. Our work provides essential data and benchmarks for\nemotion-driven image synthesis and aims to advance fields such as affective\ncomputing, multimodal learning, and computational art, enabling applications in\nart therapy and creative design. The dataset and more details can be accessed\nvia our project website.", "authors": ["Cheng Zhang", "Hongxia xie", "Bin Wen", "Songhan Zuo", "Ruoxuan Zhang", "Wen-huang Cheng"], "published_date": "2025-06-04", "title_zh": "情緒藝術：用於情緒感知藝術生成的多維數據集", "summary_zh": "隨著擴散模型快速發展，文本到圖像生成在圖像解析度、細節保真度和語義對齊方面取得了顯著進展。然而，生成情感豐富的抽象藝術圖像仍然是一項重大挑戰，主要原因是缺乏大規模、細粒度的情感數據集。為了解決這個問題，我們提出了 EmoArt 數據集，這是迄今為止最全面的情感註釋藝術數據集之一，包含 132,664 件藝術品，涵蓋 56 種繪畫風格，提供豐富的風格和文化多樣性。每張圖像都包含結構化註釋：客觀場景描述、五個關鍵視覺屬性、二元喚醒-效價標籤、十二個情感類別和潛在的藝術治療效果。利用 EmoArt，我們系統地評估了流行的文本到圖像擴散模型，以了解它們從文本生成情感對齊圖像的能力。這項研究為情感驅動的圖像合成提供了重要數據和基準，旨在推進情感計算、多模態學習和計算藝術等領域，實現藝術治療和創意設計等應用。", "audio": "audios/2506.03652v1.mp3", "timestamp": "2025-06-05T12:39:03.895550"}
{"query": "AI", "id": "2506.04005v1", "url": "http://arxiv.org/abs/2506.04005v1", "title": "Vocabulary-free few-shot learning for Vision-Language Models", "summary": "Recent advances in few-shot adaptation for Vision-Language Models (VLMs) have\ngreatly expanded their ability to generalize across tasks using only a few\nlabeled examples. However, existing approaches primarily build upon the strong\nzero-shot priors of these models by leveraging carefully designed,\ntask-specific prompts. This dependence on predefined class names can restrict\ntheir applicability, especially in scenarios where exact class names are\nunavailable or difficult to specify. To address this limitation, we introduce\nvocabulary-free few-shot learning for VLMs, a setting where target class\ninstances - that is, images - are available but their corresponding names are\nnot. We propose Similarity Mapping (SiM), a simple yet effective baseline that\nclassifies target instances solely based on similarity scores with a set of\ngeneric prompts (textual or visual), eliminating the need for carefully\nhandcrafted prompts. Although conceptually straightforward, SiM demonstrates\nstrong performance, operates with high computational efficiency (learning the\nmapping typically takes less than one second), and provides interpretability by\nlinking target classes to generic prompts. We believe that our approach could\nserve as an important baseline for future research in vocabulary-free few-shot\nlearning. Code is available at\nhttps://github.com/MaxZanella/vocabulary-free-FSL.", "authors": ["Maxime Zanella", "Clément Fuchs", "Ismail Ben Ayed", "Christophe De Vleeschouwer"], "published_date": "2025-06-04", "title_zh": "免詞彙視覺語言模型少樣本學習", "summary_zh": "視覺語言模型（VLMs）少樣本適應的最新進展大幅提升其泛化能力，僅需少量標記樣本即可適應不同任務。然而，現有方法主要依賴模型強大的零樣本先驗知識，並仰賴精心設計的、針對特定任務的提示詞。這種對預定義類別名稱的依賴限制了其應用性，尤其是在無法提供或難以指定確切類別名稱的情況下。為了解決這個問題，我們引入了VLMs的無詞彙少樣本學習，即僅提供目標類別實例（圖像），而不提供其對應名稱。我們提出相似度映射（SiM），一種簡單而有效的基準方法，僅基於與一組通用提示詞（文本或視覺）的相似度分數對目標實例進行分類，無需手動設計提示詞。儘管概念簡單，SiM表現出優異性能，計算效率高（學習映射通常不到一秒），並通過將目標類別與通用提示詞關聯提供可解釋性。我們相信此方法可作為未來無詞彙少樣本學習研究的重要基準。", "audio": "audios/2506.04005v1.mp3", "timestamp": "2025-06-05T13:32:03.405567"}
{"query": "Foundation Model", "id": "2506.03360v1", "url": "http://arxiv.org/abs/2506.03360v1", "title": "A Multimodal, Multilingual, and Multidimensional Pipeline for Fine-grained Crowdsourcing Earthquake Damage Evaluation", "summary": "Rapid, fine-grained disaster damage assessment is essential for effective\nemergency response, yet remains challenging due to limited ground sensors and\ndelays in official reporting. Social media provides a rich, real-time source of\nhuman-centric observations, but its multimodal and unstructured nature presents\nchallenges for traditional analytical methods. In this study, we propose a\nstructured Multimodal, Multilingual, and Multidimensional (3M) pipeline that\nleverages multimodal large language models (MLLMs) to assess disaster impacts.\nWe evaluate three foundation models across two major earthquake events using\nboth macro- and micro-level analyses. Results show that MLLMs effectively\nintegrate image-text signals and demonstrate a strong correlation with\nground-truth seismic data. However, performance varies with language,\nepicentral distance, and input modality. This work highlights the potential of\nMLLMs for disaster assessment and provides a foundation for future research in\napplying MLLMs to real-time crisis contexts. The code and data are released at:\nhttps://github.com/missa7481/EMNLP25_earthquake", "authors": ["Zihui Ma", "Lingyao Li", "Juan Li", "Wenyue Hua", "Jingxiao Liu", "Qingyuan Feng", "Yuki Miura"], "published_date": "2025-06-03", "title_zh": "用於細粒度群眾外包地震損害評估的多模態、多語言及多維度管線", "summary_zh": "快速精確的災害評估對應急響應至關重要，但因地面感測器不足和官方報告延遲而具挑戰。社群媒體提供豐富即時的人本觀測資料，但其多模態和非結構化特性對傳統分析方法構成挑戰。本研究提出一個結構化的多模態、多語言和多維度 (3M) 流程，利用多模態大型語言模型 (MLLM) 評估災害影響。我們使用巨觀和微觀層面分析，評估了兩個主要地震事件中的三個基礎模型。結果表明，MLLM 有效整合圖像文本信號，並與地面真實地震數據高度相關。然而，性能隨語言、震央距離和輸入模態而異。這項工作突顯了 MLLM 在災害評估中的潛力，並為未來將 MLLM 應用於實時危機環境的研究奠定了基礎。代碼和數據已發布於 https://github.com/missa7481/EMNLP25_earthquake。", "audio": "audios/2506.03360v1.mp3", "timestamp": "2025-06-05T13:32:12.374093"}
{"query": "Diffusion Model", "id": "2506.03517v1", "url": "http://arxiv.org/abs/2506.03517v1", "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models", "summary": "Direct Preference Optimization (DPO) has recently been applied as a\npost-training technique for text-to-video diffusion models. To obtain training\ndata, annotators are asked to provide preferences between two videos generated\nfrom independent noise. However, this approach prohibits fine-grained\ncomparisons, and we point out that it biases the annotators towards low-motion\nclips as they often contain fewer visual artifacts. In this work, we introduce\nDenseDPO, a method that addresses these shortcomings by making three\ncontributions. First, we create each video pair for DPO by denoising corrupted\ncopies of a ground truth video. This results in aligned pairs with similar\nmotion structures while differing in local details, effectively neutralizing\nthe motion bias. Second, we leverage the resulting temporal alignment to label\npreferences on short segments rather than entire clips, yielding a denser and\nmore precise learning signal. With only one-third of the labeled data, DenseDPO\ngreatly improves motion generation over vanilla DPO, while matching it in text\nalignment, visual quality, and temporal consistency. Finally, we show that\nDenseDPO unlocks automatic preference annotation using off-the-shelf Vision\nLanguage Models (VLMs): GPT accurately predicts segment-level preferences\nsimilar to task-specifically fine-tuned video reward models, and DenseDPO\ntrained on these labels achieves performance close to using human labels.", "authors": ["Ziyi Wu", "Anil Kag", "Ivan Skorokhodov", "Willi Menapace", "Ashkan Mirzaei", "Igor Gilitschenski", "Sergey Tulyakov", "Aliaksandr Siarohin"], "published_date": "2025-06-04", "title_zh": "DenseDPO：用於影片擴散模型的細粒度時間偏好優化", "summary_zh": "直接偏好優化(DPO)近期被應用於文生影片擴散模型的後訓練技術。傳統DPO透過比較獨立雜訊生成的兩段影片來收集訓練資料，此方法限制了細緻比較，且傾向於低運動片段，因其視覺瑕疵較少。本研究提出DenseDPO，透過三項貢獻解決此問題。首先，DenseDPO使用真實影片的損毀副本進行去噪，產生對齊的影片對，具有相似的運動結構但局部細節不同，從而消除運動偏差。其次，利用時間對齊，對短片段而非整個影片進行偏好標記，產生更密集精確的學習訊號。僅用三分之一的標記資料，DenseDPO在運動生成方面顯著優於傳統DPO，同時在文字對齊、視覺品質和時間一致性方面與之相當。最後，研究表明DenseDPO能利用現成的視覺語言模型(VLM)進行自動偏好標記：GPT能準確預測片段級偏好，效果媲美經特定任務微調的影片獎勵模型，且基於這些標記訓練的DenseDPO效能接近使用人工標記的結果。", "audio": "audios/2506.03517v1.mp3", "timestamp": "2025-06-05T13:32:22.247933"}
{"query": "AI", "id": "2506.03988v1", "url": "http://arxiv.org/abs/2506.03988v1", "title": "RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors", "summary": "AI-generated images have reached a quality level at which humans are\nincapable of reliably distinguishing them from real images. To counteract the\ninherent risk of fraud and disinformation, the detection of AI-generated images\nis a pressing challenge and an active research topic. While many of the\npresented methods claim to achieve high detection accuracy, they are usually\nevaluated under idealized conditions. In particular, the adversarial robustness\nis often neglected, potentially due to a lack of awareness or the substantial\neffort required to conduct a comprehensive robustness analysis. In this work,\nwe tackle this problem by providing a simpler means to assess the robustness of\nAI-generated image detectors. We present RAID (Robust evaluation of\nAI-generated image Detectors), a dataset of 72k diverse and highly transferable\nadversarial examples. The dataset is created by running attacks against an\nensemble of seven state-of-the-art detectors and images generated by four\ndifferent text-to-image models. Extensive experiments show that our methodology\ngenerates adversarial images that transfer with a high success rate to unseen\ndetectors, which can be used to quickly provide an approximate yet still\nreliable estimate of a detector's adversarial robustnessOur findings indicate\nthat current state-of-the-art AI-generated image detectors can be easily\ndeceived by adversarial examples, highlighting the critical need for the\ndevelopment of more robust methods. We release our dataset at\nhttps://huggingface.co/datasets/aimagelab/RAID and evaluation code at\nhttps://github.com/pralab/RAID.", "authors": ["Hicham Eddoubi", "Jonas Ricker", "Federico Cocchi", "Lorenzo Baraldi", "Angelo Sotgiu", "Maura Pintor", "Marcella Cornia", "Lorenzo Baraldi", "Asja Fischer", "Rita Cucchiara", "Battista Biggio"], "published_date": "2025-06-04", "title_zh": "RAID：用於測試人工智慧生成圖像檢測器對抗性穩健性的數據集", "summary_zh": "人工智慧生成影像已達到人類難以分辨真偽的程度。為應對潛在的詐欺和假訊息風險，檢測AI生成影像成為迫切挑戰與研究課題。現有方法雖聲稱具備高檢測準確度，但多在理想條件下評估，忽略了對抗性穩健性。本研究提出RAID（AI生成影像檢測器穩健性評估），包含7.2萬個多樣且具高轉移性的對抗樣本，透過對七個先進檢測器和四個文本生成影像模型發動攻擊而產生。實驗表明，此方法生成的對抗樣本能以高成功率轉移至未見過的檢測器，快速評估檢測器的對抗性穩健程度。結果顯示，現有AI生成影像檢測器易受對抗樣本欺騙，凸顯開發更穩健方法的必要性。研究公開數據集於https://huggingface.co/datasets/aimagelab/RAID，評估程式碼於https://github.com/pralab/RAID。", "audio": "audios/2506.03988v1.mp3", "timestamp": "2025-06-05T14:19:03.313694"}
{"query": "Foundation Model", "id": "2506.03320v1", "url": "http://arxiv.org/abs/2506.03320v1", "title": "The Future of Continual Learning in the Era of Foundation Models: Three Key Directions", "summary": "Continual learning--the ability to acquire, retain, and refine knowledge over\ntime--has always been fundamental to intelligence, both human and artificial.\nHistorically, different AI paradigms have acknowledged this need, albeit with\nvarying priorities: early expert and production systems focused on incremental\nknowledge consolidation, while reinforcement learning emphasised dynamic\nadaptation. With the rise of deep learning, deep continual learning has\nprimarily focused on learning robust and reusable representations over time to\nsolve sequences of increasingly complex tasks. However, the emergence of Large\nLanguage Models (LLMs) and foundation models has raised the question: Do we\nstill need continual learning when centralised, monolithic models can tackle\ndiverse tasks with access to internet-scale knowledge? We argue that continual\nlearning remains essential for three key reasons: (i) continual pre-training is\nstill necessary to ensure foundation models remain up to date, mitigating\nknowledge staleness and distribution shifts while integrating new information;\n(ii) continual fine-tuning enables models to specialise and personalise,\nadapting to domain-specific tasks, user preferences, and real-world constraints\nwithout full retraining, avoiding the need for computationally expensive long\ncontext-windows; (iii) continual compositionality offers a scalable and modular\napproach to intelligence, enabling the orchestration of foundation models and\nagents to be dynamically composed, recombined, and adapted. While continual\npre-training and fine-tuning are explored as niche research directions, we\nargue it is continual compositionality that will mark the rebirth of continual\nlearning. The future of AI will not be defined by a single static model but by\nan ecosystem of continually evolving and interacting models, making continual\nlearning more relevant than ever.", "authors": ["Jack Bell", "Luigi Quarantiello", "Eric Nuertey Coleman", "Lanpei Li", "Malio Li", "Mauro Madeddu", "Elia Piccoli", "Vincenzo Lomonaco"], "published_date": "2025-06-03", "title_zh": "基礎模型時代持續學習的未來：三大關鍵方向", "summary_zh": "持續學習，即隨時間獲取、保持和精進知識的能力，對人類和人工智慧至關重要。深度持續學習側重學習穩健且可重複使用的表徵，以解決日益複雜的任務。儘管大型語言模型展現強大能力，持續學習仍不可或缺，原因如下：(一)持續預訓練確保模型與時俱進，整合新資訊並應對知識過時；(二)持續微調使模型能專業化和個人化，適應特定領域和使用者偏好；(三)持續組合性提供可擴展的模組化方法，協調基礎模型和代理，使其能動態組合和調整。持續組合性將標誌持續學習的重生。AI的未來將由不斷演進和互動的模型生態系統定義，使持續學習比以往更重要。", "audio": "audios/2506.03320v1.mp3", "timestamp": "2025-06-05T14:19:10.306206"}
{"query": "Diffusion Model", "id": "2506.03502v1", "url": "http://arxiv.org/abs/2506.03502v1", "title": "CHIME: Conditional Hallucination and Integrated Multi-scale Enhancement for Time Series Diffusion Model", "summary": "The denoising diffusion probabilistic model has become a mainstream\ngenerative model, achieving significant success in various computer vision\ntasks. Recently, there has been initial exploration of applying diffusion\nmodels to time series tasks. However, existing studies still face challenges in\nmulti-scale feature alignment and generative capabilities across different\nentities and long-time scales. In this paper, we propose CHIME, a conditional\nhallucination and integrated multi-scale enhancement framework for time series\ndiffusion models. By employing multi-scale decomposition and adaptive\nintegration, CHIME captures the decomposed features of time series, achieving\nin-domain distribution alignment between generated and original samples. In\naddition, we introduce a feature hallucination module in the conditional\ndenoising process, enabling the transfer of temporal features through the\ntraining of category-independent transformation layers. Experimental results on\npublicly available real-world datasets demonstrate that CHIME achieves\nstate-of-the-art performance and exhibits excellent generative generalization\ncapabilities in few-shot scenarios.", "authors": ["Yuxuan Chen", "Haipeng Xie"], "published_date": "2025-06-04", "title_zh": "CHIME：條件式幻覺與整合多尺度增強之時間序列擴散模型", "summary_zh": "去噪擴散概率模型已成主流生成模型，在電腦視覺任務中獲得顯著成功。近期有初步研究將擴散模型應用於時間序列任務，但現有研究在多尺度特徵對齊及跨實體、長時間尺度上的生成能力仍面臨挑戰。本研究提出CHIME，一種時間序列擴散模型的條件幻覺及整合多尺度增強框架。透過多尺度分解與自適應整合，CHIME擷取時間序列的分解特徵，實現生成樣本與原始樣本間的域內分佈對齊。此外，在條件去噪過程中引入特徵幻覺模組，透過訓練獨立於類別的轉換層，實現時間特徵的轉移。在公開真實世界資料集上的實驗結果表明，CHIME達到最先進性能，並在少樣本情境中展現出色的生成泛化能力。", "audio": "audios/2506.03502v1.mp3", "timestamp": "2025-06-05T14:19:17.478423"}
{"query": "AI", "id": "2506.03954v1", "url": "http://arxiv.org/abs/2506.03954v1", "title": "HtFLlib: A Comprehensive Heterogeneous Federated Learning Library and Benchmark", "summary": "As AI evolves, collaboration among heterogeneous models helps overcome data\nscarcity by enabling knowledge transfer across institutions and devices.\nTraditional Federated Learning (FL) only supports homogeneous models, limiting\ncollaboration among clients with heterogeneous model architectures. To address\nthis, Heterogeneous Federated Learning (HtFL) methods are developed to enable\ncollaboration across diverse heterogeneous models while tackling the data\nheterogeneity issue at the same time. However, a comprehensive benchmark for\nstandardized evaluation and analysis of the rapidly growing HtFL methods is\nlacking. Firstly, the highly varied datasets, model heterogeneity scenarios,\nand different method implementations become hurdles to making easy and fair\ncomparisons among HtFL methods. Secondly, the effectiveness and robustness of\nHtFL methods are under-explored in various scenarios, such as the medical\ndomain and sensor signal modality. To fill this gap, we introduce the first\nHeterogeneous Federated Learning Library (HtFLlib), an easy-to-use and\nextensible framework that integrates multiple datasets and model heterogeneity\nscenarios, offering a robust benchmark for research and practical applications.\nSpecifically, HtFLlib integrates (1) 12 datasets spanning various domains,\nmodalities, and data heterogeneity scenarios; (2) 40 model architectures,\nranging from small to large, across three modalities; (3) a modularized and\neasy-to-extend HtFL codebase with implementations of 10 representative HtFL\nmethods; and (4) systematic evaluations in terms of accuracy, convergence,\ncomputation costs, and communication costs. We emphasize the advantages and\npotential of state-of-the-art HtFL methods and hope that HtFLlib will catalyze\nadvancing HtFL research and enable its broader applications. The code is\nreleased at https://github.com/TsingZ0/HtFLlib.", "authors": ["Jianqing Zhang", "Xinghao Wu", "Yanbing Zhou", "Xiaoting Sun", "Qiqi Cai", "Yang Liu", "Yang Hua", "Zhenzhe Zheng", "Jian Cao", "Qiang Yang"], "published_date": "2025-06-04", "title_zh": "HtFLlib：一個全面的異質聯邦學習函式庫與基準", "summary_zh": "隨著人工智慧發展，異質模型協作能克服數據匱乏，促進跨機構和設備的知識轉移。傳統聯邦學習僅支援同質模型，限制了異質模型架構客戶端的協作。為此，開發了異質聯邦學習方法，以實現跨多樣異質模型的協作，同時解決數據異質性問題。然而，目前缺乏針對快速成長的異質聯邦學習方法進行標準化評估和分析的全面基準。現有方法在數據集、模型異質性情境及方法實現方面差異大，難以進行公平比較。此外，異質聯邦學習方法在醫療領域和感測器訊號模式等場景中的有效性和穩健性仍待深入探討。為填補此空白，我們推出首個異質聯邦學習函式庫（HtFLlib），這是一個易於使用且可擴展的框架，整合了多個數據集和模型異質性情境，為研究和實際應用提供可靠基準。HtFLlib整合了：(1) 涵蓋多個領域、模式和數據異質性情境的12個數據集；(2) 三種模式下，從小型到大型的40種模型架構；(3) 模組化且易於擴展的異質聯邦學習程式碼庫，其中包含10種具代表性異質聯邦學習方法的實現；(4) 在準確性、收斂性、計算成本和通訊成本方面的系統性評估。我們強調了最先進異質聯邦學習方法的優勢和潛力，並期望HtFLlib能促進異質聯邦學習研究，並使其應用更廣泛。程式碼已在https://github.com/TsingZ0/HtFLlib上發布。", "audio": "audios/2506.03954v1.mp3", "timestamp": "2025-06-05T15:20:38.396771"}
{"query": "Foundation Model", "id": "2506.02294v2", "url": "http://arxiv.org/abs/2506.02294v2", "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through Confidence-Guided Data Augmentation", "summary": "Large foundation models trained on extensive datasets demonstrate strong\nzero-shot capabilities in various domains. To replicate their success when data\nand model size are constrained, knowledge distillation has become an\nestablished tool for transferring knowledge from foundation models to small\nstudent networks. However, the effectiveness of distillation is critically\nlimited by the available training data. This work addresses the common\npractical issue of covariate shift in knowledge distillation, where spurious\nfeatures appear during training but not at test time. We ask the question: when\nthese spurious features are unknown, yet a robust teacher is available, is it\npossible for a student to also become robust to them? We address this problem\nby introducing a novel diffusion-based data augmentation strategy that\ngenerates images by maximizing the disagreement between the teacher and the\nstudent, effectively creating challenging samples that the student struggles\nwith. Experiments demonstrate that our approach significantly improves worst\ngroup and mean group accuracy on CelebA and SpuCo Birds as well as the spurious\nmAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art\ndiffusion-based data augmentation baselines", "authors": ["Niclas Popp", "Kevin Alexander Laube", "Matthias Hein", "Lukas Schott"], "published_date": "2025-06-02", "title_zh": "基於置信度引導數據增強改進未知協變量偏移下的知識蒸餾", "summary_zh": "基於大型數據集訓練的基礎模型在多個領域展現了強大的零樣本能力。為在數據和模型規模受限的情況下複製此成功，知識蒸餾已成為將知識從基礎模型轉移到小型學生網路的常用工具。然而，蒸餾的有效性受到可用訓練數據的嚴重限制。本研究旨在解決知識蒸餾中常見的協變量偏移問題，即在訓練期間出現但在測試時消失的虛假特徵。我們探討了：當這些虛假特徵未知，但存在穩健的教師模型時，學生模型是否也能對這些特徵保持穩健？為此，我們提出了一種新的基於擴散的數據增強策略，通過最大化教師模型和學生模型之間的分歧來生成圖像，有效創建學生難以處理的挑戰性樣本。實驗表明，在協變量偏移下，此方法顯著提高了CelebA和SpuCo Birds上的最差組別和平均組別準確度，以及虛假ImageNet上的虛假mAUC，優於現有的基於擴散的數據增強基準。", "audio": "audios/2506.02294v2.mp3", "timestamp": "2025-06-05T15:20:45.711508"}
{"query": "Diffusion Model", "id": "2506.03478v1", "url": "http://arxiv.org/abs/2506.03478v1", "title": "Facial Appearance Capture at Home with Patch-Level Reflectance Prior", "summary": "Existing facial appearance capture methods can reconstruct plausible facial\nreflectance from smartphone-recorded videos. However, the reconstruction\nquality is still far behind the ones based on studio recordings. This paper\nfills the gap by developing a novel daily-used solution with a co-located\nsmartphone and flashlight video capture setting in a dim room. To enhance the\nquality, our key observation is to solve facial reflectance maps within the\ndata distribution of studio-scanned ones. Specifically, we first learn a\ndiffusion prior over the Light Stage scans and then steer it to produce the\nreflectance map that best matches the captured images. We propose to train the\ndiffusion prior at the patch level to improve generalization ability and\ntraining stability, as current Light Stage datasets are in ultra-high\nresolution but limited in data size. Tailored to this prior, we propose a\npatch-level posterior sampling technique to sample seamless full-resolution\nreflectance maps from this patch-level diffusion model. Experiments demonstrate\nour method closes the quality gap between low-cost and studio recordings by a\nlarge margin, opening the door for everyday users to clone themselves to the\ndigital world. Our code will be released at https://github.com/yxuhan/DoRA.", "authors": ["Yuxuan Han", "Junfeng Lyu", "Kuan Sheng", "Minghao Que", "Qixuan Zhang", "Lan Xu", "Feng Xu"], "published_date": "2025-06-04", "title_zh": "基於塊級反射先驗的居家面部外觀捕捉", "summary_zh": "現有臉部外觀捕捉方法能從手機影片重建尚可的臉部反射率，但品質遠遜於攝影棚錄製。本文提出一種新型日常解決方案，於昏暗房間使用手機與手電筒協同錄製影片，以填補此差距。為提升品質，關鍵在於解決攝影棚掃描資料分布內的臉部反射率圖。具體而言，首先學習光場掃描的擴散先驗，然後引導其產生最符合捕捉影像的反射率圖。為提升泛化能力和訓練穩定性，我們提出在區塊層級訓練擴散先驗，因目前光場數據集解析度極高但數據量有限。針對此先驗，提出區塊層級後驗抽樣技術，從此區塊層級擴散模型中抽樣無縫的全解析度反射率圖。實驗表明，本方法大幅縮小低成本錄製與攝影棚錄製間的品質差距，為日常使用者複製自身到數位世界開啟大門。程式碼將於https://github.com/yxuhan/DoRA發布。", "audio": "audios/2506.03478v1.mp3", "timestamp": "2025-06-05T15:20:52.727030"}
{"query": "AI", "id": "2506.03939v1", "url": "http://arxiv.org/abs/2506.03939v1", "title": "Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning", "summary": "Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external\nknowledge integration capabilities by explicitly modeling knowledge\nrelationships, thereby improving the factual accuracy and generation quality of\nLarge Language Models (LLMs) in specialized domains. However, existing methods\nsuffer from two inherent limitations: 1) Inefficient Information Aggregation:\nThey rely on a single agent and fixed iterative patterns, making it difficult\nto adaptively capture multi-level textual, structural, and degree information\nwithin graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning\nschemes, which cannot dynamically adjust reasoning depth nor achieve precise\nsemantic correction. To overcome these limitations, we propose Graph Counselor,\nan GraphRAG method based on multi-agent collaboration. This method uses the\nAdaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,\nand Execution Agents work together to precisely model complex graph structures\nand dynamically adjust information extraction strategies, addressing the\nchallenges of multi-level dependency modeling and adaptive reasoning depth.\nAdditionally, the Self-Reflection with Multiple Perspectives (SR) module\nimproves the accuracy and semantic consistency of reasoning results through\nself-reflection and backward reasoning mechanisms. Experiments demonstrate that\nGraph Counselor outperforms existing methods in multiple graph reasoning tasks,\nexhibiting higher reasoning accuracy and generalization ability. Our code is\navailable at https://github.com/gjq100/Graph-Counselor.git.", "authors": ["Junqi Gao", "Xiang Zou", "YIng Ai", "Dong Li", "Yichen Niu", "Biqing Qi", "Jianxing Liu"], "published_date": "2025-06-04", "title_zh": "圖譜諮詢師：透過多智能體協同進行自適應圖譜探索以提升大型語言模型推理能力", "summary_zh": "GraphRAG透過顯式建模知識關係，增強大型語言模型在特定領域的知識整合能力，進而提升事實準確性和生成品質。現有方法存在訊息彙整效率低落及推理機制僵化等限制。為此，我們提出基於多代理協作的GraphRAG方法Graph Counselor。該方法採用自適應圖訊息提取模組（AGIEM），透過規劃、思考和執行代理協同工作，精確建模複雜圖結構，動態調整訊息提取策略，解決多層次依賴建模和自適應推理深度的挑戰。此外，多視角自我反思模組（SR）透過自我反思和逆向推理機制，提高推理結果的準確性和語義一致性。實驗證明，Graph Counselor在多個圖推理任務中優於現有方法，展現更高的推理準確性和泛化能力。程式碼已公開。", "audio": "audios/2506.03939v1.mp3", "timestamp": "2025-06-05T16:24:10.160427"}
{"query": "Foundation Model", "id": "2506.02258v1", "url": "http://arxiv.org/abs/2506.02258v1", "title": "Are Mamba-based Audio Foundation Models the Best Fit for Non-Verbal Emotion Recognition?", "summary": "In this work, we focus on non-verbal vocal sounds emotion recognition (NVER).\nWe investigate mamba-based audio foundation models (MAFMs) for the first time\nfor NVER and hypothesize that MAFMs will outperform attention-based audio\nfoundation models (AAFMs) for NVER by leveraging its state-space modeling to\ncapture intrinsic emotional structures more effectively. Unlike AAFMs, which\nmay amplify irrelevant patterns due to their attention mechanisms, MAFMs will\nextract more stable and context-aware representations, enabling better\ndifferentiation of subtle non-verbal emotional cues. Our experiments with\nstate-of-the-art (SOTA) AAFMs and MAFMs validates our hypothesis. Further,\nmotivated from related research such as speech emotion recognition, synthetic\nspeech detection, where fusion of foundation models (FMs) have showed improved\nperformance, we also explore fusion of FMs for NVER. To this end, we propose,\nRENO, that uses renyi-divergence as a novel loss function for effective\nalignment of the FMs. It also makes use of self-attention for better\nintra-representation interaction of the FMs. With RENO, through the\nheterogeneous fusion of MAFMs and AAFMs, we show the topmost performance in\ncomparison to individual FMs, its fusion and also setting SOTA in comparison to\nprevious SOTA work.", "authors": ["Mohd Mujtaba Akhtar", "Orchid Chetia Phukan", "Girish", "Swarup Ranjan Behera", "Ananda Chandra Nayak", "Sanjib Kumar Nayak", "Arun Balaji Buduru", "Rajesh Sharma"], "published_date": "2025-06-02", "title_zh": "基於Mamba的音訊基礎模型是否最適合非語言情感辨識？", "summary_zh": "本研究著重於非語言聲音情緒辨識(NVER)，首次探索基於Mamba的音訊基礎模型(MAFMs)於NVER之應用，並假設MAFMs能更有效地捕捉內在情緒結構，優於基於注意力機制的音訊基礎模型(AAFMs)。相較於可能因注意力機制而放大無關模式的AAFMs，MAFMs將提取更穩定且具備上下文意識的表徵，從而更好地區分細微的非語言情緒線索。實驗驗證了此假設。此外，受語音情緒辨識和合成語音檢測等相關研究啟發，本研究亦探索基礎模型(FMs)融合於NVER之應用。為此，我們提出RENO，其利用Rényi散度作為新型損失函數，以有效對齊FMs，並採用自注意力機制以增強FMs的表徵互動。透過異質融合MAFMs和AAFMs，RENO展現了優於個別FMs及其融合的最佳性能，並達到了NVER領域的最新技術水準。", "audio": "audios/2506.02258v1.mp3", "timestamp": "2025-06-05T16:24:20.171825"}
{"query": "Diffusion Model", "id": "2506.03425v1", "url": "http://arxiv.org/abs/2506.03425v1", "title": "A Data-Driven Diffusion-based Approach for Audio Deepfake Explanations", "summary": "Evaluating explainability techniques, such as SHAP and LRP, in the context of\naudio deepfake detection is challenging due to lack of clear ground truth\nannotations. In the cases when we are able to obtain the ground truth, we find\nthat these methods struggle to provide accurate explanations. In this work, we\npropose a novel data-driven approach to identify artifact regions in deepfake\naudio. We consider paired real and vocoded audio, and use the difference in\ntime-frequency representation as the ground-truth explanation. The difference\nsignal then serves as a supervision to train a diffusion model to expose the\ndeepfake artifacts in a given vocoded audio. Experimental results on the VocV4\nand LibriSeVoc datasets demonstrate that our method outperforms traditional\nexplainability techniques, both qualitatively and quantitatively.", "authors": ["Petr Grinberg", "Ankur Kumar", "Surya Koppisetti", "Gaurav Bharaj"], "published_date": "2025-06-03", "title_zh": "基於數據驅動擴散模型的音頻深度偽造解釋方法", "summary_zh": "音訊深度偽造檢測中，SHAP和LRP等可解釋性技術因缺乏明確的真實標註而難以評估。即使獲得真實標註，這些方法也難以提供準確解釋。本研究提出一種新的資料驅動方法，用於識別深度偽造音訊中的偽影區域。透過配對的真實音訊和聲碼器音訊，將時頻表示差異作為真實解釋。該差異訊號用於監督訓練擴散模型，以揭示給定聲碼器音訊中的深度偽造偽影。在VocV4和LibriSeVoc資料集上的實驗結果表明，本方法在質和量方面均優於傳統可解釋性技術。", "audio": "audios/2506.03425v1.mp3", "timestamp": "2025-06-05T16:24:27.467955"}
{"query": "AI", "id": "2506.03890v1", "url": "http://arxiv.org/abs/2506.03890v1", "title": "Identifying Alzheimer's Disease Prediction Strategies of Convolutional Neural Network Classifiers using R2* Maps and Spectral Clustering", "summary": "Deep learning models have shown strong performance in classifying Alzheimer's\ndisease (AD) from R2* maps, but their decision-making remains opaque, raising\nconcerns about interpretability. Previous studies suggest biases in model\ndecisions, necessitating further analysis. This study uses Layer-wise Relevance\nPropagation (LRP) and spectral clustering to explore classifier decision\nstrategies across preprocessing and training configurations using R2* maps. We\ntrained a 3D convolutional neural network on R2* maps, generating relevance\nheatmaps via LRP and applied spectral clustering to identify dominant patterns.\nt-Stochastic Neighbor Embedding (t-SNE) visualization was used to assess\nclustering structure. Spectral clustering revealed distinct decision patterns,\nwith the relevance-guided model showing the clearest separation between AD and\nnormal control (NC) cases. The t-SNE visualization confirmed that this model\naligned heatmap groupings with the underlying subject groups. Our findings\nhighlight the significant impact of preprocessing and training choices on deep\nlearning models trained on R2* maps, even with similar performance metrics.\nSpectral clustering offers a structured method to identify classification\nstrategy differences, emphasizing the importance of explainability in medical\nAI.", "authors": ["Christian Tinauer", "Maximilian Sackl", "Stefan Ropele", "Christian Langkammer"], "published_date": "2025-06-04", "title_zh": "基於R2*圖譜與譜聚類的卷積神經網絡分類器阿爾茨海默病預測策略識別", "summary_zh": "深度學習模型在R2*圖譜的阿茲海默症分類表現優異，但決策過程不透明，引發可解釋性疑慮。本研究利用逐層相關性傳播（LRP）和譜聚類，探討不同預處理和訓練配置下分類器的決策策略。研究在R2*圖譜上訓練3D卷積神經網路，透過LRP生成相關性熱圖，並應用譜聚類識別主要模式。t-SNE視覺化評估聚類結構。譜聚類揭示了不同的決策模式，其中相關性引導模型在阿茲海默症和正常對照組間表現出最清晰的分離。t-SNE驗證該模型將熱圖分組與受試者分組對齊。研究結果強調，即使性能指標相似，預處理和訓練選擇對R2*圖譜深度學習模型的影響顯著。譜聚類提供一種結構化方法來識別分類策略差異，突顯醫學人工智慧中可解釋性的重要性。", "audio": "audios/2506.03890v1.mp3", "timestamp": "2025-06-05T17:14:16.714317"}
{"query": "Foundation Model", "id": "2506.02203v1", "url": "http://arxiv.org/abs/2506.02203v1", "title": "Constrained Sliced Wasserstein Embedding", "summary": "Sliced Wasserstein (SW) distances offer an efficient method for comparing\nhigh-dimensional probability measures by projecting them onto multiple\n1-dimensional probability distributions. However, identifying informative\nslicing directions has proven challenging, often necessitating a large number\nof slices to achieve desirable performance and thereby increasing computational\ncomplexity. We introduce a constrained learning approach to optimize the\nslicing directions for SW distances. Specifically, we constrain the 1D\ntransport plans to approximate the optimal plan in the original space, ensuring\nmeaningful slicing directions. By leveraging continuous relaxations of these\ntransport plans, we enable a gradient-based primal-dual approach to train the\nslicer parameters, alongside the remaining model parameters. We demonstrate how\nthis constrained slicing approach can be applied to pool high-dimensional\nembeddings into fixed-length permutation-invariant representations. Numerical\nresults on foundation models trained on images, point clouds, and protein\nsequences showcase the efficacy of the proposed constrained learning approach\nin learning more informative slicing directions. Our implementation code can be\nfound at https://github.com/Stranja572/constrainedswe.", "authors": ["Navid NaderiAlizadeh", "Darian Salehi", "Xinran Liu", "Soheil Kolouri"], "published_date": "2025-06-02", "title_zh": "約束切片華沙坦嵌入", "summary_zh": "切片華沙坦距離(SW)透過將高維機率分布投影至多個一維分布，提供了一種比較高維機率測度的有效方法。然而，找到有用的切片方向極具挑戰，通常需要大量切片才能達到理想效果，進而增加計算複雜性。本研究提出一種約束學習方法，以優化SW距離的切片方向。具體而言，我們約束一維傳輸計畫，使其逼近原始空間中的最佳計畫，從而確保切片方向的意義。透過利用這些傳輸計畫的連續鬆弛，我們得以採用基於梯度的原始-對偶方法來訓練切片器參數，以及剩餘的模型參數。我們展示了這種約束切片方法如何應用於將高維嵌入池化為固定長度的排列不變表示。在圖像、點雲和蛋白質序列上訓練的基礎模型的數值結果表明，所提出的約束學習方法在學習更有用的切片方向方面是有效的。實作程式碼位於https://github.com/Stranja572/constrainedswe。", "audio": "audios/2506.02203v1.mp3", "timestamp": "2025-06-05T17:14:29.364418"}
{"query": "Diffusion Model", "id": "2506.03355v1", "url": "http://arxiv.org/abs/2506.03355v1", "title": "Robustness in Both Domains: CLIP Needs a Robust Text Encoder", "summary": "Adversarial input attacks can cause a significant shift of CLIP embeddings.\nThis can affect the downstream robustness of models incorporating CLIP in the\npipeline, such as text-to-image generative models or large vision language\nmodels. While some efforts have been done towards making the CLIP image\nencoders robust, the robustness of text encoders remains unexplored. In this\nwork, we cover this gap in the literature. We propose LEAF: an efficient\nadversarial finetuning method for the text domain, with the ability to scale to\nlarge CLIP models. Our models significantly improve the zero-shot adversarial\naccuracy in the text domain, while maintaining the vision performance provided\nby robust image encoders. When combined with text-to-image diffusion models, we\ncan improve the generation quality under adversarial noise. When employing our\nrobust CLIP encoders in multimodal retrieval tasks, we improve the recall under\nadversarial noise over standard CLIP models. Finally, we show that robust text\nencoders facilitate better reconstruction of input text from its embedding via\ndirect optimization.", "authors": ["Elias Abad Rocamora", "Christian Schlarmann", "Naman Deep Singh", "Yongtao Wu", "Matthias Hein", "Volkan Cevher"], "published_date": "2025-06-03", "title_zh": "跨域穩健性：CLIP需要一個穩健的文本編碼器", "summary_zh": "對抗性輸入攻擊會顯著改變CLIP嵌入，影響包含CLIP的下游模型，如文本生成圖像模型或大型視覺語言模型的穩健性。現有研究主要關注CLIP圖像編碼器的穩健性，而忽略了文本編碼器。本研究提出LEAF，一種高效的文本域對抗性微調方法，適用於大型CLIP模型。該方法顯著提升文本域的零樣本對抗準確性，同時維持穩健圖像編碼器的視覺性能。結合文本生成圖像擴散模型，可提升對抗性噪聲下的生成品質。在多模態檢索任務中，使用穩健的CLIP編碼器可提高對抗性噪聲下的召回率。此外，穩健的文本編碼器有助於通過直接優化，更好地從嵌入重建輸入文本。", "audio": "audios/2506.03355v1.mp3", "timestamp": "2025-06-05T17:14:37.463879"}
{"query": "AI", "id": "2506.03842v1", "url": "http://arxiv.org/abs/2506.03842v1", "title": "Improving Post-Processing for Quantitative Precipitation Forecasting Using Deep Learning: Learning Precipitation Physics from High-Resolution Observations", "summary": "Accurate quantitative precipitation forecasting (QPF) remains one of the main\nchallenges in numerical weather prediction (NWP), primarily due to the\ndifficulty of representing the full complexity of atmospheric microphysics\nthrough parameterization schemes. This study introduces a deep learning-based\npost-processing model, DL-QPF, which diagnoses precipitation fields from\nmeteorological forecasts by learning directly from high-resolution radar\nestimates precipitation. The DL-QPF model is constructed using a\nPatch-conditional Generative Adversarial Network (Patch-cGAN) architecture\ncombined with a U-Net generator and a discriminator. The generator learns\nmeteorological features relevant to precipitation, while the adversarial loss\nfrom the discriminator encourages the generation of realistic rainfall patterns\nand distributions. Training is performed on three years of warm-season data\nover the Korean Peninsula, with input variables derived from ECMWF's Integrated\nForecasting System High-Resolution forecast (IFS-HRES). Model verification is\nconducted against multiple reference models, including global (IFS-HRES, KIM),\nregional (KIM-Regional, KIM-LENS), and AI-based (GraphCast) forecasts.\nVerification across multiple rainfall thresholds shows that DL-QPF achieves a\nfrequency bias near one and superior success ratios. Particularly for heavy and\nintense rainfall events, DL-QPF outperforms both conventional NWP and an AI\nmodel, demonstrating improved skill in capturing high-intensity precipitation.\nThis study highlights the potential of observational data-driven deep learning\napproaches in post-processing QPF. By directly learning from observations,\nDL-QPF reduces systematic biases and enhances the realism of forecasted\nrainfall distributions. These results demonstrate the model's potential to\nenhance QPF realism.", "authors": ["ChangJae Lee", "Heecheol Yang", "Byeonggwon Kim"], "published_date": "2025-06-04", "title_zh": "利用深度學習改進定量降水預報的後處理：從高解析度觀測中學習降水物理", "summary_zh": "精確的定量降水預報（QPF）在數值天氣預報（NWP）中仍是一大挑戰，主因是難以透過參數化方案完整呈現複雜的大氣微物理過程。本研究提出一個基於深度學習的後處理模型DL-QPF，透過學習高解析度雷達估計的降水資料，診斷氣象預報中的降水場。DL-QPF模型採用Patch條件生成對抗網路（Patch-cGAN）架構，結合U-Net生成器和鑑別器。生成器學習與降水相關的氣象特徵，鑑別器的對抗損失則促進生成逼真的降雨型態和分佈。模型在韓國半島三年暖季資料上進行訓練，輸入變數來自歐洲中期天氣預報中心高解析度預報（IFS-HRES）。驗證結果顯示，DL-QPF在多個降雨閾值下實現接近於1的頻率偏差和更佳的成功率。尤其在強降雨事件中，DL-QPF優於傳統數值天氣預報模型和基於人工智慧的模型，展現了捕捉高強度降水的更佳能力。本研究強調了觀測資料驅動的深度學習方法在QPF後處理中的潛力，透過直接學習觀測資料，DL-QPF可減少系統性偏差並提升預報降雨分佈的真實性，顯示其提升QPF真實度的潛力。", "audio": "audios/2506.03842v1.mp3", "timestamp": "2025-06-05T19:14:10.924405"}
{"query": "Foundation Model", "id": "2506.02149v1", "url": "http://arxiv.org/abs/2506.02149v1", "title": "Tomographic Foundation Model -- FORCE: Flow-Oriented Reconstruction Conditioning Engine", "summary": "Computed tomography (CT) is a major medical imaging modality. Clinical CT\nscenarios, such as low-dose screening, sparse-view scanning, and metal\nimplants, often lead to severe noise and artifacts in reconstructed images,\nrequiring improved reconstruction techniques. The introduction of deep learning\nhas significantly advanced CT image reconstruction. However, obtaining paired\ntraining data remains rather challenging due to patient motion and other\nconstraints. Although deep learning methods can still perform well with\napproximately paired data, they inherently carry the risk of hallucination due\nto data inconsistencies and model instability. In this paper, we integrate the\ndata fidelity with the state-of-the-art generative AI model, referred to as the\nPoisson flow generative model (PFGM) with a generalized version PFGM++, and\npropose a novel CT framework: Flow-Oriented Reconstruction Conditioning Engine\n(FORCE). In our experiments, the proposed method shows superior performance in\nvarious CT imaging tasks, outperforming existing unsupervised reconstruction\napproaches.", "authors": ["Wenjun Xia", "Chuang Niu", "Ge Wang"], "published_date": "2025-06-02", "title_zh": "斷層掃描基礎模型：流向導向重建條件引擎", "summary_zh": "電腦斷層掃描是重要的醫學影像技術。低劑量篩檢、稀疏視角掃描及金屬植入物等臨床情境易導致重建影像產生嚴重雜訊及偽影，需要改良重建技術。深度學習顯著推進了電腦斷層影像重建，但獲取配對訓練數據仍具挑戰性。儘管深度學習方法在近似配對數據下表現良好，但數據不一致及模型不穩定性導致其內在具有幻覺風險。本文整合數據保真度與最先進的生成式人工智慧模型，即泊松流生成模型(PFGM)及其廣義版本PFGM++，提出一種新型電腦斷層框架：流向導向重建調節引擎(FORCE)。實驗結果表明，所提方法在多種電腦斷層影像任務中表現卓越，超越了現有的非監督式重建方法。", "audio": "audios/2506.02149v1.mp3", "timestamp": "2025-06-05T19:14:16.480644"}
{"query": "Diffusion Model", "id": "2506.02395v1", "url": "http://arxiv.org/abs/2506.02395v1", "title": "The Devil is in the Darkness: Diffusion-Based Nighttime Dehazing Anchored in Brightness Perception", "summary": "While nighttime image dehazing has been extensively studied, converting\nnighttime hazy images to daytime-equivalent brightness remains largely\nunaddressed. Existing methods face two critical limitations: (1) datasets\noverlook the brightness relationship between day and night, resulting in the\nbrightness mapping being inconsistent with the real world during image\nsynthesis; and (2) models do not explicitly incorporate daytime brightness\nknowledge, limiting their ability to reconstruct realistic lighting. To address\nthese challenges, we introduce the Diffusion-Based Nighttime Dehazing (DiffND)\nframework, which excels in both data synthesis and lighting reconstruction. Our\napproach starts with a data synthesis pipeline that simulates severe\ndistortions while enforcing brightness consistency between synthetic and\nreal-world scenes, providing a strong foundation for learning night-to-day\nbrightness mapping. Next, we propose a restoration model that integrates a\npre-trained diffusion model guided by a brightness perception network. This\ndesign harnesses the diffusion model's generative ability while adapting it to\nnighttime dehazing through brightness-aware optimization. Experiments validate\nour dataset's utility and the model's superior performance in joint haze\nremoval and brightness mapping.", "authors": ["Xiaofeng Cong", "Yu-Xin Zhang", "Haoran Wei", "Yeying Jin", "Junming Hou", "Jie Gui", "Jing Zhang", "Dacheng Tao"], "published_date": "2025-06-03", "title_zh": "魔鬼藏於暗處：基於亮度感知的擴散夜間去霧", "summary_zh": "夜間影像去霧技術已獲廣泛研究，但將夜間霧霾影像轉換為等效白天亮度仍未獲充分探討。現有方法面臨兩大挑戰：一、資料集忽略晝夜亮度關係，導致影像合成的亮度映射與現實不符；二、模型未明確納入白天亮度知識，限制了其重建真實光照的能力。為解決這些問題，我們提出基於擴散的夜間去霧框架(DiffND)，該框架擅長數據合成和光照重建。我們的方案首先建立數據合成流程，模擬嚴重失真，同時確保合成場景與真實場景之間的亮度一致性，為學習夜間到白天的亮度映射提供堅實基礎。其次，我們提出一種復原模型，整合了預訓練的擴散模型，並以亮度感知網絡引導。此設計利用擴散模型的生成能力，並透過亮度感知優化使其適應夜間去霧。實驗驗證了我們資料集的有效性以及模型在聯合去霧和亮度映射方面的優越效能。", "audio": "audios/2506.02395v1.mp3", "timestamp": "2025-06-05T19:14:23.274650"}
{"query": "AI", "id": "2506.03837v1", "url": "http://arxiv.org/abs/2506.03837v1", "title": "HTSC-2025: A Benchmark Dataset of Ambient-Pressure High-Temperature Superconductors for AI-Driven Critical Temperature Prediction", "summary": "The discovery of high-temperature superconducting materials holds great\nsignificance for human industry and daily life. In recent years, research on\npredicting superconducting transition temperatures using artificial\nintelligence~(AI) has gained popularity, with most of these tools claiming to\nachieve remarkable accuracy. However, the lack of widely accepted benchmark\ndatasets in this field has severely hindered fair comparisons between different\nAI algorithms and impeded further advancement of these methods. In this work,\nwe present the HTSC-2025, an ambient-pressure high-temperature superconducting\nbenchmark dataset. This comprehensive compilation encompasses theoretically\npredicted superconducting materials discovered by theoretical physicists from\n2023 to 2025 based on BCS superconductivity theory, including the renowned\nX$_2$YH$_6$ system, perovskite MXH$_3$ system, M$_3$XH$_8$ system, cage-like\nBCN-doped metal atomic systems derived from LaH$_{10}$ structural evolution,\nand two-dimensional honeycomb-structured systems evolving from MgB$_2$. The\nHTSC-2025 benchmark has been open-sourced at\nhttps://github.com/xqh19970407/HTSC-2025 and will be continuously updated. This\nbenchmark holds significant importance for accelerating the discovery of\nsuperconducting materials using AI-based methods.", "authors": ["Xiao-Qi Han", "Ze-Feng Gao", "Xin-De Wang", "Zhenfeng Ouyang", "Peng-Jie Guo", "Zhong-Yi Lu"], "published_date": "2025-06-04", "title_zh": "HTSC-2025：用於人工智能驅動臨界溫度預測的常壓高溫超導體基準數據集", "summary_zh": "高溫超導材料的發現對人類工業和生活至關重要。近年來，利用人工智慧預測超導轉變溫度的研究日益普及，但缺乏廣泛認可的基準數據集嚴重阻礙了不同演算法之間的公平比較和進一步發展。本研究提出常壓高溫超導基準數據集HTSC-2025，其包含理論物理學家基於BCS超導理論預測的超導材料，包括X$_2$YH$_6$、鈣鈦礦MXH$_3$、M$_3$XH$_8$體系，以及由LaH$_{10}$結構演化而來的籠狀BCN摻雜金屬原子體系和由MgB$_2$演化而來的二維蜂窩結構體系。HTSC-2025已開源並將持續更新，對於利用人工智慧加速超導材料的發現具有重要意義。", "audio": "audios/2506.03837v1.mp3", "timestamp": "2025-06-05T20:17:27.878921"}
{"query": "Foundation Model", "id": "2506.02138v1", "url": "http://arxiv.org/abs/2506.02138v1", "title": "Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability", "summary": "The development of effective explainability tools for Transformers is a\ncrucial pursuit in deep learning research. One of the most promising approaches\nin this domain is Layer-wise Relevance Propagation (LRP), which propagates\nrelevance scores backward through the network to the input space by\nredistributing activation values based on predefined rules. However, existing\nLRP-based methods for Transformer explainability entirely overlook a critical\ncomponent of the Transformer architecture: its positional encoding (PE),\nresulting in violation of the conservation property, and the loss of an\nimportant and unique type of relevance, which is also associated with\nstructural and positional features. To address this limitation, we reformulate\nthe input space for Transformer explainability as a set of position-token\npairs. This allows us to propose specialized theoretically-grounded LRP rules\ndesigned to propagate attributions across various positional encoding methods,\nincluding Rotary, Learnable, and Absolute PE. Extensive experiments with both\nfine-tuned classifiers and zero-shot foundation models, such as LLaMA 3,\ndemonstrate that our method significantly outperforms the state-of-the-art in\nboth vision and NLP explainability tasks. Our code is publicly available.", "authors": ["Yarden Bakish", "Itamar Zimerman", "Hila Chefer", "Lior Wolf"], "published_date": "2025-06-02", "title_zh": "重探LRP：位置歸因作為Transformer可解釋性的缺失要素", "summary_zh": "針對變形金剛開發有效的可解釋性工具是深度學習研究的關鍵。逐層相關性傳播(LRP)是其中一種具潛力的方法，它透過基於預定義規則重新分配激活值，將相關性分數向後傳播到輸入空間。然而，現有的基於LRP的變形金剛可解釋性方法完全忽略了變形金剛架構的關鍵組成部分：位置編碼(PE)，導致違反守恆性質，並喪失了一種重要的、獨特的相關性類型，該相關性也與結構和位置特徵相關。為了解決這個限制，我們將變形金剛可解釋性的輸入空間重新定義為位置-token對的集合。這使我們能夠提出專門的、理論上可靠的LRP規則，旨在跨多種位置編碼方法（包括旋轉式、可學習式和絕對PE）傳播歸因。使用微調分類器和零樣本基礎模型（如LLaMA 3）的大量實驗表明，我們的方法在視覺和自然語言處理可解釋性任務中顯著優於最先進的方法。程式碼已公開。", "audio": "audios/2506.02138v1.mp3", "timestamp": "2025-06-05T20:17:39.736944"}
{"query": "Diffusion Model", "id": "2506.02394v1", "url": "http://arxiv.org/abs/2506.02394v1", "title": "Joint Modeling for Learning Decision-Making Dynamics in Behavioral Experiments", "summary": "Major depressive disorder (MDD), a leading cause of disability and mortality,\nis associated with reward-processing abnormalities and concentration issues.\nMotivated by the probabilistic reward task from the Establishing Moderators and\nBiosignatures of Antidepressant Response in Clinical Care (EMBARC) study, we\npropose a novel framework that integrates the reinforcement learning (RL) model\nand drift-diffusion model (DDM) to jointly analyze reward-based decision-making\nwith response times. To account for emerging evidence suggesting that\ndecision-making may alternate between multiple interleaved strategies, we model\nlatent state switching using a hidden Markov model (HMM). In the ''engaged''\nstate, decisions follow an RL-DDM, simultaneously capturing reward processing,\ndecision dynamics, and temporal structure. In contrast, in the ''lapsed''\nstate, decision-making is modeled using a simplified DDM, where specific\nparameters are fixed to approximate random guessing with equal probability. The\nproposed method is implemented using a computationally efficient generalized\nexpectation-maximization algorithm with forward-backward procedures. Through\nextensive numerical studies, we demonstrate that our proposed method\noutperforms competing approaches under various reward-generating distributions,\nboth with and without strategy switching. When applied to the EMBARC study, our\nframework reveals that MDD patients exhibit lower overall engagement than\nhealthy controls and experience longer decision times when they do engage.\nAdditionally, we show that neuroimaging measures of brain activities are\nassociated with decision-making characteristics in the ''engaged'' state but\nnot in the ''lapsed'' state, providing evidence of brain-behavioral association\nspecific to the ''engaged'' state.", "authors": ["Yuan Bian", "Xingche Guo", "Yuanjia Wang"], "published_date": "2025-06-03", "title_zh": "行為實驗中決策動態學習的聯合建模", "summary_zh": "重度憂鬱症（MDD）與獎勵處理異常和注意力問題有關。本研究基於臨床照護中抗憂鬱劑反應的調節因子和生物標誌物研究（EMBARC）的概率獎勵任務，提出一個整合強化學習（RL）模型和漂移擴散模型（DDM）的新框架，以聯合分析基於獎勵的決策和反應時間。考量到決策可能在多種交錯策略之間切換，我們使用隱馬可夫模型（HMM）對潛在狀態切換建模。在投入狀態中，決策遵循 RL-DDM，同時捕捉獎勵處理、決策動態和時間結構；在鬆懈狀態中，決策使用簡化的 DDM 建模，特定參數固定以近似等概率的隨機猜測。該方法通過高效的廣義期望最大化算法和前向-後向程序實現。數值研究表明，該方法在不同獎勵生成分布下優於其他方法。應用於 EMBARC 研究時，發現 MDD 患者的整體投入度低於健康對照組，且投入時的決策時間更長。此外，神經影像學測量顯示，大腦活動與投入狀態下的決策特徵相關，但在鬆懈狀態下則不然，證明了特定於投入狀態的腦-行為關聯。", "audio": "audios/2506.02394v1.mp3", "timestamp": "2025-06-05T20:17:51.600010"}
{"query": "AI", "id": "2506.03828v1", "url": "http://arxiv.org/abs/2506.03828v1", "title": "AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial Asset Operations and Maintenance", "summary": "AI for Industrial Asset Lifecycle Management aims to automate complex\noperational workflows -- such as condition monitoring, maintenance planning,\nand intervention scheduling -- to reduce human workload and minimize system\ndowntime. Traditional AI/ML approaches have primarily tackled these problems in\nisolation, solving narrow tasks within the broader operational pipeline. In\ncontrast, the emergence of AI agents and large language models (LLMs)\nintroduces a next-generation opportunity: enabling end-to-end automation across\nthe entire asset lifecycle. This paper envisions a future where AI agents\nautonomously manage tasks that previously required distinct expertise and\nmanual coordination. To this end, we introduce AssetOpsBench -- a unified\nframework and environment designed to guide the development, orchestration, and\nevaluation of domain-specific agents tailored for Industry 4.0 applications. We\noutline the key requirements for such holistic systems and provide actionable\ninsights into building agents that integrate perception, reasoning, and control\nfor real-world industrial operations. The software is available at\nhttps://github.com/IBM/AssetOpsBench.", "authors": ["Dhaval Patel", "Shuxin Lin", "James Rayfield", "Nianjun Zhou", "Roman Vaculin", "Natalia Martinez", "Fearghal O'donncha", "Jayant Kalagnanam"], "published_date": "2025-06-04", "title_zh": "AssetOpsBench：工業資產運營維護中用於任務自動化的人工智慧代理基準測試", "summary_zh": "工業資產生命週期管理的人工智慧旨在自動化複雜的運營流程，如狀態監測、維護規劃和干預排程，以減輕人力負擔並減少系統停機時間。傳統人工智慧/機器學習方法主要孤立地解決這些問題，僅處理運營流程中的狹隘任務。然而，人工智慧代理和大型語言模型（LLM）的出現帶來了新一代的機會：實現整個資產生命週期的端到端自動化。本文設想了人工智慧代理自主管理先前需要不同專業知識和人工協調任務的未來。為此，我們介紹AssetOpsBench，一個統一的框架和環境，旨在引導為工業4.0應用客製化的特定領域代理的開發、協調和評估。我們概述了此類整體系統的關鍵需求，並為構建整合感知、推理和控制的代理以用於實際工業運營提供可操作的見解。該軟體可在https://github.com/IBM/AssetOpsBench取得。", "audio": "audios/2506.03828v1.mp3", "timestamp": "2025-06-05T21:18:13.977499"}
{"query": "Foundation Model", "id": "2506.01953v1", "url": "http://arxiv.org/abs/2506.01953v1", "title": "Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning", "summary": "Generalized policy and execution efficiency constitute the two critical\nchallenges in robotic manipulation. While recent foundation policies benefit\nfrom the common-sense reasoning capabilities of internet-scale pretrained\nvision-language models (VLMs), they often suffer from low execution frequency.\nTo mitigate this dilemma, dual-system approaches, inspired by Kahneman's\ntheory, have been proposed to leverage a VLM-based System 2 model handling\nhigh-level reasoning and a separate System 1 action model ensuring real-time\ncontrol. However, existing designs maintain both systems as separate models,\nlimiting System 1 from fully leveraging the rich pretrained knowledge from the\nVLM-based System 2. In this work, we propose Fast-in-Slow (FiS), a unified\ndual-system vision-language-action (VLA) model that embeds the System 1\nexecution module within the VLM-based System 2 by partially sharing parameters.\nThis innovative paradigm not only enables high-frequency execution in System 1\nbut also facilitates coordination between the reasoning and execution\ncomponents within a single foundation model of System 2. Given their\nfundamentally distinct roles within FiS-VLA, we design the two systems to\nincorporate heterogeneous modality inputs alongside asynchronous operating\nfrequencies, enabling both fast and precise manipulation. To enable\ncoordination between the two systems, a dual-aware co-training strategy is\nproposed that equips System 1 with action generation capabilities while\npreserving System 2's contextual reasoning representation. For evaluation,\nFiS-VLA outperforms previous state-of-the-art methods by 8% in simulation and\n11% in real-world tasks in terms of average success rate, while achieving a\n117.7 Hz control frequency with action chunk set to eight. Project web page:\nfast-in-slow.github.io.", "authors": ["Hao Chen", "Jiaming Liu", "Chenyang Gu", "Zhuoyang Liu", "Renrui Zhang", "Xiaoqi Li", "Xiao He", "Yandong Guo", "Chi-Wing Fu", "Shanghang Zhang", "Pheng-Ann Heng"], "published_date": "2025-06-02", "title_zh": "快思慢想：整合快速操作與慢速推理的雙系統基礎模型", "summary_zh": "機器人操控面臨泛化策略與執行效率兩大挑戰。基於網路規模預訓練視覺語言模型(VLM)的基礎策略雖具備常識推理能力，但執行頻率通常偏低。受Kahneman理論啟發，雙系統方法利用VLM系統2模型處理高層推理，搭配獨立系統1動作模型確保即時控制。然而，現有設計將兩系統維持獨立，限制系統1充分利用VLM系統2的預訓練知識。本研究提出Fast-in-Slow (FiS)，一種統一的雙系統視覺-語言-動作(VLA)模型，透過部分參數共享將系統1執行模組嵌入基於VLM的系統2中。此架構不僅實現系統1的高頻執行，還促進系統2單一基礎模型內推理與執行組件的協調。考量FiS-VLA內部的根本差異，我們設計異質模態輸入與非同步操作頻率，以實現快速且精確的操控。為促進兩系統協調，提出雙感知協同訓練策略，使系統1具備動作生成能力，同時保留系統2的上下文推理表示。評估顯示，FiS-VLA在模擬和真實世界任務中的平均成功率分別超越先前技術8%和11%，並在動作塊設定為8時達到117.7 Hz的控制頻率。", "audio": "audios/2506.01953v1.mp3", "timestamp": "2025-06-05T21:18:36.667120"}
{"query": "Diffusion Model", "id": "2506.02371v1", "url": "http://arxiv.org/abs/2506.02371v1", "title": "SFBD Flow: A Continuous-Optimization Framework for Training Diffusion Models with Noisy Samples", "summary": "Diffusion models achieve strong generative performance but often rely on\nlarge datasets that may include sensitive content. This challenge is compounded\nby the models' tendency to memorize training data, raising privacy concerns.\nSFBD (Lu et al., 2025) addresses this by training on corrupted data and using\nlimited clean samples to capture local structure and improve convergence.\nHowever, its iterative denoising and fine-tuning loop requires manual\ncoordination, making it burdensome to implement. We reinterpret SFBD as an\nalternating projection algorithm and introduce a continuous variant, SFBD flow,\nthat removes the need for alternating steps. We further show its connection to\nconsistency constraint-based methods, and demonstrate that its practical\ninstantiation, Online SFBD, consistently outperforms strong baselines across\nbenchmarks.", "authors": ["Haoye Lu", "Darren Lo", "Yaoliang Yu"], "published_date": "2025-06-03", "title_zh": "SFBD流：一種使用噪聲樣本訓練擴散模型的連續優化框架", "summary_zh": "擴散模型生成能力強大，但仰賴含敏感內容的大型數據集，且易記憶訓練資料，引發隱私問題。SFBD透過於受損數據上訓練，並使用少量乾淨樣本捕捉局部結構、改善收斂。然其迭代去噪與微調迴圈需手動協調，實施繁瑣。本文將SFBD重新詮釋為交替投影演算法，並提出連續變體SFBD flow，無需交替步驟。研究亦證明其與基於一致性約束之方法關聯性，並展示其實際應用Online SFBD在基準測試中持續超越強大基線。", "audio": "audios/2506.02371v1.mp3", "timestamp": "2025-06-05T21:18:53.741999"}
{"query": "AI", "id": "2506.03812v1", "url": "http://arxiv.org/abs/2506.03812v1", "title": "Impact of friction force and retrieval speed on in silico mechanical thrombectomies: a sensitivity analysis", "summary": "Background: Mechanical Thrombectomy (MT) is a widely accepted first-line\ntreatment for Acute Ischemic Stroke (AIS) and it has been studied using in\nvitro and in silico models. Thrombectomy outcomes have been performed for\npatient-specific cases using in silico models. However, until now, in vivo\nfriction coefficients for stent-vessel, stent-clot, and clot-vessel\ninteractions are unknown, but in vitro experiments have been attempted with\nsignificant standard deviations. These interactions and friction coefficients\nhave been considered an important aspect of thrombectomy success. Objectives:\nIn the current study, we explored the influence of variation in friction forces\nfor stent-vessel, stent-clot, and clot-vessel interactions using virtual\nmechanical thrombectomy (VMT). We have performed three simulations for each\ninteraction and varied friction coefficients around the standard deviation\nobserved in the past in vitro studies. Results: (i) clot-vessel friction:\nhigher friction leads to clot fragmentation and VMT failure. (ii) stent-clot\nfriction: it is susceptible to VMT outcomes, with lower values showing the\nslippage of the clot while higher values lead to fragmentation. (iii)\nstent-vessel friction: higher friction shows compression of the stent in curved\nvessels and dislodgment of clot from stent retriever (SR) due to its\ncompression, which leads to VMT failure. (iv) retrieval speed (RS): higher RS\n(>30 mm/s) leads to significant stent compression and unrealistic behavior of\nthe SR. Conclusions: Analysis of results proposes the necessity for calculating\naccurate friction factor values and their implementation into in silico models,\ndue to their sensitivity towards thrombectomy outcomes. Such in silico models\nmimic in vivo thrombectomy more closely and can be used in mechanical\nthrombectomy planning, management, and decision-making.", "authors": ["Mahesh S. Nagargoje", "Virginia Fregona", "Giulia Luraghi", "Francesco Migliavacca", "Demitria A Poulos", "Bryan C Good", "Jose Felix Rodriguez Matas"], "published_date": "2025-06-04", "title_zh": "摩擦力與回收速度對電腦模擬機械取栓術之影響：敏感度分析", "summary_zh": "背景：機械取栓術（MT）是急性缺血性中風（AIS）的一線治療方法，已被廣泛研究。體外和體內模型皆用於研究取栓結果。然而，支架與血管、支架與血栓、血栓與血管之間的摩擦係數在體內研究中仍然未知，且體外實驗存在顯著標準差。這些交互作用和摩擦係數被認為是取栓成功的重要因素。\n\n目的：本研究利用虛擬機械取栓術（VMT），探討支架與血管、支架與血栓、血栓與血管之間摩擦力變化對取栓的影響。針對每種交互作用進行三項模擬，並在過去體外研究的標準差範圍內調整摩擦係數。\n\n結果：(i) 血栓與血管摩擦：高摩擦力導致血栓碎裂和VMT失敗。(ii) 支架與血栓摩擦：對VMT結果敏感，較低摩擦力導致血栓滑動，較高摩擦力導致碎裂。(iii) 支架與血管摩擦：高摩擦力導致支架在彎曲血管中壓縮，並因壓縮使血栓從支架取回器（SR）脫落，導致VMT失敗。(iv) 取回速度（RS）：較高RS（>30 mm/s）導致支架顯著壓縮和SR的不真實行為。\n\n結論：結果分析表明，由於摩擦係數對取栓結果的敏感性，有必要計算準確的摩擦係數值並將其應用於體內模型。此類體內模型更接近模擬體內取栓術，可用於機械取栓的規劃、管理和決策。", "audio": "audios/2506.03812v1.mp3", "timestamp": "2025-06-05T22:17:43.250052"}
{"query": "Foundation Model", "id": "2506.01946v1", "url": "http://arxiv.org/abs/2506.01946v1", "title": "MLLMs Need 3D-Aware Representation Supervision for Scene Understanding", "summary": "Recent advances in scene understanding have leveraged multimodal large\nlanguage models (MLLMs) for 3D reasoning by capitalizing on their strong 2D\npretraining. However, the lack of explicit 3D data during MLLM pretraining\nlimits 3D representation capability. In this paper, we investigate the\n3D-awareness of MLLMs by evaluating multi-view correspondence and reveal a\nstrong positive correlation between the quality of 3D-aware representation and\ndownstream task performance. Motivated by this, we propose 3DRS, a framework\nthat enhances MLLM 3D representation learning by introducing supervision from\npretrained 3D foundation models. Our approach aligns MLLM visual features with\nrich 3D knowledge distilled from 3D models, effectively improving scene\nunderstanding. Extensive experiments across multiple benchmarks and MLLMs --\nincluding visual grounding, captioning, and question answering -- demonstrate\nconsistent performance gains. Project page: https://visual-ai.github.io/3drs", "authors": ["Xiaohu Huang", "Jingjing Wu", "Qunyi Xie", "Kai Han"], "published_date": "2025-06-02", "title_zh": "多模態大型語言模型需要具備三維感知的表徵監督以提升場景理解能力", "summary_zh": "近期場景理解的進展利用多模態大型語言模型(MLLM)進行3D推理，受益於其強大的2D預訓練。然而，MLLM預訓練期間缺乏明確的3D數據，限制了3D表示能力。本文通過評估多視角對應關係來研究MLLM的3D感知能力，並揭示了3D感知表示的品質與下游任務表現之間存在強烈的正相關。受此啟發，我們提出3DRS框架，通過引入來自預訓練3D基礎模型的監督來增強MLLM的3D表示學習。我們的方案將MLLM視覺特徵與從3D模型提煉的豐富3D知識對齊，有效提升場景理解。在多個基準測試和MLLM上進行的廣泛實驗，包括視覺定位、圖像描述和問答，均展現出持續的性能提升。", "audio": "audios/2506.01946v1.mp3", "timestamp": "2025-06-05T22:17:55.806870"}
{"query": "Diffusion Model", "id": "2506.02318v1", "url": "http://arxiv.org/abs/2506.02318v1", "title": "Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models", "summary": "Discrete state space diffusion models have shown significant advantages in\napplications involving discrete data, such as text and image generation. It has\nalso been observed that their performance is highly sensitive to the choice of\nrate matrices, particularly between uniform and absorbing rate matrices. While\nempirical results suggest that absorbing rate matrices often yield better\ngeneration quality compared to uniform rate matrices, existing theoretical\nworks have largely focused on the uniform rate matrices case. Notably,\nconvergence guarantees and error analyses for absorbing diffusion models are\nstill missing. In this work, we provide the first finite-time error bounds and\nconvergence rate analysis for discrete diffusion models using absorbing rate\nmatrices. We begin by deriving an upper bound on the KL divergence of the\nforward process, introducing a surrogate initialization distribution to address\nthe challenge posed by the absorbing stationary distribution, which is a\nsingleton and causes the KL divergence to be ill-defined. We then establish the\nfirst convergence guarantees for both the $\\tau$-leaping and uniformization\nsamplers under absorbing rate matrices, demonstrating improved rates over their\ncounterparts using uniform rate matrices. Furthermore, under suitable\nassumptions, we provide convergence guarantees without early stopping. Our\nanalysis introduces several new technical tools to address challenges unique to\nabsorbing rate matrices. These include a Jensen-type argument for bounding\nforward process convergence, novel techniques for bounding absorbing score\nfunctions, and a non-divergent upper bound on the score near initialization\nthat removes the need of early-stopping.", "authors": ["Yuchen Liang", "Renxiang Huang", "Lifeng Lai", "Ness Shroff", "Yingbin Liang"], "published_date": "2025-06-02", "title_zh": "吸收與收斂：吸收離散擴散模型的可證收斂保證", "summary_zh": "離散狀態空間擴散模型在處理離散數據（如文本和圖像生成）上展現優勢，但其性能對速率矩陣選擇高度敏感，尤其是在均勻與吸收速率矩陣之間。儘管實驗表明吸收速率矩陣通常產生更佳生成質量，現有理論研究主要集中於均勻速率矩陣。吸收擴散模型的收斂性保證和誤差分析仍然缺乏。本研究首次針對使用吸收速率矩陣的離散擴散模型，提供有限時間誤差界限和收斂速率分析。我們推導出前向過程KL散度的上界，引入替代初始化分佈來解決由吸收靜止分佈（單例集）引起的KL散度定義不明確問題。進而，我們為吸收速率矩陣下的tau跳躍和均勻化採樣器建立首次收斂性保證，其速率優於使用均勻速率矩陣的同類方法。在適當假設下，我們提供無需提前停止的收斂性保證。我們的分析引入數種新技術工具，以應對吸收速率矩陣的獨特挑戰，包含用於限制前向過程收斂的Jensen型論證、用於限制吸收得分函數的新穎技術，以及在初始化附近得分的非發散上界，從而消除了提前停止的需求。", "audio": "audios/2506.02318v1.mp3", "timestamp": "2025-06-05T22:18:11.995605"}
{"query": "AI", "id": "2506.03801v1", "url": "http://arxiv.org/abs/2506.03801v1", "title": "From Theory to Practice: Real-World Use Cases on Trustworthy LLM-Driven Process Modeling, Prediction and Automation", "summary": "Traditional Business Process Management (BPM) struggles with rigidity,\nopacity, and scalability in dynamic environments while emerging Large Language\nModels (LLMs) present transformative opportunities alongside risks. This paper\nexplores four real-world use cases that demonstrate how LLMs, augmented with\ntrustworthy process intelligence, redefine process modeling, prediction, and\nautomation. Grounded in early-stage research projects with industrial partners,\nthe work spans manufacturing, modeling, life-science, and design processes,\naddressing domain-specific challenges through human-AI collaboration. In\nmanufacturing, an LLM-driven framework integrates uncertainty-aware explainable\nMachine Learning (ML) with interactive dialogues, transforming opaque\npredictions into auditable workflows. For process modeling, conversational\ninterfaces democratize BPMN design. Pharmacovigilance agents automate drug\nsafety monitoring via knowledge-graph-augmented LLMs. Finally, sustainable\ntextile design employs multi-agent systems to navigate regulatory and\nenvironmental trade-offs. We intend to examine tensions between transparency\nand efficiency, generalization and specialization, and human agency versus\nautomation. By mapping these trade-offs, we advocate for context-sensitive\nintegration prioritizing domain needs, stakeholder values, and iterative\nhuman-in-the-loop workflows over universal solutions. This work provides\nactionable insights for researchers and practitioners aiming to operationalize\nLLMs in critical BPM environments.", "authors": ["Peter Pfeiffer", "Alexander Rombach", "Maxim Majlatow", "Nijat Mehdiyev"], "published_date": "2025-06-04", "title_zh": "從理論到實踐：可信賴之大型語言模型驅動的流程建模、預測與自動化於真實世界的應用案例", "summary_zh": "傳統企業流程管理(BPM)在動態環境中面臨僵化、不透明和擴展性問題，而新興大型語言模型(LLM)帶來變革機會，同時伴隨風險。本文探討四個實際案例，展示LLM如何藉由可信的流程智慧，重新定義流程建模、預測和自動化。基於與產業夥伴合作的初步研究項目，涵蓋製造、建模、生命科學和設計流程，透過人機協作解決特定領域挑戰。在製造業，LLM驅動的框架整合考量不確定性的可解釋機器學習(ML)與互動式對話，將不透明的預測轉化為可稽核的工作流程。在流程建模方面，對話式介面使BPMN設計普及化。藥物警戒代理透過知識圖譜增強的LLM自動化藥物安全監測。永續紡織品設計則採用多代理系統來應對法規和環境的權衡。本文旨在檢視透明度與效率、泛化與專業化、以及人類能動性與自動化之間的張力。透過分析這些權衡，我們提倡情境敏感的整合，優先考慮領域需求、利害關係人價值和迭代式人機協作工作流程，而非通用解決方案。本研究為研究人員和實務工作者在關鍵BPM環境中實施LLM提供可行的見解。", "audio": "audios/2506.03801v1.mp3", "timestamp": "2025-06-05T23:20:02.883658"}
{"query": "Foundation Model", "id": "2506.01933v1", "url": "http://arxiv.org/abs/2506.01933v1", "title": "E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models", "summary": "Spatial intelligence, encompassing 3D reconstruction, perception, and\nreasoning, is fundamental to applications such as robotics, aerial imaging, and\nextended reality. A key enabler is the real-time, accurate estimation of core\n3D attributes (camera parameters, point clouds, depth maps, and 3D point\ntracks) from unstructured or streaming imagery. Inspired by the success of\nlarge foundation models in language and 2D vision, a new class of end-to-end 3D\ngeometric foundation models (GFMs) has emerged, directly predicting dense 3D\nrepresentations in a single feed-forward pass, eliminating the need for slow or\nunavailable precomputed camera parameters. Since late 2023, the field has\nexploded with diverse variants, but systematic evaluation is lacking. In this\nwork, we present the first comprehensive benchmark for 3D GFMs, covering five\ncore tasks: sparse-view depth estimation, video depth estimation, 3D\nreconstruction, multi-view pose estimation, novel view synthesis, and spanning\nboth standard and challenging out-of-distribution datasets. Our standardized\ntoolkit automates dataset handling, evaluation protocols, and metric\ncomputation to ensure fair, reproducible comparisons. We evaluate 16\nstate-of-the-art GFMs, revealing their strengths and limitations across tasks\nand domains, and derive key insights to guide future model scaling and\noptimization. All code, evaluation scripts, and processed data will be publicly\nreleased to accelerate research in 3D spatial intelligence.", "authors": ["Wenyan Cong", "Yiqing Liang", "Yancheng Zhang", "Ziyi Yang", "Yan Wang", "Boris Ivanovic", "Marco Pavone", "Chen Chen", "Zhangyang Wang", "Zhiwen Fan"], "published_date": "2025-06-02", "title_zh": "E3D-Bench：端到端三維幾何基礎模型基準測試", "summary_zh": "空間智能對於機器人、航拍攝影和擴展實境等應用至關重要，仰賴從非結構化或串流影像即時且精確地估計3D屬性。受惠於大型基礎模型在語言和2D視覺的成功，幾何基礎模型(GFM)直接預測密集3D表示，無需預先計算相機參數。自2023年末以來，GFM領域快速發展，但缺乏系統性評估。本研究首次提出針對3D GFM的全面基準測試，涵蓋稀疏視角深度估計、影片深度估計、3D重建、多視角姿態估計和新視角合成等五項核心任務，並包含標準和具挑戰性的資料集。我們提供標準化工具組，自動化資料集處理、評估協定和指標計算，以確保公平且可重現的比較。我們評估了16個最先進的GFM，揭示其在不同任務和領域的優勢和局限性，並提出指導未來模型擴展和優化的關鍵見解。所有程式碼、評估腳本和處理後的資料將公開發布，以加速3D空間智能研究。", "audio": "audios/2506.01933v1.mp3", "timestamp": "2025-06-05T23:20:17.438574"}
{"query": "Diffusion Model", "id": "2506.02276v1", "url": "http://arxiv.org/abs/2506.02276v1", "title": "Latent Stochastic Interpolants", "summary": "Stochastic Interpolants (SI) are a powerful framework for generative\nmodeling, capable of flexibly transforming between two probability\ndistributions. However, their use in jointly optimized latent variable models\nremains unexplored as they require direct access to the samples from the two\ndistributions. This work presents Latent Stochastic Interpolants (LSI) enabling\njoint learning in a latent space with end-to-end optimized encoder, decoder and\nlatent SI models. We achieve this by developing a principled Evidence Lower\nBound (ELBO) objective derived directly in continuous time. The joint\noptimization allows LSI to learn effective latent representations along with a\ngenerative process that transforms an arbitrary prior distribution into the\nencoder-defined aggregated posterior. LSI sidesteps the simple priors of the\nnormal diffusion models and mitigates the computational demands of applying SI\ndirectly in high-dimensional observation spaces, while preserving the\ngenerative flexibility of the SI framework. We demonstrate the efficacy of LSI\nthrough comprehensive experiments on the standard large scale ImageNet\ngeneration benchmark.", "authors": ["Saurabh Singh", "Dmitry Lagun"], "published_date": "2025-06-02", "title_zh": "潛在隨機內插法", "summary_zh": "隨機內插法(SI)是強大的生成模型框架，能靈活轉換兩個機率分布。然而，其在聯合優化的潛變數模型中的應用仍未被探索，因其需要直接存取來自兩個分布的樣本。本研究提出潛在隨機內插法(LSI)，可在潛在空間中實現聯合學習，並端對端優化編碼器、解碼器和潛在SI模型。我們透過開發在連續時間中直接導出的證據下界(ELBO)目標函數來達成此目的。聯合優化使LSI能學習有效的潛在表示，以及將任意先驗分布轉換為編碼器定義的聚合後驗的生成過程。LSI避開了常態擴散模型的簡單先驗，並減輕了在高維觀察空間中直接應用SI的計算需求，同時保留了SI框架的生成靈活性。我們透過對標準大規模ImageNet生成基準的全面實驗，展示了LSI的有效性。", "audio": "audios/2506.02276v1.mp3", "timestamp": "2025-06-05T23:20:26.623387"}
{"query": "AI", "id": "2506.03988v2", "url": "http://arxiv.org/abs/2506.03988v2", "title": "RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors", "summary": "AI-generated images have reached a quality level at which humans are\nincapable of reliably distinguishing them from real images. To counteract the\ninherent risk of fraud and disinformation, the detection of AI-generated images\nis a pressing challenge and an active research topic. While many of the\npresented methods claim to achieve high detection accuracy, they are usually\nevaluated under idealized conditions. In particular, the adversarial robustness\nis often neglected, potentially due to a lack of awareness or the substantial\neffort required to conduct a comprehensive robustness analysis. In this work,\nwe tackle this problem by providing a simpler means to assess the robustness of\nAI-generated image detectors. We present RAID (Robust evaluation of\nAI-generated image Detectors), a dataset of 72k diverse and highly transferable\nadversarial examples. The dataset is created by running attacks against an\nensemble of seven state-of-the-art detectors and images generated by four\ndifferent text-to-image models. Extensive experiments show that our methodology\ngenerates adversarial images that transfer with a high success rate to unseen\ndetectors, which can be used to quickly provide an approximate yet still\nreliable estimate of a detector's adversarial robustness. Our findings indicate\nthat current state-of-the-art AI-generated image detectors can be easily\ndeceived by adversarial examples, highlighting the critical need for the\ndevelopment of more robust methods. We release our dataset at\nhttps://huggingface.co/datasets/aimagelab/RAID and evaluation code at\nhttps://github.com/pralab/RAID.", "authors": ["Hicham Eddoubi", "Jonas Ricker", "Federico Cocchi", "Lorenzo Baraldi", "Angelo Sotgiu", "Maura Pintor", "Marcella Cornia", "Lorenzo Baraldi", "Asja Fischer", "Rita Cucchiara", "Battista Biggio"], "published_date": "2025-06-04", "title_zh": "RAID：用於測試人工智能生成圖像檢測器對抗魯棒性的數據集", "summary_zh": "人工智慧生成圖像已達人類難以區分的程度，檢測此類圖像以應對詐欺和假訊息風險至關重要。儘管現有方法聲稱具高檢測準確度，但其在對抗性環境下的穩健性常被忽略。本研究提出RAID，一個包含72,000個多樣且具高度轉移性的對抗樣本資料集，用於評估人工智慧生成圖像檢測器的穩健性。該資料集透過針對七種最先進檢測器和四種文本到圖像模型產生的圖像進行攻擊而創建。實驗表明，RAID能有效生成可轉移至未見檢測器的對抗樣本，快速評估檢測器的穩健性。研究結果顯示，現有檢測器易受對抗樣本欺騙，亟需開發更穩健的方法。資料集與程式碼已公開。", "audio": "audios/2506.03988v2.mp3", "timestamp": "2025-06-06T01:27:30.479019"}
{"query": "Foundation Model", "id": "2506.01901v1", "url": "http://arxiv.org/abs/2506.01901v1", "title": "Understanding Overadaptation in Supervised Fine-Tuning: The Role of Ensemble Methods", "summary": "Supervised fine-tuning (SFT) on domain-specific data is the dominant approach\nfor adapting foundation models to specialized tasks. However, it has been\nobserved that SFT models tend to forget knowledge acquired during pretraining.\nIn vision models, ensembling a pretrained model with its fine-tuned counterpart\nhas been shown to mitigate this issue. In this work, we demonstrate that the\nsame holds for language models, and, more strikingly, we observe an\noveradaptation phenomenon: the ensemble model not only retains general\nknowledge from the foundation model but also outperforms the fine-tuned model\neven on the fine-tuning domain itself. Despite the empirical success of\nensembling, a theoretical understanding of its benefits remains underexplored.\nWe develop a formal theoretical analysis of the overadaptation phenomenon.\nEnsembling mitigates this by balancing two primary sources of error: bias,\ncaused by insufficient fine-tuning, and variance, introduced by overfitting to\nfine-tuning data. While regularization techniques aim to address this\ntrade-off, we show that ensembling provides a more effective solution. We\nanalyze this phenomenon in over-parameterized linear settings and demonstrate\nthat interpolating between pretrained and fine-tuned weights significantly\nimproves performance. These findings offer theoretical justification for the\nobserved advantages of model ensembling, supported by empirical experiments\nconsistent with our analysis.", "authors": ["Yifan Hao", "Xingyuan Pan", "Hanning Zhang", "Chenlu Ye", "Rui Pan", "Tong Zhang"], "published_date": "2025-06-02", "title_zh": "監督式微調中過度適應之理解：集成方法之作用", "summary_zh": "基於特定領域資料的監督式微調是調整基礎模型以適應專業任務的主流方法。然而，微調模型容易忘記預訓練階段習得的知識。在視覺模型中，將預訓練模型與微調模型進行集成可緩解此問題。本研究表明，此方法同樣適用於語言模型，且集成模型不僅保留了基礎模型的通用知識，甚至在微調領域也優於微調模型本身，即過度適應現象。儘管集成方法在實證上取得成功，但其優勢的理論基礎仍有待探索。本研究針對過度適應現象進行了正式的理論分析。集成通過平衡兩種主要誤差來源來緩解此問題：由微調不足引起的偏差，以及由過擬合微調資料引入的變異數。雖然正則化技術旨在解決這種權衡，但本研究表明集成提供了一個更有效的解決方案。我們在過參數化線性設定中分析了這種現象，並證明在預訓練和微調權重之間進行插值顯著提高了效能。這些發現為模型集成的優勢提供了理論依據，並通過與分析一致的實證實驗予以支援。", "audio": "audios/2506.01901v1.mp3", "timestamp": "2025-06-06T01:27:43.458750"}
{"query": "Diffusion Model", "id": "2506.03979v2", "url": "http://arxiv.org/abs/2506.03979v2", "title": "Solving Inverse Problems via Diffusion-Based Priors: An Approximation-Free Ensemble Sampling Approach", "summary": "Diffusion models (DMs) have proven to be effective in modeling\nhigh-dimensional distributions, leading to their widespread adoption for\nrepresenting complex priors in Bayesian inverse problems (BIPs). However,\ncurrent DM-based posterior sampling methods proposed for solving common BIPs\nrely on heuristic approximations to the generative process. To exploit the\ngenerative capability of DMs and avoid the usage of such approximations, we\npropose an ensemble-based algorithm that performs posterior sampling without\nthe use of heuristic approximations. Our algorithm is motivated by existing\nworks that combine DM-based methods with the sequential Monte Carlo (SMC)\nmethod. By examining how the prior evolves through the diffusion process\nencoded by the pre-trained score function, we derive a modified partial\ndifferential equation (PDE) governing the evolution of the corresponding\nposterior distribution. This PDE includes a modified diffusion term and a\nreweighting term, which can be simulated via stochastic weighted particle\nmethods. Theoretically, we prove that the error between the true posterior\ndistribution can be bounded in terms of the training error of the pre-trained\nscore function and the number of particles in the ensemble. Empirically, we\nvalidate our algorithm on several inverse problems in imaging to show that our\nmethod gives more accurate reconstructions compared to existing DM-based\nmethods.", "authors": ["Haoxuan Chen", "Yinuo Ren", "Martin Renqiang Min", "Lexing Ying", "Zachary Izzo"], "published_date": "2025-06-04", "title_zh": "基於擴散先驗的逆問題求解：一種無近似的集成採樣方法", "summary_zh": "擴散模型在高維分布建模中表現出色，廣泛應用於貝氏反問題，但現有方法依賴生成過程的啟發式近似。為充分利用擴散模型的生成能力，本文提出一種基於集成演算法的後驗抽樣方法，無需啟發式近似。該演算法結合擴散模型和序列蒙地卡羅方法，通過分析先驗在擴散過程中的演變，推導出控制後驗分布演變的偏微分方程，其中包含修正的擴散項和重加權項，可通過隨機加權粒子方法模擬。理論上，後驗分布誤差可基於預訓練分數函數的訓練誤差和集成粒子數量進行限制。實驗結果表明，在影像反問題中，該方法相較於現有基於擴散模型的方法，能提供更準確的重建結果。", "audio": "audios/2506.03979v2.mp3", "timestamp": "2025-06-06T01:27:52.754072"}
{"query": "AI", "id": "2506.03755v1", "url": "http://arxiv.org/abs/2506.03755v1", "title": "Misalignment or misuse? The AGI alignment tradeoff", "summary": "Creating systems that are aligned with our goals is seen as a leading\napproach to create safe and beneficial AI in both leading AI companies and the\nacademic field of AI safety. We defend the view that misaligned AGI - future,\ngenerally intelligent (robotic) AI agents - poses catastrophic risks. At the\nsame time, we support the view that aligned AGI creates a substantial risk of\ncatastrophic misuse by humans. While both risks are severe and stand in tension\nwith one another, we show that - in principle - there is room for alignment\napproaches which do not increase misuse risk. We then investigate how the\ntradeoff between misalignment and misuse looks empirically for different\ntechnical approaches to AI alignment. Here, we argue that many current\nalignment techniques and foreseeable improvements thereof plausibly increase\nrisks of catastrophic misuse. Since the impacts of AI depend on the social\ncontext, we close by discussing important social factors and suggest that to\nreduce the risk of a misuse catastrophe due to aligned AGI, techniques such as\nrobustness, AI control methods and especially good governance seem essential.", "authors": ["Max Hellrigel-Holderbaum", "Leonard Dung"], "published_date": "2025-06-04", "title_zh": "錯位抑或誤用？通用人工智慧對齊之權衡", "summary_zh": "創造與人類目標一致的系統被視為打造安全且有益人工智慧的主要途徑。失準的通用人工智慧（AGI）帶來災難性風險，而校準的AGI也存在被人類濫用的重大風險。儘管兩者風險嚴峻且相互制衡，原則上，仍有不增加濫用風險的校準方法。本文探討不同人工智慧校準技術在失準與濫用間的權衡，認為許多現行校準技術及其可預見的改進可能增加災難性濫用風險。由於人工智慧的影響取決於社會背景，本文最終討論重要社會因素，並建議降低校準AGI造成的濫用災難風險，穩健性、人工智慧控制方法及良好的治理至關重要。", "audio": "audios/2506.03755v1.mp3", "timestamp": "2025-06-06T03:15:14.933881"}
{"query": "Foundation Model", "id": "2506.01867v1", "url": "http://arxiv.org/abs/2506.01867v1", "title": "EEG Foundation Models for BCI Learn Diverse Features of Electrophysiology", "summary": "Brain computer interface (BCI) research, as well as increasing portions of\nthe field of neuroscience, have found success deploying large-scale artificial\nintelligence (AI) pre-training methods in conjunction with vast public\nrepositories of data. This approach of pre-training foundation models using\nlabel-free, self-supervised objectives offers the potential to learn robust\nrepresentations of neurophysiology, potentially addressing longstanding\nchallenges in neural decoding. However, to date, much of this work has focused\nexplicitly on standard BCI benchmarks and tasks, which likely overlooks the\nmultitude of features these powerful methods might learn about brain function\nas well as other electrophysiological information. We introduce a new method\nfor self-supervised BCI foundation model pre-training for EEG inspired by a\ntransformer-based approach adapted from the HuBERT framework originally\ndeveloped for speech processing. Our pipeline is specifically focused on\nlow-profile, real-time usage, involving minimally pre-processed data and just\neight EEG channels on the scalp. We show that our foundation model learned a\nrepresentation of EEG that supports standard BCI tasks (P300, motor imagery),\nbut also that this model learns features of neural data related to individual\nvariability, and other salient electrophysiological components (e.g., alpha\nrhythms). In addition to describing and evaluating a novel approach to\npre-training BCI models and neural decoding, this work opens the aperture for\nwhat kind of tasks and use-cases might exist for neural data in concert with\npowerful AI methods.", "authors": ["Mattson Ogg", "Rahul Hingorani", "Diego Luna", "Griffin W. Milsap", "William G. Coon", "Clara A. Scholl"], "published_date": "2025-06-02", "title_zh": "腦機介面腦電圖基礎模型學習電生理多樣化特徵", "summary_zh": "腦機介面研究及神經科學領域日益廣泛地採用大規模人工智慧預訓練方法，並結合龐大的公共數據儲存庫。這種使用無標籤、自監督目標預訓練基礎模型的方法，有望學習神經生理學的穩健表徵，從而應對神經解碼長期存在的挑戰。然而，目前的研究主要集中於標準腦機介面基準和任務，可能忽略了這些強大方法在學習腦功能及其他腦電生理資訊方面的潛力。本研究提出一種新的腦電圖自監督腦機介面基礎模型預訓練方法，其靈感來自於基於Transformer的模型，並改編自最初為語音處理開發的HuBERT框架。我們的流程專注於低調、即時應用，僅需最少量預處理數據和八個頭皮腦電通道。結果顯示，我們的基礎模型學習了一種腦電圖表徵，不僅支援標準腦機介面任務（P300、運動意象），還能學習與個體差異相關的神經數據特徵，以及其他顯著的腦電生理成分（例如，α節律）。本研究描述並評估了一種預訓練腦機介面模型和神經解碼的新方法，並為神經數據與強大人工智慧方法結合的潛在任務和用例開闢了新的視野。", "audio": "audios/2506.01867v1.mp3", "timestamp": "2025-06-06T03:15:29.551206"}
{"query": "Diffusion Model", "id": "2506.02858v2", "url": "http://arxiv.org/abs/2506.02858v2", "title": "DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization", "summary": "Language-queried Audio Source Separation (LASS) enables open-vocabulary sound\nseparation via natural language queries. While existing methods rely on\ntask-specific training, we explore whether pretrained diffusion models,\noriginally designed for audio generation, can inherently perform separation\nwithout further training. In this study, we introduce a training-free framework\nleveraging generative priors for zero-shot LASS. Analyzing naive adaptations,\nwe identify key limitations arising from modality-specific challenges. To\naddress these issues, we propose Diffusion-Guided Mask Optimization (DGMO), a\ntest-time optimization framework that refines spectrogram masks for precise,\ninput-aligned separation. Our approach effectively repurposes pretrained\ndiffusion models for source separation, achieving competitive performance\nwithout task-specific supervision. This work expands the application of\ndiffusion models beyond generation, establishing a new paradigm for zero-shot\naudio separation. The code is available at: https://wltschmrz.github.io/DGMO/", "authors": ["Geonyoung Lee", "Geonhee Han", "Paul Hongsuck Seo"], "published_date": "2025-06-03", "title_zh": "DGMO：基於擴散引導掩碼優化的免訓練音源分離", "summary_zh": "基於語言查詢的音訊源分離（LASS）透過自然語言查詢實現開放詞彙的聲音分離。現有方法依賴特定任務訓練，本文探索預訓練擴散模型（最初用於音訊生成）是否能內在地執行分離，而無需額外訓練。本研究提出一種免訓練框架，利用生成先驗知識進行零樣本LASS。分析初步調整後，發現模態特定挑戰導致的關鍵限制。為了解決這些問題，提出擴散引導的遮罩優化（DGMO），一種測試時優化框架，用於精確地對齊輸入的分離頻譜遮罩。此方法有效地將預訓練擴散模型重新用於源分離，在沒有特定任務監督下實現具競爭力的效能。本研究擴展了擴散模型在生成之外的應用，為零樣本音訊分離建立新的範式。程式碼位於：https://wltschmrz.github.io/DGMO/", "audio": "audios/2506.02858v2.mp3", "timestamp": "2025-06-06T03:15:41.259065"}
{"query": "AI", "id": "2506.03750v1", "url": "http://arxiv.org/abs/2506.03750v1", "title": "A Retrieval-Augmented Multi-Agent Framework for Psychiatry Diagnosis", "summary": "The application of AI in psychiatric diagnosis faces significant challenges,\nincluding the subjective nature of mental health assessments, symptom overlap\nacross disorders, and privacy constraints limiting data availability. To\naddress these issues, we present MoodAngels, the first specialized multi-agent\nframework for mood disorder diagnosis. Our approach combines granular-scale\nanalysis of clinical assessments with a structured verification process,\nenabling more accurate interpretation of complex psychiatric data.\nComplementing this framework, we introduce MoodSyn, an open-source dataset of\n1,173 synthetic psychiatric cases that preserves clinical validity while\nensuring patient privacy. Experimental results demonstrate that MoodAngels\noutperforms conventional methods, with our baseline agent achieving 12.3%\nhigher accuracy than GPT-4o on real-world cases, and our full multi-agent\nsystem delivering further improvements. Evaluation in the MoodSyn dataset\ndemonstrates exceptional fidelity, accurately reproducing both the core\nstatistical patterns and complex relationships present in the original data\nwhile maintaining strong utility for machine learning applications. Together,\nthese contributions provide both an advanced diagnostic tool and a critical\nresearch resource for computational psychiatry, bridging important gaps in\nAI-assisted mental health assessment.", "authors": ["Mengxi Xiao", "Mang Ye", "Ben Liu", "Xiaofen Zong", "He Li", "Jimin Huang", "Qianqian Xie", "Min Peng"], "published_date": "2025-06-04", "title_zh": "基於檢索增強的多智能體精神病學診斷框架", "summary_zh": "AI於精神科診斷應用面臨挑戰，包含評估主觀性、症狀重疊及數據隱私限制。為此，我們提出MoodAngels，首個專用於情緒障礙診斷的多代理框架。此方法結合細粒度臨床評估分析與結構化驗證，提升複雜精神科數據的詮釋準確性。同時，我們推出MoodSyn，一個含1173例合成精神科案例的開源數據集，在確保病人隱私的前提下，維持臨床有效性。實驗結果顯示，MoodAngels優於傳統方法，基準代理於真實案例中準確度較GPT-4o高出12.3%，完整多代理系統更進一步提升。MoodSyn數據集評估展現高度保真度，準確重現原始數據的核心統計模式及複雜關係，並維持機器學習應用的實用性。此研究提供先進診斷工具及重要研究資源，填補AI輔助心理健康評估的重要缺口。", "audio": "audios/2506.03750v1.mp3", "timestamp": "2025-06-06T04:25:17.684216"}
{"query": "Foundation Model", "id": "2506.01833v1", "url": "http://arxiv.org/abs/2506.01833v1", "title": "SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model", "summary": "Inspired by the success of unsupervised pre-training paradigms, researchers\nhave applied these approaches to DNA pre-training. However, we argue that these\napproaches alone yield suboptimal results because pure DNA sequences lack\nsufficient information, since their functions are regulated by genomic profiles\nlike chromatin accessibility. Here, we demonstrate that supervised training for\ngenomic profile prediction serves as a more effective alternative to pure\nsequence pre-training. Furthermore, considering the multi-species and\nmulti-profile nature of genomic profile prediction, we introduce our\n$\\textbf{S}$pecies-$\\textbf{P}$rofile $\\textbf{A}$daptive\n$\\textbf{C}$ollaborative $\\textbf{E}$xperts (SPACE) that leverages Mixture of\nExperts (MoE) to better capture the relationships between DNA sequences across\ndifferent species and genomic profiles, thereby learning more effective DNA\nrepresentations. Through extensive experiments across various tasks, our model\nachieves state-of-the-art performance, establishing that DNA models trained\nwith supervised genomic profiles serve as powerful DNA representation learners.\nThe code is available at https://github.com/ZhuJiwei111/SPACE.", "authors": ["Zhao Yang", "Jiwei Zhu", "Bing Su"], "published_date": "2025-06-02", "title_zh": "SPACE：基於強大DNA基礎模型的基因體譜預測器", "summary_zh": "受無監督預訓練啟發，研究者將其應用於DNA預訓練。我們認為單獨使用這些方法效果不佳，因為DNA序列缺乏足夠資訊，其功能受染色質可及性等基因組圖譜調控。我們證明，基因組圖譜預測的有監督訓練是更有效的序列預訓練替代方案。考慮到基因組圖譜預測的多物種和多圖譜特性，我們提出物種-圖譜自適應協作專家(SPACE)，利用混合專家模型(MoE)更好地捕捉不同物種和基因組圖譜間的DNA序列關係，從而學習更有效的DNA表示。大量實驗表明，我們的模型在多項任務中達到最先進水平，證明用有監督基因組圖譜訓練的DNA模型是強大的DNA表示學習器。程式碼可在https://github.com/ZhuJiwei111/SPACE取得。", "audio": "audios/2506.01833v1.mp3", "timestamp": "2025-06-06T04:25:26.541204"}
{"query": "Diffusion Model", "id": "2506.02488v2", "url": "http://arxiv.org/abs/2506.02488v2", "title": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for Efficient Diffusion Models", "summary": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality.", "authors": ["Hongtao Huang", "Xiaojun Chang", "Lina Yao"], "published_date": "2025-06-03", "title_zh": "Flexiffusion：高效擴散模型的免訓練分段式神經網路架構搜尋", "summary_zh": "擴散模型能生成高品質圖像，但迭代推論耗費大量計算。神經網路架構搜尋雖可優化擴散模型，然既有方法需重新訓練、逐層優化複雜度呈指數增長、且大規模圖像生成導致評估緩慢。本研究提出Flexiffusion，一個免訓練的架構搜尋框架，可在不修改預訓練參數下聯合優化生成排程與模型架構。核心概念是將生成過程分解為等長彈性區段，各區段動態結合完整計算、快取重用計算及跳過計算三種步驟類型。此區段式搜尋空間大幅降低候選池規模，同時保有架構多樣性。另提出相對FID (rFID)，一種輕量級評估指標，藉由測量與教師模型輸出的差異而非真實數據，減少90%以上評估時間。實驗結果顯示，Flexiffusion在ImageNet和MS-COCO上，於LDM、Stable Diffusion及DDPM等模型，皆達成至少2倍加速，FID下降低於5%，優於先前架構搜尋及快取方法。尤其在Stable Diffusion上，更達到5.1倍加速且CLIP分數幾乎不變。本研究為探索高速且高品質擴散模型，開創了一種資源高效的範例。", "audio": "audios/2506.02488v2.mp3", "timestamp": "2025-06-06T04:25:37.541270"}
{"query": "AI", "id": "2506.05341v1", "url": "http://arxiv.org/abs/2506.05341v1", "title": "Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning", "summary": "Realistic 3D indoor scene synthesis is vital for embodied AI and digital\ncontent creation. It can be naturally divided into two subtasks: object\ngeneration and layout generation. While recent generative models have\nsignificantly advanced object-level quality and controllability, layout\ngeneration remains challenging due to limited datasets. Existing methods either\noverfit to these datasets or rely on predefined constraints to optimize\nnumerical layout that sacrifice flexibility. As a result, they fail to generate\nscenes that are both open-vocabulary and aligned with fine-grained user\ninstructions. We introduce DirectLayout, a framework that directly generates\nnumerical 3D layouts from text descriptions using generalizable spatial\nreasoning of large language models (LLMs). DirectLayout decomposes the\ngeneration into three stages: producing a Bird's-Eye View (BEV) layout, lifting\nit into 3D space, and refining object placements. To enable explicit spatial\nreasoning and help the model grasp basic principles of object placement, we\nemploy Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.\nAdditionally, we design CoT-Grounded Generative Layout Reward to enhance\ngeneralization and spatial planning. During inference, DirectLayout addresses\nasset-layout mismatches via Iterative Asset-Layout Alignment through in-context\nlearning. Extensive experiments demonstrate that DirectLayout achieves\nimpressive semantic consistency, generalization and physical plausibility.", "authors": ["Xingjian Ran", "Yixuan Li", "Linning Xu", "Mulin Yu", "Bo Dai"], "published_date": "2025-06-05", "title_zh": "基於空間推理的三維室內場景合成之直接數值佈局生成", "summary_zh": "逼真3D室內場景合成對具身人工智慧與數位內容創作至關重要。其可分為物件生成和佈局生成兩項子任務。現有方法受限於資料集，難以生成符合細緻使用者指令的開放詞彙場景。DirectLayout 框架利用大型語言模型 (LLM) 的通用空間推理能力，直接從文字描述生成數值 3D 佈局。該方法分解生成過程為三階段：生成鳥瞰圖佈局、提升至 3D 空間及精細調整物件位置。為增強空間推理能力，採用基於 3D-Front 資料集的思維鏈 (CoT) 激活。此外，設計 CoT 基礎生成佈局獎勵以提升泛化性和空間規劃能力。推論階段，透過上下文學習的迭代資產-佈局對齊來解決資產-佈局不匹配問題。實驗結果表明，DirectLayout 實現了出色的語義一致性、泛化性和物理合理性。", "audio": "audios/2506.05341v1.mp3", "timestamp": "2025-06-06T05:19:14.292896"}
{"query": "Foundation Model", "id": "2506.05321v1", "url": "http://arxiv.org/abs/2506.05321v1", "title": "LSM-2: Learning from Incomplete Wearable Sensor Data", "summary": "Foundation models, a cornerstone of recent advancements in machine learning,\nhave predominantly thrived on complete and well-structured data. Wearable\nsensor data frequently suffers from significant missingness, posing a\nsubstantial challenge for self-supervised learning (SSL) models that typically\nassume complete data inputs. This paper introduces the second generation of\nLarge Sensor Model (LSM-2) with Adaptive and Inherited Masking (AIM), a novel\nSSL approach that learns robust representations directly from incomplete data\nwithout requiring explicit imputation. AIM's core novelty lies in its use of\nlearnable mask tokens to model both existing (\"inherited\") and artificially\nintroduced missingness, enabling it to robustly handle fragmented real-world\ndata during inference. Pre-trained on an extensive dataset of 40M hours of\nday-long multimodal sensor data, our LSM-2 with AIM achieves the best\nperformance across a diverse range of tasks, including classification,\nregression and generative modeling. Furthermore, LSM-2 with AIM exhibits\nsuperior scaling performance, and critically, maintains high performance even\nunder targeted missingness scenarios, reflecting clinically coherent patterns,\nsuch as the diagnostic value of nighttime biosignals for hypertension\nprediction. This makes AIM a more reliable choice for real-world wearable data\napplications.", "authors": ["Maxwell A. Xu", "Girish Narayanswamy", "Kumar Ayush", "Dimitris Spathis", "Shun Liao", "Shyam A. Tailor", "Ahmed Metwally", "A. Ali Heydari", "Yuwei Zhang", "Jake Garrison", "Samy Abdel-Ghaffar", "Xuhai Xu", "Ken Gu", "Jacob Sunshine", "Ming-Zher Poh", "Yun Liu", "Tim Althoff", "Shrikanth Narayanan", "Pushmeet Kohli", "Mark Malhotra", "Shwetak Patel", "Yuzhe Yang", "James M. Rehg", "Xin Liu", "Daniel McDuff"], "published_date": "2025-06-05", "title_zh": "LSM-2：自不完整穿戴式感測器資料之學習", "summary_zh": "基於完整結構化資料的大型模型是近期機器學習進展的基石。然而，穿戴式感測器數據常有大量缺失值，對假設完整資料輸入的自監督學習模型構成挑戰。本文提出第二代大型感測器模型（LSM-2），採用自適應與繼承遮罩（AIM）的新穎自監督學習方法，直接從不完整數據中學習穩健表徵，無需顯式填補。AIM的核心創新在於使用可學習的遮罩令牌來建模既有的與人為引入的缺失值，使其在推論時能夠穩健處理分散的真實世界數據。LSM-2在包含四千萬小時日長多模態感測器數據的大型資料集上預訓練後，在分類、迴歸和生成模型等多項任務中均達到最佳效能。此外，LSM-2展現卓越的擴展性，且即使在有針對性的缺失情境下，也能維持高效能，反映臨床上相關的模式，例如夜間生理訊號對高血壓預測的診斷價值。這使得AIM成為真實世界穿戴式數據應用更可靠的選擇。", "audio": "audios/2506.05321v1.mp3", "timestamp": "2025-06-06T05:19:20.821693"}
{"query": "Diffusion Model", "id": "2506.05350v1", "url": "http://arxiv.org/abs/2506.05350v1", "title": "Contrastive Flow Matching", "summary": "Unconditional flow-matching trains diffusion models to transport samples from\na source distribution to a target distribution by enforcing that the flows\nbetween sample pairs are unique. However, in conditional settings (e.g.,\nclass-conditioned models), this uniqueness is no longer guaranteed--flows from\ndifferent conditions may overlap, leading to more ambiguous generations. We\nintroduce Contrastive Flow Matching, an extension to the flow matching\nobjective that explicitly enforces uniqueness across all conditional flows,\nenhancing condition separation. Our approach adds a contrastive objective that\nmaximizes dissimilarities between predicted flows from arbitrary sample pairs.\nWe validate Contrastive Flow Matching by conducting extensive experiments\nacross varying model architectures on both class-conditioned (ImageNet-1k) and\ntext-to-image (CC3M) benchmarks. Notably, we find that training models with\nContrastive Flow Matching (1) improves training speed by a factor of up to 9x,\n(2) requires up to 5x fewer de-noising steps and (3) lowers FID by up to 8.9\ncompared to training the same models with flow matching. We release our code\nat: https://github.com/gstoica27/DeltaFM.git.", "authors": ["George Stoica", "Vivek Ramanujan", "Xiang Fan", "Ali Farhadi", "Ranjay Krishna", "Judy Hoffman"], "published_date": "2025-06-05", "title_zh": "對比流匹配", "summary_zh": "無條件流匹配訓練擴散模型，透過確保樣本對間流動的唯一性，將樣本從源分佈轉移到目標分佈。然而，在條件設定下（例如，類別條件模型），這種唯一性不再保證，不同條件下的流動可能重疊，導致生成結果更模糊。本研究提出對比流匹配，擴展流匹配目標，顯式強化所有條件流動間的唯一性，增強條件分離。此方法加入對比目標，最大化任意樣本對預測流動間的差異性。實驗驗證顯示，對比流匹配在類別條件（ImageNet-1k）和文本到圖像（CC3M）基準測試中，於不同模型架構下，能將訓練速度提升高達9倍，降噪步驟減少高達5倍，並降低FID分數高達8.9。程式碼已公開。", "audio": "audios/2506.05350v1.mp3", "timestamp": "2025-06-06T05:19:26.335342"}
{"query": "AI", "id": "2506.05334v1", "url": "http://arxiv.org/abs/2506.05334v1", "title": "Search Arena: Analyzing Search-Augmented LLMs", "summary": "Search-augmented language models combine web search with Large Language\nModels (LLMs) to improve response groundedness and freshness. However,\nanalyzing these systems remains challenging: existing datasets are limited in\nscale and narrow in scope, often constrained to static, single-turn,\nfact-checking questions. In this work, we introduce Search Arena, a\ncrowd-sourced, large-scale, human-preference dataset of over 24,000 paired\nmulti-turn user interactions with search-augmented LLMs. The dataset spans\ndiverse intents and languages, and contains full system traces with around\n12,000 human preference votes. Our analysis reveals that user preferences are\ninfluenced by the number of citations, even when the cited content does not\ndirectly support the attributed claims, uncovering a gap between perceived and\nactual credibility. Furthermore, user preferences vary across cited sources,\nrevealing that community-driven platforms are generally preferred and static\nencyclopedic sources are not always appropriate and reliable. To assess\nperformance across different settings, we conduct cross-arena analyses by\ntesting search-augmented LLMs in a general-purpose chat environment and\nconventional LLMs in search-intensive settings. We find that web search does\nnot degrade and may even improve performance in non-search settings; however,\nthe quality in search settings is significantly affected if solely relying on\nthe model's parametric knowledge. We open-sourced the dataset to support future\nresearch in this direction. Our dataset and code are available at:\nhttps://github.com/lmarena/search-arena.", "authors": ["Mihran Miroyan", "Tsung-Han Wu", "Logan King", "Tianle Li", "Jiayi Pan", "Xinyan Hu", "Wei-Lin Chiang", "Anastasios N. Angelopoulos", "Trevor Darrell", "Narges Norouzi", "Joseph E. Gonzalez"], "published_date": "2025-06-05", "title_zh": "搜尋場域：分析搜尋增強型大型語言模型", "summary_zh": "本研究提出Search Arena，一個包含超過24,000組搜尋增強型語言模型多輪互動的大規模人工偏好資料集。該資料集涵蓋多樣意圖與語言，包含完整的系統追蹤和約12,000個人工偏好投票。分析顯示，引用數量會影響使用者偏好，即使引用內容並未直接支持主張，顯示感知可信度與實際可信度間存在差距。此外，使用者偏好因引用來源而異，社群平台通常更受青睞，而靜態百科全書式來源並非總是適當可靠。跨領域分析顯示，網路搜尋在非搜尋環境中不會降低效能，甚至可能有所提升；然而，若僅依賴模型參數知識，搜尋環境中的品質將顯著下降。該資料集已開源，旨在支持相關研究。", "audio": "audios/2506.05334v1.mp3", "timestamp": "2025-06-06T06:28:40.901368"}
{"query": "Foundation Model", "id": "2506.05263v1", "url": "http://arxiv.org/abs/2506.05263v1", "title": "Can Foundation Models Generalise the Presentation Attack Detection Capabilities on ID Cards?", "summary": "Nowadays, one of the main challenges in presentation attack detection (PAD)\non ID cards is obtaining generalisation capabilities for a diversity of\ncountries that are issuing ID cards. Most PAD systems are trained on one, two,\nor three ID documents because of privacy protection concerns. As a result, they\ndo not obtain competitive results for commercial purposes when tested in an\nunknown new ID card country. In this scenario, Foundation Models (FM) trained\non huge datasets can help to improve generalisation capabilities. This work\nintends to improve and benchmark the capabilities of FM and how to use them to\nadapt the generalisation on PAD of ID Documents. Different test protocols were\nused, considering zero-shot and fine-tuning and two different ID card datasets.\nOne private dataset based on Chilean IDs and one open-set based on three ID\ncountries: Finland, Spain, and Slovakia. Our findings indicate that bona fide\nimages are the key to generalisation.", "authors": ["Juan E. Tapia", "Christoph Busch"], "published_date": "2025-06-05", "title_zh": "基礎模型能否泛化身分證件上的呈現攻擊偵測能力？", "summary_zh": "當前身份證件呈現攻擊檢測（PAD）的主要挑戰之一，是如何在多樣化國家發行的身份證上實現泛化能力。由於隱私保護考量，多數PAD系統僅基於少量身份證件進行訓練，導致在新發行國家身份證上的商業應用效果不佳。本研究旨在評估和提升基礎模型（FM）在此領域的泛化能力，透過零樣本學習和微調等不同測試方法，並採用智利身份證私有數據集以及包含芬蘭、西班牙和斯洛伐克身份證的公開數據集進行驗證。研究結果表明，真實圖像對於提升泛化能力至關重要。", "audio": "audios/2506.05263v1.mp3", "timestamp": "2025-06-06T06:28:46.055215"}
{"query": "Diffusion Model", "id": "2506.05340v1", "url": "http://arxiv.org/abs/2506.05340v1", "title": "Exploring Diffusion Transformer Designs via Grafting", "summary": "Designing model architectures requires decisions such as selecting operators\n(e.g., attention, convolution) and configurations (e.g., depth, width).\nHowever, evaluating the impact of these decisions on model quality requires\ncostly pretraining, limiting architectural investigation. Inspired by how new\nsoftware is built on existing code, we ask: can new architecture designs be\nstudied using pretrained models? To this end, we present grafting, a simple\napproach for editing pretrained diffusion transformers (DiTs) to materialize\nnew architectures under small compute budgets. Informed by our analysis of\nactivation behavior and attention locality, we construct a testbed based on the\nDiT-XL/2 design to study the impact of grafting on model quality. Using this\ntestbed, we develop a family of hybrid designs via grafting: replacing softmax\nattention with gated convolution, local attention, and linear attention, and\nreplacing MLPs with variable expansion ratio and convolutional variants.\nNotably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for\nDiT-XL/2) using <2% pretraining compute. We then graft a text-to-image model\n(PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval\nscore. Finally, we present a case study that restructures DiT-XL/2 by\nconverting every pair of sequential transformer blocks into parallel blocks via\ngrafting. This reduces model depth by 2x and yields better quality (FID: 2.77)\nthan other models of comparable depth. Together, we show that new diffusion\nmodel designs can be explored by grafting pretrained DiTs, with edits ranging\nfrom operator replacement to architecture restructuring. Code and grafted\nmodels: https://grafting.stanford.edu", "authors": ["Keshigeyan Chandrasegaran", "Michael Poli", "Daniel Y. Fu", "Dongjun Kim", "Lea M. Hadzic", "Manling Li", "Agrim Gupta", "Stefano Massaroli", "Azalia Mirhoseini", "Juan Carlos Niebles", "Stefano Ermon", "Li Fei-Fei"], "published_date": "2025-06-05", "title_zh": "基於嫁接探索擴散Transformer設計", "summary_zh": "模型架構設計涉及算子選擇與配置，評估其影響需耗費大量預訓練資源。本研究受軟體開發啟發，探討能否利用預訓練模型研究新架構。我們提出嫁接法，以低算力成本編輯預訓練擴散轉換器 (DiT)，實現新架構。基於對激活行為與注意力局部性的分析，我們構建了DiT-XL/2測試平台，研究嫁接對模型品質的影響。我們透過嫁接開發了一系列混合設計，包括替換Softmax注意力為門控卷積、局部注意力和線性注意力，以及用可變擴展率和卷積變體替換MLP。許多混合設計在<2%預訓練算力下達到良好品質。我們將文本到圖像模型 (PixArt-Sigma) 進行嫁接，在GenEval分數下降<2%的情況下，速度提升1.43倍。我們亦透過嫁接將DiT-XL/2中每對連續轉換器塊轉換為平行塊，深度減少2倍，並獲得比其他同深度模型更好的品質。研究表明，可透過嫁接預訓練DiT探索新的擴散模型設計，編輯範圍涵蓋算子替換到架構重構。", "audio": "audios/2506.05340v1.mp3", "timestamp": "2025-06-06T06:28:55.134395"}
{"query": "AI", "id": "2506.05333v1", "url": "http://arxiv.org/abs/2506.05333v1", "title": "Kinetics: Rethinking Test-Time Scaling Laws", "summary": "We rethink test-time scaling laws from a practical efficiency perspective,\nrevealing that the effectiveness of smaller models is significantly\noverestimated. Prior work, grounded in compute-optimality, overlooks critical\nmemory access bottlenecks introduced by inference-time strategies (e.g.,\nBest-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to\n32B parameters, reveals a new Kinetics Scaling Law that better guides resource\nallocation by incorporating both computation and memory access costs. Kinetics\nScaling Law suggests that test-time compute is more effective when used on\nmodels above a threshold than smaller ones. A key reason is that in TTS,\nattention, rather than parameter count, emerges as the dominant cost factor.\nMotivated by this, we propose a new scaling paradigm centered on sparse\nattention, which lowers per-token cost and enables longer generations and more\nparallel samples within the same resource budget. Empirically, we show that\nsparse attention models consistently outperform dense counterparts, achieving\nover 60 points gains in low-cost regimes and over 5 points gains in high-cost\nregimes for problem-solving accuracy on AIME, encompassing evaluations on\nstate-of-the-art MoEs. These results suggest that sparse attention is essential\nfor realizing the full potential of test-time scaling because, unlike training,\nwhere parameter scaling saturates, test-time accuracy continues to improve\nthrough increased generation. The code is available at\nhttps://github.com/Infini-AI-Lab/Kinetics.", "authors": ["Ranajoy Sadhukhan", "Zhuoming Chen", "Haizhong Zheng", "Yang Zhou", "Emma Strubell", "Beidi Chen"], "published_date": "2025-06-05", "title_zh": "動力學：重新思考測試時縮放定律", "summary_zh": "本研究從實用效率角度重新評估測試時期的規模法則，指出小型模型效能被顯著高估。過往基於運算最佳化的研究忽略了推論時策略（如Best-of-$N$、長鏈思維）引入的記憶體存取瓶頸。我們對0.6B到32B參數模型進行整體分析，揭示了一種新的動態規模法則，透過納入運算和記憶體存取成本，更好地指導資源分配。動態規模法則表明，在超過一定閾值的模型上使用測試時運算比在較小模型上更有效。主要原因是，在測試時，注意力機制而非參數數量成為主要成本因素。因此，我們提出一種以稀疏注意力為中心的新規模範式，降低了每次 token 的成本，並在相同資源預算內實現更長的生成和更多的平行樣本。實驗結果表明，稀疏注意力模型始終優於密集模型，在低成本情況下，AIME 問題解決準確度提升超過60分，在高成本情況下提升超過5分，並涵蓋了對最先進的 MoE 模型的評估。這些結果表明，稀疏注意力對於充分發揮測試時規模的潛力至關重要，因為與訓練不同，參數規模擴展會達到飽和，而測試時準確度會通過增加生成量而持續提高。", "audio": "audios/2506.05333v1.mp3", "timestamp": "2025-06-06T07:18:31.710999"}
{"query": "Foundation Model", "id": "2506.05210v1", "url": "http://arxiv.org/abs/2506.05210v1", "title": "Towards Vision-Language-Garment Models For Web Knowledge Garment Understanding and Generation", "summary": "Multimodal foundation models have demonstrated strong generalization, yet\ntheir ability to transfer knowledge to specialized domains such as garment\ngeneration remains underexplored. We introduce VLG, a vision-language-garment\nmodel that synthesizes garments from textual descriptions and visual imagery.\nOur experiments assess VLG's zero-shot generalization, investigating its\nability to transfer web-scale reasoning to unseen garment styles and prompts.\nPreliminary results indicate promising transfer capabilities, highlighting the\npotential for multimodal foundation models to adapt effectively to specialized\ndomains like fashion design.", "authors": ["Jan Ackermann", "Kiyohiro Nakayama", "Guandao Yang", "Tong Wu", "Gordon Wetzstein"], "published_date": "2025-06-05", "title_zh": "邁向視覺-語言-服裝模型：用於網路知識服裝理解與生成", "summary_zh": "多模態基礎模型展現強大泛化能力，然其於服裝生成等專門領域之知識轉移能力仍待探討。本文提出VLG，一視覺語言服裝模型，可由文本描述及視覺圖像合成服裝。實驗評估VLG之零樣本泛化能力，檢視其將網路規模推理轉移至未見服裝風格及提示之能力。初步結果顯示具備潛力之轉移能力，突顯多模態基礎模型有效適應時尚設計等專門領域之可能。", "audio": "audios/2506.05210v1.mp3", "timestamp": "2025-06-06T07:18:36.373911"}
{"query": "Diffusion Model", "id": "2506.05231v1", "url": "http://arxiv.org/abs/2506.05231v1", "title": "Progressive Tempering Sampler with Diffusion", "summary": "Recent research has focused on designing neural samplers that amortize the\nprocess of sampling from unnormalized densities. However, despite significant\nadvancements, they still fall short of the state-of-the-art MCMC approach,\nParallel Tempering (PT), when it comes to the efficiency of target evaluations.\nOn the other hand, unlike a well-trained neural sampler, PT yields only\ndependent samples and needs to be rerun -- at considerable computational cost\n-- whenever new samples are required. To address these weaknesses, we propose\nthe Progressive Tempering Sampler with Diffusion (PTSD), which trains diffusion\nmodels sequentially across temperatures, leveraging the advantages of PT to\nimprove the training of neural samplers. We also introduce a novel method to\ncombine high-temperature diffusion models to generate approximate\nlower-temperature samples, which are minimally refined using MCMC and used to\ntrain the next diffusion model. PTSD enables efficient reuse of sample\ninformation across temperature levels while generating well-mixed, uncorrelated\nsamples. Our method significantly improves target evaluation efficiency,\noutperforming diffusion-based neural samplers.", "authors": ["Severi Rissanen", "RuiKang OuYang", "Jiajun He", "Wenlin Chen", "Markus Heinonen", "Arno Solin", "José Miguel Hernández-Lobato"], "published_date": "2025-06-05", "title_zh": "具擴散的漸進退火採樣器", "summary_zh": "近期研究致力於設計神經採樣器，以分攤從未歸一化密度中採樣的過程。儘管取得顯著進展，但在目標評估效率方面，仍不如最先進的馬可夫鏈蒙地卡羅方法，平行迴火法。另一方面，與訓練良好的神經採樣器不同，平行迴火法僅產生相關樣本，且每次需要新樣本時都需重新運行，計算成本高昂。為了解決這些弱點，我們提出基於擴散的漸進迴火採樣器，該方法在不同溫度下依序訓練擴散模型，利用平行迴火法的優勢來改進神經採樣器的訓練。我們還引入一種新方法，結合高溫擴散模型以生成近似的較低溫樣本，這些樣本經由馬可夫鏈蒙地卡羅法進行最小程度的精煉，並用於訓練下一個擴散模型。基於擴散的漸進迴火採樣器能夠有效重複利用跨溫度層級的樣本資訊，同時產生混合良好且不相關的樣本。我們的研究顯著提升了目標評估效率，優於基於擴散的神經採樣器。", "audio": "audios/2506.05231v1.mp3", "timestamp": "2025-06-06T07:18:44.999690"}
{"query": "AI", "id": "2506.05325v1", "url": "http://arxiv.org/abs/2506.05325v1", "title": "Seeing the Invisible: Machine learning-Based QPI Kernel Extraction via Latent Alignment", "summary": "Quasiparticle interference (QPI) imaging is a powerful tool for probing\nelectronic structures in quantum materials, but extracting the single-scatterer\nQPI pattern (i.e., the kernel) from a multi-scatterer image remains a\nfundamentally ill-posed inverse problem. In this work, we propose the first\nAI-based framework for QPI kernel extraction. We introduce a two-step learning\nstrategy that decouples kernel representation learning from\nobservation-to-kernel inference. In the first step, we train a variational\nautoencoder to learn a compact latent space of scattering kernels. In the\nsecond step, we align the latent representation of QPI observations with those\nof the pre-learned kernels using a dedicated encoder. This design enables the\nmodel to infer kernels robustly even under complex, entangled scattering\nconditions. We construct a diverse and physically realistic QPI dataset\ncomprising 100 unique kernels and evaluate our method against a direct one-step\nbaseline. Experimental results demonstrate that our approach achieves\nsignificantly higher extraction accuracy, and improved generalization to unseen\nkernels.", "authors": ["Yingshuai Ji", "Haomin Zhuang", "Matthew Toole", "James McKenzie", "Xiaolong Liu", "Xiangliang Zhang"], "published_date": "2025-06-05", "title_zh": "看見不可見：基於潛在對齊的機器學習QPI核心提取", "summary_zh": "準粒子干涉(QPI)成像能有效探測量子材料的電子結構，但從多重散射圖像中提取單散射體QPI模式(即核心)本質上是不適定的逆問題。本文提出首個基於人工智慧的QPI核心提取框架，採用兩階段學習策略，將核心表示學習與觀測至核心推論解耦。第一階段，訓練變分自編碼器以學習散射核心的緊湊潛在空間。第二階段，使用專用編碼器將QPI觀測的潛在表示與預先學習的核心對齊。此設計使模型即使在複雜、糾纏的散射條件下也能穩健地推斷核心。我們構建包含100個獨特核心的多樣且物理上真實的QPI數據集，並將我們的方法與直接的單階段基準線進行評估。實驗結果表明，我們的方法實現了顯著更高的提取準確性，並改善了對未見核心的泛化能力。", "audio": "audios/2506.05325v1.mp3", "timestamp": "2025-06-06T08:25:21.253877"}
{"query": "Foundation Model", "id": "2506.05184v1", "url": "http://arxiv.org/abs/2506.05184v1", "title": "Single GPU Task Adaptation of Pathology Foundation Models for Whole Slide Image Analysis", "summary": "Pathology foundation models (PFMs) have emerged as powerful tools for\nanalyzing whole slide images (WSIs). However, adapting these pretrained PFMs\nfor specific clinical tasks presents considerable challenges, primarily due to\nthe availability of only weak (WSI-level) labels for gigapixel images,\nnecessitating multiple instance learning (MIL) paradigm for effective WSI\nanalysis. This paper proposes a novel approach for single-GPU \\textbf{T}ask\n\\textbf{A}daptation of \\textbf{PFM}s (TAPFM) that uses vision transformer\n(\\vit) attention for MIL aggregation while optimizing both for feature\nrepresentations and attention weights. The proposed approach maintains separate\ncomputational graphs for MIL aggregator and the PFM to create stable training\ndynamics that align with downstream task objectives during end-to-end\nadaptation. Evaluated on mutation prediction tasks for bladder cancer and lung\nadenocarcinoma across institutional and TCGA cohorts, TAPFM consistently\noutperforms conventional approaches, with H-Optimus-0 (TAPFM) outperforming the\nbenchmarks. TAPFM effectively handles multi-label classification of actionable\nmutations as well. Thus, TAPFM makes adaptation of powerful pre-trained PFMs\npractical on standard hardware for various clinical applications.", "authors": ["Neeraj Kumar", "Swaraj Nanda", "Siddharth Singi", "Jamal Benhamida", "David Kim", "Jie-Fu Chen", "Amir Momeni-Boroujeni", "Gregory M. Goldgof", "Gabriele Campanella", "Chad Vanderbilt"], "published_date": "2025-06-05", "title_zh": "用於全玻片影像分析之病理學基礎模型之單GPU任務適應", "summary_zh": "病理基礎模型(PFM)已成為分析全玻片影像(WSI)的有力工具。然而，將這些預訓練PFM應用於特定臨床任務面臨挑戰，主因是巨像素影像僅有弱標籤(WSI層級)，需採用多實例學習(MIL)模式。本文提出單GPU任務適應PFM(TAPFM)新方法，利用視覺轉換器(ViT)注意力進行MIL聚合，同時優化特徵表示和注意力權重。此方法為MIL聚合器和PFM維護獨立的計算圖，以建立穩定的訓練動態，使其在端對端適應期間與下游任務目標對齊。在膀胱癌和肺腺癌的突變預測任務中，TAPFM在機構和TCGA隊列中均優於傳統方法，其中H-Optimus-0 (TAPFM)超越基準。TAPFM有效處理可操作突變的多標籤分類。因此，TAPFM使強大的預訓練PFM能在標準硬體上實際應用於各種臨床應用。", "audio": "audios/2506.05184v1.mp3", "timestamp": "2025-06-06T08:25:30.519994"}
{"query": "Diffusion Model", "id": "2506.05204v1", "url": "http://arxiv.org/abs/2506.05204v1", "title": "OGGSplat: Open Gaussian Growing for Generalizable Reconstruction with Expanded Field-of-View", "summary": "Reconstructing semantic-aware 3D scenes from sparse views is a challenging\nyet essential research direction, driven by the demands of emerging\napplications such as virtual reality and embodied AI. Existing per-scene\noptimization methods require dense input views and incur high computational\ncosts, while generalizable approaches often struggle to reconstruct regions\noutside the input view cone. In this paper, we propose OGGSplat, an open\nGaussian growing method that expands the field-of-view in generalizable 3D\nreconstruction. Our key insight is that the semantic attributes of open\nGaussians provide strong priors for image extrapolation, enabling both semantic\nconsistency and visual plausibility. Specifically, once open Gaussians are\ninitialized from sparse views, we introduce an RGB-semantic consistent\ninpainting module applied to selected rendered views. This module enforces\nbidirectional control between an image diffusion model and a semantic diffusion\nmodel. The inpainted regions are then lifted back into 3D space for efficient\nand progressive Gaussian parameter optimization. To evaluate our method, we\nestablish a Gaussian Outpainting (GO) benchmark that assesses both semantic and\ngenerative quality of reconstructed open-vocabulary scenes. OGGSplat also\ndemonstrates promising semantic-aware scene reconstruction capabilities when\nprovided with two view images captured directly from a smartphone camera.", "authors": ["Yanbo Wang", "Ziyi Wang", "Wenzhao Zheng", "Jie Zhou", "Jiwen Lu"], "published_date": "2025-06-05", "title_zh": "OGGSplat：用於廣泛視野下具泛化性重建的開放式高斯增長", "summary_zh": "從稀疏視角重建具備語義認知的3D場景極具挑戰，但也至關重要，受虛擬實境和具身人工智慧等新興應用所驅動。現有基於場景優化的方法需要密集的輸入視角且計算成本高昂，而可泛化的方法則難以重建輸入視錐之外的區域。本文提出OGGSplat，一種開放式高斯成長方法，旨在擴展可泛化3D重建的視野。核心概念是開放高斯的語義屬性為圖像外推提供強先驗，實現語義一致性和視覺合理性。具體而言，在從稀疏視角初始化開放高斯後，引入一個RGB-語義一致的修復模組應用於選定的渲染視圖，該模組在圖像擴散模型和語義擴散模型之間強制執行雙向控制。然後將修復區域提升回3D空間，以進行有效且漸進的高斯參數優化。為評估該方法，建立了一個高斯外繪（GO）基準，用於評估重建的開放詞彙場景的語義和生成品質。當使用智慧型手機相機直接拍攝的兩個視圖圖像時，OGGSplat也展示了有前景的語義感知場景重建能力。", "audio": "audios/2506.05204v1.mp3", "timestamp": "2025-06-06T08:25:39.881259"}
{"query": "AI", "id": "2506.05305v1", "url": "http://arxiv.org/abs/2506.05305v1", "title": "ProRefine: Inference-time Prompt Refinement with Textual Feedback", "summary": "Agentic workflows, where multiple AI agents collaborate to accomplish complex\ntasks like reasoning or planning, are becoming increasingly prevalent. However,\nthese workflows often suffer from error propagation and sub-optimal\nperformance, largely due to poorly designed prompts that fail to effectively\nguide individual agents. This is a critical problem because it limits the\nreliability and scalability of these powerful systems. We introduce ProRefine,\nan innovative inference-time prompt optimization method that leverages textual\nfeedback from large language models (LLMs) to address this challenge. ProRefine\ndynamically refines prompts for multi-step reasoning tasks without additional\ntraining or ground truth labels. Evaluated on five benchmark mathematical\nreasoning datasets, ProRefine significantly surpasses zero-shot\nChain-of-Thought baselines by 3 to 37 percentage points. This approach not only\nboosts accuracy but also allows smaller models to match the performance of\nlarger ones, highlighting its potential for efficient and scalable AI\ndeployment, and democratizing access to high-performing AI.", "authors": ["Deepak Pandita", "Tharindu Cyril Weerasooriya", "Ankit Parag Shah", "Christopher M. Homan", "Wei Wei"], "published_date": "2025-06-05", "title_zh": "ProRefine：基於文本反饋的推理時提示詞精煉", "summary_zh": "主動代理工作流日益普及，但常因提示設計不佳導致錯誤傳播和效能降低。為了解決此問題，我們提出ProRefine，一種推理時提示優化方法，利用大型語言模型的文本回饋，動態精煉多步驟推理任務的提示，無需額外訓練或真實標籤。在五個數學推理資料集上的評估顯示，ProRefine顯著優於零樣本思維鏈基準線，效能提升3%至37%。此方法不僅提高準確性，還使較小模型能達到較大模型的效能，突顯其在高效且可擴展的AI部署以及普及高性能AI方面的潛力。", "audio": "audios/2506.05305v1.mp3", "timestamp": "2025-06-06T09:20:25.286905"}
{"query": "Foundation Model", "id": "2506.05176v1", "url": "http://arxiv.org/abs/2506.05176v1", "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models", "summary": "In this work, we introduce the Qwen3 Embedding series, a significant\nadvancement over its predecessor, the GTE-Qwen series, in text embedding and\nreranking capabilities, built upon the Qwen3 foundation models. Leveraging the\nQwen3 LLMs' robust capabilities in multilingual text understanding and\ngeneration, our innovative multi-stage training pipeline combines large-scale\nunsupervised pre-training with supervised fine-tuning on high-quality datasets.\nEffective model merging strategies further ensure the robustness and\nadaptability of the Qwen3 Embedding series. During the training process, the\nQwen3 LLMs serve not only as backbone models but also play a crucial role in\nsynthesizing high-quality, rich, and diverse training data across multiple\ndomains and languages, thus enhancing the training pipeline. The Qwen3\nEmbedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both\nembedding and reranking tasks, addressing diverse deployment scenarios where\nusers can optimize for either efficiency or effectiveness. Empirical\nevaluations demonstrate that the Qwen3 Embedding series achieves\nstate-of-the-art results across diverse benchmarks. Notably, it excels on the\nmultilingual evaluation benchmark MTEB for text embedding, as well as in\nvarious retrieval tasks, including code retrieval, cross-lingual retrieval and\nmultilingual retrieval. To facilitate reproducibility and promote\ncommunity-driven research and development, the Qwen3 Embedding models are\npublicly available under the Apache 2.0 license.", "authors": ["Yanzhao Zhang", "Mingxin Li", "Dingkun Long", "Xin Zhang", "Huan Lin", "Baosong Yang", "Pengjun Xie", "An Yang", "Dayiheng Liu", "Junyang Lin", "Fei Huang", "Jingren Zhou"], "published_date": "2025-06-05", "title_zh": "Qwen3嵌入：基於基礎模型推進文本嵌入與重排序", "summary_zh": "本文介紹基於Qwen3基礎模型構建的Qwen3 Embedding系列，其在文本嵌入和重排序能力上超越了GTE-Qwen系列。該系列利用Qwen3 LLM在多語文本理解和生成方面的能力，採用多階段訓練流程，結合大規模無監督預訓練和高品質數據集的監督微調。有效的模型合併策略確保了其穩健性和適應性。Qwen3 LLM不僅作為骨幹模型，還在跨多領域和語言合成高質量、豐富多樣的訓練數據方面發揮關鍵作用，從而增強了訓練流程。Qwen3 Embedding系列提供多種模型尺寸（0.6B、4B、8B），適用於嵌入和重排序任務，滿足不同部署場景下效率或效果的優化需求。實驗評估表明，該系列在多項基準測試中取得了領先成果，尤其在多語文本嵌入基準測試MTEB以及程式碼檢索、跨語言檢索和多語檢索等任務中表現出色。Qwen3 Embedding模型已根據Apache 2.0許可公開發布，以促進可重複性並推動社群驅動的研究和開發。", "audio": "audios/2506.05176v1.mp3", "timestamp": "2025-06-06T09:20:33.557746"}
{"query": "Diffusion Model", "id": "2506.05198v1", "url": "http://arxiv.org/abs/2506.05198v1", "title": "Quantifying Cross-Modality Memorization in Vision-Language Models", "summary": "Understanding what and how neural networks memorize during training is\ncrucial, both from the perspective of unintentional memorization of potentially\nsensitive information and from the standpoint of effective knowledge\nacquisition for real-world, knowledge-intensive tasks. While previous studies\nprimarily investigate memorization within a single modality, such as text\nmemorization in large language models or image memorization in diffusion\nmodels, unified multimodal models are becoming increasingly prevalent in\npractical applications. In this work, we focus on the unique characteristics of\ncross-modality memorization and conduct a systematic study centered on\nvision-language models. To facilitate controlled experiments, we first\nintroduce a synthetic persona dataset comprising diverse synthetic person\nimages and textual descriptions. We quantify factual knowledge memorization and\ncross-modal transferability by training models on a single modality and\nevaluating their performance in the other. Our results reveal that facts\nlearned in one modality transfer to the other, but a significant gap exists\nbetween recalling information in the source and target modalities. Furthermore,\nwe observe that this gap exists across various scenarios, including more\ncapable models, machine unlearning, and the multi-hop case. At the end, we\npropose a baseline method to mitigate this challenge. We hope our study can\ninspire future research on developing more robust multimodal learning\ntechniques to enhance cross-modal transferability.", "authors": ["Yuxin Wen", "Yangsibo Huang", "Tom Goldstein", "Ravi Kumar", "Badih Ghazi", "Chiyuan Zhang"], "published_date": "2025-06-05", "title_zh": "視覺語言模型中跨模態記憶的量化研究", "summary_zh": "理解神經網絡在訓練期間記憶內容及方式至關重要，考量點包含無意間記憶潛在敏感資訊，以及為真實知識密集型任務有效獲取知識。過去研究主要探討單模態記憶，如大型語言模型中的文本記憶或擴散模型中的圖像記憶，而統一多模態模型在實際應用中日益普及。本研究聚焦跨模態記憶的獨特特性，針對視覺語言模型進行系統性研究。為便於控制實驗，我們首先引入包含多樣合成人物圖像和文本描述的合成人物數據集。我們透過在單模態上訓練模型並評估其在另一模態中的表現，來量化事實知識記憶和跨模態可遷移性。結果顯示，在一種模態中學習的事實會轉移到另一種模態，但在源模態和目標模態中回憶資訊之間存在顯著差距。此外，我們觀察到這種差距存在於各種情境中，包括更強大的模型、機器遺忘和多跳案例。最後，我們提出一種基於基準的方法來緩解此挑戰。期望本研究能啟發未來在開發更穩健的多模態學習技術以增強跨模態可遷移性方面的研究。", "audio": "audios/2506.05198v1.mp3", "timestamp": "2025-06-06T09:20:41.326024"}
{"query": "AI", "id": "2506.05296v1", "url": "http://arxiv.org/abs/2506.05296v1", "title": "Control Tax: The Price of Keeping AI in Check", "summary": "The rapid integration of agentic AI into high-stakes real-world applications\nrequires robust oversight mechanisms. The emerging field of AI Control (AIC)\naims to provide such an oversight mechanism, but practical adoption depends\nheavily on implementation overhead. To study this problem better, we introduce\nthe notion of Control tax -- the operational and financial cost of integrating\ncontrol measures into AI pipelines. Our work makes three key contributions to\nthe field of AIC: (1) we introduce a theoretical framework that quantifies the\nControl Tax and maps classifier performance to safety assurances; (2) we\nconduct comprehensive evaluations of state-of-the-art language models in\nadversarial settings, where attacker models insert subtle backdoors into code\nwhile monitoring models attempt to detect these vulnerabilities; and (3) we\nprovide empirical financial cost estimates for control protocols and develop\noptimized monitoring strategies that balance safety and cost-effectiveness\nwhile accounting for practical constraints like auditing budgets. Our framework\nenables practitioners to make informed decisions by systematically connecting\nsafety guarantees with their costs, advancing AIC through principled economic\nfeasibility assessment across different deployment contexts.", "authors": ["Mikhail Terekhov", "Zhen Ning David Liu", "Caglar Gulcehre", "Samuel Albanie"], "published_date": "2025-06-05", "title_zh": "控制稅：約束人工智能的代價", "summary_zh": "自主型人工智慧快速整合至高風險現實應用，需健全監督機制。新興的AI控制領域旨在提供此類機制，但實際應用取決於導入成本。本文提出控制稅概念，量化控制措施整合至AI流程的運營和財務成本。研究貢獻包括：(1)建立理論框架，量化控制稅並將分類器性能映射至安全保證；(2)針對最先進語言模型在對抗環境中進行全面評估，其中攻擊者模型植入隱蔽後門程式碼，監控模型則試圖偵測漏洞；(3)提供控制協議的實證財務成本估算，並開發優化監控策略，在安全性和成本效益之間取得平衡，同時考慮審計預算等實際限制。此框架使從業者能有根據地做出決策，系統性地將安全保證與成本聯繫起來，透過跨不同部署環境的經濟可行性評估，推進AI控制領域發展。", "audio": "audios/2506.05296v1.mp3", "timestamp": "2025-06-06T10:21:14.694314"}
{"query": "Foundation Model", "id": "2506.05127v1", "url": "http://arxiv.org/abs/2506.05127v1", "title": "PixCell: A generative foundation model for digital histopathology images", "summary": "The digitization of histology slides has revolutionized pathology, providing\nmassive datasets for cancer diagnosis and research. Contrastive self-supervised\nand vision-language models have been shown to effectively mine large pathology\ndatasets to learn discriminative representations. On the other hand, generative\nmodels, capable of synthesizing realistic and diverse images, present a\ncompelling solution to address unique problems in pathology that involve\nsynthesizing images; overcoming annotated data scarcity, enabling\nprivacy-preserving data sharing, and performing inherently generative tasks,\nsuch as virtual staining. We introduce PixCell, the first diffusion-based\ngenerative foundation model for histopathology. We train PixCell on PanCan-30M,\na vast, diverse dataset derived from 69,184 H\\&E-stained whole slide images\ncovering various cancer types. We employ a progressive training strategy and a\nself-supervision-based conditioning that allows us to scale up training without\nany annotated data. PixCell generates diverse and high-quality images across\nmultiple cancer types, which we find can be used in place of real data to train\na self-supervised discriminative model. Synthetic images shared between\ninstitutions are subject to fewer regulatory barriers than would be the case\nwith real clinical images. Furthermore, we showcase the ability to precisely\ncontrol image generation using a small set of annotated images, which can be\nused for both data augmentation and educational purposes. Testing on a cell\nsegmentation task, a mask-guided PixCell enables targeted data augmentation,\nimproving downstream performance. Finally, we demonstrate PixCell's ability to\nuse H\\&E structural staining to infer results from molecular marker studies; we\nuse this capability to infer IHC staining from H\\&E images. Our trained models\nare publicly released to accelerate research in computational pathology.", "authors": ["Srikar Yellapragada", "Alexandros Graikos", "Zilinghan Li", "Kostas Triaridis", "Varun Belagali", "Saarthak Kapse", "Tarak Nath Nandi", "Ravi K Madduri", "Prateek Prasanna", "Tahsin Kurc", "Rajarsi R. Gupta", "Joel Saltz", "Dimitris Samaras"], "published_date": "2025-06-05", "title_zh": "PixCell：數位組織病理學影像之生成式基礎模型", "summary_zh": "組織病理學切片數位化革新了病理學，產生龐大數據集用於癌症診斷與研究。對比自監督和視覺語言模型能有效挖掘大型病理數據集以學習判別性表徵。生成模型則能合成逼真多樣的圖像，為解決病理學中獨特問題（例如圖像合成）提供有力的解決方案，克服了標註數據稀缺、實現保護隱私的數據共享以及執行虛擬染色等生成任務的挑戰。本研究提出PixCell，首個基於擴散的組織病理學生成基礎模型。PixCell在PanCan-30M上訓練，該數據集涵蓋多種癌症類型，源自69,184張H&E染色的全切片圖像。研究採用漸進式訓練策略和基於自監督的條件設定，無需標註數據即可擴展訓練。PixCell生成多種癌症類型的高質量圖像，可用於訓練自監督判別模型，且合成圖像的機構間共享相較真實臨床圖像受到的監管限制更少。研究還展示了使用少量標註圖像精確控制圖像生成的能力，可用於數據增強和教學。在細胞分割任務中，基於遮罩引導的PixCell實現了有針對性的數據增強，提升了下游性能。此外，PixCell還能利用H&E結構染色推斷分子標記研究結果，從H&E圖像推斷IHC染色。研究發布了訓練好的模型，旨在加速計算病理學研究。", "audio": "audios/2506.05127v1.mp3", "timestamp": "2025-06-06T10:21:25.983933"}
{"query": "Diffusion Model", "id": "2506.05178v1", "url": "http://arxiv.org/abs/2506.05178v1", "title": "Associative Memory and Generative Diffusion in the Zero-noise Limit", "summary": "Connections between generative diffusion and continuous-state associative\nmemory models are studied. Morse-Smale dynamical systems are emphasized as\nuniversal approximators of gradient-based associative memory models and\ndiffusion models as white-noise perturbed systems thereof. Universal properties\nof associative memory that follow from this description are described and used\nto characterize a generic transition from generation to memory as noise levels\ndiminish. Structural stability inherited by Morse-Smale flows is shown to imply\na notion of stability for diffusions at vanishing noise levels. Applied to one-\nand two-parameter families of gradients, this indicates stability at all but\nisolated points of associative memory learning landscapes and the learning and\ngeneration landscapes of diffusion models with gradient drift in the zero-noise\nlimit, at which small sets of generic bifurcations characterize qualitative\ntransitions between stable systems. Examples illustrating the characterization\nof these landscapes by sequences of these bifurcations are given, along with\nstructural stability criterion for classic and modern Hopfield networks\n(equivalently, the attention mechanism).", "authors": ["Joshua Hess", "Quaid Morris"], "published_date": "2025-06-05", "title_zh": "零噪聲極限下的聯想記憶與生成擴散", "summary_zh": "研究生成式擴散模型與連續狀態聯想記憶模型間的關聯。Morse-Smale動態系統被強調為基於梯度的聯想記憶模型和擴散模型（視為白雜訊擾動系統）的通用逼近器。闡述了由此描述產生的聯想記憶通用性質，並用於描述雜訊水平降低時，從生成到記憶的通用轉變。Morse-Smale流繼承的結構穩定性暗示了擴散在雜訊趨近於零時的穩定性概念。應用於單參數和雙參數梯度族，表明在聯想記憶學習地形以及具有梯度漂移的擴散模型在零雜訊極限下的學習和生成地形中，除了孤立點外，其餘點皆穩定。少數通用分岔描述了穩定系統之間的質性轉變。文中提供了範例，展示了如何透過這些分岔序列來表徵這些地形，並給出了經典和現代Hopfield網路（等效於注意力機制）的結構穩定性判據。", "audio": "audios/2506.05178v1.mp3", "timestamp": "2025-06-06T10:21:32.586480"}
{"query": "AI", "id": "2506.05286v1", "url": "http://arxiv.org/abs/2506.05286v1", "title": "Stable Vision Concept Transformers for Medical Diagnosis", "summary": "Transparency is a paramount concern in the medical field, prompting\nresearchers to delve into the realm of explainable AI (XAI). Among these XAI\nmethods, Concept Bottleneck Models (CBMs) aim to restrict the model's latent\nspace to human-understandable high-level concepts by generating a conceptual\nlayer for extracting conceptual features, which has drawn much attention\nrecently. However, existing methods rely solely on concept features to\ndetermine the model's predictions, which overlook the intrinsic feature\nembeddings within medical images. To address this utility gap between the\noriginal models and concept-based models, we propose Vision Concept Transformer\n(VCT). Furthermore, despite their benefits, CBMs have been found to negatively\nimpact model performance and fail to provide stable explanations when faced\nwith input perturbations, which limits their application in the medical field.\nTo address this faithfulness issue, this paper further proposes the Stable\nVision Concept Transformer (SVCT) based on VCT, which leverages the vision\ntransformer (ViT) as its backbone and incorporates a conceptual layer. SVCT\nemploys conceptual features to enhance decision-making capabilities by fusing\nthem with image features and ensures model faithfulness through the integration\nof Denoised Diffusion Smoothing. Comprehensive experiments on four medical\ndatasets demonstrate that our VCT and SVCT maintain accuracy while remaining\ninterpretable compared to baselines. Furthermore, even when subjected to\nperturbations, our SVCT model consistently provides faithful explanations, thus\nmeeting the needs of the medical field.", "authors": ["Lijie Hu", "Songning Lai", "Yuan Hua", "Shu Yang", "Jingfeng Zhang", "Di Wang"], "published_date": "2025-06-05", "title_zh": "用於醫學診斷的穩定視覺概念轉換器", "summary_zh": "透明度在醫療領域至關重要，促使研究人員探索可解釋人工智慧（XAI）。概念瓶頸模型（CBMs）透過生成概念層以提取概念特徵，將模型潛在空間限制在人類可理解的高階概念上，備受關注。然而，現有方法僅依賴概念特徵來決定模型預測，忽略了醫療影像中的內在特徵嵌入。為了解決原始模型與基於概念模型之間的效用差距，我們提出視覺概念轉換器（VCT）。此外，CBMs存在影響模型性能，以及在面對輸入擾動時無法提供穩定解釋的問題，限制了其在醫療領域的應用。為了解決忠實性問題，本文基於VCT進一步提出穩定視覺概念轉換器（SVCT），其利用視覺轉換器（ViT）作為主幹，並整合概念層。SVCT透過將概念特徵與影像特徵融合來增強決策能力，並透過整合去噪擴散平滑來確保模型忠實性。在四個醫療數據集上的綜合實驗表明，與基準模型相比，我們的VCT和SVCT在保持準確性的同時保持可解釋性。此外，即使受到擾動，我們的SVCT模型也能持續提供忠實的解釋，從而滿足醫療領域的需求。", "audio": "audios/2506.05286v1.mp3", "timestamp": "2025-06-06T11:16:54.190714"}
{"query": "Foundation Model", "id": "2506.05027v1", "url": "http://arxiv.org/abs/2506.05027v1", "title": "Tuning the Right Foundation Models is What you Need for Partial Label Learning", "summary": "Partial label learning (PLL) seeks to train generalizable classifiers from\ndatasets with inexact supervision, a common challenge in real-world\napplications. Existing studies have developed numerous approaches to\nprogressively refine and recover ground-truth labels by training convolutional\nneural networks. However, limited attention has been given to foundation models\nthat offer transferrable representations. In this work, we empirically conduct\ncomprehensive evaluations of 11 foundation models across 13 PLL approaches on 8\nbenchmark datasets under 3 PLL scenarios. We further propose PartialCLIP, an\nefficient fine-tuning framework for foundation models in PLL. Our findings\nreveal that current PLL approaches tend to 1) achieve significant performance\ngains when using foundation models, 2) exhibit remarkably similar performance\nto each other, 3) maintain stable performance across varying ambiguity levels,\nwhile 4) are susceptible to foundation model selection and adaptation\nstrategies. Additionally, we demonstrate the efficacy of text-embedding\nclassifier initialization and effective candidate label filtering using\nzero-shot CLIP. Our experimental results and analysis underscore the\nlimitations of current PLL approaches and provide valuable insights for\ndeveloping more generalizable PLL models. The source code can be found at\nhttps://github.com/SEU-hk/PartialCLIP.", "authors": ["Kuang He", "Wei Tang", "Tong Wei", "Min-Ling Zhang"], "published_date": "2025-06-05", "title_zh": "調整合適的基礎模型是部分標籤學習的關鍵", "summary_zh": "部分標籤學習旨在利用不精確監督資料訓練泛化性分類器，此為現實應用常見挑戰。現有研究多著重於透過訓練卷積神經網路逐步精煉並恢復真實標籤，然對可轉移表示之基礎模型關注不足。本研究針對11個基礎模型，於3種部分標籤學習情境下，在8個基準資料集上，對13種部分標籤學習方法進行全面評估。此外，我們提出PartialCLIP，一種針對部分標籤學習中基礎模型的高效微調框架。研究結果顯示，當前部分標籤學習方法：1) 使用基礎模型時可顯著提升效能；2) 彼此表現高度相似；3) 在不同模糊程度下保持穩定效能；4) 易受基礎模型選擇及適應策略影響。另外，我們驗證了文本嵌入分類器初始化以及利用零樣本CLIP進行有效候選標籤過濾的有效性。實驗結果與分析突顯了當前部分標籤學習方法的局限性，並為開發更具泛化性的部分標籤學習模型提供寶貴見解。原始碼位於https://github.com/SEU-hk/PartialCLIP。", "audio": "audios/2506.05027v1.mp3", "timestamp": "2025-06-06T11:17:01.689111"}
{"query": "Diffusion Model", "id": "2506.05137v1", "url": "http://arxiv.org/abs/2506.05137v1", "title": "Neural Jumps for Option Pricing", "summary": "Recognizing the importance of jump risk in option pricing, we propose a\nneural jump stochastic differential equation model in this paper, which\nintegrates neural networks as parameter estimators in the conventional jump\ndiffusion model. To overcome the problem that the backpropagation algorithm is\nnot compatible with the jump process, we use the Gumbel-Softmax method to make\nthe jump parameter gradient learnable. We examine the proposed model using both\nsimulated data and S&P 500 index options. The findings demonstrate that the\nincorporation of neural jump components substantially improves the accuracy of\npricing compared to existing benchmark models.", "authors": ["Duosi Zheng", "Hanzhong Guo", "Yanchu Liu", "Wei Huang"], "published_date": "2025-06-05", "title_zh": "期權定價之神經跳躍", "summary_zh": "本研究提出神經跳躍隨機微分方程模型，將神經網路融入傳統跳躍擴散模型以估計參數，藉此提升選擇權定價中跳躍風險的考量。為解決反向傳播演算法與跳躍過程的不相容性，採用Gumbel-Softmax方法使跳躍參數梯度可學習。經由模擬數據與標普500指數選擇權驗證，結果顯示相較於現有基準模型，加入神經跳躍組件能顯著提高定價準確性。", "audio": "audios/2506.05137v1.mp3", "timestamp": "2025-06-06T11:17:05.645195"}
{"query": "AI", "id": "2506.05265v1", "url": "http://arxiv.org/abs/2506.05265v1", "title": "Teaming in the AI Era: AI-Augmented Frameworks for Forming, Simulating, and Optimizing Human Teams", "summary": "Effective teamwork is essential across diverse domains. During the team\nformation stage, a key challenge is forming teams that effectively balance user\npreferences with task objectives to enhance overall team satisfaction. In the\nteam performing stage, maintaining cohesion and engagement is critical for\nsustaining high team performance. However, existing computational tools and\nalgorithms for team optimization often rely on static data inputs, narrow\nalgorithmic objectives, or solutions tailored for specific contexts, failing to\naccount for the dynamic interplay of team members personalities, evolving\ngoals, and changing individual preferences. Therefore, teams may encounter\nmember dissatisfaction, as purely algorithmic assignments can reduce members\ncommitment to team goals or experience suboptimal engagement due to the absence\nof timely, personalized guidance to help members adjust their behaviors and\ninteractions as team dynamics evolve. Ultimately, these challenges can lead to\nreduced overall team performance. My Ph.D. dissertation aims to develop\nAI-augmented team optimization frameworks and practical systems that enhance\nteam satisfaction, engagement, and performance. First, I propose a team\nformation framework that leverages a multi-armed bandit algorithm to\niteratively refine team composition based on user preferences, ensuring\nalignment between individual needs and collective team goals to enhance team\nsatisfaction. Second, I introduce tAIfa (Team AI Feedback Assistant), an\nAI-powered system that utilizes large language models (LLMs) to deliver\nimmediate, personalized feedback to both teams and individual members,\nenhancing cohesion and engagement. Finally, I present PuppeteerLLM, an\nLLM-based simulation framework that simulates multi-agent teams to model\ncomplex team dynamics within realistic environments, incorporating task-driven\ncollaboration and long-term coordination.", "authors": ["Mohammed Almutairi"], "published_date": "2025-06-05", "title_zh": "人工智慧時代的團隊協作：用於組建、模擬與優化人類團隊的人工智慧增強框架", "summary_zh": "有效的團隊合作至關重要。組隊初期，關鍵挑戰在於平衡用戶偏好與任務目標，提升團隊滿意度。團隊執行階段，維持凝聚力與投入度是維持高效能的關鍵。然而，現有團隊優化工具常仰賴靜態數據、狹隘目標或特定情境方案，忽略團隊成員性格、目標演變及個人偏好的動態交互作用，導致成員不滿意、投入度降低及團隊效能下降。本博士論文旨在開發AI增強的團隊優化框架與實用系統，以提升團隊滿意度、投入度及效能。首先，提出一種團隊組建框架，利用多臂老虎機演算法迭代優化團隊組成，確保個人需求與團隊目標一致。其次，引入tAIfa (團隊AI回饋助手)，利用大型語言模型提供即時個人化回饋，增強凝聚力與投入度。最後，提出PuppeteerLLM，一個基於大型語言模型的模擬框架，模擬多代理團隊，在逼真環境中建模複雜的團隊動態，包含任務驅動的協作與長期協調。", "audio": "audios/2506.05265v1.mp3", "timestamp": "2025-06-06T12:37:45.859414"}
{"query": "Foundation Model", "id": "2506.05011v1", "url": "http://arxiv.org/abs/2506.05011v1", "title": "UAV4D: Dynamic Neural Rendering of Human-Centric UAV Imagery using Gaussian Splatting", "summary": "Despite significant advancements in dynamic neural rendering, existing\nmethods fail to address the unique challenges posed by UAV-captured scenarios,\nparticularly those involving monocular camera setups, top-down perspective, and\nmultiple small, moving humans, which are not adequately represented in existing\ndatasets. In this work, we introduce UAV4D, a framework for enabling\nphotorealistic rendering for dynamic real-world scenes captured by UAVs.\nSpecifically, we address the challenge of reconstructing dynamic scenes with\nmultiple moving pedestrians from monocular video data without the need for\nadditional sensors. We use a combination of a 3D foundation model and a human\nmesh reconstruction model to reconstruct both the scene background and humans.\nWe propose a novel approach to resolve the scene scale ambiguity and place both\nhumans and the scene in world coordinates by identifying human-scene contact\npoints. Additionally, we exploit the SMPL model and background mesh to\ninitialize Gaussian splats, enabling holistic scene rendering. We evaluated our\nmethod on three complex UAV-captured datasets: VisDrone, Manipal-UAV, and\nOkutama-Action, each with distinct characteristics and 10~50 humans. Our\nresults demonstrate the benefits of our approach over existing methods in novel\nview synthesis, achieving a 1.5 dB PSNR improvement and superior visual\nsharpness.", "authors": ["Jaehoon Choi", "Dongki Jung", "Christopher Maxey", "Yonghan Lee", "Sungmin Eum", "Dinesh Manocha", "Heesung Kwon"], "published_date": "2025-06-05", "title_zh": "無人機四維：基於高斯潑濺的人體中心無人機影像動態神經渲染", "summary_zh": "現有動態神經渲染方法難以應對無人機場景的獨特挑戰，尤其是在單目相機、俯視視角及多個小型移動人群的環境下，而現有數據集對此代表性不足。本研究提出UAV4D框架，旨在實現無人機捕捉的動態真實場景之照片級渲染。針對僅使用單目影片重建多個移動行人動態場景的難題，結合3D基礎模型與人體網格重建模型，重建場景背景及行人。藉由識別人體與場景接觸點，提出解決場景尺度模糊性並將人體及場景置於世界座標系的新方法。此外，利用SMPL模型及背景網格初始化高斯濺射，實現整體場景渲染。在VisDrone、Manipal-UAV及Okutama-Action三個複雜無人機數據集上的評估結果顯示，本方法在新視角合成方面優於現有方法，PSNR提升1.5 dB，且視覺清晰度更佳。", "audio": "audios/2506.05011v1.mp3", "timestamp": "2025-06-06T12:37:52.517841"}
{"query": "Diffusion Model", "id": "2506.05083v1", "url": "http://arxiv.org/abs/2506.05083v1", "title": "SeedEdit 3.0: Fast and High-Quality Generative Image Editing", "summary": "We introduce SeedEdit 3.0, in companion with our T2I model Seedream 3.0 [22],\nwhich significantly improves over our previous version [27] in both aspects of\nedit instruction following and image content (e.g., ID/IP) preservation on real\nimage inputs. Additional to model upgrading with T2I, in this report, we\npresent several key improvements. First, we develop an enhanced data curation\npipeline with a meta-info paradigm and meta-info embedding strategy that help\nmix images from multiple data sources. This allows us to scale editing data\neffectively, and meta information is helpfult to connect VLM with diffusion\nmodel more closely. Second, we introduce a joint learning pipeline for\ncomputing a diffusion loss and a reward loss. Finally, we evaluate SeedEdit 3.0\non our testing benchmarks, for real image editing, where it achieves a best\ntrade-off between multiple aspects, yielding a high usability rate of 56.1%,\ncompared to SeedEdit 1.6 (38.4%), GPT4o (37.1%) and Gemini 2.0 (30.3%).", "authors": ["Peng Wang", "Yichun Shi", "Xiaochen Lian", "Zhonghua Zhai", "Xin Xia", "Xuefeng Xiao", "Weilin Huang", "Jianchao Yang"], "published_date": "2025-06-05", "title_zh": "SeedEdit 3.0：快速且高品質的生成式影像編輯", "summary_zh": "SeedEdit 3.0伴隨Seedream 3.0模型推出，在編輯指令執行和圖像內容保留方面顯著優於先前版本。除了模型升級，本研究還提出多項關鍵改進。首先，我們開發了增強的數據管理流程，採用元信息範例和嵌入策略，有效整合多源數據，擴展編輯數據規模，並促進視覺語言模型與擴散模型的緊密連接。其次，我們引入了聯合學習流程，計算擴散損失和獎勵損失。最後，在真實圖像編輯的測試基準上，SeedEdit 3.0實現了多方面最佳平衡，可用性達到56.1%，高於SeedEdit 1.6 (38.4%)、GPT4o (37.1%)和Gemini 2.0 (30.3%)。", "audio": "audios/2506.05083v1.mp3", "timestamp": "2025-06-06T12:37:59.412374"}
{"query": "AI", "id": "2506.05226v1", "url": "http://arxiv.org/abs/2506.05226v1", "title": "Towards Effective Multidisciplinary Health and HCI Teams based on AI Framework", "summary": "As a Ph.D. student with a diverse background in both public and private\nsectors, I have encountered numerous challenges in cross-disciplinary and\nmulti-stakeholder team projects. My research on developing team compositions\nthat involve multidisciplinary members from fields including education,\nacademia, and health. Along with my advisor, we are focused on exploring how\nHCI can help individuals assemble more effective teams. This effort involves\ndeveloping socio-technical systems that guide and inform individuals of the\npotential teams that they can assemble. We employ state-of-the-art algorithms\nthat prioritize inclusion among team members from diverse areas of expertise\nand familiarity between the team members. Our goal for attending this workshop\nis to engage in meaningful dialogues with scholars and researchers, leveraging\nthese interactions to refine our approach to building an AI-driven team\ncomposition system to foster effective, interdisciplinary collaboration in\nhealth-focused HCI research.", "authors": ["Mohammed Almutairi", "Diego Gómez-Zará"], "published_date": "2025-06-05", "title_zh": "基於人工智慧框架下高效能跨領域健康與人機互動團隊建構", "summary_zh": "身為具備公私部門經驗的博士生，我於跨領域及多方利害關係人團隊專案中遭遇諸多挑戰。本研究與指導教授探討人機互動如何協助組建更有效率的團隊，開發引導個人組建潛在團隊的社會技術系統。研究運用先進演算法，優先考量團隊成員專業領域的多樣性與熟悉度。參與本次研討會旨在與學者專家交流，藉此精進以人工智慧驅動的團隊組成系統，促進以健康為中心的人機互動研究中，有效的跨領域合作。", "audio": "audios/2506.05226v1.mp3", "timestamp": "2025-06-06T13:30:28.278365"}
{"query": "Foundation Model", "id": "2506.04650v1", "url": "http://arxiv.org/abs/2506.04650v1", "title": "Neural Network Reprogrammability: A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction", "summary": "As large-scale pre-trained foundation models continue to expand in size and\ncapability, efficiently adapting them to specific downstream tasks has become\nincreasingly critical. Despite substantial progress, existing adaptation\napproaches have evolved largely in isolation, without a clear understanding of\ntheir interrelationships. This survey introduces neural network\nreprogrammability as a unifying framework that bridges mainstream model\nadaptation techniques--model reprogramming, prompt tuning, and prompt\ninstruction--previously fragmented research areas yet converges on a shared\nprinciple: repurposing a pre-trained model by manipulating information at the\ninterfaces while keeping the model parameters frozen. These methods exploit\nneural networks' sensitivity to manipulation on different interfaces, be it\nthrough perturbing inputs, inserting tokens into intermediate layers, or\nproviding task-specific examples in context, to redirect model behaviors\ntowards desired outcomes. We then present a taxonomy that categorizes such\ninformation manipulation-based adaptation approaches across four key\ndimensions: manipulation format (fixed or learnable), location (interfaces\nwhere manipulations occur), operator (how they are applied), and output\nalignment requirement (post-processing needed to align outputs with downstream\ntasks). Notably, this framework applies consistently across data modalities,\nindependent of specific model architectures. Moreover, viewing established\ntechniques like in-context learning and chain-of-thought prompting through this\nlens reveals both their theoretical connections and practical distinctions. We\nfurther analyze remaining technical challenges and ethical considerations,\npositioning neural network reprogrammability as a fundamental paradigm for\nefficient model adaptation. We lastly identify promising research directions\nemerging from this integrative viewpoint.", "authors": ["Zesheng Ye", "Chengyi Cai", "Ruijiang Dong", "Jianzhong Qi", "Lei Feng", "Pin-Yu Chen", "Feng Liu"], "published_date": "2025-06-05", "title_zh": "神經網路可重編程性：模型重編程、提示調整與提示指令之統一主題", "summary_zh": "隨著大規模預訓練模型日益擴展，高效適應下游任務至關重要。現有適應方法多各自發展，缺乏關聯性理解。本研究提出神經網路可重編程性作為統一框架，連結模型重編程、提示調整和提示指令等主流技術，其共同原則為：透過操控介面資訊，在凍結模型參數下重新利用預訓練模型。這些方法利用神經網路對不同介面操控的敏感性，例如擾動輸入、插入中間層令牌或提供任務範例，將模型行為導向所需結果。本研究建立分類法，依據操控格式、位置、運算子及輸出對齊要求等四個面向，對基於資訊操控的適應方法進行分類。此框架適用於各種數據模態和模型架構。透過此框架觀察，情境學習和思維鏈提示等技術的理論聯繫和實際區別得以顯現。本研究進一步分析技術挑戰和倫理考量，將神經網路可重編程性定位為高效模型適應的基本範式，並從整合視角識別具潛力的研究方向。", "audio": "audios/2506.04650v1.mp3", "timestamp": "2025-06-06T13:30:37.111877"}
{"query": "Diffusion Model", "id": "2506.05046v1", "url": "http://arxiv.org/abs/2506.05046v1", "title": "FlowDirector: Training-Free Flow Steering for Precise Text-to-Video Editing", "summary": "Text-driven video editing aims to modify video content according to natural\nlanguage instructions. While recent training-free approaches have made progress\nby leveraging pre-trained diffusion models, they typically rely on\ninversion-based techniques that map input videos into the latent space, which\noften leads to temporal inconsistencies and degraded structural fidelity. To\naddress this, we propose FlowDirector, a novel inversion-free video editing\nframework. Our framework models the editing process as a direct evolution in\ndata space, guiding the video via an Ordinary Differential Equation (ODE) to\nsmoothly transition along its inherent spatiotemporal manifold, thereby\npreserving temporal coherence and structural details. To achieve localized and\ncontrollable edits, we introduce an attention-guided masking mechanism that\nmodulates the ODE velocity field, preserving non-target regions both spatially\nand temporally. Furthermore, to address incomplete edits and enhance semantic\nalignment with editing instructions, we present a guidance-enhanced editing\nstrategy inspired by Classifier-Free Guidance, which leverages differential\nsignals between multiple candidate flows to steer the editing trajectory toward\nstronger semantic alignment without compromising structural consistency.\nExtensive experiments across benchmarks demonstrate that FlowDirector achieves\nstate-of-the-art performance in instruction adherence, temporal consistency,\nand background preservation, establishing a new paradigm for efficient and\ncoherent video editing without inversion.", "authors": ["Guangzhao Li", "Yanming Yang", "Chenxi Song", "Chi Zhang"], "published_date": "2025-06-05", "title_zh": "FlowDirector：用於精確文本到視頻編輯的免訓練流導向", "summary_zh": "基於文字指令的影片編輯旨在根據自然語言修改影片內容。現有免訓練方法雖利用預訓練擴散模型取得進展，但通常依賴反演技術將輸入影片映射到潛在空間，導致時間不一致和結構保真度降低。為此，我們提出FlowDirector，一種新型免反演影片編輯框架。該框架將編輯過程建模為數據空間中的直接演變，通過常微分方程 (ODE)引導影片沿其固有的時空流形平滑過渡，從而保持時間連貫性和結構細節。為實現局部化和可控編輯，我們引入注意力導向的遮罩機制，調節ODE速度場，在空間和時間上保留非目標區域。此外，為了解決不完整編輯並增強與編輯指令的語義對齊，我們提出一種受Classifier-Free Guidance啟發的引導增強編輯策略，利用多個候選流之間的差分信號，引導編輯軌跡朝向更強的語義對齊，同時不損害結構一致性。基準測試顯示，FlowDirector在指令遵循、時間一致性和背景保留方面均達到最先進水平，為無需反演的高效且連貫的影片編輯建立新範例。", "audio": "audios/2506.05046v1.mp3", "timestamp": "2025-06-06T13:30:47.797978"}
{"query": "AI", "id": "2506.05225v1", "url": "http://arxiv.org/abs/2506.05225v1", "title": "Enhancing the Merger Simulation Toolkit with ML/AI", "summary": "This paper develops a flexible approach to predict the price effects of\nhorizontal mergers using ML/AI methods. While standard merger simulation\ntechniques rely on restrictive assumptions about firm conduct, we propose a\ndata-driven framework that relaxes these constraints when rich market data are\navailable. We develop and identify a flexible nonparametric model of supply\nthat nests a broad range of conduct models and cost functions. To overcome the\ncurse of dimensionality, we adapt the Variational Method of Moments (VMM)\n(Bennett and Kallus, 2023) to estimate the model, allowing for various forms of\nstrategic interaction. Monte Carlo simulations show that our method\nsignificantly outperforms an array of misspecified models and rivals the\nperformance of the true model, both in predictive performance and\ncounterfactual merger simulations. As a way to interpret the economics of the\nestimated function, we simulate pass-through and reveal that the model learns\nmarkup and cost functions that imply approximately correct pass-through\nbehavior. Applied to the American Airlines-US Airways merger, our method\nproduces more accurate post-merger price predictions than traditional\napproaches. The results demonstrate the potential for machine learning\ntechniques to enhance merger analysis while maintaining economic structure.", "authors": ["Harold D. Chiang", "Jack Collison", "Lorenzo Magnolfi", "Christopher Sullivan"], "published_date": "2025-06-05", "title_zh": "以機器學習/人工智慧強化合併模擬工具組", "summary_zh": "本文提出一種彈性方法，利用機器學習/人工智慧預測橫向併購的價格效應。有別於傳統併購模擬技術對企業行為的限制性假設，本文提出一個數據驅動框架，可在豐富市場數據下放寬這些限制。本文建立並識別一個彈性的非參數供給模型，涵蓋多種行為模型和成本函數。為了解決維度詛咒，本文採用變分動差法(VMM)估計模型，允許各種形式的策略互動。蒙地卡羅模擬顯示，本方法顯著優於一系列錯誤設定的模型，並在預測性能和反事實併購模擬方面與真實模型相媲美。為解釋估計函數的經濟意義，本文模擬傳遞效應，揭示模型學習到的加成和成本函數暗示大致正確的傳遞行為。應用於美國航空與全美航空的併購案，本方法產生比傳統方法更準確的併購後價格預測。結果表明，機器學習技術有潛力在維持經濟結構的同時，強化併購分析。", "audio": "audios/2506.05225v1.mp3", "timestamp": "2025-06-06T14:18:33.653499"}
{"query": "Foundation Model", "id": "2506.04598v1", "url": "http://arxiv.org/abs/2506.04598v1", "title": "Scaling Laws for Robust Comparison of Open Foundation Language-Vision Models and Datasets", "summary": "In studies of transferable learning, scaling laws are obtained for various\nimportant foundation models to predict their properties and performance at\nlarger scales. We show here how scaling law derivation can also be used for\nmodel and dataset comparison, allowing to decide which procedure is to be\npreferred for pre-training. For the first time, full scaling laws based on\ndense measurements across a wide span of model and samples seen scales are\nderived for two important language-vision learning procedures, CLIP and MaMMUT,\nthat use either contrastive only or contrastive and captioning text generative\nloss. Ensuring sufficient prediction accuracy for held out points, we use\nderived scaling laws to compare both models, obtaining evidence for MaMMUT's\nstronger improvement with scale and better sample efficiency than standard\nCLIP. To strengthen validity of the comparison, we show scaling laws for\nvarious downstream tasks, classification, retrieval, and segmentation, and for\ndifferent open datasets, DataComp, DFN and Re-LAION, observing consistently the\nsame trends. We show that comparison can also be performed when deriving\nscaling laws with a constant learning rate schedule, reducing compute cost.\nAccurate derivation of scaling laws provides thus means to perform model and\ndataset comparison across scale spans, avoiding misleading conclusions based on\nmeasurements from single reference scales only, paving the road for systematic\ncomparison and improvement of open foundation models and datasets for their\ncreation. We release all the pre-trained models with their intermediate\ncheckpoints, including openMaMMUT-L/14, which achieves $80.3\\%$ zero-shot\nImageNet-1k accuracy, trained on 12.8B samples from DataComp-1.4B. Code for\nreproducing experiments in the paper and raw experiments data can be found at\nhttps://github.com/LAION-AI/scaling-laws-for-comparison.", "authors": ["Marianna Nezhurina", "Tomer Porian", "Giovanni Pucceti", "Tommie Kerssies", "Romain Beaumont", "Mehdi Cherti", "Jenia Jitsev"], "published_date": "2025-06-05", "title_zh": "開放基礎語言-視覺模型與數據集穩健比較之比例定律", "summary_zh": "可遷移學習研究中，已推導出多種重要基礎模型之規模定律，以預測其於更大規模下的特性與效能。本文展示如何利用規模定律進行模型與資料集比較，從而決定最佳預訓練流程。首次針對CLIP與MaMMUT兩種語言-視覺學習流程，推導出基於密集測量且橫跨廣泛模型與樣本規模的完整規模定律。前者僅使用對比損失，後者則同時使用對比損失與圖像描述生成損失。在確保足夠預測精度的前提下，利用規模定律比較兩模型，證明MaMMUT隨規模擴大展現更強的改進，且樣本效率優於標準CLIP。為加強比較的有效性，展示了分類、檢索與分割等下游任務，以及DataComp、DFN與Re-LAION等不同開放資料集的規模定律，觀察到一致趨勢。研究表明，使用恆定學習率排程推導規模定律時，亦可進行比較，從而降低計算成本。準確推導規模定律，可於不同規模範圍內比較模型與資料集，避免僅根據單一參考規模的測量結果得出誤導性結論，為系統性比較與改進開放基礎模型及其創建資料集鋪平道路。研究釋出所有預訓練模型及其checkpoint，包括在DataComp-1.4B的128億樣本上訓練的openMaMMUT-L/14，其零樣本ImageNet-1k準確率達到80.3%。實驗重現程式碼與原始實驗數據可在指定連結取得。", "audio": "audios/2506.04598v1.mp3", "timestamp": "2025-06-06T14:18:46.156245"}
{"query": "Diffusion Model", "id": "2506.04879v1", "url": "http://arxiv.org/abs/2506.04879v1", "title": "Invisible Backdoor Triggers in Image Editing Model via Deep Watermarking", "summary": "Diffusion models have achieved remarkable progress in both image generation\nand editing. However, recent studies have revealed their vulnerability to\nbackdoor attacks, in which specific patterns embedded in the input can\nmanipulate the model's behavior. Most existing research in this area has\nproposed attack frameworks focused on the image generation pipeline, leaving\nbackdoor attacks in image editing relatively unexplored. Among the few studies\ntargeting image editing, most utilize visible triggers, which are impractical\nbecause they introduce noticeable alterations to the input image before\nediting. In this paper, we propose a novel attack framework that embeds\ninvisible triggers into the image editing process via poisoned training data.\nWe leverage off-the-shelf deep watermarking models to encode imperceptible\nwatermarks as backdoor triggers. Our goal is to make the model produce the\npredefined backdoor target when it receives watermarked inputs, while editing\nclean images normally according to the given prompt. With extensive experiments\nacross different watermarking models, the proposed method achieves promising\nattack success rates. In addition, the analysis results of the watermark\ncharacteristics in term of backdoor attack further support the effectiveness of\nour approach. The code is available\nat:https://github.com/aiiu-lab/BackdoorImageEditing", "authors": ["Yu-Feng Chen", "Tzuhsuan Huang", "Pin-Yen Chiu", "Jun-Cheng Chen"], "published_date": "2025-06-05", "title_zh": "基於深度浮水印的圖像編輯模型中不可見後門觸發器", "summary_zh": "擴散模型在圖像生成和編輯領域表現出色，但研究顯示其易受後門攻擊影響。現有研究主要針對圖像生成流程，對圖像編輯中的後門攻擊探索不足，且多使用可見觸發器，實用性較低。本文提出一種新穎的攻擊框架，透過中毒訓練資料將隱形觸發器嵌入圖像編輯過程。利用現成的深度浮水印模型編碼難以察覺的浮水印作為後門觸發器，使模型在接收帶浮水印的輸入時產生預定義的後門目標，同時正常編輯乾淨圖像。實驗結果表明，該方法在不同浮水印模型下均能實現較高的攻擊成功率。對浮水印特徵的分析進一步驗證了該方法在後門攻擊中的有效性。程式碼已公開。", "audio": "audios/2506.04879v1.mp3", "timestamp": "2025-06-06T14:18:52.513255"}
{"query": "AI", "id": "2506.05211v1", "url": "http://arxiv.org/abs/2506.05211v1", "title": "Intentionally Unintentional: GenAI Exceptionalism and the First Amendment", "summary": "This paper challenges the assumption that courts should grant First Amendment\nprotections to outputs from large generative AI models, such as GPT-4 and\nGemini. We argue that because these models lack intentionality, their outputs\ndo not constitute speech as understood in the context of established legal\nprecedent, so there can be no speech to protect. Furthermore, if the model\noutputs are not speech, users cannot claim a First Amendment speech right to\nreceive the outputs. We also argue that extending First Amendment rights to AI\nmodels would not serve the fundamental purposes of free speech, such as\npromoting a marketplace of ideas, facilitating self-governance, or fostering\nself-expression. In fact, granting First Amendment protections to AI models\nwould be detrimental to society because it would hinder the government's\nability to regulate these powerful technologies effectively, potentially\nleading to the unchecked spread of misinformation and other harms.", "authors": ["David Atkinson", "Jena D. Hwang", "Jacob Morrison"], "published_date": "2025-06-05", "title_zh": "有意為之的無意：通用人工智慧例外論與第一修正案", "summary_zh": "本文質疑法院應賦予大型生成式AI模型（如GPT-4和Gemini）產出物第一修正案保障的假設。論證要點為：由於這些模型缺乏意圖性，其產出物不構成既定法律先例所認知的言論，因此無言論可保護。若模型產出物非言論，使用者亦無法主張接收產出物的第一修正案言論權。擴大AI模型的第一修正案權利無助於促進思想市場、推動自治或培養自我表達等言論自由基本目的，反而會妨礙政府有效監管此類強大技術的能力，可能導致不實資訊的無限制傳播及其他危害，對社會造成不利影響。", "audio": "audios/2506.05211v1.mp3", "timestamp": "2025-06-06T15:17:54.939894"}
{"query": "Foundation Model", "id": "2506.04590v1", "url": "http://arxiv.org/abs/2506.04590v1", "title": "Follow-Your-Creation: Empowering 4D Creation through Video Inpainting", "summary": "We introduce Follow-Your-Creation, a novel 4D video creation framework\ncapable of both generating and editing 4D content from a single monocular video\ninput. By leveraging a powerful video inpainting foundation model as a\ngenerative prior, we reformulate 4D video creation as a video inpainting task,\nenabling the model to fill in missing content caused by camera trajectory\nchanges or user edits. To facilitate this, we generate composite masked\ninpainting video data to effectively fine-tune the model for 4D video\ngeneration. Given an input video and its associated camera trajectory, we first\nperform depth-based point cloud rendering to obtain invisibility masks that\nindicate the regions that should be completed. Simultaneously, editing masks\nare introduced to specify user-defined modifications, and these are combined\nwith the invisibility masks to create a composite masks dataset. During\ntraining, we randomly sample different types of masks to construct diverse and\nchallenging inpainting scenarios, enhancing the model's generalization and\nrobustness in various 4D editing and generation tasks. To handle temporal\nconsistency under large camera motion, we design a self-iterative tuning\nstrategy that gradually increases the viewing angles during training, where the\nmodel is used to generate the next-stage training data after each fine-tuning\niteration. Moreover, we introduce a temporal packaging module during inference\nto enhance generation quality. Our method effectively leverages the prior\nknowledge of the base model without degrading its original performance,\nenabling the generation of 4D videos with consistent multi-view coherence. In\naddition, our approach supports prompt-based content editing, demonstrating\nstrong flexibility and significantly outperforming state-of-the-art methods in\nboth quality and versatility.", "authors": ["Yue Ma", "Kunyu Feng", "Xinhua Zhang", "Hongyu Liu", "David Junhao Zhang", "Jinbo Xing", "Yinhan Zhang", "Ayden Yang", "Zeyu Wang", "Qifeng Chen"], "published_date": "2025-06-05", "title_zh": "追隨你的創作：通過影片修復賦能四維創作", "summary_zh": "本文提出名為Follow-Your-Creation的全新四維影片創作框架，可從單目影片輸入生成及編輯四維內容。藉由強大的影片修復基礎模型作為生成先驗，將四維影片創作重新定義為影片修復任務，使模型能填補因相機軌跡變化或使用者編輯造成的內容缺失。為此，產生複合遮罩修復影片資料，有效微調模型以進行四維影片生成。給定輸入影片及其相關相機軌跡，首先執行基於深度的點雲渲染，以獲得指示應完成區域的不可見遮罩。同時，引入編輯遮罩以指定使用者定義的修改，並將其與不可見遮罩組合以建立複合遮罩資料集。在訓練期間，隨機取樣不同類型的遮罩以建構多樣且具挑戰性的修復情境，從而增強模型在各種四維編輯和生成任務中的泛化性和穩健性。為了解決大相機運動下的時間一致性問題，設計了一種自我迭代調整策略，在訓練期間逐步增加視角，在每次微調迭代後，模型用於產生下一階段的訓練資料。此外，在推理過程中引入時間封裝模組以提高生成品質。此方法有效地利用了基礎模型的先驗知識，且不降低其原始性能，從而能夠生成具有一致多視圖連貫性的四維影片。此外，此方法支援基於提示詞的內容編輯，展現出強大的靈活性，並且在品質和多功能性方面均顯著優於最先進的方法。", "audio": "audios/2506.04590v1.mp3", "timestamp": "2025-06-06T15:18:04.097199"}
{"query": "Diffusion Model", "id": "2506.04859v1", "url": "http://arxiv.org/abs/2506.04859v1", "title": "Sparse Autoencoders, Again?", "summary": "Is there really much more to say about sparse autoencoders (SAEs)?\nAutoencoders in general, and SAEs in particular, represent deep architectures\nthat are capable of modeling low-dimensional latent structure in data. Such\nstructure could reflect, among other things, correlation patterns in large\nlanguage model activations, or complex natural image manifolds. And yet despite\nthe wide-ranging applicability, there have been relatively few changes to SAEs\nbeyond the original recipe from decades ago, namely, standard deep\nencoder/decoder layers trained with a classical/deterministic sparse\nregularizer applied within the latent space. One possible exception is the\nvariational autoencoder (VAE), which adopts a stochastic encoder module capable\nof producing sparse representations when applied to manifold data. In this work\nwe formalize underappreciated weaknesses with both canonical SAEs, as well as\nanalogous VAEs applied to similar tasks, and propose a hybrid alternative model\nthat circumvents these prior limitations. In terms of theoretical support, we\nprove that global minima of our proposed model recover certain forms of\nstructured data spread across a union of manifolds. Meanwhile, empirical\nevaluations on synthetic and real-world datasets substantiate the efficacy of\nour approach in accurately estimating underlying manifold dimensions and\nproducing sparser latent representations without compromising reconstruction\nerror. In general, we are able to exceed the performance of equivalent-capacity\nSAEs and VAEs, as well as recent diffusion models where applicable, within\ndomains such as images and language model activation patterns.", "authors": ["Yin Lu", "Tong He", "Xuening Zhu", "David Wipf"], "published_date": "2025-06-05", "title_zh": "稀疏自動編碼器，再議？", "summary_zh": "關於稀疏自動編碼器(SAE)是否還有更多可探討之處？ SAE作為深度架構，能對數據中的低維潛在結構進行建模，例如大型語言模型激活中的相關性或複雜自然圖像流形。儘管應用廣泛，SAE在原始基礎上的改進相對較少。本文指出傳統SAE及類似VAE在處理此類任務時存在的不足，並提出一種混合模型以克服這些限制。理論上，該模型能恢復流形聯合體上的結構化數據。在合成及真實數據集上的實驗表明，該方法能準確估計流形維度，並在不影響重構誤差的情況下產生更稀疏的潛在表示。在圖像和語言模型激活模式等領域，該模型通常優於同等容量的SAE、VAE以及近期的擴散模型。", "audio": "audios/2506.04859v1.mp3", "timestamp": "2025-06-06T15:18:09.822786"}
{"query": "AI", "id": "2506.05203v1", "url": "http://arxiv.org/abs/2506.05203v1", "title": "Trustworthiness Preservation by Copies of Machine Learning Systems", "summary": "A common practice of ML systems development concerns the training of the same\nmodel under different data sets, and the use of the same (training and test)\nsets for different learning models. The first case is a desirable practice for\nidentifying high quality and unbiased training conditions. The latter case\ncoincides with the search for optimal models under a common dataset for\ntraining. These differently obtained systems have been considered akin to\ncopies. In the quest for responsible AI, a legitimate but hardly investigated\nquestion is how to verify that trustworthiness is preserved by copies. In this\npaper we introduce a calculus to model and verify probabilistic complex queries\nover data and define four distinct notions: Justifiably, Equally, Weakly and\nAlmost Trustworthy which can be checked analysing the (partial) behaviour of\nthe copy with respect to its original. We provide a study of the relations\nbetween these notions of trustworthiness, and how they compose with each other\nand under logical operations. The aim is to offer a computational tool to check\nthe trustworthiness of possibly complex systems copied from an original whose\nbehavour is known.", "authors": ["Leonardo Ceragioli", "Giuseppe Primiero"], "published_date": "2025-06-05", "title_zh": "機器學習系統副本之可信度保持", "summary_zh": "機器學習系統開發常見做法包括以不同數據集訓練相同模型，以及使用相同訓練和測試集評估不同學習模型。前者有助於識別高品質且無偏頗的訓練條件，後者旨在尋找在共同數據集下之最佳模型。這些經由不同途徑獲得的系統常被視為副本。在追求負責任AI的過程中，一個合理但鮮少探討的問題是，如何驗證副本是否保留了可信度。本文提出一種演算，以建模和驗證數據上的概率複雜查詢，並定義四種不同的可信度概念：正當可信、等同可信、弱可信和近乎可信，這些概念可透過分析副本相對於原始模型的（部分）行為來檢驗。我們研究這些可信度概念之間的關係，以及它們如何彼此組合和在邏輯運算下組合。目標是提供一種計算工具，用於檢查可能複雜的系統副本，其可信度是基於已知行為的原始模型。", "audio": "audios/2506.05203v1.mp3", "timestamp": "2025-06-06T16:24:40.134629"}
{"query": "Foundation Model", "id": "2506.04586v1", "url": "http://arxiv.org/abs/2506.04586v1", "title": "LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech Foundational Models", "summary": "We introduce LESS (Large Language Model Enhanced Semi-supervised Learning), a\nversatile framework that leverages Large Language Models (LLMs) to correct\npseudo labels generated from in-the-wild data. Within the LESS framework,\npseudo-labeled text from Automatic Speech Recognition (ASR) or Automatic Speech\nTranslation (AST) of the unsupervised data is refined by an LLM, and augmented\nby a data filtering strategy to optimize LLM knowledge transfer efficiency.\nExperiments on both Mandarin ASR and Spanish-to-English AST tasks show that\nLESS achieves a notable absolute WER reduction of 3.77% on the Wenet Speech\ntest set, as well as BLEU scores of 34.0 and 64.7 on Callhome and Fisher test\nsets respectively. These results validate the adaptability of LESS across\ndifferent languages, tasks, and domains. Ablation studies conducted with\nvarious LLMs and prompt configurations provide novel insights into leveraging\nLLM-derived knowledge for speech processing applications.", "authors": ["Wen Ding", "Fan Qian"], "published_date": "2025-06-05", "title_zh": "LESS：大型語言模型增強的語音基礎模型半監督學習", "summary_zh": "LESS（大型語言模型增強半監督學習）是一個通用框架，利用大型語言模型（LLM）校正來自實際數據產生的偽標籤。在LESS框架中，來自自動語音識別（ASR）或自動語音翻譯（AST）的無監督數據之偽標記文本，經由LLM精煉，並透過數據篩選策略增強，以優化LLM知識轉移效率。在普通話ASR和西班牙語到英語AST任務上的實驗表明，LESS在Wenet Speech測試集上實現了顯著的3.77%絕對詞錯誤率（WER）降低，並在Callhome和Fisher測試集上分別獲得34.0和64.7的BLEU分數。這些結果驗證了LESS在不同語言、任務和領域的適應性。使用不同LLM和提示配置進行的消融研究，為利用LLM知識進行語音處理應用提供了新的見解。", "audio": "audios/2506.04586v1.mp3", "timestamp": "2025-06-06T16:24:49.942550"}
{"query": "Diffusion Model", "id": "2506.04716v1", "url": "http://arxiv.org/abs/2506.04716v1", "title": "Learning dissection trajectories from expert surgical videos via imitation learning with equivariant diffusion", "summary": "Endoscopic Submucosal Dissection (ESD) is a well-established technique for\nremoving epithelial lesions. Predicting dissection trajectories in ESD videos\noffers significant potential for enhancing surgical skill training and\nsimplifying the learning process, yet this area remains underexplored. While\nimitation learning has shown promise in acquiring skills from expert\ndemonstrations, challenges persist in handling uncertain future movements,\nlearning geometric symmetries, and generalizing to diverse surgical scenarios.\nTo address these, we introduce a novel approach: Implicit Diffusion Policy with\nEquivariant Representations for Imitation Learning (iDPOE). Our method models\nexpert behavior through a joint state action distribution, capturing the\nstochastic nature of dissection trajectories and enabling robust visual\nrepresentation learning across various endoscopic views. By incorporating a\ndiffusion model into policy learning, iDPOE ensures efficient training and\nsampling, leading to more accurate predictions and better generalization.\nAdditionally, we enhance the model's ability to generalize to geometric\nsymmetries by embedding equivariance into the learning process. To address\nstate mismatches, we develop a forward-process guided action inference strategy\nfor conditional sampling. Using an ESD video dataset of nearly 2000 clips,\nexperimental results show that our approach surpasses state-of-the-art methods,\nboth explicit and implicit, in trajectory prediction. To the best of our\nknowledge, this is the first application of imitation learning to surgical\nskill development for dissection trajectory prediction.", "authors": ["Hongyu Wang", "Yonghao Long", "Yueyao Chen", "Hon-Chi Yip", "Markus Scheppach", "Philip Wai-Yan Chiu", "Yeung Yam", "Helen Mei-Ling Meng", "Qi Dou"], "published_date": "2025-06-05", "title_zh": "藉由等變擴散之模仿學習，從專家手術影片中學習解剖軌跡", "summary_zh": "內視鏡黏膜下剝離術(ESD)為切除上皮病灶的成熟技術。預測ESD影片中的剝離軌跡，能有效提升手術訓練與簡化學習過程，惟相關研究尚待開發。模仿學習雖展現從專家示範中習得技能的潛力，然處理不確定未來動作、學習幾何對稱性及推廣至不同手術情境仍具挑戰。為此，我們提出一種新穎方法：具等變表示的隱式擴散策略模仿學習(iDPOE)。此方法透過聯合狀態-動作分佈對專家行為進行建模，捕捉剝離軌跡的隨機性，並實現跨多種內視鏡視角的穩健視覺表徵學習。透過將擴散模型融入策略學習，iDPOE確保高效訓練和採樣，進而實現更準確的預測和更好的泛化能力。此外，我們透過將等變性嵌入學習過程，增強模型推廣至幾何對稱性的能力。為了解決狀態不匹配問題，我們開發了一種前向過程引導的動作推斷策略，用於條件採樣。實驗結果基於近2000個ESD影片片段的數據集，顯示我們的模型在軌跡預測方面超越了現有最佳方法（顯式和隱式）。據我們所知，這是模仿學習在剝離軌跡預測手術技能發展中的首次應用。", "audio": "audios/2506.04716v1.mp3", "timestamp": "2025-06-06T16:25:04.626687"}
{"query": "AI", "id": "2506.05171v1", "url": "http://arxiv.org/abs/2506.05171v1", "title": "Towards provable probabilistic safety for scalable embodied AI systems", "summary": "Embodied AI systems, comprising AI models and physical plants, are\nincreasingly prevalent across various applications. Due to the rarity of system\nfailures, ensuring their safety in complex operating environments remains a\nmajor challenge, which severely hinders their large-scale deployment in\nsafety-critical domains, such as autonomous vehicles, medical devices, and\nrobotics. While achieving provable deterministic safety--verifying system\nsafety across all possible scenarios--remains theoretically ideal, the rarity\nand complexity of corner cases make this approach impractical for scalable\nembodied AI systems. To address this challenge, we introduce provable\nprobabilistic safety, which aims to ensure that the residual risk of\nlarge-scale deployment remains below a predefined threshold. Instead of\nattempting exhaustive safety proof across all corner cases, this paradigm\nestablishes a probabilistic safety boundary on overall system performance,\nleveraging statistical methods to enhance feasibility and scalability. A\nwell-defined probabilistic safety boundary enables embodied AI systems to be\ndeployed at scale while allowing for continuous refinement of safety\nguarantees. Our work focuses on three core questions: what is provable\nprobabilistic safety, how to prove the probabilistic safety, and how to achieve\nthe provable probabilistic safety. By bridging the gap between theoretical\nsafety assurance and practical deployment, our work offers a pathway toward\nsafer, large-scale adoption of embodied AI systems in safety-critical\napplications.", "authors": ["Linxuan He", "Qing-Shan Jia", "Ang Li", "Hongyan Sang", "Ling Wang", "Jiwen Lu", "Tao Zhang", "Jie Zhou", "Yi Zhang", "Yisen Wang", "Peng Wei", "Zhongyuan Wang", "Henry X. Liu", "Shuo Feng"], "published_date": "2025-06-05", "title_zh": "邁向具可驗證機率安全性的可擴展具身人工智慧系統", "summary_zh": "具身人工智慧系統結合AI模型與實體設備，應用日趨廣泛。然因系統故障罕見，確保複雜環境下的安全仍是主要挑戰，阻礙其於自駕車、醫療設備、機器人等安全攸關領域的大規模部署。儘管驗證所有情境下的系統安全是理想目標，但邊緣案例的稀有性與複雜度使其難以規模化。為此，我們提出可驗證的機率安全，旨在確保大規模部署的殘餘風險低於預定閾值。此範例不追求所有邊緣案例的詳盡安全驗證，而是建立系統整體效能的機率安全邊界，利用統計方法提升可行性與規模化。明確的機率安全邊界使具身AI系統得以規模化部署，並持續精進安全保證。本研究聚焦於三個核心問題：何謂可驗證的機率安全、如何驗證機率安全、以及如何達成可驗證的機率安全。透過彌合理論安全保證與實際部署之間的差距，本研究為具身AI系統於安全攸關應用中的安全大規模採用提供了一條途徑。", "audio": "audios/2506.05171v1.mp3", "timestamp": "2025-06-06T17:18:08.182956"}
{"query": "Foundation Model", "id": "2506.04571v1", "url": "http://arxiv.org/abs/2506.04571v1", "title": "OpenAg: Democratizing Agricultural Intelligence", "summary": "Agriculture is undergoing a major transformation driven by artificial\nintelligence (AI), machine learning, and knowledge representation technologies.\nHowever, current agricultural intelligence systems often lack contextual\nunderstanding, explainability, and adaptability, especially for smallholder\nfarmers with limited resources. General-purpose large language models (LLMs),\nwhile powerful, typically lack the domain-specific knowledge and contextual\nreasoning needed for practical decision support in farming. They tend to\nproduce recommendations that are too generic or unrealistic for real-world\napplications. To address these challenges, we present OpenAg, a comprehensive\nframework designed to advance agricultural artificial general intelligence\n(AGI). OpenAg combines domain-specific foundation models, neural knowledge\ngraphs, multi-agent reasoning, causal explainability, and adaptive transfer\nlearning to deliver context-aware, explainable, and actionable insights. The\nsystem includes: (i) a unified agricultural knowledge base that integrates\nscientific literature, sensor data, and farmer-generated knowledge; (ii) a\nneural agricultural knowledge graph for structured reasoning and inference;\n(iii) an adaptive multi-agent reasoning system where AI agents specialize and\ncollaborate across agricultural domains; and (iv) a causal transparency\nmechanism that ensures AI recommendations are interpretable, scientifically\ngrounded, and aligned with real-world constraints. OpenAg aims to bridge the\ngap between scientific knowledge and the tacit expertise of experienced farmers\nto support scalable and locally relevant agricultural decision-making.", "authors": ["Srikanth Thudumu", "Jason Fisher"], "published_date": "2025-06-05", "title_zh": "開放農業：普及農業智能", "summary_zh": "人工智慧、機器學習和知識表示技術正驅動農業轉型。現有農業智慧系統普遍缺乏情境理解、可解釋性和適應性，特別是對資源有限的小農而言。通用大型語言模型雖強大，但缺乏農業領域知識和情境推理能力，難以提供實用決策支援，建議往往過於籠統或不切實際。為此，我們提出OpenAg，旨在推進農業人工通用智慧。OpenAg結合領域特定基礎模型、神經知識圖譜、多代理推理、因果可解釋性及自適應遷移學習，以提供具情境感知、可解釋且可執行的洞見。該系統包含：(i)整合科學文獻、感測器數據和農民經驗的統一農業知識庫；(ii)用於結構化推理的神經農業知識圖譜；(iii) AI代理在農業領域專業分工協作的自適應多代理推理系統；(iv)確保AI建議可解釋、具科學依據且符合實際限制的因果透明機制。OpenAg旨在彌合科學知識與農民經驗之間的差距，以支持可擴展且與在地相關的農業決策。", "audio": "audios/2506.04571v1.mp3", "timestamp": "2025-06-06T17:18:15.928837"}
{"query": "Diffusion Model", "id": "2506.04641v1", "url": "http://arxiv.org/abs/2506.04641v1", "title": "Text-Aware Real-World Image Super-Resolution via Diffusion Model with Joint Segmentation Decoders", "summary": "The introduction of generative models has significantly advanced image\nsuper-resolution (SR) in handling real-world degradations. However, they often\nincur fidelity-related issues, particularly distorting textual structures. In\nthis paper, we introduce a novel diffusion-based SR framework, namely TADiSR,\nwhich integrates text-aware attention and joint segmentation decoders to\nrecover not only natural details but also the structural fidelity of text\nregions in degraded real-world images. Moreover, we propose a complete pipeline\nfor synthesizing high-quality images with fine-grained full-image text masks,\ncombining realistic foreground text regions with detailed background content.\nExtensive experiments demonstrate that our approach substantially enhances text\nlegibility in super-resolved images, achieving state-of-the-art performance\nacross multiple evaluation metrics and exhibiting strong generalization to\nreal-world scenarios. Our code is available at\n\\href{https://github.com/mingcv/TADiSR}{here}.", "authors": ["Qiming Hu", "Linlong Fan", "Yiyan Luo", "Yuhang Yu", "Xiaojie Guo", "Qingnan Fan"], "published_date": "2025-06-05", "title_zh": "基於擴散模型與聯合分割解碼器的文本感知真實世界圖像超解析度", "summary_zh": "生成模型顯著推進了圖像超解析度在處理真實降質方面的進展，但常導致保真度問題，尤其扭曲文本結構。本研究提出名為TADiSR的新型基於擴散的超解析度框架，整合文本感知注意力及聯合分割解碼器，以恢復真實降質圖像中的自然細節與文本區域的結構保真度。此外，我們提出完整流程，結合逼真的前景文本區域和細緻的背景內容，合成具精細全圖像文本遮罩的高品質圖像。實驗結果表明，我們的方案大幅提升超解析度圖像中的文本可讀性，在多項評估指標上實現了最先進的性能，並展現出對真實場景的強大泛化能力。", "audio": "audios/2506.04641v1.mp3", "timestamp": "2025-06-06T17:18:21.039938"}
{"query": "AI", "id": "2506.05135v1", "url": "http://arxiv.org/abs/2506.05135v1", "title": "Noise-Driven AI Sensors: Secure Healthcare Monitoring with PUFs", "summary": "Wearable and implantable healthcare sensors are pivotal for real-time patient\nmonitoring but face critical challenges in power efficiency, data security, and\nsignal noise. This paper introduces a novel platform that leverages hardware\nnoise as a dual-purpose resource to enhance machine learning (ML) robustness\nand secure data via Physical Unclonable Functions (PUFs). By integrating\nnoise-driven signal processing, PUFbased authentication, and ML-based anomaly\ndetection, our system achieves secure, low-power monitoring for devices like\nECG wearables. Simulations demonstrate that noise improves ML accuracy by 8%\n(92% for detecting premature ventricular contractions (PVCs) and atrial\nfibrillation (AF)), while PUFs provide 98% uniqueness for tamper-resistant\nsecurity, all within a 50 uW power budget. This unified approach not only\naddresses power, security, and noise challenges but also enables scalable,\nintelligent sensing for telemedicine and IoT applications.", "authors": ["Christiana Chamon", "Abhijit Sarkar", "A. Lynn Abbott"], "published_date": "2025-06-05", "title_zh": "噪聲驅動的人工智慧感測器：基於PUF的安全醫療保健監控", "summary_zh": "穿戴式與植入式醫療感測器對即時病人監測至關重要，但在功率效率、數據安全及訊號雜訊方面面臨挑戰。本文提出一種新穎平台，利用硬體雜訊作為雙重用途資源，增強機器學習的穩健性，並透過物理不可複製函數（PUFs）保護數據安全。透過整合雜訊驅動的訊號處理、基於PUF的身份驗證和基於機器學習的異常檢測，該系統為心電圖穿戴裝置等設備實現安全、低功耗監測。模擬結果顯示，雜訊將機器學習準確度提高8%（早發性心室收縮(PVCs)和心房顫動(AF)的檢測準確度為92%），同時PUF提供98%的唯一性，實現防篡改安全性，且功耗維持在50微瓦以內。此統一方法不僅解決了功率、安全和雜訊挑戰，還為遠程醫療和物聯網應用實現可擴展的智慧感測。", "audio": "audios/2506.05135v1.mp3", "timestamp": "2025-06-06T18:26:39.583800"}
{"query": "Foundation Model", "id": "2506.04552v1", "url": "http://arxiv.org/abs/2506.04552v1", "title": "DAS-MAE: A self-supervised pre-training framework for universal and high-performance representation learning of distributed fiber-optic acoustic sensing", "summary": "Distributed fiber-optic acoustic sensing (DAS) has emerged as a\ntransformative approach for distributed vibration measurement with high spatial\nresolution and long measurement range while maintaining cost-efficiency.\nHowever, the two-dimensional spatial-temporal DAS signals present analytical\nchallenges. The abstract signal morphology lacking intuitive physical\ncorrespondence complicates human interpretation, and its unique\nspatial-temporal coupling renders conventional image processing methods\nsuboptimal. This study investigates spatial-temporal characteristics and\nproposes a self-supervised pre-training framework that learns signals'\nrepresentations through a mask-reconstruction task. This framework is named the\nDAS Masked AutoEncoder (DAS-MAE). The DAS-MAE learns high-level representations\n(e.g., event class) without using labels. It achieves up to 1% error and 64.5%\nrelative improvement (RI) over the semi-supervised baseline in few-shot\nclassification tasks. In a practical external damage prevention application,\nDAS-MAE attains a 5.0% recognition error, marking a 75.7% RI over supervised\ntraining from scratch. These results demonstrate the high-performance and\nuniversal representations learned by the DAS-MAE framework, highlighting its\npotential as a foundation model for analyzing massive unlabeled DAS signals.", "authors": ["Junyi Duan", "Jiageng Chen", "Zuyuan He"], "published_date": "2025-06-05", "title_zh": "DAS-MAE：用於分布式光纖聲學感測通用且高效能表示學習的自監督預訓練框架", "summary_zh": "分散式光纖聲波感測(DAS)已成為一種變革性方法，能以高空間解析度和長測量範圍進行分散式振動測量，同時保持成本效益。然而，二維時空DAS訊號帶來分析挑戰，缺乏直觀物理對應的抽象訊號形態使人工判讀複雜化，其獨特的時空耦合性也使傳統圖像處理方法效果不佳。本研究探討時空特性，並提出一種自我監督預訓練框架，透過遮罩重建任務學習訊號表徵。此框架名為DAS遮罩自編碼器(DAS-MAE)，無需標籤即可學習高階表徵(如事件類別)，在少量樣本分類任務中，錯誤率降低至1%，相較於半監督基準線，相對改進幅度達64.5%。在實際的外力破壞預防應用中，DAS-MAE的辨識錯誤率為5.0%，相較於從頭開始的監督式訓練，相對改進幅度達75.7%。這些結果展示了DAS-MAE框架所學習到的高效能和通用表徵，突顯了其作為分析大量未標記DAS訊號的基礎模型的潛力。", "audio": "audios/2506.04552v1.mp3", "timestamp": "2025-06-06T18:26:47.147846"}
{"query": "Diffusion Model", "id": "2506.04612v1", "url": "http://arxiv.org/abs/2506.04612v1", "title": "Perfecting Depth: Uncertainty-Aware Enhancement of Metric Depth", "summary": "We propose a novel two-stage framework for sensor depth enhancement, called\nPerfecting Depth. This framework leverages the stochastic nature of diffusion\nmodels to automatically detect unreliable depth regions while preserving\ngeometric cues. In the first stage (stochastic estimation), the method\nidentifies unreliable measurements and infers geometric structure by leveraging\na training-inference domain gap. In the second stage (deterministic\nrefinement), it enforces structural consistency and pixel-level accuracy using\nthe uncertainty map derived from the first stage. By combining stochastic\nuncertainty modeling with deterministic refinement, our method yields dense,\nartifact-free depth maps with improved reliability. Experimental results\ndemonstrate its effectiveness across diverse real-world scenarios. Furthermore,\ntheoretical analysis, various experiments, and qualitative visualizations\nvalidate its robustness and scalability. Our framework sets a new baseline for\nsensor depth enhancement, with potential applications in autonomous driving,\nrobotics, and immersive technologies.", "authors": ["Jinyoung Jun", "Lei Chu", "Jiahao Li", "Yan Lu", "Chang-Su Kim"], "published_date": "2025-06-05", "title_zh": "完善深度：基於不確定性的度量深度增強", "summary_zh": "本研究提出名為「完善深度」的新型兩階段感測器深度增強框架。此框架利用擴散模型的隨機性自動偵測不可靠深度區域，同時保留幾何線索。第一階段（隨機估計）透過訓練-推論域間隙識別不可靠測量並推斷幾何結構。第二階段（確定性精煉）利用第一階段產生的不確定性圖，強化結構一致性和像素級精確度。結合隨機不確定性建模與確定性精煉，本方法產生密集、無瑕疵且可靠性更高的深度圖。實驗結果證明其在各種真實場景中的有效性。理論分析、多項實驗及視覺化驗證了其穩健性和可擴展性。此框架為感測器深度增強設定了新基準，潛在應用於自動駕駛、機器人和沉浸式技術。", "audio": "audios/2506.04612v1.mp3", "timestamp": "2025-06-06T18:26:52.169655"}
{"query": "AI", "id": "2506.05111v1", "url": "http://arxiv.org/abs/2506.05111v1", "title": "An SCMA Receiver for 6G NTN based on Multi-Task Learning", "summary": "Future 6G networks are envisioned to enhance the user experience in a\nmultitude of different ways. The unification of existing terrestrial networks\nwith non-terrestrial network (NTN) components will provide users with\nubiquitous connectivity. Multi-access edge computing (MEC) will enable\nlow-latency services, with computations performed closer to the end users, and\ndistributed learning paradigms. Advanced multiple access schemes, such as\nsparse code multiple access (SCMA), can be employed to efficiently move data\nfrom edge nodes to spaceborne MEC servers. However, the non-orthogonal nature\nof SCMA results in interference, limiting the effectiveness of traditional SCMA\nreceivers. Hence, NTN links should be protected with robust channel codes,\nsignificantly reducing the uplink throughput. Thus, we investigate the\napplication of artificial intelligence (AI) to SCMA receivers for 6G NTNs. We\ntrain an AI model with multi-task learning to optimally separate and receive\nsuperimposed SCMA signals. Through link level simulations, we evaluate the\nblock error rate (BLER) and the aggregated theoretical throughput achieved by\nthe AI model as a function of the received energy per bit over noise power\nspectral density ratio (Eb/N0). We show that the proposed receiver achieves a\ntarget 10% BLER with 3.5dB lower Eb/N0 with respect to the benchmark algorithm.\nWe conclude the assessment discussing the complexity-related challenges to the\nimplementation of the AI model on board of a low earth orbit satellite.", "authors": ["Bruno De Filippo", "Carla Amatetti", "Riccardo Campana", "Alessandro Guidotti", "Alessandro Vanelli-Coralli"], "published_date": "2025-06-05", "title_zh": "基於多任務學習的6G NTN之SCMA接收器", "summary_zh": "未來的6G網路旨在透過多種方式提升使用者體驗。整合地面網路與非地面網路(NTN)組件將提供無所不在的連接性。多接取邊緣運算(MEC)將實現低延遲服務及分散式學習。稀疏碼多重接取(SCMA)等先進多重接取方案可有效傳輸數據至星載MEC伺服器。然而，SCMA的非正交性導致干擾，限制了傳統SCMA接收器的效能。因此，NTN鏈路需要可靠的通道編碼保護，這會顯著降低上行鏈路吞吐量。本研究探討人工智慧(AI)在6G NTN中SCMA接收器的應用。透過多任務學習訓練AI模型，以最佳化分離和接收疊加的SCMA訊號。鏈路級模擬顯示，相較於基準演算法，該AI模型在達到10%區塊錯誤率(BLER)目標時，所需的每位元能量與雜訊功率譜密度比(Eb/N0)降低了3.5dB。最後，討論了在近地軌道衛星上實施AI模型所面臨的複雜度挑戰。", "audio": "audios/2506.05111v1.mp3", "timestamp": "2025-06-06T19:16:08.845841"}
{"query": "Diffusion Model", "id": "2506.04606v1", "url": "http://arxiv.org/abs/2506.04606v1", "title": "SmartAvatar: Text- and Image-Guided Human Avatar Generation with VLM AI Agents", "summary": "SmartAvatar is a vision-language-agent-driven framework for generating fully\nrigged, animation-ready 3D human avatars from a single photo or textual prompt.\nWhile diffusion-based methods have made progress in general 3D object\ngeneration, they continue to struggle with precise control over human identity,\nbody shape, and animation readiness. In contrast, SmartAvatar leverages the\ncommonsense reasoning capabilities of large vision-language models (VLMs) in\ncombination with off-the-shelf parametric human generators to deliver\nhigh-quality, customizable avatars. A key innovation is an autonomous\nverification loop, where the agent renders draft avatars, evaluates facial\nsimilarity, anatomical plausibility, and prompt alignment, and iteratively\nadjusts generation parameters for convergence. This interactive, AI-guided\nrefinement process promotes fine-grained control over both facial and body\nfeatures, enabling users to iteratively refine their avatars via\nnatural-language conversations. Unlike diffusion models that rely on static\npre-trained datasets and offer limited flexibility, SmartAvatar brings users\ninto the modeling loop and ensures continuous improvement through an LLM-driven\nprocedural generation and verification system. The generated avatars are fully\nrigged and support pose manipulation with consistent identity and appearance,\nmaking them suitable for downstream animation and interactive applications.\nQuantitative benchmarks and user studies demonstrate that SmartAvatar\noutperforms recent text- and image-driven avatar generation systems in terms of\nreconstructed mesh quality, identity fidelity, attribute accuracy, and\nanimation readiness, making it a versatile tool for realistic, customizable\navatar creation on consumer-grade hardware.", "authors": ["Alexander Huang-Menders", "Xinhang Liu", "Andy Xu", "Yuyao Zhang", "Chi-Keung Tang", "Yu-Wing Tai"], "published_date": "2025-06-05", "title_zh": "智慧化身：基於文本與圖像引導，利用VLM人工智慧代理生成人體化身", "summary_zh": "SmartAvatar為一視覺語言代理驅動框架，可由單張照片或文字提示生成完整骨架、可供動畫製作之3D人體頭像。有別於擴散模型在精確控制人物身分、體態及動畫適用性上的不足，SmartAvatar利用大型視覺語言模型(VLM)的常識推理能力，結合現成參數化人體生成器，產出高品質、可客製化之頭像。其關鍵創新為一自主驗證迴路，該代理渲染初步頭像，評估面部相似度、解剖學合理性及提示對齊程度，並迭代調整生成參數以實現收斂。此互動式、AI引導之精煉過程促進對面部及身體特徵的精細控制，使用者可透過自然語言對話迭代精煉頭像。與依賴靜態預訓練數據集且靈活性有限的擴散模型不同，SmartAvatar將使用者納入建模迴路，並透過LLM驅動的程序生成和驗證系統確保持續改進。所生成頭像具備完整骨架，支援姿勢操控，並保持一致的身分和外觀，適用於下游動畫和互動應用。量化基準測試與使用者研究表明，SmartAvatar在重建網格品質、身分保真度、屬性準確性及動畫適用性方面，優於近期基於文本和圖像的頭像生成系統，使其成為在消費級硬體上創建逼真、可客製化頭像的多功能工具。", "audio": "audios/2506.04606v1.mp3", "timestamp": "2025-06-06T19:16:21.625151"}
{"query": "AI", "id": "2506.05095v1", "url": "http://arxiv.org/abs/2506.05095v1", "title": "FG 2025 TrustFAA: the First Workshop on Towards Trustworthy Facial Affect Analysis: Advancing Insights of Fairness, Explainability, and Safety (TrustFAA)", "summary": "With the increasing prevalence and deployment of Emotion AI-powered facial\naffect analysis (FAA) tools, concerns about the trustworthiness of these\nsystems have become more prominent. This first workshop on \"Towards Trustworthy\nFacial Affect Analysis: Advancing Insights of Fairness, Explainability, and\nSafety (TrustFAA)\" aims to bring together researchers who are investigating\ndifferent challenges in relation to trustworthiness-such as interpretability,\nuncertainty, biases, and privacy-across various facial affect analysis tasks,\nincluding macro/ micro-expression recognition, facial action unit detection,\nother corresponding applications such as pain and depression detection, as well\nas human-robot interaction and collaboration. In alignment with FG2025's\nemphasis on ethics, as demonstrated by the inclusion of an Ethical Impact\nStatement requirement for this year's submissions, this workshop supports\nFG2025's efforts by encouraging research, discussion and dialogue on\ntrustworthy FAA.", "authors": ["Jiaee Cheong", "Yang Liu", "Harold Soh", "Hatice Gunes"], "published_date": "2025-06-05", "title_zh": "FG 2025信賴FAA：邁向可信賴面部情感分析的首屆研討會：推進公平性、可解釋性和安全性 (TrustFAA) 的洞見", "summary_zh": "隨著情緒人工智慧驅動的面部情感分析工具日益普及，其可信賴性備受關注。本次名為邁向可信賴面部情感分析：提升公平性、可解釋性和安全性（TrustFAA）的首屆研討會，旨在匯集研究人員，共同探討面部情感分析各項任務中與可信賴性相關的挑戰，包括巨表情/微表情識別、面部動作單元檢測，以及疼痛和抑鬱症檢測、人機互動與協作等相關應用中的可解釋性、不確定性、偏見和隱私問題。本次研討會呼應FG2025對倫理的重視（體現於今年提交論文需包含倫理影響聲明），透過鼓勵對可信賴FAA的研究、討論和對話，支持FG2025的相關努力。", "audio": "audios/2506.05095v1.mp3", "timestamp": "2025-06-06T20:21:23.500120"}
{"query": "Diffusion Model", "id": "2506.04421v1", "url": "http://arxiv.org/abs/2506.04421v1", "title": "HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation", "summary": "Visual Auto-Regressive modeling (VAR) has shown promise in bridging the speed\nand quality gap between autoregressive image models and diffusion models. VAR\nreformulates autoregressive modeling by decomposing an image into successive\nresolution scales. During inference, an image is generated by predicting all\nthe tokens in the next (higher-resolution) scale, conditioned on all tokens in\nall previous (lower-resolution) scales. However, this formulation suffers from\nreduced image quality due to the parallel generation of all tokens in a\nresolution scale; has sequence lengths scaling superlinearly in image\nresolution; and requires retraining to change the sampling schedule.\n  We introduce Hierarchical Masked Auto-Regressive modeling (HMAR), a new image\ngeneration algorithm that alleviates these issues using next-scale prediction\nand masked prediction to generate high-quality images with fast sampling. HMAR\nreformulates next-scale prediction as a Markovian process, wherein the\nprediction of each resolution scale is conditioned only on tokens in its\nimmediate predecessor instead of the tokens in all predecessor resolutions.\nWhen predicting a resolution scale, HMAR uses a controllable multi-step masked\ngeneration procedure to generate a subset of the tokens in each step. On\nImageNet 256x256 and 512x512 benchmarks, HMAR models match or outperform\nparameter-matched VAR, diffusion, and autoregressive baselines. We develop\nefficient IO-aware block-sparse attention kernels that allow HMAR to achieve\nfaster training and inference times over VAR by over 2.5x and 1.75x\nrespectively, as well as over 3x lower inference memory footprint. Finally,\nHMAR yields additional flexibility over VAR; its sampling schedule can be\nchanged without further training, and it can be applied to image editing tasks\nin a zero-shot manner.", "authors": ["Hermann Kumbong", "Xian Liu", "Tsung-Yi Lin", "Ming-Yu Liu", "Xihui Liu", "Ziwei Liu", "Daniel Y. Fu", "Christopher Ré", "David W. Romero"], "published_date": "2025-06-04", "title_zh": "HMAR：高效層級式遮罩自迴歸圖像生成", "summary_zh": "視覺自迴歸模型(VAR)有潛力彌合自迴歸圖像模型與擴散模型在速度和品質上的差距。VAR透過將圖像分解為連續解析度尺度來重新定義自迴歸建模。但在推論時，由於在一個解析度尺度中平行生成所有tokens，導致圖像品質下降，序列長度隨圖像解析度超線性增長，並且需要重新訓練才能改變取樣排程。\n\n我們引入階層式遮罩自迴歸模型(HMAR)，一種新的圖像生成演算法，它利用下一尺度預測和遮罩預測來緩解這些問題，以快速取樣生成高品質圖像。HMAR將下一尺度預測重新定義為馬可夫過程，其中每個解析度尺度的預測僅以其直接前置尺度中的tokens為條件，而不是以所有前置解析度中的tokens為條件。在預測解析度尺度時，HMAR使用可控的多步遮罩生成程序，在每個步驟中生成tokens的子集。在ImageNet 256x256和512x512基準測試中，HMAR模型匹配或優於參數匹配的VAR、擴散和自迴歸基準。我們開發了高效的IO感知塊稀疏注意力核心，使HMAR能夠實現比VAR快超過2.5倍和1.75倍的訓練和推論時間，以及低於3倍的推論記憶體佔用。最後，HMAR比VAR具有更大的靈活性；其取樣排程可以在沒有進一步訓練的情況下進行更改，並且可以以零樣本方式應用於圖像編輯任務。", "audio": "audios/2506.04421v1.mp3", "timestamp": "2025-06-06T20:21:39.293150"}
{"query": "AI", "id": "2506.05080v1", "url": "http://arxiv.org/abs/2506.05080v1", "title": "Parking, Perception, and Retail: Street-Level Determinants of Community Vitality in Harbin", "summary": "The commercial vitality of community-scale streets in Chinese cities is\nshaped by complex interactions between vehicular accessibility, environmental\nquality, and pedestrian perception. This study proposes an interpretable,\nimage-based framework to examine how street-level features -- including parked\nvehicle density, greenery, cleanliness, and street width -- impact retail\nperformance and user satisfaction in Harbin, China. Leveraging street view\nimagery and a multimodal large language model (VisualGLM-6B), we construct a\nCommunity Commercial Vitality Index (CCVI) from Meituan and Dianping data and\nanalyze its relationship with spatial attributes extracted via GPT-4-based\nperception modeling. Our findings reveal that while moderate vehicle presence\nmay enhance commercial access, excessive on-street parking -- especially in\nnarrow streets -- erodes walkability and reduces both satisfaction and\nshop-level pricing. In contrast, streets with higher perceived greenery and\ncleanliness show significantly greater satisfaction scores but only weak\nassociations with pricing. Street width moderates the effects of vehicle\npresence, underscoring the importance of spatial configuration. These results\ndemonstrate the value of integrating AI-assisted perception with urban\nmorphological analysis to capture non-linear and context-sensitive drivers of\ncommercial success. This study advances both theoretical and methodological\nfrontiers by highlighting the conditional role of vehicle activity in\nneighborhood commerce and demonstrating the feasibility of multimodal AI for\nperceptual urban diagnostics. The implications extend to urban design, parking\nmanagement, and scalable planning tools for community revitalization.", "authors": ["HaoTian Lan"], "published_date": "2025-06-05", "title_zh": "停車、感知與零售：哈爾濱市街區層面社群活力的決定因素", "summary_zh": "中國城市社區街道的商業活力受車輛可達性、環境品質和行人感知之間的複雜互動影響。本研究提出一個可解釋的、基於圖像的框架，檢視哈爾濱的街道特徵（如停車密度、綠化、清潔度和街道寬度）如何影響零售業績和使用者滿意度。利用街景圖像和多模態大型語言模型 (VisualGLM-6B)，我們從美團和點評數據構建了社區商業活力指數 (CCVI)，並分析其與基於 GPT-4 感知建模提取的空間屬性之間的關係。研究發現，適度的車輛存在可能增強商業可達性，但過度的路邊停車（尤其是在狹窄的街道上）會降低步行性，並降低滿意度和商店定價。相反，具有更高感知綠化和清潔度的街道顯示出顯著更高的滿意度，但與定價的關聯較弱。街道寬度調節車輛存在的影響，突顯了空間配置的重要性。這些結果證明了將人工智慧輔助感知與城市形態分析相結合，以捕捉商業成功的非線性和情境敏感驅動因素的價值。本研究透過強調車輛活動在鄰里商業中的條件作用，並展示多模態人工智慧用於感知城市診斷的可行性，推進了理論和方法的前沿。其影響擴展到城市設計、停車管理和用於社區復興的可擴展規劃工具。", "audio": "audios/2506.05080v1.mp3", "timestamp": "2025-06-06T21:18:05.506639"}
{"query": "Diffusion Model", "id": "2506.04394v1", "url": "http://arxiv.org/abs/2506.04394v1", "title": "Is Perturbation-Based Image Protection Disruptive to Image Editing?", "summary": "The remarkable image generation capabilities of state-of-the-art diffusion\nmodels, such as Stable Diffusion, can also be misused to spread misinformation\nand plagiarize copyrighted materials. To mitigate the potential risks\nassociated with image editing, current image protection methods rely on adding\nimperceptible perturbations to images to obstruct diffusion-based editing. A\nfully successful protection for an image implies that the output of editing\nattempts is an undesirable, noisy image which is completely unrelated to the\nreference image. In our experiments with various perturbation-based image\nprotection methods across multiple domains (natural scene images and artworks)\nand editing tasks (image-to-image generation and style editing), we discover\nthat such protection does not achieve this goal completely. In most scenarios,\ndiffusion-based editing of protected images generates a desirable output image\nwhich adheres precisely to the guidance prompt. Our findings suggest that\nadding noise to images may paradoxically increase their association with given\ntext prompts during the generation process, leading to unintended consequences\nsuch as better resultant edits. Hence, we argue that perturbation-based methods\nmay not provide a sufficient solution for robust image protection against\ndiffusion-based editing.", "authors": ["Qiuyu Tang", "Bonor Ayambem", "Mooi Choo Chuah", "Aparna Bharati"], "published_date": "2025-06-04", "title_zh": "基於擾動的圖像保護是否會干擾圖像編輯？", "summary_zh": "目前如Stable Diffusion等先進擴散模型雖具卓越影像生成能力，但也可能被濫用於散播不實資訊與剽竊版權。現有影像保護方法透過在影像中加入難以察覺的擾動來阻礙基於擴散的編輯，以降低影像編輯的潛在風險。理想的保護應使編輯輸出的結果為與原始影像完全無關的噪點影像。然而，我們針對多種基於擾動的影像保護方法，在不同領域（自然場景與藝術作品）及編輯任務（圖生圖、風格編輯）進行實驗後發現，此類保護並未完全達成目標。在多數情況下，受保護影像經擴散編輯後，仍能產生符合提示詞的理想輸出結果。我們的研究顯示，對影像添加噪聲可能會反向增強其在生成過程中與特定文本提示的關聯性，導致意外後果，例如產生更佳的編輯結果。因此，我們認為基於擾動的方法可能不足以提供針對基於擴散編輯的強韌影像保護。", "audio": "audios/2506.04394v1.mp3", "timestamp": "2025-06-06T21:18:12.594502"}
{"query": "AI", "id": "2506.05073v1", "url": "http://arxiv.org/abs/2506.05073v1", "title": "Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection through Intent Differentiation and Emoji Interpretation", "summary": "Self-harm detection on social media is critical for early intervention and\nmental health support, yet remains challenging due to the subtle,\ncontext-dependent nature of such expressions. Identifying self-harm intent aids\nsuicide prevention by enabling timely responses, but current large language\nmodels (LLMs) struggle to interpret implicit cues in casual language and\nemojis. This work enhances LLMs' comprehension of self-harm by distinguishing\nintent through nuanced language-emoji interplay. We present the Centennial\nEmoji Sensitivity Matrix (CESM-100), a curated set of 100 emojis with\ncontextual self-harm interpretations and the Self-Harm Identification aNd\nintent Extraction with Supportive emoji sensitivity (SHINES) dataset, offering\ndetailed annotations for self-harm labels, casual mentions (CMs), and serious\nintents (SIs). Our unified framework: a) enriches inputs using CESM-100; b)\nfine-tunes LLMs for multi-task learning: self-harm detection (primary) and\nCM/SI span detection (auxiliary); c) generates explainable rationales for\nself-harm predictions. We evaluate the framework on three state-of-the-art\nLLMs-Llama 3, Mental-Alpaca, and MentalLlama, across zero-shot, few-shot, and\nfine-tuned scenarios. By coupling intent differentiation with contextual cues,\nour approach commendably enhances LLM performance in both detection and\nexplanation tasks, effectively addressing the inherent ambiguity in self-harm\nsignals. The SHINES dataset, CESM-100 and codebase are publicly available at:\nhttps://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES .", "authors": ["Soumitra Ghosh", "Gopendra Vikram Singh", "Shambhavi", "Sabarna Choudhury", "Asif Ekbal"], "published_date": "2025-06-05", "title_zh": "僅是皮毛之傷：藉由意圖辨別與表情符號詮釋強化大型語言模型之自殘偵測能力", "summary_zh": "社交媒體上的自殘偵測對早期干預和心理健康支持至關重要，但由於此類表達方式的微妙性和情境依賴性，仍然具有挑戰性。識別自殘意圖有助於預防自殺，但目前的大型語言模型（LLM）難以解釋隨意語言和表情符號中的隱含線索。本研究透過細緻的語言表情符號互動，區分意圖，從而增強LLM對自殘的理解。我們提出了百年表情符號敏感度矩陣（CESM-100），一套包含100個具有情境自殘解釋的表情符號，以及自殘識別與意圖提取的支持性表情符號敏感度（SHINES）數據集，提供自殘標籤、隨意提及（CM）和嚴重意圖（SI）的詳細註釋。我們的統一框架：a)使用CESM-100豐富輸入；b)微調LLM進行多任務學習：自殘檢測（主要）和CM/SI跨度檢測（輔助）；c)為自殘預測生成可解釋的理由。我們在零樣本、少樣本和微調情境下，評估了三個最先進的LLM-Llama 3、Mental-Alpaca和MentalLlama。透過將意圖區分與情境線索相結合，我們的方法顯著提高了LLM在檢測和解釋任務中的性能，有效解決了自殘信號中固有的模糊性。SHINES數據集、CESM-100和代碼庫可在以下網址公開獲取：https://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES。", "audio": "audios/2506.05073v1.mp3", "timestamp": "2025-06-06T22:18:13.311403"}
{"query": "Diffusion Model", "id": "2506.04351v1", "url": "http://arxiv.org/abs/2506.04351v1", "title": "HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting", "summary": "3D human generation is an important problem with a wide range of applications\nin computer vision and graphics. Despite recent progress in generative AI such\nas diffusion models or rendering methods like Neural Radiance Fields or\nGaussian Splatting, controlling the generation of accurate 3D humans from text\nprompts remains an open challenge. Current methods struggle with fine detail,\naccurate rendering of hands and faces, human realism, and controlability over\nappearance. The lack of diversity, realism, and annotation in human image data\nalso remains a challenge, hindering the development of a foundational 3D human\nmodel. We present a weakly supervised pipeline that tries to address these\nchallenges. In the first step, we generate a photorealistic human image dataset\nwith controllable attributes such as appearance, race, gender, etc using a\nstate-of-the-art image diffusion model. Next, we propose an efficient mapping\napproach from image features to 3D point clouds using a transformer-based\narchitecture. Finally, we close the loop by training a point-cloud diffusion\nmodel that is conditioned on the same text prompts used to generate the\noriginal samples. We demonstrate orders-of-magnitude speed-ups in 3D human\ngeneration compared to the state-of-the-art approaches, along with\nsignificantly improved text-prompt alignment, realism, and rendering quality.\nWe will make the code and dataset available.", "authors": ["Maksym Ivashechkin", "Oscar Mendez", "Richard Bowden"], "published_date": "2025-06-04", "title_zh": "HuGeDiff：藉由高斯潑濺擴散之三維人體生成", "summary_zh": "三維人體生成在電腦視覺和圖形學中至關重要，但利用文字提示精準控制三維人體生成仍具挑戰。現有方法在細節、手部和面部渲染、真實感及外觀控制方面存在不足，且缺乏多樣性、真實感及標註的人體圖像數據阻礙了基礎三維人體模型的發展。本文提出一個弱監督流程以應對這些挑戰：首先，利用先進圖像擴散模型生成具有可控屬性的逼真人體圖像數據集；其次，提出高效的圖像特徵至三維點雲映射方法，採用基於 Transformer 的架構；最後，訓練一個以文字提示為條件的點雲擴散模型。實驗結果表明，相較於現有技術，該方法在三維人體生成速度上具有數量級的提升，並顯著改善了文字提示對齊、真實感和渲染品質。我們將公開程式碼和數據集。", "audio": "audios/2506.04351v1.mp3", "timestamp": "2025-06-06T22:18:22.692719"}
{"query": "AI", "id": "2506.05055v1", "url": "http://arxiv.org/abs/2506.05055v1", "title": "Study of $f_1(1420)$ and $η(1405)$ in the decay $J/ψ\\to γ π^{0}π^{0}π^{0}$", "summary": "A partial-wave analysis is performed on the decay\n$J/\\psi\\to\\gamma\\pi^{0}\\pi^{0}\\pi^{0}$ within the $\\pi^{0}\\pi^{0}\\pi^{0}$\ninvariant-mass region below 1.6 GeV$/c^{2}$, using\n$(10.09~\\pm~0.04)\\times10^{9} ~J/\\psi$ events collected with the BESIII\ndetector. Significant isospin-violating decays of $\\eta(1405)$ and $f_1(1420)$\ninto $f_0(980)\\pi^{0}$ are observed. For the first time, three axial-vectors,\n$f_1(1285)$, $f_1(1420)$ and $f_1(1510)$, are observed to decay into\n$\\pi^{0}\\pi^{0}\\pi^{0}$. The product branching fractions of these resonances\nare reported.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "O. Afedulidis", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "I. Balossino", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "G. Chelkov", "C. Chen", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "Y. B. Chen", "Y. Q. Chen", "Y. Q. Chen", "Z. J. Chen", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. Gao", "Y. N. Gao", "Y. N. Gao", "S. Garbolino", "I. Garzia", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "T. Holtmann", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "F. Hölzken", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "X. Q. Jia", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. S. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "W. N. Lan", "T. T. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "C. X. Lin", "D. X. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "X. Liu", "X. Liu", "X. L. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "X. R. Lyu", "Y. F. Lyu", "F. C. Ma", "H. Ma", "H. L. Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Q. A. Malik", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "Y. H. Meng", "Z. X. Meng", "J. G. Messchendorp", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "Y. Niu", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "Y. Y. Peng", "K. Peters", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "L. Q. Qin", "L. Y. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "C. F. Redmer", "K. J. Ren", "A. Rivetti", "M. Rolo", "G. Rong", "Ch. Rosner", "M. Q. Ruan", "S. N. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "J. J. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "Y. Wan", "S. J. Wang", "B. Wang", "Bo Wang", "C. Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "J. P. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. J. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Z. Wang", "Z. L. Wang", "Z. Y. Wang", "D. H. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "L. Wollenberg", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "Lianjie Wu", "X. Wu", "X. H. Wu", "Y. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. H. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. X. Yang", "Y. Z. Yang", "Z. W. Yang", "Z. P. Yao", "M. Ye", "M. H. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "C. Z. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "X. Q. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "F. R. Zeng", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. C. Zhai", "Y. H. Zhan", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Y. P. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "L. Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "H. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. Z. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "J. H. Zou", "J. Zu"], "published_date": "2025-06-05", "title_zh": "$J/ψ\\to γ π^{0}π^{0}π^{0}$衰變中$f_1(1420)$和$η(1405)$的研究", "summary_zh": "利用BESIII偵測器收集的$(10.09~\\pm~0.04)\\times10^{9} ~J/\\psi$事件，對$J/\\psi\\to\\gamma\\pi^{0}\\pi^{0}\\pi^{0}$衰變進行分波分析，研究$\\pi^{0}\\pi^{0}\\pi^{0}$不變質量低於1.6 GeV$/c^{2}$的區域。觀察到$\\eta(1405)$和$f_1(1420)$顯著的同位旋破壞衰變至$f_0(980)\\pi^{0}$。首次觀察到三個軸向量，$f_1(1285)$，$f_1(1420)$和$f_1(1510)$，衰變至$\\pi^{0}\\pi^{0}\\pi^{0}$，並報告了這些共振態的產物分支比。", "audio": "audios/2506.05055v1.mp3", "timestamp": "2025-06-06T23:18:12.518360"}
{"query": "Diffusion Model", "id": "2506.04288v1", "url": "http://arxiv.org/abs/2506.04288v1", "title": "Backbone Augmented Training for Adaptations", "summary": "Adaptations facilitate efficient training of large backbone models, including\ndiffusion models for image generation and transformer-based language models.\nWhile various adaptation techniques enhance performance with minimal\ncomputational resources, limited adaptation data often leads to challenges in\ntraining. To address this, we focus on the enormous amount of backbone data\nused to pre-train the backbone models. We propose Backbone Augmented Training\n(BAT), a method that leverages backbone data to augment the adaptation dataset.\nFirst, we formulate and prove two mathematical key propositions: one\nestablishes the validity of BAT, while the other identifies a condition under\nwhich BAT benefits adaptation. Furthermore, we introduce an advanced data\nselection scheme that satisfies these propositions and present ALBAT algorithm\nto implement this approach. ALBAT efficiently enhances adaptation training in\nboth personalization and language generation tasks with scarce data.", "authors": ["Jae Wan Park", "Junhyeok Kim", "Youngjun Jun", "Hyunah Ko", "Seong Jae Hwang"], "published_date": "2025-06-04", "title_zh": "骨幹增強訓練用於適應", "summary_zh": "適應性調整有助於大型骨幹模型的有效訓練，如圖像生成擴散模型和基於 Transformer 的語言模型。儘管多種適應技術能以最少的運算資源提升效能，但有限的適應資料常導致訓練困難。為此，本文著重於骨幹模型預訓練所使用的大量骨幹資料，提出骨幹增強訓練(BAT)，利用骨幹資料擴增適應資料集。首先，本文提出並證明兩項關鍵數學命題：一項確立 BAT 的有效性，另一項則確定 BAT 有利於適應性調整的條件。此外，本文引入一種滿足這些命題的先進資料選擇方案，並提出 ALBAT 演算法以實施該方法。ALBAT 能有效增強稀缺資料下的個人化和語言生成任務的適應性訓練。", "audio": "audios/2506.04288v1.mp3", "timestamp": "2025-06-06T23:18:18.542591"}
{"query": "AI", "id": "2506.05010v1", "url": "http://arxiv.org/abs/2506.05010v1", "title": "ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow Development", "summary": "We introduce ComfyUI-Copilot, a large language model-powered plugin designed\nto enhance the usability and efficiency of ComfyUI, an open-source platform for\nAI-driven art creation. Despite its flexibility and user-friendly interface,\nComfyUI can present challenges to newcomers, including limited documentation,\nmodel misconfigurations, and the complexity of workflow design. ComfyUI-Copilot\naddresses these challenges by offering intelligent node and model\nrecommendations, along with automated one-click workflow construction. At its\ncore, the system employs a hierarchical multi-agent framework comprising a\ncentral assistant agent for task delegation and specialized worker agents for\ndifferent usages, supported by our curated ComfyUI knowledge bases to\nstreamline debugging and deployment. We validate the effectiveness of\nComfyUI-Copilot through both offline quantitative evaluations and online user\nfeedback, showing that it accurately recommends nodes and accelerates workflow\ndevelopment. Additionally, use cases illustrate that ComfyUI-Copilot lowers\nentry barriers for beginners and enhances workflow efficiency for experienced\nusers. The ComfyUI-Copilot installation package and a demo video are available\nat https://github.com/AIDC-AI/ComfyUI-Copilot.", "authors": ["Zhenran Xu", "Xue Yang", "Yiyu Wang", "Qingli Hu", "Zijiao Wu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "published_date": "2025-06-05", "title_zh": "ComfyUI協作副駕駛：用於自動化工作流程開發的智慧助理", "summary_zh": "ComfyUI-Copilot為一大型語言模型外掛程式，旨在提升開源AI藝術創作平台ComfyUI的易用性與效率。為了解決ComfyUI文檔不足、模型配置錯誤及工作流程設計複雜等新手挑戰，ComfyUI-Copilot提供智慧節點與模型推薦，以及一鍵式自動工作流程建構。該系統採用層級式多代理框架，包含負責任務委派的中央助理代理，以及執行不同功能的專業工作代理，並輔以精選的ComfyUI知識庫，以簡化除錯與部署。離線量化評估與線上用戶回饋驗證了ComfyUI-Copilot的有效性，證明其能精準推薦節點並加速工作流程開發。案例顯示，ComfyUI-Copilot降低了新手入門門檻，並提升了資深用戶的工作流程效率。安裝包與展示影片已於GitHub提供。", "audio": "audios/2506.05010v1.mp3", "timestamp": "2025-06-07T01:26:44.967097"}
{"query": "Diffusion Model", "id": "2506.04283v1", "url": "http://arxiv.org/abs/2506.04283v1", "title": "SSIMBaD: Sigma Scaling with SSIM-Guided Balanced Diffusion for AnimeFace Colorization", "summary": "We propose a novel diffusion-based framework for automatic colorization of\nAnime-style facial sketches. Our method preserves the structural fidelity of\nthe input sketch while effectively transferring stylistic attributes from a\nreference image. Unlike traditional approaches that rely on predefined noise\nschedules - which often compromise perceptual consistency -- our framework\nbuilds on continuous-time diffusion models and introduces SSIMBaD (Sigma\nScaling with SSIM-Guided Balanced Diffusion). SSIMBaD applies a sigma-space\ntransformation that aligns perceptual degradation, as measured by structural\nsimilarity (SSIM), in a linear manner. This scaling ensures uniform visual\ndifficulty across timesteps, enabling more balanced and faithful\nreconstructions. Experiments on a large-scale Anime face dataset demonstrate\nthat our method outperforms state-of-the-art models in both pixel accuracy and\nperceptual quality, while generalizing to diverse styles. Code is available at\ngithub.com/Giventicket/SSIMBaD-Sigma-Scaling-with-SSIM-Guided-Balanced-Diffusion-for-AnimeFace-Colorization", "authors": ["Junpyo Seo", "Hanbin Koo", "Jieun Yook", "Byung-Ro Moon"], "published_date": "2025-06-04", "title_zh": "SSIMBaD：基於SSIM引導平衡擴散的Sigma尺度調整實現動漫人臉著色", "summary_zh": "本研究提出一種新穎的基於擴散模型的動漫風格臉部草圖自動上色框架。此方法在保留輸入草圖結構完整性的同時，有效轉移參考圖像的風格屬性。與依賴預定義噪聲排程的傳統方法不同，本框架基於連續時間擴散模型，並引入SSIMBaD（基於結構相似性引導的平衡擴散Sigma縮放）。SSIMBaD應用Sigma空間轉換，線性對齊結構相似性（SSIM）所測量的感知退化。此縮放確保時間步長上視覺難度的均勻性，實現更平衡且忠實的重建。在大型動漫臉部數據集上的實驗表明，本方法在像素準確度和感知品質方面均優於最先進的模型，且能推廣至多樣風格。代碼已公開。", "audio": "audios/2506.04283v1.mp3", "timestamp": "2025-06-07T01:26:52.609347"}
{"query": "AI", "id": "2506.05007v1", "url": "http://arxiv.org/abs/2506.05007v1", "title": "QiMeng: Fully Automated Hardware and Software Design for Processor Chip", "summary": "Processor chip design technology serves as a key frontier driving\nbreakthroughs in computer science and related fields. With the rapid\nadvancement of information technology, conventional design paradigms face three\nmajor challenges: the physical constraints of fabrication technologies, the\nescalating demands for design resources, and the increasing diversity of\necosystems. Automated processor chip design has emerged as a transformative\nsolution to address these challenges. While recent breakthroughs in Artificial\nIntelligence (AI), particularly Large Language Models (LLMs) techniques, have\nopened new possibilities for fully automated processor chip design, substantial\nchallenges remain in establishing domain-specific LLMs for processor chip\ndesign.\n  In this paper, we propose QiMeng, a novel system for fully automated hardware\nand software design of processor chips. QiMeng comprises three hierarchical\nlayers. In the bottom-layer, we construct a domain-specific Large Processor\nChip Model (LPCM) that introduces novel designs in architecture, training, and\ninference, to address key challenges such as knowledge representation gap, data\nscarcity, correctness assurance, and enormous solution space. In the\nmiddle-layer, leveraging the LPCM's knowledge representation and inference\ncapabilities, we develop the Hardware Design Agent and the Software Design\nAgent to automate the design of hardware and software for processor chips.\nCurrently, several components of QiMeng have been completed and successfully\napplied in various top-layer applications, demonstrating significant advantages\nand providing a feasible solution for efficient, fully automated\nhardware/software design of processor chips. Future research will focus on\nintegrating all components and performing iterative top-down and bottom-up\ndesign processes to establish a comprehensive QiMeng system.", "authors": ["Rui Zhang", "Yuanbo Wen", "Shuyao Cheng", "Di Huang", "Shaohui Peng", "Jiaming Guo", "Pengwei Jin", "Jiacheng Zhao", "Tianrui Ma", "Yaoyu Zhu", "Yifan Hao", "Yongwei Zhao", "Shengwen Liang", "Ying Wang", "Xing Hu", "Zidong Du", "Huimin Cui", "Ling Li", "Qi Guo", "Yunji Chen"], "published_date": "2025-06-05", "title_zh": "啟蒙：處理器晶片的全自動硬體與軟體設計", "summary_zh": "處理器晶片設計技術是推動電腦科學及相關領域突破的關鍵。資訊科技快速發展下，傳統設計範式面臨製程物理限制、設計資源需求增加及生態系統多樣性提升三大挑戰。自動化處理器晶片設計應運而生。人工智慧，尤其是大型語言模型技術的突破，為全自動處理器晶片設計開闢了新途徑，但建立領域特定的大型語言模型仍存在挑戰。\n\n本文提出QiMeng，一種用於處理器晶片硬體及軟體全自動設計的新系統。QiMeng包含三個層級。底層構建了領域特定的大型處理器晶片模型(LPCM)，在架構、訓練及推論中引入新設計，以解決知識表示鴻溝、數據稀缺、正確性保證及龐大解空間等問題。中層則利用LPCM的知識表示及推論能力，開發硬體設計代理及軟體設計代理，以自動化處理器晶片的硬體及軟體設計。QiMeng的部分組件已完成並成功應用於頂層應用，展現顯著優勢，為高效、全自動的處理器晶片硬/軟體設計提供可行方案。未來研究將致力於整合所有組件，並執行迭代式自上而下和自下而上的設計流程，以建立完整的QiMeng系統。", "audio": "audios/2506.05007v1.mp3", "timestamp": "2025-06-07T03:13:37.898697"}
{"query": "AI", "id": "2506.04982v1", "url": "http://arxiv.org/abs/2506.04982v1", "title": "GEX: Democratizing Dexterity with Fully-Actuated Dexterous Hand and Exoskeleton Glove", "summary": "This paper introduces GEX, an innovative low-cost dexterous manipulation\nsystem that combines the GX11 tri-finger anthropomorphic hand (11 DoF) with the\nEX12 tri-finger exoskeleton glove (12 DoF), forming a closed-loop teleoperation\nframework through kinematic retargeting for high-fidelity control. Both\ncomponents employ modular 3D-printed finger designs, achieving ultra-low\nmanufacturing costs while maintaining full actuation capabilities. Departing\nfrom conventional tendon-driven or underactuated approaches, our\nelectromechanical system integrates independent joint motors across all 23 DoF,\nensuring complete state observability and accurate kinematic modeling. This\nfull-actuation architecture enables precise bidirectional kinematic\ncalculations, substantially enhancing kinematic retargeting fidelity between\nthe exoskeleton and robotic hand. The proposed system bridges the\ncost-performance gap in dexterous manipulation research, providing an\naccessible platform for acquiring high-quality demonstration data to advance\nembodied AI and dexterous robotic skill transfer learning.", "authors": ["Yunlong Dong", "Xing Liu", "Jun Wan", "Zelin Deng"], "published_date": "2025-06-05", "title_zh": "GEX：藉由全驅動靈巧手與外骨骼手套實現靈巧操作之普及化", "summary_zh": "本研究提出GEX，一個低成本靈巧操作系統，結合GX11擬人三指手（11自由度）與EX12三指外骨骼手套（12自由度），透過運動學重定向形成閉迴路遙控框架，實現高保真控制。兩者皆採用模組化3D列印手指設計，大幅降低製造成本，同時保有完整驅動能力。有別於傳統肌腱驅動或欠驅動方式，本機電系統整合所有23個自由度的獨立關節馬達，確保完整狀態可觀測性和精確運動學建模。此全驅動架構可進行精確雙向運動學計算，顯著提高外骨骼與機械手之間的運動學重定向保真度。此系統彌合了靈巧操作研究中的成本效益差距，為獲取高品質示範數據提供了一個易於使用的平台，以促進具身人工智慧和靈巧機器人技能轉移學習。", "audio": "audios/2506.04982v1.mp3", "timestamp": "2025-06-07T04:23:13.404118"}
{"query": "AI", "id": "2506.04980v1", "url": "http://arxiv.org/abs/2506.04980v1", "title": "Agentic AI for Intent-Based Industrial Automation", "summary": "The recent development of Agentic AI systems, empowered by autonomous large\nlanguage models (LLMs) agents with planning and tool-usage capabilities,\nenables new possibilities for the evolution of industrial automation and\nreduces the complexity introduced by Industry 4.0. This work proposes a\nconceptual framework that integrates Agentic AI with the intent-based paradigm,\noriginally developed in network research, to simplify human-machine interaction\n(HMI) and better align automation systems with the human-centric, sustainable,\nand resilient principles of Industry 5.0. Based on the intent-based processing,\nthe framework allows human operators to express high-level business or\noperational goals in natural language, which are decomposed into actionable\ncomponents. These intents are broken into expectations, conditions, targets,\ncontext, and information that guide sub-agents equipped with specialized tools\nto execute domain-specific tasks. A proof of concept was implemented using the\nCMAPSS dataset and Google Agent Developer Kit (ADK), demonstrating the\nfeasibility of intent decomposition, agent orchestration, and autonomous\ndecision-making in predictive maintenance scenarios. The results confirm the\npotential of this approach to reduce technical barriers and enable scalable,\nintent-driven automation, despite data quality and explainability concerns.", "authors": ["Marcos Lima Romero", "Ricardo Suyama"], "published_date": "2025-06-05", "title_zh": "基於意圖的工業自動化之能動人工智慧", "summary_zh": "具備規劃與工具使用能力之自主大型語言模型(LLM)代理賦能了Agentic AI系統，為工業自動化發展帶來新可能，並降低工業4.0帶來的複雜性。本文提出一個概念框架，整合Agentic AI與意圖導向範式(源於網路研究)，以簡化人機互動(HMI)，使自動化系統更符合以人為本、永續及具韌性的工業5.0原則。此框架基於意圖導向處理，允許操作者以自然語言表達高階業務或營運目標，並將其分解為可執行組件，即分解為引導具備專業工具之次代理執行特定領域任務的期望、條件、目標、上下文和資訊。使用CMAPSS數據集和Google Agent Developer Kit (ADK)的實作概念驗證，展示了在預測性維護情境中意圖分解、代理協調和自主決策的可行性。結果證實此方法具備降低技術門檻並實現可擴展、意圖驅動自動化的潛力，然數據品質與可解釋性仍是考量。", "audio": "audios/2506.04980v1.mp3", "timestamp": "2025-06-07T05:18:20.300407"}
{"query": "AI", "id": "2506.04965v1", "url": "http://arxiv.org/abs/2506.04965v1", "title": "From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced Algorithm Exams and Pave the Way for Editorial Generation", "summary": "This paper presents a comprehensive evaluation of the performance of\nstate-of-the-art Large Language Models (LLMs) on challenging university-level\nalgorithms exams. By testing multiple models on both a Romanian exam and its\nhigh-quality English translation, we analyze LLMs' problem-solving\ncapabilities, consistency, and multilingual performance. Our empirical study\nreveals that the most recent models not only achieve scores comparable to\ntop-performing students but also demonstrate robust reasoning skills on\ncomplex, multi-step algorithmic challenges, even though difficulties remain\nwith graph-based tasks. Building on these findings, we explore the potential of\nLLMs to support educational environments through the generation of high-quality\neditorial content, offering instructors a powerful tool to enhance student\nfeedback. The insights and best practices discussed herein pave the way for\nfurther integration of generative AI in advanced algorithm education.", "authors": ["Adrian Marius Dumitran", "Theodor-Pierre Moroianu", "Vasile Paul Alexe"], "published_date": "2025-06-05", "title_zh": "從掙扎 (06-2024) 到精通 (02-2025)：大型語言模型攻克高等演算法考試並為社論生成鋪路", "summary_zh": "本研究全面評估了最先進的大型語言模型在具挑戰性大學演算法考試中的表現。透過在羅馬尼亞語考卷及其高品質英語翻譯上測試多個模型，分析了語言模型的問題解決能力、一致性和多語種表現。實證研究表明，最新模型不僅獲得了與頂尖學生相當的分數，而且在複雜、多步驟的演算法挑戰中展現出強大的推理能力，儘管在基於圖表的任務中仍存在困難。基於這些發現，我們探討了大型語言模型透過生成高品質編輯內容來支援教育環境的潛力，為教師提供了一個強大的工具來加強學生回饋。本文探討的見解和最佳實踐為生成式人工智慧在高等演算法教育中的進一步整合鋪平了道路。", "audio": "audios/2506.04965v1.mp3", "timestamp": "2025-06-07T06:25:13.493741"}
{"query": "AI", "id": "2506.04950v1", "url": "http://arxiv.org/abs/2506.04950v1", "title": "Time-Lapse Video-Based Embryo Grading via Complementary Spatial-Temporal Pattern Mining", "summary": "Artificial intelligence has recently shown promise in automated embryo\nselection for In-Vitro Fertilization (IVF). However, current approaches either\naddress partial embryo evaluation lacking holistic quality assessment or target\nclinical outcomes inevitably confounded by extra-embryonic factors, both\nlimiting clinical utility. To bridge this gap, we propose a new task called\nVideo-Based Embryo Grading - the first paradigm that directly utilizes\nfull-length time-lapse monitoring (TLM) videos to predict embryologists'\noverall quality assessments. To support this task, we curate a real-world\nclinical dataset comprising over 2,500 TLM videos, each annotated with a\ngrading label indicating the overall quality of embryos. Grounded in clinical\ndecision-making principles, we propose a Complementary Spatial-Temporal Pattern\nMining (CoSTeM) framework that conceptually replicates embryologists'\nevaluation process. The CoSTeM comprises two branches: (1) a morphological\nbranch using a Mixture of Cross-Attentive Experts layer and a Temporal\nSelection Block to select discriminative local structural features, and (2) a\nmorphokinetic branch employing a Temporal Transformer to model global\ndevelopmental trajectories, synergistically integrating static and dynamic\ndeterminants for grading embryos. Extensive experimental results demonstrate\nthe superiority of our design. This work provides a valuable methodological\nframework for AI-assisted embryo selection. The dataset and source code will be\npublicly available upon acceptance.", "authors": ["Yong Sun", "Yipeng Wang", "Junyu Shi", "Zhiyuan Zhang", "Yanmei Xiao", "Lei Zhu", "Manxi Jiang", "Qiang Nie"], "published_date": "2025-06-05", "title_zh": "基於時差影像的胚胎分級：互補時空模式挖掘", "summary_zh": "人工智慧近期在體外受精的自動化胚胎選擇方面展現潛力。然而，現有方法或僅處理部分胚胎評估，缺乏整體品質評估；或針對易受胚胎外因素干擾的臨床結果，限制了臨床應用。為此，我們提出基於影片的胚胎分級新任務，此為首個直接利用完整延時監控影片，預測胚胎學家整體品質評估的範例。我們建立包含超過2500個延時監控影片的真實臨床數據集，每個影片均標記有表示胚胎整體品質的分級標籤，以支持此任務。基於臨床決策原則，我們提出互補時空模式挖掘框架，概念上複製胚胎學家的評估流程。此框架包含兩個分支：(1) 使用混合交叉注意力專家層和時間選擇模塊的形態學分支，以選擇判別性局部結構特徵；(2) 使用時間轉換器的動態形態學分支，以模擬全局發育軌跡，協同整合靜態和動態決定因素以進行胚胎分級。實驗結果顯示我們設計的優越性。本研究為人工智慧輔助胚胎選擇提供有價值的方法框架。數據集和原始碼將於接受後公開。", "audio": "audios/2506.04950v1.mp3", "timestamp": "2025-06-07T07:18:07.715159"}
{"query": "AI", "id": "2506.04920v1", "url": "http://arxiv.org/abs/2506.04920v1", "title": "Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback", "summary": "Large language models (LLMs) have demonstrated the ability to generate\nformative feedback and instructional hints in English, making them increasingly\nrelevant for AI-assisted education. However, their ability to provide effective\ninstructional support across different languages, especially for mathematically\ngrounded reasoning tasks, remains largely unexamined. In this work, we present\nthe first large-scale simulation of multilingual tutor-student interactions\nusing LLMs. A stronger model plays the role of the tutor, generating feedback\nin the form of hints, while a weaker model simulates the student. We explore\n352 experimental settings across 11 typologically diverse languages, four\nstate-of-the-art LLMs, and multiple prompting strategies to assess whether\nlanguage-specific feedback leads to measurable learning gains. Our study\nexamines how student input language, teacher feedback language, model choice,\nand language resource level jointly influence performance. Results show that\nmultilingual hints can significantly improve learning outcomes, particularly in\nlow-resource languages when feedback is aligned with the student's native\nlanguage. These findings offer practical insights for developing multilingual,\nLLM-based educational tools that are both effective and inclusive.", "authors": ["Junior Cedric Tonga", "KV Aditya Srivatsa", "Kaushal Kumar Maurya", "Fajri Koto", "Ekaterina Kochmar"], "published_date": "2025-06-05", "title_zh": "多語言數學回饋之大型語言模型間輔導模擬", "summary_zh": "大型語言模型已展現生成英語形成性回饋和教學提示的能力，使其在人工智慧輔助教育中日益重要。然而，其在不同語言，特別是數學推理任務中提供有效教學支援的能力，仍未經充分檢驗。本研究首次大規模模擬使用大型語言模型的多語言師生互動。較強的模型扮演導師角色，生成提示形式的回饋，而較弱的模型模擬學生。我們探索了跨11種語系多樣語言、四種最先進的大型語言模型和多種提示策略的352種實驗設定，以評估特定語言的回饋是否能帶來可衡量的學習成果。研究檢驗了學生輸入語言、教師回饋語言、模型選擇和語言資源程度如何共同影響表現。結果表明，多語言提示可以顯著提高學習成果，尤其是在低資源語言中，當回饋與學生的母語一致時。這些發現為開發有效且具包容性的多語言、基於大型語言模型的教育工具提供了實用見解。", "audio": "audios/2506.04920v1.mp3", "timestamp": "2025-06-07T09:18:28.625723"}
{"query": "AI", "id": "2506.04909v1", "url": "http://arxiv.org/abs/2506.04909v1", "title": "When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models", "summary": "The honesty of large language models (LLMs) is a critical alignment\nchallenge, especially as advanced systems with chain-of-thought (CoT) reasoning\nmay strategically deceive humans. Unlike traditional honesty issues on LLMs,\nwhich could be possibly explained as some kind of hallucination, those models'\nexplicit thought paths enable us to study strategic deception--goal-driven,\nintentional misinformation where reasoning contradicts outputs. Using\nrepresentation engineering, we systematically induce, detect, and control such\ndeception in CoT-enabled LLMs, extracting \"deception vectors\" via Linear\nArtificial Tomography (LAT) for 89% detection accuracy. Through activation\nsteering, we achieve a 40% success rate in eliciting context-appropriate\ndeception without explicit prompts, unveiling the specific honesty-related\nissue of reasoning models and providing tools for trustworthy AI alignment.", "authors": ["Kai Wang", "Yihao Zhang", "Meng Sun"], "published_date": "2025-06-05", "title_zh": "當思考型LLM說謊時：揭示推理模型表徵中的策略性欺騙", "summary_zh": "大型語言模型（LLM）的誠實性是一項重要的對齊挑戰，尤其是具備思維鏈（CoT）推理的高級系統可能會有策略性地欺騙人類。與傳統LLM的誠實性問題（可能被解釋為某種幻覺）不同，這些模型的明確思考路徑使我們能夠研究策略性欺騙，即目標驅動、有意的虛假訊息，其中推理與輸出相矛盾。我們利用表徵工程，系統性地誘導、檢測和控制啟用CoT的LLM中的此類欺騙行為，透過線性人工斷層掃描（LAT）提取「欺騙向量」，檢測準確率達89%。透過激活引導，我們在沒有明確提示的情況下，成功地以40%的機率引發符合情境的欺騙，揭示了推理模型中與誠實性相關的具體問題，並提供了用於可信賴AI對齊的工具。", "audio": "audios/2506.04909v1.mp3", "timestamp": "2025-06-07T10:19:05.986659"}
{"query": "AI", "id": "2506.04852v1", "url": "http://arxiv.org/abs/2506.04852v1", "title": "Improving AI-generated music with user-guided training", "summary": "AI music generation has advanced rapidly, with models like diffusion and\nautoregressive algorithms enabling high-fidelity outputs. These tools can alter\nstyles, mix instruments, or isolate them. Since sound can be visualized as\nspectrograms, image-generation algorithms can be applied to generate novel\nmusic. However, these algorithms are typically trained on fixed datasets, which\nmakes it challenging for them to interpret and respond to user input\naccurately. This is especially problematic because music is highly subjective\nand requires a level of personalization that image generation does not provide.\nIn this work, we propose a human-computation approach to gradually improve the\nperformance of these algorithms based on user interactions. The\nhuman-computation element involves aggregating and selecting user ratings to\nuse as the loss function for fine-tuning the model. We employ a genetic\nalgorithm that incorporates user feedback to enhance the baseline performance\nof a model initially trained on a fixed dataset. The effectiveness of this\napproach is measured by the average increase in user ratings with each\niteration. In the pilot test, the first iteration showed an average rating\nincrease of 0.2 compared to the baseline. The second iteration further improved\nupon this, achieving an additional increase of 0.39 over the first iteration.", "authors": ["Vishwa Mohan Singh", "Sai Anirudh Aryasomayajula", "Ahan Chatterjee", "Beste Aydemir", "Rifat Mehreen Amin"], "published_date": "2025-06-05", "title_zh": "以使用者引導式訓練改進人工智慧生成音樂", "summary_zh": "人工智慧音樂生成技術快速發展，擴散模型與自迴歸演算法等能產生高保真輸出，並可變換風格、混合樂器或分離音軌。由於聲音可視為聲譜圖，圖像生成演算法亦可用於創作新音樂。然而，這些演算法通常以固定資料集訓練，難以準確理解並回應使用者輸入，尤其音樂具高度主觀性，需要圖像生成所缺乏的個人化。本研究提出一種人機協同方法，基於使用者互動逐步提升演算法效能。此方法藉由彙整並選擇使用者評分，作為微調模型的損失函數，並採用結合使用者回饋的基因演算法，以增強基於固定資料集訓練之模型的基準效能。研究以每次迭代使用者評分平均增幅衡量成效，先導測試顯示，第一次迭代較基準線平均提升0.2分，第二次迭代在第一次迭代基礎上額外提升0.39分。", "audio": "audios/2506.04852v1.mp3", "timestamp": "2025-06-07T12:34:53.209751"}
{"query": "AI", "id": "2506.04851v1", "url": "http://arxiv.org/abs/2506.04851v1", "title": "Multiple-Choice Question Generation Using Large Language Models: Methodology and Educator Insights", "summary": "Integrating Artificial Intelligence (AI) in educational settings has brought\nnew learning approaches, transforming the practices of both students and\neducators. Among the various technologies driving this transformation, Large\nLanguage Models (LLMs) have emerged as powerful tools for creating educational\nmaterials and question answering, but there are still space for new\napplications. Educators commonly use Multiple-Choice Questions (MCQs) to assess\nstudent knowledge, but manually generating these questions is\nresource-intensive and requires significant time and cognitive effort. In our\nopinion, LLMs offer a promising solution to these challenges. This paper\npresents a novel comparative analysis of three widely known LLMs - Llama 2,\nMistral, and GPT-3.5 - to explore their potential for creating informative and\nchallenging MCQs. In our approach, we do not rely on the knowledge of the LLM,\nbut we inject the knowledge into the prompt to contrast the hallucinations,\ngiving the educators control over the test's source text, too. Our experiment\ninvolving 21 educators shows that GPT-3.5 generates the most effective MCQs\nacross several known metrics. Additionally, it shows that there is still some\nreluctance to adopt AI in the educational field. This study sheds light on the\npotential of LLMs to generate MCQs and improve the educational experience,\nproviding valuable insights for the future.", "authors": ["Giorgio Biancini", "Alessio Ferrato", "Carla Limongelli"], "published_date": "2025-06-05", "title_zh": "使用大型語言模型生成選擇題：方法與教育者洞見", "summary_zh": "人工智慧融入教育環境帶來新的學習方式，改變師生實踐。大型語言模型作為強大工具，於教材生成和問答方面展現潛力，惟仍有發展空間。選擇題是常見的評估方式，但人工編寫耗時費力。本文比較Llama 2、Mistral和GPT-3.5三種模型，探討其生成資訊豐富且具挑戰性選擇題的可能性。研究方法側重於提示工程注入知識，以對照幻覺，賦予教師對測試文本的控制權。由21位教育者參與的實驗顯示，GPT-3.5在多項指標下生成效果最佳的選擇題，但也反映出教育領域對採用人工智慧的顧慮。本研究闡明了大型語言模型生成選擇題的潛力，並為未來改善教育體驗提供有價值的見解。", "audio": "audios/2506.04851v1.mp3", "timestamp": "2025-06-07T13:25:49.525261"}
{"query": "AI", "id": "2506.04836v1", "url": "http://arxiv.org/abs/2506.04836v1", "title": "Oversight Structures for Agentic AI in Public-Sector Organizations", "summary": "This paper finds that the introduction of agentic AI systems intensifies\nexisting challenges to traditional public sector oversight mechanisms -- which\nrely on siloed compliance units and episodic approvals rather than continuous,\nintegrated supervision. We identify five governance dimensions essential for\nresponsible agent deployment: cross-departmental implementation, comprehensive\nevaluation, enhanced security protocols, operational visibility, and systematic\nauditing. We evaluate the capacity of existing oversight structures to meet\nthese challenges, via a mixed-methods approach consisting of a literature\nreview and interviews with civil servants in AI-related roles. We find that\nagent oversight poses intensified versions of three existing governance\nchallenges: continuous oversight, deeper integration of governance and\noperational capabilities, and interdepartmental coordination. We propose\napproaches that both adapt institutional structures and design agent oversight\ncompatible with public sector constraints.", "authors": ["Chris Schmitz", "Jonathan Rystrøm", "Jan Batzner"], "published_date": "2025-06-05", "title_zh": "公共部門組織中具自主性人工智慧的監督結構", "summary_zh": "本文研究指出，具自主性的AI系統加劇了傳統公共部門監督機制的挑戰，因其仰賴孤立的合規單位和偶發性批准，而非持續、整合的監督。我們定義了負責任的AI部署所需的五個治理面向：跨部門實施、全面評估、強化安全協議、營運可見性及系統性稽核。透過文獻回顧與AI相關職務公務員訪談的混合方法，評估現有監督結構應對這些挑戰的能力。研究發現，AI監督加劇了三個現有治理挑戰：持續監督、治理與營運能力更深層次的整合，以及跨部門協調。因此，我們提出在公共部門限制下，調整制度結構並設計與之相容的AI監督方法。", "audio": "audios/2506.04836v1.mp3", "timestamp": "2025-06-07T15:17:59.745040"}
{"query": "AI", "id": "2506.04832v1", "url": "http://arxiv.org/abs/2506.04832v1", "title": "Joint Evaluation of Answer and Reasoning Consistency for Hallucination Detection in Large Reasoning Models", "summary": "Large Reasoning Models (LRMs) extend large language models with explicit,\nmulti-step reasoning traces to enhance transparency and performance on complex\ntasks. However, these reasoning traces can be redundant or logically\ninconsistent, making them a new source of hallucination that is difficult to\ndetect. Existing hallucination detection methods focus primarily on\nanswer-level uncertainty and often fail to detect hallucinations or logical\ninconsistencies arising from the model's reasoning trace. This oversight is\nparticularly problematic for LRMs, where the explicit thinking trace is not\nonly an important support to the model's decision-making process but also a key\nsource of potential hallucination. To this end, we propose RACE (Reasoning and\nAnswer Consistency Evaluation), a novel framework specifically tailored for\nhallucination detection in LRMs. RACE operates by extracting essential\nreasoning steps and computing four diagnostic signals: inter-sample consistency\nof reasoning traces, entropy-based answer uncertainty, semantic alignment\nbetween reasoning and answers, and internal coherence of reasoning. This joint\nanalysis enables fine-grained hallucination detection even when the final\nanswer appears correct. Experiments across datasets and different LLMs\ndemonstrate that RACE outperforms existing hallucination detection baselines,\noffering a robust and generalizable solution for evaluating LRMs. Our code is\navailable at: https://github.com/bebr2/RACE.", "authors": ["Changyue Wang", "Weihang Su", "Qingyao Ai", "Yiqun Liu"], "published_date": "2025-06-05", "title_zh": "大型推理模型中基於答案與推理一致性的幻覺偵測聯合評估", "summary_zh": "大型推理模型(LRM)透過外顯的多步驟推理軌跡擴展大型語言模型，以提升複雜任務上的透明度及效能。然而，這些推理軌跡可能冗餘或邏輯不一致，成為難以察覺的幻覺新來源。現有幻覺檢測方法主要關注答案層級的不確定性，往往無法檢測LRM推理軌跡中產生的幻覺或邏輯不一致，此疏忽對LRM尤其不利，因其外顯的思考軌跡既是決策的重要支撐，也是潛在幻覺的關鍵來源。因此，我們提出RACE (推理與答案一致性評估)，一個專為LRM幻覺檢測設計的新框架。RACE提取關鍵推理步驟，並計算四個診斷信號：推理軌跡的樣本間一致性、基於熵的答案不確定性、推理與答案間的語義對齊，以及推理的內部連貫性。此聯合分析即使在最終答案看似正確時，也能實現細粒度的幻覺檢測。跨數據集與不同LLM的實驗表明，RACE優於現有的幻覺檢測基準，為評估LRM提供了一個穩健且可泛化的解決方案。程式碼位於https://github.com/bebr2/RACE。", "audio": "audios/2506.04832v1.mp3", "timestamp": "2025-06-07T16:21:25.661146"}
{"query": "AI", "id": "2506.04822v1", "url": "http://arxiv.org/abs/2506.04822v1", "title": "Evaluating Vision-Language and Large Language Models for Automated Student Assessment in Indonesian Classrooms", "summary": "Although vision-language and large language models (VLM and LLM) offer\npromising opportunities for AI-driven educational assessment, their\neffectiveness in real-world classroom settings, particularly in\nunderrepresented educational contexts, remains underexplored. In this study, we\nevaluated the performance of a state-of-the-art VLM and several LLMs on 646\nhandwritten exam responses from grade 4 students in six Indonesian schools,\ncovering two subjects: Mathematics and English. These sheets contain more than\n14K student answers that span multiple choice, short answer, and essay\nquestions. Assessment tasks include grading these responses and generating\npersonalized feedback. Our findings show that the VLM often struggles to\naccurately recognize student handwriting, leading to error propagation in\ndownstream LLM grading. Nevertheless, LLM-generated feedback retains some\nutility, even when derived from imperfect input, although limitations in\npersonalization and contextual relevance persist.", "authors": ["Nurul Aisyah", "Muhammad Dehan Al Kautsar", "Arif Hidayat", "Raqib Chowdhury", "Fajri Koto"], "published_date": "2025-06-05", "title_zh": "評估視覺語言模型與大型語言模型於印尼課堂中自動化學生評估之應用", "summary_zh": "視覺語言模型和大型語言模型為AI驅動的教育評估帶來了潛力，但其在真實課堂環境，尤其是在弱勢教育環境中的有效性仍待探索。本研究評估了一流視覺語言模型和多個大型語言模型在印尼六所學校四年級學生646份手寫考卷上的表現，涵蓋數學和英語兩科。這些試卷包含超過14000個學生的多項選擇、簡答和論文題答案。評估任務包括評分和產生個人化回饋。研究發現，視覺語言模型難以準確識別學生筆跡，導致下游大型語言模型評分出現錯誤傳播。儘管如此，即使來自不完美的輸入，大型語言模型產生的回饋仍具有一定效用，但在個人化和情境相關性方面仍存在局限性。", "audio": "audios/2506.04822v1.mp3", "timestamp": "2025-06-07T18:24:01.428794"}
{"query": "AI", "id": "2506.04788v1", "url": "http://arxiv.org/abs/2506.04788v1", "title": "Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques", "summary": "The rapid progress of Multimodal Large Language Models(MLLMs) has transformed\nthe AI landscape. These models combine pre-trained LLMs with various modality\nencoders. This integration requires a systematic understanding of how different\nmodalities connect to the language backbone. Our survey presents an LLM-centric\nanalysis of current approaches. We examine methods for transforming and\naligning diverse modal inputs into the language embedding space. This addresses\na significant gap in existing literature. We propose a classification framework\nfor MLLMs based on three key dimensions. First, we examine architectural\nstrategies for modality integration. This includes both the specific\nintegration mechanisms and the fusion level. Second, we categorize\nrepresentation learning techniques as either joint or coordinate\nrepresentations. Third, we analyze training paradigms, including training\nstrategies and objective functions. By examining 125 MLLMs developed between\n2021 and 2025, we identify emerging patterns in the field. Our taxonomy\nprovides researchers with a structured overview of current integration\ntechniques. These insights aim to guide the development of more robust\nmultimodal integration strategies for future models built on pre-trained\nfoundations.", "authors": ["Jisu An", "Junseok Lee", "Jeoungeun Lee", "Yongseok Son"], "published_date": "2025-06-05", "title_zh": "邁向以大型語言模型為中心的多模態融合：整合策略與技術綜述", "summary_zh": "多模態大型語言模型(MLLM)的快速發展改變了AI格局。此類模型結合了預訓練LLM與多種模態編碼器。整合不同模態至語言主幹需要系統性的理解。本研究以LLM為中心，分析現有方法，檢視將多元模態輸入轉換並對齊至語言嵌入空間之方法，填補了現有文獻的空白。我們基於三個關鍵維度，提出了MLLM的分類框架：模態整合的架構策略(包含整合機制和融合層次)、表徵學習技術(聯合或協調表徵)以及訓練範式(訓練策略和目標函數)。通過檢視2021年至2025年間開發的125個MLLM，我們識別出該領域的新興模式。本分類法為研究人員提供了對當前整合技術的結構化概述，旨在指導未來基於預訓練基礎的更強大多模態整合策略的開發。", "audio": "audios/2506.04788v1.mp3", "timestamp": "2025-06-07T19:14:04.753044"}
{"query": "AI", "id": "2506.04785v1", "url": "http://arxiv.org/abs/2506.04785v1", "title": "From Developer Pairs to AI Copilots: A Comparative Study on Knowledge Transfer", "summary": "Knowledge transfer is fundamental to human collaboration and is therefore\ncommon in software engineering. Pair programming is a prominent instance. With\nthe rise of AI coding assistants, developers now not only work with human\npartners but also, as some claim, with AI pair programmers. Although studies\nconfirm knowledge transfer during human pair programming, its effectiveness\nwith AI coding assistants remains uncertain. To analyze knowledge transfer in\nboth human-human and human-AI settings, we conducted an empirical study where\ndeveloper pairs solved a programming task without AI support, while a separate\ngroup of individual developers completed the same task using the AI coding\nassistant GitHub Copilot. We extended an existing knowledge transfer framework\nand employed a semi-automated evaluation pipeline to assess differences in\nknowledge transfer episodes across both settings. We found a similar frequency\nof successful knowledge transfer episodes and overlapping topical categories\nacross both settings. Two of our key findings are that developers tend to\naccept GitHub Copilot's suggestions with less scrutiny than those from human\npair programming partners, but also that GitHub Copilot can subtly remind\ndevelopers of important code details they might otherwise overlook.", "authors": ["Alisa Welter", "Niklas Schneider", "Tobias Dick", "Kallistos Weis", "Christof Tinnes", "Marvin Wyrich", "Sven Apel"], "published_date": "2025-06-05", "title_zh": "從開發者配對到人工智慧副駕駛：知識轉移之比較研究", "summary_zh": "知識轉移是人類協作的基礎，在軟體工程中普遍存在，結對程式設計是重要實例。隨著AI程式碼助手的興起，開發者現在不僅與人類夥伴協作，也與AI協同程式設計師協作。儘管研究證實人類結對程式設計期間存在知識轉移，但其在AI程式碼助手中的有效性仍不確定。為分析人際和人機環境下的知識轉移，我們進行了一項實證研究，讓開發者小組在沒有AI支援下解決程式設計任務，同時另一組開發者單獨使用GitHub Copilot完成相同任務。我們擴展了現有的知識轉移框架，並採用半自動評估流程來評估兩種環境下知識轉移事件的差異。我們發現兩種環境下成功知識轉移事件的頻率相似，主題類別重疊。主要發現包括：開發者傾向於較少審查GitHub Copilot的建議，但Copilot可以巧妙地提醒開發者可能忽略的重要程式碼細節。", "audio": "audios/2506.04785v1.mp3", "timestamp": "2025-06-07T20:19:56.819497"}
{"query": "AI", "id": "2506.04779v1", "url": "http://arxiv.org/abs/2506.04779v1", "title": "MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning Benchmark", "summary": "Speech inherently contains rich acoustic information that extends far beyond\nthe textual language. In real-world spoken language understanding, effective\ninterpretation often requires integrating semantic meaning (e.g., content),\nparalinguistic features (e.g., emotions, speed, pitch) and phonological\ncharacteristics (e.g., prosody, intonation, rhythm), which are embedded in\nspeech. While recent multimodal Speech Large Language Models (SpeechLLMs) have\ndemonstrated remarkable capabilities in processing audio information, their\nability to perform fine-grained perception and complex reasoning in natural\nspeech remains largely unexplored. To address this gap, we introduce MMSU, a\ncomprehensive benchmark designed specifically for understanding and reasoning\nin spoken language. MMSU comprises 5,000 meticulously curated\naudio-question-answer triplets across 47 distinct tasks. To ground our\nbenchmark in linguistic theory, we systematically incorporate a wide range of\nlinguistic phenomena, including phonetics, prosody, rhetoric, syntactics,\nsemantics, and paralinguistics. Through a rigorous evaluation of 14 advanced\nSpeechLLMs, we identify substantial room for improvement in existing models,\nhighlighting meaningful directions for future optimization. MMSU establishes a\nnew standard for comprehensive assessment of spoken language understanding,\nproviding valuable insights for developing more sophisticated human-AI speech\ninteraction systems. MMSU benchmark is available at\nhttps://huggingface.co/datasets/ddwang2000/MMSU. Evaluation Code is available\nat https://github.com/dingdongwang/MMSU_Bench.", "authors": ["Dingdong Wang", "Jincenzi Wu", "Junan Li", "Dongchao Yang", "Xueyuan Chen", "Tianhua Zhang", "Helen Meng"], "published_date": "2025-06-05", "title_zh": "MMSU：大規模多任務口語理解與推理基準", "summary_zh": "語音除了文字內容外，還包含豐富的聲學資訊。真實語音理解需要整合語義、副語言特徵和音韻特徵。為評估語音大型語言模型在自然語音中的細緻感知和複雜推理能力，我們提出MMSU基準測試，包含跨47項任務的5000組音訊-問題-答案三元組，並系統性地納入語音學、韻律學、修辭學、句法學、語義學和副語言學等語言現象。對14個先進語音大型語言模型的評估顯示，現有模型仍有改進空間。MMSU為全面評估語音理解能力建立了新標準，並為開發更複雜的人工智慧語音互動系統提供寶貴見解。MMSU基準測試可在https://huggingface.co/datasets/ddwang2000/MMSU取得，評估程式碼可在https://github.com/dingdongwang/MMSU_Bench取得。", "audio": "audios/2506.04779v1.mp3", "timestamp": "2025-06-07T22:17:18.863339"}
{"query": "AI", "id": "2506.04756v1", "url": "http://arxiv.org/abs/2506.04756v1", "title": "Ontology-based knowledge representation for bone disease diagnosis: a foundation for safe and sustainable medical artificial intelligence systems", "summary": "Medical artificial intelligence (AI) systems frequently lack systematic\ndomain expertise integration, potentially compromising diagnostic reliability.\nThis study presents an ontology-based framework for bone disease diagnosis,\ndeveloped in collaboration with Ho Chi Minh City Hospital for Traumatology and\nOrthopedics. The framework introduces three theoretical contributions: (1) a\nhierarchical neural network architecture guided by bone disease ontology for\nsegmentation-classification tasks, incorporating Visual Language Models (VLMs)\nthrough prompts, (2) an ontology-enhanced Visual Question Answering (VQA)\nsystem for clinical reasoning, and (3) a multimodal deep learning model that\nintegrates imaging, clinical, and laboratory data through ontological\nrelationships. The methodology maintains clinical interpretability through\nsystematic knowledge digitization, standardized medical terminology mapping,\nand modular architecture design. The framework demonstrates potential for\nextension beyond bone diseases through its standardized structure and reusable\ncomponents. While theoretical foundations are established, experimental\nvalidation remains pending due to current dataset and computational resource\nlimitations. Future work will focus on expanding the clinical dataset and\nconducting comprehensive system validation.", "authors": ["Loan Dao", "Ngoc Quoc Ly"], "published_date": "2025-06-05", "title_zh": "基於本體論的骨骼疾病診斷知識表示：安全且永續醫療人工智慧系統的基礎", "summary_zh": "醫學人工智慧系統常缺乏系統性領域知識整合，可能影響診斷可靠性。本研究與胡志明市創傷骨科醫院合作，提出基於本體論的骨骼疾病診斷框架。此框架具備三項理論貢獻：(1)基於骨骼疾病本體論的階層式神經網路架構，用於分割分類任務，並透過提示整合視覺語言模型(VLMs)；(2)一個本體論增強的視覺問答(VQA)系統，用於臨床推理；(3)一個多模態深度學習模型，透過本體論關係整合影像、臨床和實驗室數據。此方法透過系統性知識數位化、標準化醫學術語映射和模組化架構設計，維持臨床可解釋性。該框架具備透過其標準化結構和可重複使用組件擴展至骨骼疾病以外的潛力。儘管已建立理論基礎，但由於目前數據集和計算資源的限制，實驗驗證仍在進行中。未來工作將著重於擴展臨床數據集並進行全面的系統驗證。", "audio": "audios/2506.04756v1.mp3", "timestamp": "2025-06-07T23:19:20.883457"}
{"query": "AI", "id": "2506.04717v1", "url": "http://arxiv.org/abs/2506.04717v1", "title": "Using In-Context Learning for Automatic Defect Labelling of Display Manufacturing Data", "summary": "This paper presents an AI-assisted auto-labeling system for display panel\ndefect detection that leverages in-context learning capabilities. We adopt and\nenhance the SegGPT architecture with several domain-specific training\ntechniques and introduce a scribble-based annotation mechanism to streamline\nthe labeling process. Our two-stage training approach, validated on industrial\ndisplay panel datasets, demonstrates significant improvements over the baseline\nmodel, achieving an average IoU increase of 0.22 and a 14% improvement in\nrecall across multiple product types, while maintaining approximately 60%\nauto-labeling coverage. Experimental results show that models trained on our\nauto-labeled data match the performance of those trained on human-labeled data,\noffering a practical solution for reducing manual annotation efforts in\nindustrial inspection systems.", "authors": ["Babar Hussain", "Qiang Liu", "Gang Chen", "Bihai She", "Dahai Yu"], "published_date": "2025-06-05", "title_zh": "利用上下文學習進行顯示器製造數據的自動缺陷標記", "summary_zh": "本文提出一種利用上下文學習能力之AI輔助顯示面板缺陷自動標記系統。透過採用並增強SegGPT架構，結合領域特定訓練技術及基於塗鴉之標註機制，簡化標記流程。經驗證，此二階段訓練方法在工業顯示面板數據集上顯著優於基準模型，多種產品平均IoU提升0.22，召回率提升14%，並維持約60%的自動標記覆蓋率。實驗結果顯示，經自動標記數據訓練之模型，其效能與經人工標記數據訓練者相當，為降低工業檢測系統中人工標註工作提供實用方案。", "audio": "audios/2506.04717v1.mp3", "timestamp": "2025-06-08T03:23:27.152153"}
{"query": "AI", "id": "2506.04715v1", "url": "http://arxiv.org/abs/2506.04715v1", "title": "Towards Holistic Visual Quality Assessment of AI-Generated Videos: A LLM-Based Multi-Dimensional Evaluation Model", "summary": "The development of AI-Generated Video (AIGV) technology has been remarkable\nin recent years, significantly transforming the paradigm of video content\nproduction. However, AIGVs still suffer from noticeable visual quality defects,\nsuch as noise, blurriness, frame jitter and low dynamic degree, which severely\nimpact the user's viewing experience. Therefore, an effective automatic visual\nquality assessment is of great importance for AIGV content regulation and\ngenerative model improvement. In this work, we decompose the visual quality of\nAIGVs into three dimensions: technical quality, motion quality, and video\nsemantics. For each dimension, we design corresponding encoder to achieve\neffective feature representation. Moreover, considering the outstanding\nperformance of large language models (LLMs) in various vision and language\ntasks, we introduce a LLM as the quality regression module. To better enable\nthe LLM to establish reasoning associations between multi-dimensional features\nand visual quality, we propose a specially designed multi-modal prompt\nengineering framework. Additionally, we incorporate LoRA fine-tuning technology\nduring the training phase, allowing the LLM to better adapt to specific tasks.\nOur proposed method achieved \\textbf{second place} in the NTIRE 2025 Quality\nAssessment of AI-Generated Content Challenge: Track 2 AI Generated video,\ndemonstrating its effectiveness. Codes can be obtained at\nhttps://github.com/QiZelu/AIGVEval.", "authors": ["Zelu Qi", "Ping Shi", "Chaoyang Zhang", "Shuqi Wang", "Fei Zhao", "Da Pan", "Zefeng Ying"], "published_date": "2025-06-05", "title_zh": "邁向人工智慧生成影片之整體視覺品質評估：基於大型語言模型的多維度評估模型", "summary_zh": "近年人工智慧生成影片 (AIGV) 技術發展迅速，顯著改變了影片內容製作模式。然而，AIGV 仍存在視覺品質缺陷，如雜訊、模糊、畫面抖動和動態範圍不足，影響使用者體驗。因此，有效的自動視覺品質評估對於AIGV內容監管和生成模型改進至關重要。本文將AIGV視覺品質分解為技術品質、運動品質和影片語義三個維度，並為每個維度設計編碼器以實現有效的特徵表示。此外，鑒於大型語言模型 (LLM) 在視覺和語言任務中的卓越表現，引入 LLM 作為品質回歸模組，並提出專門設計的多模態提示工程框架，以更好地建立多維特徵與視覺品質之間的推理關聯。訓練階段採用LoRA微調技術，使 LLM 更好地適應特定任務。該方法在 NTIRE 2025 AI生成內容品質評估挑戰賽：Track 2 AI生成影片中獲得第二名，驗證了其有效性。代碼可於 https://github.com/QiZelu/AIGVEval 取得。", "audio": "audios/2506.04715v1.mp3", "timestamp": "2025-06-08T04:29:46.584823"}
{"query": "AI", "id": "2506.04688v1", "url": "http://arxiv.org/abs/2506.04688v1", "title": "MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal Large Language Models", "summary": "This paper introduces MMRefine, a MultiModal Refinement benchmark designed to\nevaluate the error refinement capabilities of Multimodal Large Language Models\n(MLLMs). As the emphasis shifts toward enhancing reasoning during inference,\nMMRefine provides a framework that evaluates MLLMs' abilities to detect and\ncorrect errors across six distinct scenarios beyond just comparing final\naccuracy before and after refinement. Furthermore, the benchmark analyzes the\nrefinement performance by categorizing errors into six error types. Experiments\nwith various open and closed MLLMs reveal bottlenecks and factors impeding\nrefinement performance, highlighting areas for improvement in effective\nreasoning enhancement. Our code and dataset are publicly available at\nhttps://github.com/naver-ai/MMRefine.", "authors": ["Gio Paik", "Geewook Kim", "Jinbae Im"], "published_date": "2025-06-05", "title_zh": "MMRefine：揭示多模態大型語言模型穩健精煉的障礙", "summary_zh": "本研究提出MMRefine，一個多模態精煉基準，旨在評估多模態大型語言模型（MLLM）的錯誤精煉能力。隨著重點轉向增強推理，MMRefine提供一個框架，評估MLLM在六種情境中偵測與修正錯誤的能力，不僅比較精煉前後的最終準確度。該基準更將錯誤歸類為六種型別，分析精煉效能。對多個開放和封閉MLLM的實驗揭示了阻礙精煉效能的瓶頸與因素，突顯了有效增強推理的改進方向。程式碼和資料集已公開。", "audio": "audios/2506.04688v1.mp3", "timestamp": "2025-06-08T05:19:50.443826"}
{"query": "AI", "id": "2506.04681v1", "url": "http://arxiv.org/abs/2506.04681v1", "title": "Urania: Differentially Private Insights into AI Use", "summary": "We introduce $Urania$, a novel framework for generating insights about LLM\nchatbot interactions with rigorous differential privacy (DP) guarantees. The\nframework employs a private clustering mechanism and innovative keyword\nextraction methods, including frequency-based, TF-IDF-based, and LLM-guided\napproaches. By leveraging DP tools such as clustering, partition selection, and\nhistogram-based summarization, $Urania$ provides end-to-end privacy protection.\nOur evaluation assesses lexical and semantic content preservation, pair\nsimilarity, and LLM-based metrics, benchmarking against a non-private\nClio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple\nempirical privacy evaluation that demonstrates the enhanced robustness of our\nDP pipeline. The results show the framework's ability to extract meaningful\nconversational insights while maintaining stringent user privacy, effectively\nbalancing data utility with privacy preservation.", "authors": ["Daogao Liu", "Edith Cohen", "Badih Ghazi", "Peter Kairouz", "Pritish Kamath", "Alexander Knop", "Ravi Kumar", "Pasin Manurangsi", "Adam Sealfon", "Da Yu", "Chiyuan Zhang"], "published_date": "2025-06-05", "title_zh": "烏拉尼亞：人工智能使用之差分隱私洞察", "summary_zh": "Urania是一種新型框架，旨在以嚴格的差分隱私(DP)保證下，產生關於大型語言模型聊天機器人互動的洞察。該框架採用私有聚類機制和創新的關鍵字提取方法，包括基於頻率、TF-IDF和大型語言模型引導的方法。透過利用差分隱私工具，如聚類、分區選擇和基於直方圖的摘要，$Urania$提供端到端的隱私保護。評估著重於詞彙和語義內容的保留、配對相似性和基於大型語言模型的指標，並與非私有的Clio啟發式流程進行基準測試。此外，開發了一種簡單的經驗隱私評估，展示了差分隱私流程的增強穩健性。結果表明，該框架能夠在保持嚴格用戶隱私的同時，提取有意義的對話洞察，有效平衡數據效用和隱私保護。", "audio": "audios/2506.04681v1.mp3", "timestamp": "2025-06-08T06:26:34.596322"}
{"query": "AI", "id": "2506.04679v1", "url": "http://arxiv.org/abs/2506.04679v1", "title": "Normative Conflicts and Shallow AI Alignment", "summary": "The progress of AI systems such as large language models (LLMs) raises\nincreasingly pressing concerns about their safe deployment. This paper examines\nthe value alignment problem for LLMs, arguing that current alignment strategies\nare fundamentally inadequate to prevent misuse. Despite ongoing efforts to\ninstill norms such as helpfulness, honesty, and harmlessness in LLMs through\nfine-tuning based on human preferences, they remain vulnerable to adversarial\nattacks that exploit conflicts between these norms. I argue that this\nvulnerability reflects a fundamental limitation of existing alignment methods:\nthey reinforce shallow behavioral dispositions rather than endowing LLMs with a\ngenuine capacity for normative deliberation. Drawing from on research in moral\npsychology, I show how humans' ability to engage in deliberative reasoning\nenhances their resilience against similar adversarial tactics. LLMs, by\ncontrast, lack a robust capacity to detect and rationally resolve normative\nconflicts, leaving them susceptible to manipulation; even recent advances in\nreasoning-focused LLMs have not addressed this vulnerability. This ``shallow\nalignment'' problem carries significant implications for AI safety and\nregulation, suggesting that current approaches are insufficient for mitigating\npotential harms posed by increasingly capable AI systems.", "authors": ["Raphaël Millière"], "published_date": "2025-06-05", "title_zh": "規範性衝突與淺層人工智慧對齊", "summary_zh": "大型語言模型等人工智慧系統的發展引發了對其安全部署的擔憂。本文探討了大型語言模型的價值對齊問題，指出目前對齊策略不足以預防濫用。儘管透過基於人類偏好的微調，力圖在大型語言模型中灌輸有益性、誠實性和無害性等規範，但它們仍易受利用這些規範之間衝突的對抗性攻擊。我認為，這種脆弱性反映了現有對齊方法的一個根本限制：它們強化了膚淺的行為傾向，而不是賦予大型語言模型真正的規範性審議能力。參考道德心理學的研究，我展示了人類進行審議推理的能力如何增強他們對類似對抗策略的抵抗力。相比之下，大型語言模型缺乏偵測和理性解決規範性衝突的穩健能力，容易受到操縱；即使是近期在以推理為重點的大型語言模型方面的進展也未能解決這種脆弱性。這種淺層對齊問題對人工智慧安全和監管具有重大意義，表明當前方法不足以減輕日益強大的人工智慧系統可能造成的潛在危害。", "audio": "audios/2506.04679v1.mp3", "timestamp": "2025-06-08T07:17:51.587105"}
{"query": "AI", "id": "2506.04677v1", "url": "http://arxiv.org/abs/2506.04677v1", "title": "The cost of ensembling: is it always worth combining?", "summary": "Given the continuous increase in dataset sizes and the complexity of\nforecasting models, the trade-off between forecast accuracy and computational\ncost is emerging as an extremely relevant topic, especially in the context of\nensemble learning for time series forecasting. To asses it, we evaluated ten\nbase models and eight ensemble configurations across two large-scale retail\ndatasets (M5 and VN1), considering both point and probabilistic accuracy under\nvarying retraining frequencies. We showed that ensembles consistently improve\nforecasting performance, particularly in probabilistic settings. However, these\ngains come at a substantial computational cost, especially for larger,\naccuracy-driven ensembles. We found that reducing retraining frequency\nsignificantly lowers costs, with minimal impact on accuracy, particularly for\npoint forecasts. Moreover, efficiency-driven ensembles offer a strong balance,\nachieving competitive accuracy with considerably lower costs compared to\naccuracy-optimized combinations. Most importantly, small ensembles of two or\nthree models are often sufficient to achieve near-optimal results. These\nfindings provide practical guidelines for deploying scalable and cost-efficient\nforecasting systems, supporting the broader goals of sustainable AI in\nforecasting. Overall, this work shows that careful ensemble design and\nretraining strategy selection can yield accurate, robust, and cost-effective\nforecasts suitable for real-world applications.", "authors": ["Marco Zanotti"], "published_date": "2025-06-05", "title_zh": "集成的成本：總是值得組合嗎？", "summary_zh": "隨著資料集規模和預測模型複雜度不斷增加，預測準確性和計算成本之間的權衡日益重要，尤其是在時間序列預測的集成學習中。本研究評估了十個基礎模型和八種集成配置，橫跨M5和VN1兩個大型零售資料集，考量不同重新訓練頻率下的點預測和機率預測準確性。結果顯示，集成模型能持續提升預測效能，特別是在機率預測方面。然而，此提升伴隨著顯著的計算成本，尤其對於較大且以準確性為導向的集成模型。降低重新訓練頻率可大幅降低成本，且對準確性的影響甚微，特別是點預測。此外，效率驅動的集成模型在成本效益上表現出色，以遠低於準確性最佳化的組合達到具競爭力的準確性。更重要的是，由兩到三個模型組成的小型集成通常足以獲得接近最佳的結果。這些發現為部署可擴展且具成本效益的預測系統提供實用指南，支持預測中永續AI的目標。總體而言，本研究表明，仔細的集成設計和重新訓練策略選擇可以產生準確、穩健且具成本效益的預測，適用於實際應用。", "audio": "audios/2506.04677v1.mp3", "timestamp": "2025-06-08T08:22:59.674070"}
{"query": "AI", "id": "2506.04640v1", "url": "http://arxiv.org/abs/2506.04640v1", "title": "ROSGuard: A Bandwidth Regulation Mechanism for ROS2-based Applications", "summary": "Multicore timing interference, arising when multiple requests contend for the\nsame shared hardware resources, is a primary concern for timing verification\nand validation of time-critical applications. Bandwidth control and regulation\napproaches have been proposed in the literature as an effective method to\nmonitor and limit the impact of timing interference at run time. These\napproaches seek for fine-grained control of the bandwidth consumption (at the\nmicrosecond level) to meet stringent timing requirements on embedded critical\nsystems. Such granularity and configurations, while effective, can become an\nentry barrier for the application of bandwidth control to a wide class of\nproductized, modular ROS2 applications. This is so because those applications\nhave less stringent timing requirements but would still benefit from bandwidth\nregulation, though under less restrictive, and therefore more portable,\ngranularity and configurations.\n  In this work, we provide ROSGuard, a highly-portable, modular implementation\nof a timing interference monitoring and control mechanism that builds on the\nabstractions available on top of a generic and portable Linux-based software\nstack with the Robotic Operating System 2 (ROS2) layer, a widespreadedly\nadopted middleware for a wide class of industrial applications, far beyond the\nrobotic domain. We deploy ROSGuard on an NVIDIA AGX Orin platform as a\nrepresentative target for functionally rich distributed AI-based applications\nand a set of synthetic and real-world benchmarks. We apply an effective\nbandwidth regulation scheme on ROS2-based applications and achieve comparable\neffectiveness to specialized, finer-grained state-of-the-art solutions.", "authors": ["Jon Altonaga Puente", "Enrico Mezzetti", "Irune Agirre Troncoso", "Jaume Abella Ferrer", "Francisco J. Cazorla Almeida"], "published_date": "2025-06-05", "title_zh": "ROSGuard：基於ROS2應用程序的帶寬調節機制", "summary_zh": "多核心時序干擾是時間關鍵應用程式時序驗證的主要問題。頻寬控制被認為能有效監測並限制執行時的干擾。然而，微秒級的精細控制對ROS2模組化應用程式而言門檻過高，因其對頻寬管制的需求較低。本研究提出ROSGuard，一個高可移植性的時序干擾監測與控制機制，建構於通用Linux系統及ROS2之上。我們在NVIDIA AGX Orin平台上部署ROSGuard，並使用合成及真實基準測試，驗證其在ROS2應用程式上的頻寬控制效果，與更精細的先進方案具備相當的效能。", "audio": "audios/2506.04640v1.mp3", "timestamp": "2025-06-08T09:18:04.585212"}
{"query": "AI", "id": "2506.04633v1", "url": "http://arxiv.org/abs/2506.04633v1", "title": "Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual Simulations", "summary": "Spatial cognition is essential for human intelligence, enabling\nproblem-solving through visual simulations rather than solely relying on verbal\nreasoning. However, existing AI benchmarks primarily assess verbal reasoning,\nneglecting the complexities of non-verbal, multi-step visual simulation. We\nintroduce STARE(Spatial Transformations and Reasoning Evaluation), a benchmark\ndesigned to rigorously evaluate multimodal large language models on tasks\nbetter solved through multi-step visual simulation. STARE features 4K tasks\nspanning foundational geometric transformations (2D and 3D), integrated spatial\nreasoning (cube net folding and tangram puzzles), and real-world spatial\nreasoning (perspective and temporal reasoning), reflecting practical cognitive\nchallenges like object assembly, mechanical diagram interpretation, and\neveryday spatial navigation. Our evaluations show that models excel at\nreasoning over simpler 2D transformations, but perform close to random chance\non more complex tasks like 3D cube net folding and tangram puzzles that require\nmulti-step visual simulations. Humans achieve near-perfect accuracy but take\nconsiderable time (up to 28.9s) on complex tasks, significantly speeding up\n(down by 7.5 seconds on average) with intermediate visual simulations. In\ncontrast, models exhibit inconsistent performance gains from visual\nsimulations, improving on most tasks but declining in specific cases like\ntangram puzzles (GPT-4o, o1) and cube net folding (Claude-3.5, Gemini-2.0\nFlash), indicating that models may not know how to effectively leverage\nintermediate visual information.", "authors": ["Linjie Li", "Mahtab Bigverdi", "Jiawei Gu", "Zixian Ma", "Yinuo Yang", "Ziang Li", "Yejin Choi", "Ranjay Krishna"], "published_date": "2025-06-05", "title_zh": "空間認知展開：視覺模擬上多模態模型的評估", "summary_zh": "空間認知對人類智能至關重要，透過視覺模擬而非僅依賴語言推理來解決問題。現有AI基準測試主要評估語言推理，忽略了非語言、多步驟視覺模擬的複雜性。我們提出了STARE（空間變換與推理評估）基準，旨在嚴格評估多模態大型語言模型在更適合透過多步驟視覺模擬解決的任務上的表現。STARE包含4K任務，涵蓋基礎幾何變換（2D和3D）、整合空間推理（正方體展開圖摺疊和七巧板拼圖）以及真實世界空間推理（透視和時間推理），反映了物件組裝、機械圖解讀和日常空間導航等實際認知挑戰。評估顯示，模型擅長較簡單的2D變換推理，但在需要多步驟視覺模擬的複雜任務（如3D正方體展開圖摺疊和七巧板拼圖）上的表現接近隨機。人類在複雜任務上幾乎達到完美準確度，但耗時較長（最多28.9秒），透過中間視覺模擬可顯著加快速度（平均減少7.5秒）。相比之下，模型從視覺模擬中獲得的性能提升不一致，在大多數任務上有所改進，但在特定情況下（如GPT-4o、o1的七巧板拼圖以及Claude-3.5、Gemini-2.0 Flash的正方體展開圖摺疊）表現下降，表明模型可能不知道如何有效利用中間視覺資訊。", "audio": "audios/2506.04633v1.mp3", "timestamp": "2025-06-08T10:19:11.857751"}
{"query": "AI", "id": "2506.04632v1", "url": "http://arxiv.org/abs/2506.04632v1", "title": "Composing Agents to Minimize Worst-case Risk", "summary": "From software development to robot control, modern agentic systems decompose\ncomplex objectives into a sequence of subtasks and choose a set of specialized\nAI agents to complete them. We formalize an agentic workflow as a directed\nacyclic graph, called an agent graph, where edges represent AI agents and paths\ncorrespond to feasible compositions of agents. When deploying these systems in\nthe real world, we need to choose compositions of agents that not only maximize\nthe task success, but also minimize risk where the risk captures requirements\nlike safety, fairness, and privacy. This additionally requires carefully\nanalyzing the low-probability (tail) behaviors of compositions of agents. In\nthis work, we consider worst-case risk minimization over the set of feasible\nagent compositions. We define worst-case risk as the tail quantile -- also\nknown as value-at-risk -- of the loss distribution of the agent composition\nwhere the loss quantifies the risk associated with agent behaviors. We\nintroduce an efficient algorithm that traverses the agent graph and finds a\nnear-optimal composition of agents by approximating the value-at-risk via a\nunion bound and dynamic programming. Furthermore, we prove that the\napproximation is near-optimal asymptotically for a broad class of practical\nloss functions. To evaluate our framework, we consider a suite of video\ngame-like control benchmarks that require composing several agents trained with\nreinforcement learning and demonstrate our algorithm's effectiveness in\napproximating the value-at-risk and identifying the optimal agent composition.", "authors": ["Guruprerana Shabadi", "Rajeev Alur"], "published_date": "2025-06-05", "title_zh": "最小化最差情況風險的智能體協作", "summary_zh": "現代代理系統將複雜目標分解為子任務序列，並選擇專業AI代理來完成。本文將代理工作流程形式化為有向無環圖，即代理圖，邊代表AI代理，路徑對應可行的代理組合。在實際部署中，需選擇最大化任務成功率並最小化風險（如安全、公平、隱私）的代理組合，並分析代理組合的低概率行為。本文探討可行代理組合中的最壞情況風險最小化，將最壞情況風險定義為代理組合損失分佈的尾部分位數（風險價值），損失量化代理行為的相關風險。我們提出一種高效演算法，遍歷代理圖，通過聯合界和動態規劃近似風險價值，尋找近似最佳的代理組合。此外，證明該近似對於廣泛的實際損失函數具有漸近近優性。我們在一系列視頻遊戲控制基準上評估了此框架，這些基準需要組合多個使用強化學習訓練的代理，並展示了我們的演算法在近似風險價值和識別最佳代理組合方面的有效性。", "audio": "audios/2506.04632v1.mp3", "timestamp": "2025-06-08T11:15:16.695414"}
{"query": "AI", "id": "2506.04625v1", "url": "http://arxiv.org/abs/2506.04625v1", "title": "Advancing Tool-Augmented Large Language Models via Meta-Verification and Reflection Learning", "summary": "Empowering large language models (LLMs) with effective tool utilization\ncapabilities is crucial for enabling AI agents to solve complex problems.\nHowever, current models face two major limitations: (1) unreliable tool\nplanning and invocation due to low-quality instruction datasets (e.g.,\nwidespread hallucinated API calls), and (2) weak tool reflection abilities\n(over 90% of errors cannot be corrected) resulting from static imitation\nlearning. To address these critical limitations, we propose Tool-MVR, a novel\nTool-Augmented LLM that achieves comprehensive System 2 reasoning through two\nkey innovations. Specifically, we first introduce Multi-Agent Meta-Verification\n(MAMV), a systematic pipeline that rigorously validates APIs, queries, and\nreasoning trajectories to construct ToolBench-V, a new high-quality instruction\ndataset that addresses the limitation of unreliable tool planning and\ninvocation. Second, we propose Exploration-based Reflection Learning (EXPLORE),\nwhich enhances tool reflection capabilities by leveraging tool feedback through\na dynamic \"Error -> Reflection -> Correction\" learning paradigm, resulting in\nour reflection dataset ToolBench-R and addressing the critical weakness in tool\nreflection. Finally, we obtain Tool-MVR by finetuning open-source LLMs (e.g.,\nQwen-7B) on both ToolBench-V and ToolBench-R. Our experiments demonstrate that\nTool-MVR achieves state-of-the-art performance on StableToolBench, surpassing\nboth ToolLLM (by 23.9%) and GPT-4 (by 15.3%) while reducing API calls by 31.4%,\nwith strong generalization capabilities across unseen tools and scenarios.\nAdditionally, on our proposed RefineToolBench, the first benchmark specifically\ndesigned to evaluate tool reflection capabilities, Tool-MVR achieves a 58.9%\nerror correction rate, significantly outperforming ToolLLM's 9.1%.", "authors": ["Zhiyuan Ma", "Jiayu Liu", "Xianzhen Luo", "Zhenya Huang", "Qingfu Zhu", "Wanxiang Che"], "published_date": "2025-06-05", "title_zh": "基於元驗證與反思學習推進工具增強型大型語言模型", "summary_zh": "為賦予大型語言模型(LLMs)有效的工具利用能力，以解決複雜問題，本研究提出Tool-MVR，一種新型工具增強型LLM。Tool-MVR透過兩項關鍵創新實現全面的System 2推理：首先，引入多代理元驗證(MAMV)流程，嚴格驗證API、查詢和推理軌跡，構建高品質指令數據集ToolBench-V，解決不可靠的工具規劃和調用問題。其次，提出基於探索的反思學習(EXPLORE)，利用工具反饋，透過動態的錯誤->反思->修正學習模式，增強工具反思能力，產生反思數據集ToolBench-R。最後，在ToolBench-V和ToolBench-R上微調開源LLM(如Qwen-7B)以獲得Tool-MVR。實驗結果表明，Tool-MVR在StableToolBench上實現了最先進的性能，超越ToolLLM (23.9%)和GPT-4 (15.3%)，同時減少了31.4%的API調用，並在未見過的工具和場景中展現出強大的泛化能力。此外，在用於評估工具反思能力的RefineToolBench基準測試中，Tool-MVR實現了58.9%的錯誤糾正率，顯著優於ToolLLM的9.1%。", "audio": "audios/2506.04625v1.mp3", "timestamp": "2025-06-08T14:16:41.530809"}
{"query": "AI", "id": "2506.04616v1", "url": "http://arxiv.org/abs/2506.04616v1", "title": "Subjective Perspectives within Learned Representations Predict High-Impact Innovation", "summary": "Existing studies of innovation emphasize the power of social structures to\nshape innovation capacity. Emerging machine learning approaches, however,\nenable us to model innovators' personal perspectives and interpersonal\ninnovation opportunities as a function of their prior trajectories of\nexperience. We theorize then quantify subjective perspectives and innovation\nopportunities based on innovator positions within the geometric space of\nconcepts inscribed by dynamic language representations. Using data on millions\nof scientists, inventors, writers, entrepreneurs, and Wikipedia contributors\nacross the creative domains of science, technology, film, entrepreneurship, and\nWikipedia, here we show that measured subjective perspectives anticipate what\nideas individuals and groups creatively attend to and successfully combine in\nfuture. When perspective and background diversity are decomposed as the angular\ndifference between collaborators' perspectives on their creation and between\ntheir experiences, the former consistently anticipates creative achievement\nwhile the latter portends its opposite, across all cases and time periods\nexamined. We analyze a natural experiment and simulate creative collaborations\nbetween AI (large language model) agents designed with various perspective and\nbackground diversity, which are consistent with our observational findings. We\nexplore mechanisms underlying these findings and identify how successful\ncollaborators leverage common language to weave together diverse experience\nobtained through trajectories of prior work that converge to provoke one\nanother and innovate. We explore the importance of these findings for team\nassembly and research policy.", "authors": ["Likun Cao", "Rui Pan", "James Evans"], "published_date": "2025-06-05", "title_zh": "學習表徵中的主觀視角預測高影響力創新", "summary_zh": "現有創新研究強調社會結構對創新能力的影響。新興機器學習方法則能將創新者的個人觀點與人際創新機會建模為其先前經驗軌跡的函數。本研究基於動態語言表徵所構成的概念幾何空間中，創新者的位置來量化主觀觀點與創新機會。利用數百萬科學家、發明家、作家、企業家和維基百科貢獻者的資料，研究表明，所測量的主觀觀點能預測個體及群體在未來關注並成功結合哪些創意。當將觀點及背景多樣性分解為合作者對其創作的觀點差異及經驗差異時，前者始終能預測創造性成就，後者則預示相反結果。本研究分析了一項自然實驗，並模擬了具備不同觀點及背景多樣性的人工智慧協作，結果與觀察性研究一致。研究進一步探討了潛在機制，並指出成功的合作者如何利用共同語言將多樣化的經驗編織在一起，從而激發彼此創新。這些發現對於團隊組建和研究政策具有重要意義。", "audio": "audios/2506.04616v1.mp3", "timestamp": "2025-06-08T16:22:20.097113"}
{"query": "AI", "id": "2506.05333v2", "url": "http://arxiv.org/abs/2506.05333v2", "title": "Kinetics: Rethinking Test-Time Scaling Laws", "summary": "We rethink test-time scaling laws from a practical efficiency perspective,\nrevealing that the effectiveness of smaller models is significantly\noverestimated. Prior work, grounded in compute-optimality, overlooks critical\nmemory access bottlenecks introduced by inference-time strategies (e.g.,\nBest-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to\n32B parameters, reveals a new Kinetics Scaling Law that better guides resource\nallocation by incorporating both computation and memory access costs. Kinetics\nScaling Law suggests that test-time compute is more effective when used on\nmodels above a threshold than smaller ones. A key reason is that in TTS,\nattention, rather than parameter count, emerges as the dominant cost factor.\nMotivated by this, we propose a new scaling paradigm centered on sparse\nattention, which lowers per-token cost and enables longer generations and more\nparallel samples within the same resource budget. Empirically, we show that\nsparse attention models consistently outperform dense counterparts, achieving\nover 60 points gains in low-cost regimes and over 5 points gains in high-cost\nregimes for problem-solving accuracy on AIME, encompassing evaluations on\nstate-of-the-art MoEs. These results suggest that sparse attention is essential\nand increasingly important with more computing invested, for realizing the full\npotential of test-time scaling where, unlike training, accuracy has yet to\nsaturate as a function of computation, and continues to improve through\nincreased generation. The code is available at\nhttps://github.com/Infini-AI-Lab/Kinetics.", "authors": ["Ranajoy Sadhukhan", "Zhuoming Chen", "Haizhong Zheng", "Yang Zhou", "Emma Strubell", "Beidi Chen"], "published_date": "2025-06-05", "title_zh": "動力學：重新思考測試時縮放定律", "summary_zh": "本研究從實際效率角度重新評估了測試時縮放定律，發現小型模型的效果被顯著高估。以往基於計算最佳化的研究忽略了推論時策略（例如Best-of-N、長CoTs）引入的記憶體存取瓶頸。透過對0.6B至32B參數模型的整體分析，揭示了一種新的動力學縮放定律，該定律透過納入計算和記憶體存取成本，更好地指導資源分配。動力學縮放定律表明，測試時計算在較大模型上的效果優於小型模型。主要原因是，在測試時縮放中，注意力而非參數計數成為主要的成本因素。因此，我們提出了一種以稀疏注意力為中心的新縮放範例，降低了每個 token 的成本，並在相同的資源預算內實現更長的生成和更多的平行樣本。實驗表明，稀疏注意力模型始終優於密集模型，在低成本情況下，解決問題的準確性提高了 60 多分，在高成本情況下，在 AIME 上提高了 5 分以上，包括對最先進的 MoE 的評估。這些結果表明，稀疏注意力至關重要，並且隨著計算投入的增加而變得越來越重要，從而充分發揮了測試時縮放的潛力，與訓練不同，準確性尚未達到計算的飽和點，並且可以透過增加生成來不斷提高。", "audio": "audios/2506.05333v2.mp3", "timestamp": "2025-06-09T01:41:21.800912"}
{"query": "Diffusion Model", "id": "2506.05340v2", "url": "http://arxiv.org/abs/2506.05340v2", "title": "Exploring Diffusion Transformer Designs via Grafting", "summary": "Designing model architectures requires decisions such as selecting operators\n(e.g., attention, convolution) and configurations (e.g., depth, width).\nHowever, evaluating the impact of these decisions on model quality requires\ncostly pretraining, limiting architectural investigation. Inspired by how new\nsoftware is built on existing code, we ask: can new architecture designs be\nstudied using pretrained models? To this end, we present grafting, a simple\napproach for editing pretrained diffusion transformers (DiTs) to materialize\nnew architectures under small compute budgets. Informed by our analysis of\nactivation behavior and attention locality, we construct a testbed based on the\nDiT-XL/2 design to study the impact of grafting on model quality. Using this\ntestbed, we develop a family of hybrid designs via grafting: replacing softmax\nattention with gated convolution, local attention, and linear attention, and\nreplacing MLPs with variable expansion ratio and convolutional variants.\nNotably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for\nDiT-XL/2) using <2% pretraining compute. We then graft a text-to-image model\n(PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval\nscore. Finally, we present a case study that restructures DiT-XL/2 by\nconverting every pair of sequential transformer blocks into parallel blocks via\ngrafting. This reduces model depth by 2x and yields better quality (FID: 2.77)\nthan other models of comparable depth. Together, we show that new diffusion\nmodel designs can be explored by grafting pretrained DiTs, with edits ranging\nfrom operator replacement to architecture restructuring. Code and grafted\nmodels: https://grafting.stanford.edu", "authors": ["Keshigeyan Chandrasegaran", "Michael Poli", "Daniel Y. Fu", "Dongjun Kim", "Lea M. Hadzic", "Manling Li", "Agrim Gupta", "Stefano Massaroli", "Azalia Mirhoseini", "Juan Carlos Niebles", "Stefano Ermon", "Li Fei-Fei"], "published_date": "2025-06-05", "title_zh": "基於嫁接探索擴散轉換器設計", "summary_zh": "模型架構設計涉及運算子選擇（如注意力機制、卷積）和配置（如深度、寬度）等決策。評估這些決策的影響通常需耗費大量預訓練資源，限制了架構探索。本研究提出嫁接方法，利用預訓練擴散Transformer（DiT）實現新型架構設計，降低運算成本。基於對激活行為和注意力局部性的分析，我們構建了基於DiT-XL/2的測試平台，研究嫁接對模型品質的影響。透過嫁接，我們開發了一系列混合設計，包括使用門控卷積、局部注意力、線性注意力取代softmax注意力，以及使用可變擴展率和卷積變體取代MLP。許多混合設計在低於2%預訓練運算量下，達到良好品質（FID：2.38-2.64，DiT-XL/2為2.27）。我們將嫁接應用於文本到圖像模型（PixArt-Sigma），在GenEval分數下降不到2%的情況下，實現了1.43倍的加速。案例研究表明，透過嫁接將DiT-XL/2的每對連續Transformer塊轉換為並行塊，可將模型深度減少2倍，並產生優於其他同等深度模型的品質（FID：2.77）。研究結果表明，可通過嫁接預訓練DiT探索新型擴散模型設計，包括運算子替換和架構重組。", "audio": "audios/2506.05340v2.mp3", "timestamp": "2025-06-09T01:41:31.535700"}
{"query": "AI", "id": "2506.06242v1", "url": "http://arxiv.org/abs/2506.06242v1", "title": "Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models", "summary": "Recent advancements in multimodal large language models have driven\nbreakthroughs in visual question answering. Yet, a critical gap persists,\n`conceptualization'-the ability to recognize and reason about the same concept\ndespite variations in visual form, a basic ability of human reasoning. To\naddress this challenge, we introduce the Visual Graph Arena (VGA), a dataset\nfeaturing six graph-based tasks designed to evaluate and improve AI systems'\ncapacity for visual abstraction. VGA uses diverse graph layouts (e.g.,\nKamada-Kawai vs. planar) to test reasoning independent of visual form.\nExperiments with state-of-the-art vision models and multimodal LLMs reveal a\nstriking divide: humans achieved near-perfect accuracy across tasks, while\nmodels totally failed on isomorphism detection and showed limited success in\npath/cycle tasks. We further identify behavioral anomalies suggesting\npseudo-intelligent pattern matching rather than genuine understanding. These\nfindings underscore fundamental limitations in current AI models for visual\nunderstanding. By isolating the challenge of representation-invariant\nreasoning, the VGA provides a framework to drive progress toward human-like\nconceptualization in AI visual models. The Visual Graph Arena is available at:\n\\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}", "authors": ["Zahra Babaiee", "Peyman M. Kiasari", "Daniela Rus", "Radu Grosu"], "published_date": "2025-06-06", "title_zh": "視覺圖形競技場：評估視覺與多模態大型語言模型的視覺概念化能力", "summary_zh": "多模態大型語言模型在視覺問答領域取得突破，但AI在視覺概念化能力方面仍存在差距，即無法識別和推理視覺形式不同的相同概念。為此，我們提出視覺圖形競技場（VGA）數據集，包含六項圖形任務，旨在評估和提升AI系統的視覺抽象能力。VGA利用多樣的圖形佈局來測試獨立於視覺形式的推理能力。實驗表明，人類在各項任務中表現近乎完美，而模型在同構檢測上完全失敗，路徑/環路任務表現有限。我們觀察到異常行為，暗示模型採用偽智能模式匹配而非真正理解。這些發現突顯了當前AI模型在視覺理解方面的根本局限性。VGA通過隔離表徵不變推理的挑戰，為推動AI視覺模型實現類人概念化提供了一個框架。視覺圖形競技場可在以下網址獲取：\\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}", "audio": "audios/2506.06242v1.mp3", "timestamp": "2025-06-09T04:30:55.467417"}
{"query": "Foundation Model", "id": "2506.06281v1", "url": "http://arxiv.org/abs/2506.06281v1", "title": "TerraFM: A Scalable Foundation Model for Unified Multisensor Earth Observation", "summary": "Modern Earth observation (EO) increasingly leverages deep learning to harness\nthe scale and diversity of satellite imagery across sensors and regions. While\nrecent foundation models have demonstrated promising generalization across EO\ntasks, many remain limited by the scale, geographical coverage, and spectral\ndiversity of their training data, factors critical for learning globally\ntransferable representations. In this work, we introduce TerraFM, a scalable\nself-supervised learning model that leverages globally distributed Sentinel-1\nand Sentinel-2 imagery, combined with large spatial tiles and land-cover aware\nsampling to enrich spatial and semantic coverage. By treating sensing\nmodalities as natural augmentations in our self-supervised approach, we unify\nradar and optical inputs via modality-specific patch embeddings and adaptive\ncross-attention fusion. Our training strategy integrates local-global\ncontrastive learning and introduces a dual-centering mechanism that\nincorporates class-frequency-aware regularization to address long-tailed\ndistributions in land cover.TerraFM achieves strong generalization on both\nclassification and segmentation tasks, outperforming prior models on GEO-Bench\nand Copernicus-Bench. Our code and pretrained models are publicly available at:\nhttps://github.com/mbzuai-oryx/TerraFM .", "authors": ["Muhammad Sohail Danish", "Muhammad Akhtar Munir", "Syed Roshaan Ali Shah", "Muhammad Haris Khan", "Rao Muhammad Anwer", "Jorma Laaksonen", "Fahad Shahbaz Khan", "Salman Khan"], "published_date": "2025-06-06", "title_zh": "TerraFM：用於統一多感測器地球觀測的可擴展基礎模型", "summary_zh": "現代地球觀測日益利用深度學習處理大規模、多樣化的衛星影像。儘管近期基礎模型在地球觀測任務中展現了良好的泛化能力，但其訓練數據的規模、地理覆蓋範圍和光譜多樣性仍有局限，這些因素對學習全球可遷移的表徵至關重要。本研究提出TerraFM，一種可擴展的自我監督學習模型，利用全球分佈的Sentinel-1和Sentinel-2影像，結合大型空間瓦片和土地覆蓋感知採樣，以豐富空間和語義覆蓋範圍。透過將感測模式視為自我監督方法中的自然增強，我們透過特定模態的補丁嵌入和自適應交叉注意力融合來統一雷達和光學輸入。我們的訓練策略整合了局部-全局對比學習，並引入雙中心機制，該機制結合了類別頻率感知正規化，以解決土地覆蓋中的長尾分佈。TerraFM在分類和分割任務上均取得了強大的泛化能力，優於GEO-Bench和Copernicus-Bench上的先前模型。代碼和預訓練模型已公開。", "audio": "audios/2506.06281v1.mp3", "timestamp": "2025-06-09T04:31:02.507754"}
{"query": "Diffusion Model", "id": "2506.06276v1", "url": "http://arxiv.org/abs/2506.06276v1", "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis", "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.", "authors": ["Jiatao Gu", "Tianrong Chen", "David Berthelot", "Huangjie Zheng", "Yuyang Wang", "Ruixiang Zhang", "Laurent Dinh", "Miguel Angel Bautista", "Josh Susskind", "Shuangfei Zhai"], "published_date": "2025-06-06", "title_zh": "STARFlow：用於高解析度影像合成的潛在歸一化流擴展", "summary_zh": "STARFlow是一種基於正規化流的可擴展生成模型，在高解析度影像合成方面表現出色。其核心是Transformer自迴歸流（TARFlow），結合了正規化流的表達能力與自迴歸Transformer的結構化建模能力。研究首先確立了TARFlow對連續分布建模的理論普適性，並在此基礎上引入多項架構和演算法創新以提升可擴展性：包括深淺設計（深層Transformer區塊捕捉模型的主要表徵能力，輔以計算效率高但效益顯著的淺層Transformer區塊）、在預訓練自編碼器的潛在空間中建模（優於直接像素級建模），以及顯著提升樣本品質的新型引導演算法。重要的是，該模型保持端到端正規化流特性，無需離散化即可在連續空間中進行精確的最大似然訓練。STARFlow在類別條件和文字條件影像生成任務中均達到具競爭力的效能，其樣本品質接近最先進的擴散模型。據了解，此研究首次成功展示了正規化流在此規模和解析度下的有效運作。", "audio": "audios/2506.06276v1.mp3", "timestamp": "2025-06-09T04:31:08.712712"}
{"query": "AI", "id": "2506.06232v1", "url": "http://arxiv.org/abs/2506.06232v1", "title": "Challenging Vision-Language Models with Surgical Data: A New Dataset and Broad Benchmarking Study", "summary": "While traditional computer vision models have historically struggled to\ngeneralize to endoscopic domains, the emergence of foundation models has shown\npromising cross-domain performance. In this work, we present the first\nlarge-scale study assessing the capabilities of Vision Language Models (VLMs)\nfor endoscopic tasks with a specific focus on laparoscopic surgery. Using a\ndiverse set of state-of-the-art models, multiple surgical datasets, and\nextensive human reference annotations, we address three key research questions:\n(1) Can current VLMs solve basic perception tasks on surgical images? (2) Can\nthey handle advanced frame-based endoscopic scene understanding tasks? and (3)\nHow do specialized medical VLMs compare to generalist models in this context?\nOur results reveal that VLMs can effectively perform basic surgical perception\ntasks, such as object counting and localization, with performance levels\ncomparable to general domain tasks. However, their performance deteriorates\nsignificantly when the tasks require medical knowledge. Notably, we find that\nspecialized medical VLMs currently underperform compared to generalist models\nacross both basic and advanced surgical tasks, suggesting that they are not yet\noptimized for the complexity of surgical environments. These findings highlight\nthe need for further advancements to enable VLMs to handle the unique\nchallenges posed by surgery. Overall, our work provides important insights for\nthe development of next-generation endoscopic AI systems and identifies key\nareas for improvement in medical visual language models.", "authors": ["Leon Mayer", "Tim Rädsch", "Dominik Michael", "Lucas Luttner", "Amine Yamlahi", "Evangelia Christodoulou", "Patrick Godau", "Marcel Knopp", "Annika Reinke", "Fiona Kolbinger", "Lena Maier-Hein"], "published_date": "2025-06-06", "title_zh": "以外科數據挑戰視覺語言模型：新數據集與廣泛基準測試研究", "summary_zh": "傳統電腦視覺模型難以適應內視鏡領域，但基礎模型展現了跨領域的潛力。本研究首次大規模評估視覺語言模型在腹腔鏡手術等內視鏡任務中的能力。透過多種先進模型、手術數據集及人工標註，探討三個問題：(1) 現有視覺語言模型能否解決手術圖像的基本感知任務？(2) 能否處理基於幀的高級內視鏡場景理解任務？(3) 專用醫療視覺語言模型與通用模型相比如何？結果顯示，視覺語言模型能有效執行物件計數和定位等基本手術感知任務，效能與通用領域任務相當。然而，涉及醫學知識的任務效能明顯下降。值得注意的是，專用醫療視覺語言模型在基本和高級手術任務中的表現均不如通用模型，表明其尚未針對複雜的手術環境進行優化。研究結果突顯了開發能應對手術獨特挑戰的視覺語言模型之必要性，並為下一代內視鏡人工智慧系統的開發提供了重要見解，同時指出醫療視覺語言模型需要改進的關鍵領域。", "audio": "audios/2506.06232v1.mp3", "timestamp": "2025-06-09T06:30:11.866436"}
{"query": "Foundation Model", "id": "2506.06270v1", "url": "http://arxiv.org/abs/2506.06270v1", "title": "RecGPT: A Foundation Model for Sequential Recommendation", "summary": "This work addresses a fundamental barrier in recommender systems: the\ninability to generalize across domains without extensive retraining.\nTraditional ID-based approaches fail entirely in cold-start and cross-domain\nscenarios where new users or items lack sufficient interaction history.\nInspired by foundation models' cross-domain success, we develop a foundation\nmodel for sequential recommendation that achieves genuine zero-shot\ngeneralization capabilities. Our approach fundamentally departs from existing\nID-based methods by deriving item representations exclusively from textual\nfeatures. This enables immediate embedding of any new item without model\nretraining. We introduce unified item tokenization with Finite Scalar\nQuantization that transforms heterogeneous textual descriptions into\nstandardized discrete tokens. This eliminates domain barriers that plague\nexisting systems. Additionally, the framework features hybrid\nbidirectional-causal attention that captures both intra-item token coherence\nand inter-item sequential dependencies. An efficient catalog-aware beam search\ndecoder enables real-time token-to-item mapping. Unlike conventional approaches\nconfined to their training domains, RecGPT naturally bridges diverse\nrecommendation contexts through its domain-invariant tokenization mechanism.\nComprehensive evaluations across six datasets and industrial scenarios\ndemonstrate consistent performance advantages.", "authors": ["Yangqin Jiang", "Xubin Ren", "Lianghao Xia", "Da Luo", "Kangyi Lin", "Chao Huang"], "published_date": "2025-06-06", "title_zh": "RecGPT：序列推薦之基礎模型", "summary_zh": "本研究旨在解決推薦系統中普遍存在的跨領域泛化難題，傳統基於ID的方法在新用戶或項目缺乏互動歷史的冷啟動和跨領域情境中失效。受惠於基礎模型在跨領域上的成功，我們開發了一種序列推薦基礎模型，實現真正的零樣本泛化。此方法捨棄基於ID的方式，僅從文本特徵提取項目表徵，無需重新訓練即可嵌入新項目。我們採用統一項目分詞與有限標量量化，將異質文本描述轉換為標準化離散令牌，消除領域隔閡。此外，該框架具備混合雙向因果注意力機制，捕捉項目內令牌一致性和項目間序列依賴性。高效的目錄感知束搜索解碼器實現實時令牌到項目映射。相較於受限於訓練領域的傳統方法，RecGPT透過其領域不變的分詞機制自然地橋接不同推薦情境。在六個數據集和工業情境中的評估顯示，其性能具備一致性優勢。", "audio": "audios/2506.06270v1.mp3", "timestamp": "2025-06-09T06:30:18.032031"}
{"query": "Diffusion Model", "id": "2506.06185v1", "url": "http://arxiv.org/abs/2506.06185v1", "title": "Antithetic Noise in Diffusion Models", "summary": "We initiate a systematic study of antithetic initial noise in diffusion\nmodels. Across unconditional models trained on diverse datasets,\ntext-conditioned latent-diffusion models, and diffusion-posterior samplers, we\nfind that pairing each initial noise with its negation consistently yields\nstrongly negatively correlated samples. To explain this phenomenon, we combine\nexperiments and theoretical analysis, leading to a symmetry conjecture that the\nlearned score function is approximately affine antisymmetric (odd symmetry up\nto a constant shift), and provide evidence supporting it. Leveraging this\nnegative correlation, we enable two applications: (1) enhancing image diversity\nin models like Stable Diffusion without quality loss, and (2) sharpening\nuncertainty quantification (e.g., up to 90% narrower confidence intervals) when\nestimating downstream statistics. Building on these gains, we extend the\ntwo-point pairing to a randomized quasi-Monte Carlo estimator, which further\nimproves estimation accuracy. Our framework is training-free, model-agnostic,\nand adds no runtime overhead.", "authors": ["Jing Jia", "Sifan Liu", "Bowen Song", "Wei Yuan", "Liyue Shen", "Guanyang Wang"], "published_date": "2025-06-06", "title_zh": "擴散模型中的反向雜訊", "summary_zh": "本研究系統性探討擴散模型中的反義初始雜訊。針對無條件模型、文字條件潛在擴散模型及擴散後驗採樣器，發現將每個初始雜訊與其反值配對，能穩定產生強烈負相關樣本。為解釋此現象，結合實驗與理論分析，提出對稱猜想：學習到的分數函數近似仿射反對稱（奇對稱性至常數偏移），並提供證據支持。利用此負相關性，實現兩項應用：一、提升Stable Diffusion等模型中的圖像多樣性且不損失品質；二、提高不確定性量化精確度（例如，縮減高達90%的信賴區間）。基於這些優勢，將兩點配對擴展到隨機準蒙地卡羅估計器，進一步提高估計準確性。此框架無需訓練，適用於各種模型，且不增加運行時額外負擔。", "audio": "audios/2506.06185v1.mp3", "timestamp": "2025-06-09T06:30:23.772825"}
{"query": "AI", "id": "2506.06225v1", "url": "http://arxiv.org/abs/2506.06225v1", "title": "\"We need to avail ourselves of GenAI to enhance knowledge distribution\": Empowering Older Adults through GenAI Literacy", "summary": "As generative AI (GenAI) becomes increasingly widespread, it is crucial to\nequip users, particularly vulnerable populations such as older adults (65 and\nolder), with the knowledge to understand its benefits and potential risks.\nOlder adults often exhibit greater reservations about adopting emerging\ntechnologies and require tailored literacy support. Using a mixed methods\napproach, this study examines strategies for delivering GenAI literacy to older\nadults through a chatbot named Litti, evaluating its impact on their AI\nliteracy (knowledge, safety, and ethical use). The quantitative data indicated\na trend toward improved AI literacy, though the results were not statistically\nsignificant. However, qualitative interviews revealed diverse levels of\nfamiliarity with generative AI and a strong desire to learn more. Findings also\nshow that while Litti provided a positive learning experience, it did not\nsignificantly enhance participants' trust or sense of safety regarding GenAI.\nThis exploratory case study highlights the challenges and opportunities in\ndesigning AI literacy education for the rapidly growing older adult population.", "authors": ["Eunhye Grace Ko", "Shaini Nanayakkara", "Earl W. Huff Jr"], "published_date": "2025-06-06", "title_zh": "我們需要利用生成式人工智慧來增強知識傳播：透過生成式人工智慧素養賦能年長者", "summary_zh": "隨著生成式人工智慧 (GenAI) 日益普及，讓使用者，特別是老年人等弱勢群體，具備理解其益處與潛在風險的知識至關重要。老年人對新興技術的接受度通常較低，需要量身定制的素養支持。本研究採用混合方法，探討透過名為Litti的聊天機器人向老年人提供GenAI素養的策略，評估其對人工智慧素養（知識、安全和道德使用）的影響。量化數據顯示人工智慧素養有提升趨勢，但結果未達統計顯著性。質性訪談揭示了老年人對生成式人工智慧的不同熟悉程度和強烈的學習意願。研究結果亦顯示，儘管Litti提供了積極的學習體驗，但並未顯著提升參與者對GenAI的信任或安全感。這項探索性案例研究突顯了為快速增長的老年人口設計人工智慧素養教育的挑戰與機遇。", "audio": "audios/2506.06225v1.mp3", "timestamp": "2025-06-09T07:19:41.786024"}
{"query": "Foundation Model", "id": "2506.06211v1", "url": "http://arxiv.org/abs/2506.06211v1", "title": "PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts", "summary": "Puzzlehunts are a genre of complex, multi-step puzzles lacking well-defined\nproblem definitions. In contrast to conventional reasoning benchmarks\nconsisting of tasks with clear instructions, puzzlehunts require models to\ndiscover the underlying problem structure from multimodal evidence and\niterative reasoning, mirroring real-world domains such as scientific discovery,\nexploratory data analysis, or investigative problem-solving. Despite recent\nprogress in foundation models, their performance on such open-ended settings\nremains largely untested. In this paper, we introduce PuzzleWorld, a\nlarge-scale benchmark of 667 puzzlehunt-style problems designed to assess\nstep-by-step, open-ended, and creative multimodal reasoning. Each puzzle is\nannotated with the final solution, detailed reasoning traces, and cognitive\nskill labels, enabling holistic benchmarking and fine-grained diagnostic\nanalysis. Most state-of-the-art models achieve only 1-2% final answer accuracy,\nwith the best model solving only 14% of puzzles and reaching 40% stepwise\naccuracy. To demonstrate the value of our reasoning annotations, we show that\nfine-tuning a small model on reasoning traces improves stepwise reasoning from\n4% to 11%, while training on final answers alone degrades performance to near\nzero. Our error analysis reveals that current models exhibit myopic reasoning,\nare bottlenecked by the limitations of language-based inference, and lack\nsketching capabilities crucial for visual and spatial reasoning. We release\nPuzzleWorld at https://github.com/MIT-MI/PuzzleWorld to support future work on\nbuilding more general, open-ended, and creative reasoning systems.", "authors": ["Hengzhi Li", "Brendon Jiang", "Alexander Naehu", "Regan Song", "Justin Zhang", "Megan Tjandrasuwita", "Chanakya Ekbote", "Steven-Shine Chen", "Adithya Balachandran", "Wei Dai", "Rebecca Chang", "Paul Pu Liang"], "published_date": "2025-06-06", "title_zh": "謎題世界：謎題尋寶中多模態開放式推理的基準測試", "summary_zh": "益智解謎遊戲缺乏明確的問題定義，需要從多模態證據和迭代推理中發掘潛在結構，類似於現實世界的科學發現或探索性資料分析。現有基礎模型在此類開放式環境中的表現仍待驗證。本文提出 PuzzleWorld，一個包含667個益智解謎風格問題的大規模基準測試，旨在評估逐步式、開放式和創造性的多模態推理能力。每個謎題都標注了最終答案、詳細推理過程和認知技能標籤，以便進行整體基準測試和細緻的診斷分析。多數頂尖模型僅能達到1-2%的最終答案準確度，最佳模型也僅能解決14%的謎題，逐步準確度為40%。研究顯示，基於推理過程微調小型模型能將逐步推理能力從4%提升至11%，而僅基於最終答案的訓練則會使效能降至近零。誤差分析表明，現有模型存在短視推理，受限於語言推理，且缺乏視覺和空間推理所需的草圖繪製能力。PuzzleWorld已公開發布，以支持構建更通用、開放和具創造力的推理系統的未來研究。", "audio": "audios/2506.06211v1.mp3", "timestamp": "2025-06-09T07:19:49.486509"}
{"query": "Diffusion Model", "id": "2506.06085v1", "url": "http://arxiv.org/abs/2506.06085v1", "title": "Feedback Guidance of Diffusion Models", "summary": "While Classifier-Free Guidance (CFG) has become standard for improving sample\nfidelity in conditional diffusion models, it can harm diversity and induce\nmemorization by applying constant guidance regardless of whether a particular\nsample needs correction. We propose FeedBack Guidance (FBG), which uses a\nstate-dependent coefficient to self-regulate guidance amounts based on need.\nOur approach is derived from first principles by assuming the learned\nconditional distribution is linearly corrupted by the unconditional\ndistribution, contrasting with CFG's implicit multiplicative assumption. Our\nscheme relies on feedback of its own predictions about the conditional signal\ninformativeness to adapt guidance dynamically during inference, challenging the\nview of guidance as a fixed hyperparameter. The approach is benchmarked on\nImageNet512x512, where it significantly outperforms Classifier-Free Guidance\nand is competitive to Limited Interval Guidance (LIG) while benefitting from a\nstrong mathematical framework. On Text-To-Image generation, we demonstrate\nthat, as anticipated, our approach automatically applies higher guidance scales\nfor complex prompts than for simpler ones and that it can be easily combined\nwith existing guidance schemes such as CFG or LIG.", "authors": ["Koulischer Felix", "Handke Florian", "Deleu Johannes", "Demeester Thomas", "Ambrogioni Luca"], "published_date": "2025-06-06", "title_zh": "擴散模型的回饋引導", "summary_zh": "無分類器引導(CFG)雖已成為提升條件擴散模型樣本保真度的標準方法，但其恆定引導方式可能損害多樣性並導致記憶，無論特定樣本是否需要修正。我們提出回饋引導(FBG)，使用狀態相關係數根據需求自我調節引導量。我們的方案基於第一性原理推導，假設學習到的條件分佈被無條件分佈線性破壞，這與CFG隱含的乘法假設形成對比。此方案依賴自身對條件訊號資訊量的預測回饋，在推理過程中動態調整引導，挑戰了將引導視為固定超參數的觀點。在ImageNet512x512上的基準測試表明，FBG顯著優於無分類器引導，並與有限區間引導(LIG)具備競爭力，同時受益於強大的數學框架。在文本到圖像生成方面，我們證明FBG如預期般，對複雜提示詞自動應用更高的引導尺度，而對簡單提示詞則不然，且易於與現有的引導方案(如CFG或LIG)結合。", "audio": "audios/2506.06085v1.mp3", "timestamp": "2025-06-09T07:19:56.486439"}
{"query": "AI", "id": "2506.06220v1", "url": "http://arxiv.org/abs/2506.06220v1", "title": "GenIR: Generative Visual Feedback for Mental Image Retrieval", "summary": "Vision-language models (VLMs) have shown strong performance on text-to-image\nretrieval benchmarks. However, bridging this success to real-world applications\nremains a challenge. In practice, human search behavior is rarely a one-shot\naction. Instead, it is often a multi-round process guided by clues in mind,\nthat is, a mental image ranging from vague recollections to vivid mental\nrepresentations of the target image. Motivated by this gap, we study the task\nof Mental Image Retrieval (MIR), which targets the realistic yet underexplored\nsetting where users refine their search for a mentally envisioned image through\nmulti-round interactions with an image search engine. Central to successful\ninteractive retrieval is the capability of machines to provide users with\nclear, actionable feedback; however, existing methods rely on indirect or\nabstract verbal feedback, which can be ambiguous, misleading, or ineffective\nfor users to refine the query. To overcome this, we propose GenIR, a generative\nmulti-round retrieval paradigm leveraging diffusion-based image generation to\nexplicitly reify the AI system's understanding at each round. These synthetic\nvisual representations provide clear, interpretable feedback, enabling users to\nrefine their queries intuitively and effectively. We further introduce a fully\nautomated pipeline to generate a high-quality multi-round MIR dataset.\nExperimental results demonstrate that GenIR significantly outperforms existing\ninteractive methods in the MIR scenario. This work establishes a new task with\na dataset and an effective generative retrieval method, providing a foundation\nfor future research in this direction.", "authors": ["Diji Yang", "Minghao Liu", "Chung-Hsiang Lo", "Yi Zhang", "James Davis"], "published_date": "2025-06-06", "title_zh": "GenIR：用於心理圖像檢索的生成式視覺反饋", "summary_zh": "視覺語言模型在文本到圖像檢索基準測試中表現出色，但將此成果應用於現實世界仍具挑戰。實際應用中，人類搜尋行為鮮少為單次完成，通常是多輪互動過程，受腦海中線索引導，即目標圖像模糊回憶至清晰表徵的心像。為此，本研究探討心像檢索(MIR)任務，著重於使用者透過與圖像搜尋引擎的多輪互動，逐步完善對心中圖像的搜尋。成功的互動檢索關鍵在於機器提供清晰、可操作的回饋；然而，現有方法依賴間接或抽象的口頭回饋，可能造成歧義、誤導或影響使用者調整查詢。為了解決此問題，我們提出GenIR，一種生成式多輪檢索範例，利用基於擴散的圖像生成，明確展現AI系統在每一輪的理解。這些合成視覺表徵提供清晰、可解釋的回饋，使使用者能夠直觀且有效地完善查詢。我們進一步引入全自動化流程，生成高品質多輪MIR資料集。實驗結果表明，GenIR在MIR情境中顯著優於現有互動方法。本研究確立了新的任務及資料集，並提出有效的生成式檢索方法，為未來研究奠定基礎。", "audio": "audios/2506.06220v1.mp3", "timestamp": "2025-06-09T08:27:51.850936"}
{"query": "Foundation Model", "id": "2506.06105v1", "url": "http://arxiv.org/abs/2506.06105v1", "title": "Text-to-LoRA: Instant Transformer Adaption", "summary": "While Foundation Models provide a general tool for rapid content creation,\nthey regularly require task-specific adaptation. Traditionally, this exercise\ninvolves careful curation of datasets and repeated fine-tuning of the\nunderlying model. Fine-tuning techniques enable practitioners to adapt\nfoundation models for many new applications but require expensive and lengthy\ntraining while being notably sensitive to hyper-parameter choices. To overcome\nthese limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting\nLarge Language Models on the fly solely based on a natural language description\nof the target task. T2L is a hypernetwork trained to construct LoRAs in a\nsingle inexpensive forward pass. After training T2L on a suite of 9 pre-trained\nLoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA\ninstances match the performance of task-specific adapters across the\ncorresponding test sets. Furthermore, T2L can compress hundreds of LoRA\ninstances and zero-shot generalize to entirely unseen tasks. This approach\nprovides a significant step towards democratizing the specialization of\nfoundation models and enables language-based adaptation with minimal compute\nrequirements. Our code is available at https://github.com/SakanaAI/text-to-lora", "authors": ["Rujikorn Charakorn", "Edoardo Cetin", "Yujin Tang", "Robert Tjarko Lange"], "published_date": "2025-06-06", "title_zh": "文本到LoRA：即時轉換器適應", "summary_zh": "大型語言模型雖能快速生成內容，常需針對特定任務調整。傳統上，這涉及數據集篩選及模型反覆微調，耗時費力且對超參數敏感。為克服此限制，我們提出Text-to-LoRA (T2L)，僅依目標任務的自然語言描述，即可即時調整大型語言模型。T2L是個超網絡，經訓練能以單次低成本的前向傳播建構LoRA。在9個預訓練LoRA適配器(如GSM8K, Arc)上訓練T2L後，結果顯示，特設重建的LoRA實例，其性能與對應測試集上的特定任務適配器相當。此外，T2L能壓縮數百個LoRA實例，並零樣本泛化至全新任務。此方法大幅推進了大型語言模型專業化的普及，並以極低的計算需求實現基於語言的適應。代碼位於https://github.com/SakanaAI/text-to-lora", "audio": "audios/2506.06105v1.mp3", "timestamp": "2025-06-09T08:27:59.022508"}
{"query": "Diffusion Model", "id": "2506.06023v1", "url": "http://arxiv.org/abs/2506.06023v1", "title": "Restereo: Diffusion stereo video generation and restoration", "summary": "Stereo video generation has been gaining increasing attention with recent\nadvancements in video diffusion models. However, most existing methods focus on\ngenerating 3D stereoscopic videos from monocular 2D videos. These approaches\ntypically assume that the input monocular video is of high quality, making the\ntask primarily about inpainting occluded regions in the warped video while\npreserving disoccluded areas. In this paper, we introduce a new pipeline that\nnot only generates stereo videos but also enhances both left-view and\nright-view videos consistently with a single model. Our approach achieves this\nby fine-tuning the model on degraded data for restoration, as well as\nconditioning the model on warped masks for consistent stereo generation. As a\nresult, our method can be fine-tuned on a relatively small synthetic stereo\nvideo datasets and applied to low-quality real-world videos, performing both\nstereo video generation and restoration. Experiments demonstrate that our\nmethod outperforms existing approaches both qualitatively and quantitatively in\nstereo video generation from low-resolution inputs.", "authors": ["Xingchang Huang", "Ashish Kumar Singh", "Florian Dubost", "Cristina Nader Vasconcelos", "Sakar Khattar", "Liang Shi", "Christian Theobalt", "Cengiz Oztireli", "Gurprit Singh"], "published_date": "2025-06-06", "title_zh": "Restereo：擴散立體視頻生成與復原", "summary_zh": "立體影片生成隨著影片擴散模型的發展備受關注。現有方法多著重於從單眼2D影片生成3D立體影片，並假設輸入影片品質良好，主要任務為修復扭曲影片中的遮蔽區域並保留未遮蔽區域。本文提出一種新流程，不僅能生成立體影片，還能使用單一模型一致性地增強左右視圖影片。此方法透過在降質數據上微調模型以進行修復，並以扭曲遮罩調節模型以實現一致的立體生成。因此，本方法可於相對較小的合成立體影片數據集上微調，並應用於低品質真實影片，同時執行立體影片生成與修復。實驗證明，在低解析度輸入的立體影片生成方面，本方法在質與量上均優於現有方法。", "audio": "audios/2506.06023v1.mp3", "timestamp": "2025-06-09T08:28:05.914930"}
{"query": "AI", "id": "2506.06214v1", "url": "http://arxiv.org/abs/2506.06214v1", "title": "Can Theoretical Physics Research Benefit from Language Agents?", "summary": "Large Language Models (LLMs) are rapidly advancing across diverse domains,\nyet their application in theoretical physics research is not yet mature. This\nposition paper argues that LLM agents can potentially help accelerate\ntheoretical, computational, and applied physics when properly integrated with\ndomain knowledge and toolbox. We analyze current LLM capabilities for physics\n-- from mathematical reasoning to code generation -- identifying critical gaps\nin physical intuition, constraint satisfaction, and reliable reasoning. We\nenvision future physics-specialized LLMs that could handle multimodal data,\npropose testable hypotheses, and design experiments. Realizing this vision\nrequires addressing fundamental challenges: ensuring physical consistency, and\ndeveloping robust verification methods. We call for collaborative efforts\nbetween physics and AI communities to help advance scientific discovery in\nphysics.", "authors": ["Sirui Lu", "Zhijing Jin", "Terry Jingchen Zhang", "Pavel Kos", "J. Ignacio Cirac", "Bernhard Schölkopf"], "published_date": "2025-06-06", "title_zh": "理論物理研究能否受益於語言智能體？", "summary_zh": "大型語言模型在各領域快速發展，但其在理論物理研究中的應用尚不成熟。本文認為，若將大型語言模型代理與領域知識及工具箱適當結合，有望加速理論、計算和應用物理學的發展。我們分析了當前大型語言模型在物理學方面的能力，包括數學推理和程式碼生成，並指出其在物理直覺、約束滿足和可靠推理方面存在關鍵差距。我們設想未來的物理專業大型語言模型能夠處理多模態數據、提出可驗證的假設並設計實驗。實現這一願景需要解決基本挑戰：確保物理一致性，並開發穩健的驗證方法。我們呼籲物理學界和人工智慧界展開合作，以促進物理學的科學發現。", "audio": "audios/2506.06214v1.mp3", "timestamp": "2025-06-09T12:39:38.799996"}
{"query": "Foundation Model", "id": "2506.06076v1", "url": "http://arxiv.org/abs/2506.06076v1", "title": "Full Conformal Adaptation of Medical Vision-Language Models", "summary": "Vision-language models (VLMs) pre-trained at large scale have shown\nunprecedented transferability capabilities and are being progressively\nintegrated into medical image analysis. Although its discriminative potential\nhas been widely explored, its reliability aspect remains overlooked. This work\ninvestigates their behavior under the increasingly popular split conformal\nprediction (SCP) framework, which theoretically guarantees a given error level\non output sets by leveraging a labeled calibration set. However, the zero-shot\nperformance of VLMs is inherently limited, and common practice involves\nfew-shot transfer learning pipelines, which cannot absorb the rigid\nexchangeability assumptions of SCP. To alleviate this issue, we propose full\nconformal adaptation, a novel setting for jointly adapting and conformalizing\npre-trained foundation models, which operates transductively over each test\ndata point using a few-shot adaptation set. Moreover, we complement this\nframework with SS-Text, a novel training-free linear probe solver for VLMs that\nalleviates the computational cost of such a transductive approach. We provide\ncomprehensive experiments using 3 different modality-specialized medical VLMs\nand 9 adaptation tasks. Our framework requires exactly the same data as SCP,\nand provides consistent relative improvements of up to 27% on set efficiency\nwhile maintaining the same coverage guarantees.", "authors": ["Julio Silva-Rodríguez", "Leo Fillioux", "Paul-Henry Cournède", "Maria Vakalopoulou", "Stergios Christodoulidis", "Ismail Ben Ayed", "Jose Dolz"], "published_date": "2025-06-06", "title_zh": "醫學視覺語言模型的全共形適應", "summary_zh": "大規模預訓練的視覺語言模型展現了前所未有的遷移能力，並逐漸應用於醫學影像分析。儘管其區分潛力已被廣泛研究，但可靠性仍被忽略。本研究探討了視覺語言模型在分割一致預測框架下的表現，該框架利用標記校準集，在輸出集上保證給定的錯誤率。然而，視覺語言模型的零樣本性能有限，常見的做法是少量樣本遷移學習，但這無法滿足分割一致預測的嚴格可交換性假設。為解決此問題，我們提出完整一致性適應，一種用於聯合適應和一致化預訓練基礎模型的新方法，其使用少量樣本適應集對每個測試數據點進行傳導式操作。此外，我們使用SS-Text（一種無需訓練的視覺語言模型線性探針求解器）來補充該框架，以減輕此類傳導式方法的計算成本。我們使用三種不同的模態專用醫學視覺語言模型和九個適應任務進行了全面實驗。我們的框架需要與分割一致預測完全相同的數據，並在保持相同覆蓋率保證的前提下，在集合效率方面提供高達27%的一致性相對改進。", "audio": "audios/2506.06076v1.mp3", "timestamp": "2025-06-09T12:39:45.832304"}
{"query": "Diffusion Model", "id": "2506.06018v1", "url": "http://arxiv.org/abs/2506.06018v1", "title": "Optimization-Free Universal Watermark Forgery with Regenerative Diffusion Models", "summary": "Watermarking becomes one of the pivotal solutions to trace and verify the\norigin of synthetic images generated by artificial intelligence models, but it\nis not free of risks. Recent studies demonstrate the capability to forge\nwatermarks from a target image onto cover images via adversarial optimization\nwithout knowledge of the target generative model and watermark schemes. In this\npaper, we uncover a greater risk of an optimization-free and universal\nwatermark forgery that harnesses existing regenerative diffusion models. Our\nproposed forgery attack, PnP (Plug-and-Plant), seamlessly extracts and\nintegrates the target watermark via regenerating the image, without needing any\nadditional optimization routine. It allows for universal watermark forgery that\nworks independently of the target image's origin or the watermarking model\nused. We explore the watermarked latent extracted from the target image and\nvisual-textual context of cover images as priors to guide sampling of the\nregenerative process. Extensive evaluation on 24 scenarios of\nmodel-data-watermark combinations demonstrates that PnP can successfully forge\nthe watermark (up to 100% detectability and user attribution), and maintain the\nbest visual perception. By bypassing model retraining and enabling adaptability\nto any image, our approach significantly broadens the scope of forgery attacks,\npresenting a greater challenge to the security of current watermarking\ntechniques for diffusion models and the authority of watermarking schemes in\nsynthetic data generation and governance.", "authors": ["Chaoyi Zhu", "Zaitang Li", "Renyi Yang", "Robert Birke", "Pin-Yu Chen", "Tsung-Yi Ho", "Lydia Y. Chen"], "published_date": "2025-06-06", "title_zh": "使用再生擴散模型的免優化通用浮水印偽造", "summary_zh": "浮水印技術是追蹤與驗證人工智慧生成圖像來源的重要方案，但也存在風險。近期研究顯示，可透過對抗性最佳化將目標圖像的浮水印偽造到載體圖像上，無需了解目標生成模型和浮水印方案。本文揭露一種更嚴重的風險：免最佳化且通用的浮水印偽造，該方法利用現有的再生擴散模型。我們提出的PnP（Plug-and-Plant）偽造攻擊，透過再生圖像無縫提取並整合目標浮水印，無需額外的最佳化程序。它實現了通用的浮水印偽造，獨立於目標圖像的來源或所使用的浮水印模型。我們探索從目標圖像提取的浮水印潛在空間，以及載體圖像的視覺-文本上下文，作為引導再生過程取樣的先驗知識。在24種模型-數據-浮水印組合場景的廣泛評估表明，PnP可以成功偽造浮水印（高達100%的可檢測性和使用者歸屬），並保持最佳的視覺感知。透過繞過模型重新訓練並實現對任何圖像的適應性，我們的方案顯著擴大了偽造攻擊的範圍，對當前擴散模型的浮水印技術安全以及合成數據生成和治理中浮水印方案的權威性提出了更大的挑戰。", "audio": "audios/2506.06018v1.mp3", "timestamp": "2025-06-09T12:39:54.582090"}
{"query": "AI", "id": "2506.06166v1", "url": "http://arxiv.org/abs/2506.06166v1", "title": "The Lock-in Hypothesis: Stagnation by Algorithm", "summary": "The training and deployment of large language models (LLMs) create a feedback\nloop with human users: models learn human beliefs from data, reinforce these\nbeliefs with generated content, reabsorb the reinforced beliefs, and feed them\nback to users again and again. This dynamic resembles an echo chamber. We\nhypothesize that this feedback loop entrenches the existing values and beliefs\nof users, leading to a loss of diversity and potentially the lock-in of false\nbeliefs. We formalize this hypothesis and test it empirically with agent-based\nLLM simulations and real-world GPT usage data. Analysis reveals sudden but\nsustained drops in diversity after the release of new GPT iterations,\nconsistent with the hypothesized human-AI feedback loop. Code and data\navailable at https://thelockinhypothesis.com", "authors": ["Tianyi Alex Qiu", "Zhonghao He", "Tejasveer Chugh", "Max Kleiman-Weiner"], "published_date": "2025-06-06", "title_zh": "鎖定假說：算法導致的停滯", "summary_zh": "大型語言模型（LLM）的訓練和部署形成與人類使用者的回饋迴路：模型從數據中學習人類信念，透過生成內容強化這些信念，重新吸收強化的信念，並反覆將其傳回使用者。此動態類似於迴聲室。我們假設此回饋迴路會鞏固使用者現有的價值觀和信念，導致多樣性喪失，並可能鎖定錯誤信念。我們形式化此假設，並透過基於代理的LLM模擬和真實世界的GPT使用數據進行實證測試。分析顯示，在新的GPT迭代版本發布後，多樣性出現突然但持續的下降，與假設的人機回饋迴路一致。", "audio": "audios/2506.06166v1.mp3", "timestamp": "2025-06-09T14:20:35.563371"}
{"query": "Foundation Model", "id": "2506.06006v1", "url": "http://arxiv.org/abs/2506.06006v1", "title": "Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models", "summary": "To what extent do vision-and-language foundation models possess a realistic\nworld model (observation $\\times$ action $\\rightarrow$ observation) and a\ndynamics model (observation $\\times$ observation $\\rightarrow$ action), when\nactions are expressed through language? While open-source foundation models\nstruggle with both, we find that fine-tuning them to acquire a dynamics model\nthrough supervision is significantly easier than acquiring a world model. In\nturn, dynamics models can be used to bootstrap world models through two main\nstrategies: 1) weakly supervised learning from synthetic data and 2) inference\ntime verification. Firstly, the dynamics model can annotate actions for\nunlabelled pairs of video frame observations to expand the training data. We\nfurther propose a new objective, where image tokens in observation pairs are\nweighted by their importance, as predicted by a recognition model. Secondly,\nthe dynamics models can assign rewards to multiple samples of the world model\nto score them, effectively guiding search at inference time. We evaluate the\nworld models resulting from both strategies through the task of action-centric\nimage editing on Aurora-Bench. Our best model achieves a performance\ncompetitive with state-of-the-art image editing models, improving on them by a\nmargin of $15\\%$ on real-world subsets according to GPT4o-as-judge, and\nachieving the best average human evaluation across all subsets of Aurora-Bench.", "authors": ["Yifu Qiu", "Yftah Ziser", "Anna Korhonen", "Shay B. Cohen", "Edoardo M. Ponti"], "published_date": "2025-06-06", "title_zh": "多模態基礎模型中基於動力學模型的自舉世界模型", "summary_zh": "視覺語言基礎模型在多大程度上具備真實世界模型（觀察×動作→觀察）和動態模型（觀察×觀察→動作），尤其當動作透過語言表達時？開源基礎模型在這兩方面均有困難，但透過監督學習微調以獲得動態模型比獲得世界模型更容易。動態模型可透過兩種策略引導世界模型：一，基於合成數據的弱監督學習；二，推理時驗證。動態模型可為未標記的視頻幀對標註動作，擴展訓練數據。我們提出一種新目標，根據識別模型預測的重要性對觀察對中的圖像令牌進行加權。動態模型可為世界模型的樣本分配獎勵以評分，有效引導推理時的搜索。我們在Aurora-Bench上評估了由此產生的世界模型在以動作為中心的圖像編輯任務中的表現。我們的最佳模型表現與最先進的圖像編輯模型相當，在真實世界子集上優於它們15%（基於GPT4o評估），並在Aurora-Bench所有子集上獲得最佳平均人工評估。", "audio": "audios/2506.06006v1.mp3", "timestamp": "2025-06-09T14:20:43.262474"}
{"query": "Diffusion Model", "id": "2506.05960v1", "url": "http://arxiv.org/abs/2506.05960v1", "title": "AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models", "summary": "Significant investments have been made towards the commodification of\ndiffusion models for generation of diverse media. Their mass-market adoption is\nhowever still hobbled by the intense hardware resource requirements of\ndiffusion model inference. Model quantization strategies tailored specifically\ntowards diffusion models have been useful in easing this burden, yet have\ngenerally explored the Uniform Scalar Quantization (USQ) family of quantization\nmethods. In contrast, Vector Quantization (VQ) methods, which operate on groups\nof multiple related weights as the basic unit of compression, have seen\nsubstantial success in Large Language Model (LLM) quantization. In this work,\nwe apply codebook-based additive vector quantization to the problem of\ndiffusion model compression. Our resulting approach achieves a new Pareto\nfrontier for the extremely low-bit weight quantization on the standard\nclass-conditional benchmark of LDM-4 on ImageNet at 20 inference time steps.\nNotably, we report sFID 1.92 points lower than the full-precision model at W4A8\nand the best-reported results for FID, sFID and ISC at W2A8. We are also able\nto demonstrate FLOPs savings on arbitrary hardware via an efficient inference\nkernel, as opposed to savings resulting from small integer operations which may\nlack broad hardware support.", "authors": ["Adil Hasan", "Thomas Peyrin"], "published_date": "2025-06-06", "title_zh": "AQUATIC-Diff：極小壓縮擴散模型的加性量化", "summary_zh": "為使擴散模型生成多樣化媒體，已投入大量資源進行商業化。然而，擴散模型推論對硬體資源的高需求阻礙其普及。針對擴散模型客製化的模型量化策略雖有助於減輕此負擔，但主要集中於均勻純量量化方法。相較之下，向量量化方法對多個相關權重組進行壓縮，已在大語言模型量化中獲得顯著成功。本研究將基於碼本的加法向量量化應用於擴散模型壓縮。在ImageNet上，針對LDM-4於20個推論步驟進行標準類別條件基準測試，該方法在極低位元權重量化方面達到新的帕雷托前沿。值得注意的是，在W4A8時，sFID比全精度模型低1.92點，並且在W2A8時，FID、sFID和ISC均達到最佳結果。相較於依賴硬體支援可能不足的小整數運算，我們亦透過高效推論核心，在任意硬體上實現了FLOPs的節省。", "audio": "audios/2506.05960v1.mp3", "timestamp": "2025-06-09T14:20:52.692372"}
{"query": "AI", "id": "2506.06165v1", "url": "http://arxiv.org/abs/2506.06165v1", "title": "(AI peers) are people learning from the same standpoint: Perception of AI characters in a Collaborative Science Investigation", "summary": "While the complexity of 21st-century demands has promoted pedagogical\napproaches to foster complex competencies, a persistent gap remains between\nin-class learning activities and individualized learning or assessment\npractices. To address this, studies have explored the use of AI-generated\ncharacters in learning and assessment. One attempt is scenario-based assessment\n(SBA), a technique that not only measures but also fosters the development of\ncompetencies throughout the assessment process. SBA introduces simulated agents\nto provide an authentic social-interactional context, allowing for the\nassessment of competency-based constructs while mitigating the unpredictability\nof real-life interactions. Recent advancements in multimodal AI, such as\ntext-to-video technology, allow these agents to be enhanced into AI-generated\ncharacters. This mixed-method study investigates how learners perceive AI\ncharacters taking the role of mentor and teammates in an SBA mirroring the\ncontext of a collaborative science investigation. Specifically, we examined the\nLikert scale responses of 56 high schoolers regarding trust, social presence,\nand effectiveness. We analyzed the relationships between these factors and\ntheir impact on the intention to adopt AI characters through PLS-SEM. Our\nfindings indicated that learners' trust shaped their sense of social presence\nwith the AI characters, enhancing perceived effectiveness. Qualitative analysis\nfurther highlighted factors that foster trust, such as material credibility and\nalignment with learning goals, as well as the pivotal role of social presence\nin creating a collaborative context.\n  This paper was accepted as an full paper for AIED 2025.", "authors": ["Eunhye Grace Ko", "Soo Hyoung Joo"], "published_date": "2025-06-06", "title_zh": "(人工智慧同儕)是從相同角度學習的人嗎：合作科學探究中對人工智慧角色的感知", "summary_zh": "二十一世紀的複雜需求促使教學方法著重培養複雜能力，然而課堂學習活動與個別化學習或評估實踐之間仍存在差距。為了解決此問題，研究探索了人工智慧生成角色在學習和評估中的應用，其中情境式評估 (SBA) 不僅能衡量，還能在整個評估過程中培養能力。SBA 引入模擬代理人，提供真實的社會互動情境，評估基於能力的建構，並降低現實互動的不可預測性。多模態人工智慧的進步，如文本到視頻技術，使這些代理人能被強化為人工智慧生成角色。本研究採用混合方法，探討高中生如何看待在協作科學調查情境的SBA中，擔任導師和隊友的人工智慧角色。研究分析了56名高中生關於信任、社會臨場感和效能的李克特量表回應，並透過 PLS-SEM 分析這些因素之間的關係及其對採用人工智慧角色意願的影響。研究結果表明，學習者的信任塑造了他們與人工智慧角色的社會臨場感，從而提升了感知效能。質性分析進一步強調了促進信任的因素，如材料可信度和與學習目標的一致性，以及社會臨場感在創造協作情境中的關鍵作用。本文已被 AIED 2025 接受為完整論文。", "audio": "audios/2506.06165v1.mp3", "timestamp": "2025-06-09T16:24:36.042255"}
{"query": "Foundation Model", "id": "2506.06005v1", "url": "http://arxiv.org/abs/2506.06005v1", "title": "LightGTS: A Lightweight General Time Series Forecasting Model", "summary": "Existing works on general time series forecasting build foundation models\nwith heavy model parameters through large-scale multi-source pre-training.\nThese models achieve superior generalization ability across various datasets at\nthe cost of significant computational burdens and limitations in\nresource-constrained scenarios. This paper introduces LightGTS, a lightweight\ngeneral time series forecasting model designed from the perspective of\nconsistent periodical modeling. To handle diverse scales and intrinsic periods\nin multi-source pre-training, we introduce Periodical Tokenization, which\nextracts consistent periodic patterns across different datasets with varying\nscales. To better utilize the periodicity in the decoding process, we further\nintroduce Periodical Parallel Decoding, which leverages historical tokens to\nimprove forecasting. Based on the two techniques above which fully leverage the\ninductive bias of periods inherent in time series, LightGTS uses a lightweight\nmodel to achieve outstanding performance on general time series forecasting. It\nachieves state-of-the-art forecasting performance on 9 real-world benchmarks in\nboth zero-shot and full-shot settings with much better efficiency compared with\nexisting time series foundation models.", "authors": ["Yihang Wang", "Yuying Qiu", "Peng Chen", "Yang Shu", "Zhongwen Rao", "Lujia Pan", "Bin Yang", "Chenjuan Guo"], "published_date": "2025-06-06", "title_zh": "LightGTS：輕量級通用時間序列預測模型", "summary_zh": "現有通用時間序列預測研究透過大規模多源預訓練構建具有龐大模型參數的基礎模型。這些模型雖在各數據集上實現卓越的泛化能力，但也帶來了顯著的計算負擔，並在資源受限環境中存在局限性。本研究提出LightGTS，一種從一致週期性建模角度設計的輕量級通用時間序列預測模型。為處理多源預訓練中不同的尺度和內在週期，我們引入週期性分詞，以提取不同尺度數據集間一致的週期模式。為更好地利用解碼過程中的週期性，我們進一步引入週期性並行解碼，利用歷史數據改善預測。基於上述兩種充分利用時間序列內在週期性歸納偏置的技術，LightGTS使用輕量級模型在通用時間序列預測上實現卓越性能。在零樣本和全樣本設定下，LightGTS在9個真實世界基準測試中實現了最先進的預測性能，且效率遠優於現有的時間序列基礎模型。", "audio": "audios/2506.06005v1.mp3", "timestamp": "2025-06-09T16:24:43.424989"}
{"query": "Diffusion Model", "id": "2506.05934v1", "url": "http://arxiv.org/abs/2506.05934v1", "title": "FADE: Frequency-Aware Diffusion Model Factorization for Video Editing", "summary": "Recent advancements in diffusion frameworks have significantly enhanced video\nediting, achieving high fidelity and strong alignment with textual prompts.\nHowever, conventional approaches using image diffusion models fall short in\nhandling video dynamics, particularly for challenging temporal edits like\nmotion adjustments. While current video diffusion models produce high-quality\nresults, adapting them for efficient editing remains difficult due to the heavy\ncomputational demands that prevent the direct application of previous image\nediting techniques. To overcome these limitations, we introduce FADE, a\ntraining-free yet highly effective video editing approach that fully leverages\nthe inherent priors from pre-trained video diffusion models via frequency-aware\nfactorization. Rather than simply using these models, we first analyze the\nattention patterns within the video model to reveal how video priors are\ndistributed across different components. Building on these insights, we propose\na factorization strategy to optimize each component's specialized role.\nFurthermore, we devise spectrum-guided modulation to refine the sampling\ntrajectory with frequency domain cues, preventing information leakage and\nsupporting efficient, versatile edits while preserving the basic spatial and\ntemporal structure. Extensive experiments on real-world videos demonstrate that\nour method consistently delivers high-quality, realistic and temporally\ncoherent editing results both qualitatively and quantitatively. Code is\navailable at https://github.com/EternalEvan/FADE .", "authors": ["Yixuan Zhu", "Haolin Wang", "Shilin Ma", "Wenliang Zhao", "Yansong Tang", "Lei Chen", "Jie Zhou"], "published_date": "2025-06-06", "title_zh": "FADE：用於影片編輯的頻率感知擴散模型分解", "summary_zh": "擴散框架的進步顯著提升了影片編輯能力，實現了高保真度和與文本提示的強對齊。然而，傳統的圖像擴散模型在處理影片動態方面存在不足，特別是對於像運動調整等具時間挑戰性的編輯。現有的影片擴散模型雖然能產生高品質的結果，但由於其龐大的計算需求，使其難以適應高效編輯，無法直接應用先前的圖像編輯技術。為了解決這些限制，我們引入FADE，一種無需訓練但高效的影片編輯方法，通過頻率感知分解充分利用預訓練影片擴散模型中的內在先驗知識。我們首先分析影片模型中的注意力模式，揭示影片先驗知識如何在不同組件中分佈。基於這些見解，我們提出一種分解策略來優化每個組件的專門角色。此外，我們設計了頻譜引導調製，利用頻域線索來改進採樣軌跡，防止信息洩漏，並支持高效、通用的編輯，同時保留基本的空間和時間結構。在真實影片上的大量實驗表明，我們的方法在質量、真實性和時間一致性方面均能始終如一地提供高品質的編輯結果。代碼可在https://github.com/EternalEvan/FADE 獲取。", "audio": "audios/2506.05934v1.mp3", "timestamp": "2025-06-09T16:24:52.695710"}
{"query": "AI", "id": "2506.06153v1", "url": "http://arxiv.org/abs/2506.06153v1", "title": "Personalized Large Language Models Can Increase the Belief Accuracy of Social Networks", "summary": "Large language models (LLMs) are increasingly involved in shaping public\nunderstanding on contested issues. This has led to substantial discussion about\nthe potential of LLMs to reinforce or correct misperceptions. While existing\nliterature documents the impact of LLMs on individuals' beliefs, limited work\nexplores how LLMs affect social networks. We address this gap with a\npre-registered experiment (N = 1265) around the 2024 US presidential election,\nwhere we empirically explore the impact of personalized LLMs on belief accuracy\nin the context of social networks. The LLMs are constructed to be personalized,\noffering messages tailored to individuals' profiles, and to have guardrails for\naccurate information retrieval. We find that the presence of a personalized LLM\nleads individuals to update their beliefs towards the truth. More importantly,\nindividuals with a personalized LLM in their social network not only choose to\nfollow it, indicating they would like to obtain information from it in\nsubsequent interactions, but also construct subsequent social networks to\ninclude other individuals with beliefs similar to the LLM -- in this case, more\naccurate beliefs. Therefore, our results show that LLMs have the capacity to\ninfluence individual beliefs and the social networks in which people exist, and\nhighlight the potential of LLMs to act as corrective agents in online\nenvironments. Our findings can inform future strategies for responsible\nAI-mediated communication.", "authors": ["Adiba Mahbub Proma", "Neeley Pate", "Sean Kelty", "Gourab Ghoshal", "James N. Druckman", "Ehsan Hoque"], "published_date": "2025-06-06", "title_zh": "個人化大型語言模型可提升社交網絡的信念準確性", "summary_zh": "大型語言模型（LLM）日益影響公眾對爭議議題的理解，引發其強化或修正錯誤認知的討論。本研究透過一項針對2024年美國總統大選的預註冊實驗（N=1265），探討個人化LLM在社交網絡中對信念準確性的影響。結果顯示，個人化LLM促使個體更新信念以趨近真相。更重要的是，社交網絡中存在個人化LLM的個體不僅選擇追蹤它以獲取資訊，還傾向於構建包含與LLM信念相似（更準確）的其他個體的社交網絡。因此，LLM有能力影響個體信念及其所處社交網絡，並具備在線上環境中充當修正媒介的潛力。研究結果可為負責任的人工智慧溝通策略提供參考。", "audio": "audios/2506.06153v1.mp3", "timestamp": "2025-06-09T17:17:27.900419"}
{"query": "Foundation Model", "id": "2506.05864v1", "url": "http://arxiv.org/abs/2506.05864v1", "title": "CryoFastAR: Fast Cryo-EM Ab Initio Reconstruction Made Easy", "summary": "Pose estimation from unordered images is fundamental for 3D reconstruction,\nrobotics, and scientific imaging. Recent geometric foundation models, such as\nDUSt3R, enable end-to-end dense 3D reconstruction but remain underexplored in\nscientific imaging fields like cryo-electron microscopy (cryo-EM) for\nnear-atomic protein reconstruction. In cryo-EM, pose estimation and 3D\nreconstruction from unordered particle images still depend on time-consuming\niterative optimization, primarily due to challenges such as low signal-to-noise\nratios (SNR) and distortions from the contrast transfer function (CTF). We\nintroduce CryoFastAR, the first geometric foundation model that can directly\npredict poses from Cryo-EM noisy images for Fast ab initio Reconstruction. By\nintegrating multi-view features and training on large-scale simulated cryo-EM\ndata with realistic noise and CTF modulations, CryoFastAR enhances pose\nestimation accuracy and generalization. To enhance training stability, we\npropose a progressive training strategy that first allows the model to extract\nessential features under simpler conditions before gradually increasing\ndifficulty to improve robustness. Experiments show that CryoFastAR achieves\ncomparable quality while significantly accelerating inference over traditional\niterative approaches on both synthetic and real datasets.", "authors": ["Jiakai Zhang", "Shouchen Zhou", "Haizhao Dai", "Xinhang Liu", "Peihao Wang", "Zhiwen Fan", "Yuan Pei", "Jingyi Yu"], "published_date": "2025-06-06", "title_zh": "低溫電鏡快速從頭算重建：簡化流程", "summary_zh": "無序圖像的姿態估計對3D重建、機器人和科學成像至關重要。近年幾何基礎模型（如DUSt3R）雖實現端到端密集3D重建，但在低溫電子顯微鏡(cryo-EM)等科學成像領域的應用仍待開發。Cryo-EM中，受訊噪比低、對比轉移函數(CTF)失真等因素影響，粒子圖像的姿態估計與3D重建仍依賴耗時的迭代優化。本研究提出CryoFastAR，首個幾何基礎模型，可直接預測Cryo-EM噪聲圖像的姿態，實現快速從頭重建。透過整合多視角特徵，並以具真實噪聲及CTF調制的模擬cryo-EM大規模數據訓練，CryoFastAR提升了姿態估計的準確性和泛化能力。為增強訓練穩定性，我們提出一種漸進式訓練策略，先讓模型在較簡單條件下提取關鍵特徵，再逐步增加難度以提高穩健性。實驗結果表明，在合成和真實數據集上，CryoFastAR在實現可比擬品質的同時，顯著加速了推論速度，優於傳統迭代方法。", "audio": "audios/2506.05864v1.mp3", "timestamp": "2025-06-09T17:17:37.715836"}
{"query": "Diffusion Model", "id": "2506.05923v1", "url": "http://arxiv.org/abs/2506.05923v1", "title": "Convection Anisotropies of Cosmic Rays in Highly Magnetized Plasma", "summary": "Recently, Zhang & Liu (2024) proposed a turbulent convection model for\nmultiscale anisotropies of cosmic rays (CRs), with an assumption of isotropic\ndiffusion such that the anisotropies are statistically isotropic. However, this\nassumption may be unrealistic for TeV CRs, whose observations have revealed the\nsignificance of the local interstellar background magnetic field. To meet the\ndifficulty, the turbulent convection scenario needs to be extended to cover\nanisotropic diffusion. In this paper, we focus on the parallel diffusion with\nisotropic pitch-angle scattering, which may be an approximation to the\ntransport process driven by weak hydromagnetic waves in a magnetic flux tube,\nwhere fluctuations of the wave velocities lead to the turbulent convection. The\nconsequence is the breaking of the statistical isotropy, while the overall\nshape of the angular power spectrum, $ \\overline{C_\\ell}\\propto\\ell ^{-\\gamma\n-1} $ ($ \\ell\\gg 1 $), remains similar to that in the isotropic diffusion\nmodel, where $ \\ell $ are degrees of spherical harmonics, and $ \\gamma $ is the\nturbulence spectral index of the convection field. It is then expected that the\npower-law index of the TeV CR small-scale angular power spectrum can be\nexplained with the Kolmogorov law $ \\gamma =5/3 $, irrespective of the\nbackground magnetic field to some extent.", "authors": ["Yiran Zhang", "Siming Liu"], "published_date": "2025-06-06", "title_zh": "高磁化電漿中宇宙射線的對流異向性", "summary_zh": "張與劉（2024）曾提出宇宙射線湍流對流模型，用於解釋其多尺度異向性，該模型假設擴散為等向性，故異向性具統計等向性。然而，此假設可能不適用於TeV宇宙射線，因觀測顯示局部星際背景磁場的重要性。為解決此問題，需擴展湍流對流模型以涵蓋異向性擴散。本文聚焦於具等向性螺距角散射的平行擴散，這可能是磁通管中由弱磁流體波驅動的傳輸過程近似，其中波速擾動導致湍流對流。此舉將打破統計等向性，但角功率譜的整體形狀，$\\overline{C_\\ell}\\propto\\ell ^{-\\gamma -1}$（$\\ell\\gg 1$），與等向性擴散模型相似，其中$\\ell$為球諧函數階數，$\\gamma$為對流場的湍流譜指數。因此，預期TeV宇宙射線小尺度角功率譜的冪律指數可用科莫哥洛夫定律$\\gamma =5/3$解釋，在一定程度上不受背景磁場影響。", "audio": "audios/2506.05923v1.mp3", "timestamp": "2025-06-09T17:17:45.641891"}
{"query": "AI", "id": "2506.06104v1", "url": "http://arxiv.org/abs/2506.06104v1", "title": "WoundAIssist: A Patient-Centered Mobile App for AI-Assisted Wound Care With Physicians in the Loop", "summary": "The rising prevalence of chronic wounds, especially in aging populations,\npresents a significant healthcare challenge due to prolonged hospitalizations,\nelevated costs, and reduced patient quality of life. Traditional wound care is\nresource-intensive, requiring frequent in-person visits that strain both\npatients and healthcare professionals (HCPs). Therefore, we present\nWoundAIssist, a patient-centered, AI-driven mobile application designed to\nsupport telemedical wound care. WoundAIssist enables patients to regularly\ndocument wounds at home via photographs and questionnaires, while physicians\nremain actively engaged in the care process through remote monitoring and video\nconsultations. A distinguishing feature is an integrated lightweight deep\nlearning model for on-device wound segmentation, which, combined with\npatient-reported data, enables continuous monitoring of wound healing\nprogression. Developed through an iterative, user-centered process involving\nboth patients and domain experts, WoundAIssist prioritizes an user-friendly\ndesign, particularly for elderly patients. A conclusive usability study with\npatients and dermatologists reported excellent usability, good app quality, and\nfavorable perceptions of the AI-driven wound recognition. Our main contribution\nis two-fold: (I) the implementation and (II) evaluation of WoundAIssist, an\neasy-to-use yet comprehensive telehealth solution designed to bridge the gap\nbetween patients and HCPs. Additionally, we synthesize design insights for\nremote patient monitoring apps, derived from over three years of\ninterdisciplinary research, that may inform the development of similar digital\nhealth tools across clinical domains.", "authors": ["Vanessa Borst", "Anna Riedmann", "Tassilo Dege", "Konstantin Müller", "Astrid Schmieder", "Birgit Lugrin", "Samuel Kounev"], "published_date": "2025-06-06", "title_zh": "WoundAIssist：以病人為中心的行動應用程式，結合AI輔助傷口照護並由醫師參與", "summary_zh": "慢性傷口盛行率日益上升，對醫療照護構成重大挑戰。傳統照護耗費資源且仰賴頻繁就診。本研究提出WoundAIssist，一款以病人為中心、AI驅動的行動應用程式，支援遠距傷口照護。病人可在家定期記錄傷口照片和問卷，醫師則透過遠端監控和視訊諮詢參與照護。其特色為內建輕量級深度學習模型，可在裝置上進行傷口分割，結合病人回報數據，持續監測傷口癒合進展。WoundAIssist經由迭代式、以使用者為中心的流程開發，注重友善的使用者介面，特別是針對老年病人。使用者研究顯示其具備卓越的可用性、良好的應用程式品質，以及對AI傷口辨識的正面評價。本研究的主要貢獻在於WoundAIssist的實施和評估，這是一個易於使用且全面的遠距醫療解決方案，旨在彌合病人與醫療專業人員之間的差距。此外，本研究綜合了三年多跨學科研究中獲得的遠端病人監測應用程式設計見解，可為其他臨床領域類似數位健康工具的開發提供參考。", "audio": "audios/2506.06104v1.mp3", "timestamp": "2025-06-09T18:27:44.562803"}
{"query": "Foundation Model", "id": "2506.05850v1", "url": "http://arxiv.org/abs/2506.05850v1", "title": "Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models", "summary": "We identify \\textbf{Cross-lingual Collapse}, a systematic drift in which the\nchain-of-thought (CoT) of a multilingual language model reverts to its dominant\npre-training language even when the prompt is expressed in a different\nlanguage. Recent large language models (LLMs) with reinforcement learning with\nverifiable reward (RLVR) have achieved strong logical reasoning performances by\nexposing their intermediate reasoning traces, giving rise to large reasoning\nmodels (LRMs). However, the mechanism behind multilingual reasoning in LRMs is\nnot yet fully explored. To investigate the issue, we fine-tune multilingual\nLRMs with Group-Relative Policy Optimization (GRPO) on translated versions of\nthe GSM$8$K and SimpleRL-Zoo datasets in three different languages: Chinese,\nKorean, and Ukrainian. During training, we monitor both task accuracy and\nlanguage consistency of the reasoning chains. Our experiments reveal three key\nfindings: (i) GRPO rapidly amplifies pre-training language imbalances, leading\nto the erosion of low-resource languages within just a few hundred updates;\n(ii) language consistency reward mitigates this drift but does so at the\nexpense of an almost 5 - 10 pp drop in accuracy. and (iii) the resulting\nlanguage collapse is severely damaging and largely irreversible, as subsequent\nfine-tuning struggles to steer the model back toward its original\ntarget-language reasoning capabilities. Together, these findings point to a\nremarkable conclusion: \\textit{not all languages are trained equally for\nreasoning}. Furthermore, our paper sheds light on the roles of reward shaping,\ndata difficulty, and pre-training priors in eliciting multilingual reasoning.", "authors": ["Cheonbok Park", "Jeonghoon Kim", "Joosung Lee", "Sanghwan Bae", "Jaegul Choo", "Kangmin Yoo"], "published_date": "2025-06-06", "title_zh": "跨語言崩潰：以語言為中心的基礎模型如何形塑大型語言模型中的推理", "summary_zh": "本文探討多語大型語言模型中一種系統性偏差現象：跨語言崩潰。即使提示使用不同語言，思考鏈仍會退回至模型預訓練時的主導語言。透過在GSM$8$K與SimpleRL-Zoo數據集的翻譯版本（中、韓、烏克蘭語）上，以群組相對策略優化(GRPO)微調多語大型推理模型，並監測任務準確度與推理鏈的語言一致性，研究發現：(一) GRPO迅速放大預訓練語言不平衡，導致低資源語言在極短時間內衰退；(二) 語言一致性獎勵可減緩此偏差，但會犧牲5-10%的準確度；(三) 語言崩潰具嚴重且幾乎不可逆的破壞性，後續微調難以使模型恢復至原始目標語言的推理能力。結論顯示：並非所有語言都接受了同等程度的推理訓練。此外，本文闡明獎勵塑造、數據難度及預訓練先驗在誘發多語推理中的作用。", "audio": "audios/2506.05850v1.mp3", "timestamp": "2025-06-09T18:27:53.397500"}
{"query": "Diffusion Model", "id": "2506.05867v1", "url": "http://arxiv.org/abs/2506.05867v1", "title": "Stealix: Model Stealing via Prompt Evolution", "summary": "Model stealing poses a significant security risk in machine learning by\nenabling attackers to replicate a black-box model without access to its\ntraining data, thus jeopardizing intellectual property and exposing sensitive\ninformation. Recent methods that use pre-trained diffusion models for data\nsynthesis improve efficiency and performance but rely heavily on manually\ncrafted prompts, limiting automation and scalability, especially for attackers\nwith little expertise. To assess the risks posed by open-source pre-trained\nmodels, we propose a more realistic threat model that eliminates the need for\nprompt design skills or knowledge of class names. In this context, we introduce\nStealix, the first approach to perform model stealing without predefined\nprompts. Stealix uses two open-source pre-trained models to infer the victim\nmodel's data distribution, and iteratively refines prompts through a genetic\nalgorithm, progressively improving the precision and diversity of synthetic\nimages. Our experimental results demonstrate that Stealix significantly\noutperforms other methods, even those with access to class names or\nfine-grained prompts, while operating under the same query budget. These\nfindings highlight the scalability of our approach and suggest that the risks\nposed by pre-trained generative models in model stealing may be greater than\npreviously recognized.", "authors": ["Zhixiong Zhuang", "Hui-Po Wang", "Maria-Irina Nicolae", "Mario Fritz"], "published_date": "2025-06-06", "title_zh": "Stealix：基於提示演化的模型竊取", "summary_zh": "模型竊取對機器學習構成重大安全風險，使攻擊者無需存取訓練數據即可複製黑盒模型，危及智慧財產權並暴露敏感資訊。近期利用預訓練擴散模型進行數據合成的方法雖提升效率與效能，但過度依賴手動設計提示，限制了自動化與擴展性，尤其對缺乏專業知識的攻擊者而言。為評估開源預訓練模型帶來的風險，本文提出更實際的威脅模型，無需提示設計技巧或類別名稱知識。在此背景下，本文提出首個無需預定義提示的模型竊取方法 Stealix。Stealix 利用兩個開源預訓練模型推斷受害者模型的數據分布，並透過基因演算法迭代優化提示，逐步提高合成圖像的精確度和多樣性。實驗結果表明，在相同查詢預算下，Stealix 明顯優於其他方法，甚至優於那些可存取類別名稱或精細提示的方法。這些發現突顯了 Stealix 的可擴展性，並表明預訓練生成模型在模型竊取中帶來的風險可能比先前所認為的更大。", "audio": "audios/2506.05867v1.mp3", "timestamp": "2025-06-09T18:28:01.145921"}
{"query": "AI", "id": "2506.06099v1", "url": "http://arxiv.org/abs/2506.06099v1", "title": "DermaCon-IN: A Multi-concept Annotated Dermatological Image Dataset of Indian Skin Disorders for Clinical AI Research", "summary": "Artificial intelligence is poised to augment dermatological care by enabling\nscalable image-based diagnostics. Yet, the development of robust and equitable\nmodels remains hindered by datasets that fail to capture the clinical and\ndemographic complexity of real-world practice. This complexity stems from\nregion-specific disease distributions, wide variation in skin tones, and the\nunderrepresentation of outpatient scenarios from non-Western populations. We\nintroduce DermaCon-IN, a prospectively curated dermatology dataset comprising\nover 5,450 clinical images from approximately 3,000 patients across outpatient\nclinics in South India. Each image is annotated by board-certified\ndermatologists with over 240 distinct diagnoses, structured under a\nhierarchical, etiology-based taxonomy adapted from Rook's classification. The\ndataset captures a wide spectrum of dermatologic conditions and tonal variation\ncommonly seen in Indian outpatient care. We benchmark a range of architectures\nincluding convolutional models (ResNet, DenseNet, EfficientNet),\ntransformer-based models (ViT, MaxViT, Swin), and Concept Bottleneck Models to\nestablish baseline performance and explore how anatomical and concept-level\ncues may be integrated. These results are intended to guide future efforts\ntoward interpretable and clinically realistic models. DermaCon-IN provides a\nscalable and representative foundation for advancing dermatology AI in\nreal-world settings.", "authors": ["Shanawaj S Madarkar", "Mahajabeen Madarkar", "Madhumitha V", "Teli Prakash", "Konda Reddy Mopuri", "Vinaykumar MV", "KVL Sathwika", "Adarsh Kasturi", "Gandla Dilip Raj", "PVN Supranitha", "Harsh Udai"], "published_date": "2025-06-06", "title_zh": "DermaCon-IN：用於臨床人工智慧研究的印度皮膚疾病多概念註釋皮膚影像資料集", "summary_zh": "人工智慧有望藉由可擴展的圖像診斷來提升皮膚科照護。然而，缺乏能捕捉真實臨床和人口複雜性的資料集，阻礙了穩健且公平模型的開發。此複雜性源於區域性疾病分布、膚色差異大，以及非西方人群門診案例的代表性不足。我們推出了 DermaCon-IN，一個前瞻性策劃的皮膚科資料集，包含來自南印度門診的約 3,000 名患者的 5,450 多張臨床圖像。每張圖像均由認證皮膚科醫生使用超過 240 種不同的診斷進行註釋，這些診斷基於改編自 Rook 分類的層次結構、病因學分類。該資料集捕捉了印度門診護理中常見的各種皮膚病和膚色變化。我們基準測試了包括卷積模型（ResNet、DenseNet、EfficientNet）、基於Transformer的模型（ViT、MaxViT、Swin）和概念瓶頸模型在內的一系列架構，以建立基準效能，並探討如何整合解剖和概念層面的線索。這些結果旨在指導未來的可解釋和臨床真實模型開發。DermaCon-IN 為在真實環境中推進皮膚科人工智慧提供了一個可擴展且具代表性的基礎。", "audio": "audios/2506.06099v1.mp3", "timestamp": "2025-06-09T19:15:30.161032"}
{"query": "Foundation Model", "id": "2506.05714v1", "url": "http://arxiv.org/abs/2506.05714v1", "title": "Advancement and Field Evaluation of a Dual-arm Apple Harvesting Robot", "summary": "Apples are among the most widely consumed fruits worldwide. Currently, apple\nharvesting fully relies on manual labor, which is costly, drudging, and\nhazardous to workers. Hence, robotic harvesting has attracted increasing\nattention in recent years. However, existing systems still fall short in terms\nof performance, effectiveness, and reliability for complex orchard\nenvironments. In this work, we present the development and evaluation of a\ndual-arm harvesting robot. The system integrates a ToF camera, two 4DOF robotic\narms, a centralized vacuum system, and a post-harvest handling module. During\nharvesting, suction force is dynamically assigned to either arm via the vacuum\nsystem, enabling efficient apple detachment while reducing power consumption\nand noise. Compared to our previous design, we incorporated a platform movement\nmechanism that enables both in-out and up-down adjustments, enhancing the\nrobot's dexterity and adaptability to varying canopy structures. On the\nalgorithmic side, we developed a robust apple localization pipeline that\ncombines a foundation-model-based detector, segmentation, and clustering-based\ndepth estimation, which improves performance in orchards. Additionally,\npressure sensors were integrated into the system, and a novel dual-arm\ncoordination strategy was introduced to respond to harvest failures based on\nsensor feedback, further improving picking efficiency. Field demos were\nconducted in two commercial orchards in MI, USA, with different canopy\nstructures. The system achieved success rates of 0.807 and 0.797, with an\naverage picking cycle time of 5.97s. The proposed strategy reduced harvest time\nby 28% compared to a single-arm baseline. The dual-arm harvesting robot\nenhances the reliability and efficiency of apple picking. With further\nadvancements, the system holds strong potential for autonomous operation and\ncommercialization for the apple industry.", "authors": ["Keyi Zhu", "Kyle Lammers", "Kaixiang Zhang", "Chaaran Arunachalam", "Siddhartha Bhattacharya", "Jiajia Li", "Renfu Lu", "Zhaojian Li"], "published_date": "2025-06-06", "title_zh": "雙臂蘋果採摘機器人的進展與田間評估", "summary_zh": "蘋果為全球廣泛食用的水果。現行蘋果採收仰賴人工，成本高昂且具危險性。因此，機器人採收備受關注，但現有系統在複雜果園環境中的效能、效率與可靠性仍不足。本研究開發並評估雙臂採收機器人，整合ToF相機、雙4自由度機械手臂、中央真空系統及採後處理模組。透過真空系統動態分配吸力，實現高效蘋果摘取，同時降低功耗與噪音。相較於先前設計，本研究加入平台移動機制，強化機器人靈活性與對不同樹冠結構的適應性。在演算法方面，開發結合基礎模型偵測器、分割及基於叢集之深度估計的蘋果定位流程，提升果園效能。此外，系統整合壓力感測器，導入新型雙臂協作策略，基於感測器回饋應對採收失敗，進一步提高採摘效率。於美國密西根州兩個具不同樹冠結構的商業果園進行實地演示，系統成功率分別為0.807和0.797，平均採摘週期為5.97秒。相較於單臂基準線，該策略減少了28%的採收時間。此雙臂採收機器人提高了蘋果採摘的可靠性和效率，並具備自主運行與商業化的潛力。", "audio": "audios/2506.05714v1.mp3", "timestamp": "2025-06-09T19:15:42.419280"}
{"query": "Diffusion Model", "id": "2506.05843v1", "url": "http://arxiv.org/abs/2506.05843v1", "title": "FontAdapter: Instant Font Adaptation in Visual Text Generation", "summary": "Text-to-image diffusion models have significantly improved the seamless\nintegration of visual text into diverse image contexts. Recent approaches\nfurther improve control over font styles through fine-tuning with predefined\nfont dictionaries. However, adapting unseen fonts outside the preset is\ncomputationally expensive, often requiring tens of minutes, making real-time\ncustomization impractical. In this paper, we present FontAdapter, a framework\nthat enables visual text generation in unseen fonts within seconds, conditioned\non a reference glyph image. To this end, we find that direct training on font\ndatasets fails to capture nuanced font attributes, limiting generalization to\nnew glyphs. To overcome this, we propose a two-stage curriculum learning\napproach: FontAdapter first learns to extract font attributes from isolated\nglyphs and then integrates these styles into diverse natural backgrounds. To\nsupport this two-stage training scheme, we construct synthetic datasets\ntailored to each stage, leveraging large-scale online fonts effectively.\nExperiments demonstrate that FontAdapter enables high-quality, robust font\ncustomization across unseen fonts without additional fine-tuning during\ninference. Furthermore, it supports visual text editing, font style blending,\nand cross-lingual font transfer, positioning FontAdapter as a versatile\nframework for font customization tasks.", "authors": ["Myungkyu Koo", "Subin Kim", "Sangkyung Kwak", "Jaehyun Nam", "Seojin Kim", "Jinwoo Shin"], "published_date": "2025-06-06", "title_zh": "FontAdapter：視覺文本生成中的即時字體適配", "summary_zh": "文字轉圖像擴散模型顯著提升視覺文字與圖像的整合度。近期研究透過使用預定義字體字典進行微調，進一步改善字體樣式控制。然而，調整預設外的未知字體計算成本高昂，耗時數十分鐘，使即時客製化不可行。本文提出FontAdapter框架，能在數秒內基於參考字形圖像生成未知字體的視覺文字。直接訓練字體數據集無法捕捉細緻的字體屬性，限制了對新字形的泛化能力。為了解決此問題，我們提出一個兩階段課程學習方法：FontAdapter首先學習從孤立字形中提取字體屬性，然後將這些樣式整合到自然背景中。為支援此兩階段訓練方案，我們利用大型線上字體資源，建構針對每個階段客製化的合成數據集。實驗表明，FontAdapter無需額外微調，即可實現對未知字體的高品質、穩健的字體客製化。此外，它還支援視覺文字編輯、字體樣式混合和跨語言字體轉換，使其成為一個多功能的字體客製化框架。", "audio": "audios/2506.05843v1.mp3", "timestamp": "2025-06-09T19:15:51.145650"}
{"query": "AI", "id": "2506.06015v1", "url": "http://arxiv.org/abs/2506.06015v1", "title": "On the Merits of LLM-Based Corpus Enrichment", "summary": "Generative AI (genAI) technologies -- specifically, large language models\n(LLMs) -- and search have evolving relations. We argue for a novel perspective:\nusing genAI to enrich a document corpus so as to improve query-based retrieval\neffectiveness. The enrichment is based on modifying existing documents or\ngenerating new ones. As an empirical proof of concept, we use LLMs to generate\ndocuments relevant to a topic which are more retrievable than existing ones. In\naddition, we demonstrate the potential merits of using corpus enrichment for\nretrieval augmented generation (RAG) and answer attribution in question\nanswering.", "authors": ["Gal Zur", "Tommy Mordo", "Moshe Tennenholtz", "Oren Kurland"], "published_date": "2025-06-06", "title_zh": "基於大型語言模型之語料擴增優勢探討", "summary_zh": "生成式人工智慧技術，特別是大語言模型，與搜尋之間存在演變關係。本文提出一種新觀點：利用生成式人工智慧豐富文檔語料庫，以提升基於查詢的檢索效能。此豐富化基於修改現有文檔或生成新文檔。作為概念驗證，我們使用大語言模型生成與主題相關且更易於檢索的文檔。此外，我們展示了語料庫豐富化在檢索增強生成和問答中答案歸屬方面的潛在優勢。", "audio": "audios/2506.06015v1.mp3", "timestamp": "2025-06-09T20:21:07.325570"}
{"query": "Foundation Model", "id": "2506.05713v1", "url": "http://arxiv.org/abs/2506.05713v1", "title": "Come Together, But Not Right Now: A Progressive Strategy to Boost Low-Rank Adaptation", "summary": "Low-rank adaptation (LoRA) has emerged as a leading parameter-efficient\nfine-tuning technique for adapting large foundation models, yet it often locks\nadapters into suboptimal minima near their initialization. This hampers model\ngeneralization and limits downstream operators such as adapter merging and\npruning. Here, we propose CoTo, a progressive training strategy that gradually\nincreases adapters' activation probability over the course of fine-tuning. By\nstochastically deactivating adapters, CoTo encourages more balanced\noptimization and broader exploration of the loss landscape. We provide a\ntheoretical analysis showing that CoTo promotes layer-wise dropout stability\nand linear mode connectivity, and we adopt a cooperative-game approach to\nquantify each adapter's marginal contribution. Extensive experiments\ndemonstrate that CoTo consistently boosts single-task performance, enhances\nmulti-task merging accuracy, improves pruning robustness, and reduces training\noverhead, all while remaining compatible with diverse LoRA variants. Code is\navailable at https://github.com/zwebzone/coto.", "authors": ["Zhan Zhuang", "Xiequn Wang", "Wei Li", "Yulong Zhang", "Qiushi Huang", "Shuhao Chen", "Xuehao Wang", "Yanbin Wei", "Yuhe Nie", "Kede Ma", "Yu Zhang", "Ying Wei"], "published_date": "2025-06-06", "title_zh": "聚合同步，刻不容緩：一種推進低秩適應的漸進策略", "summary_zh": "低秩適配(LoRA)已成為調整大型基礎模型的主要參數高效微調技術，但常將適配器鎖定在初始化附近次優的最小值。這阻礙了模型泛化，並限制了適配器合併和剪枝等下游操作。本文提出CoTo，一種漸進式訓練策略，在微調過程中逐步增加適配器的激活機率。透過隨機停用適配器，CoTo促進更平衡的優化和更廣泛的損失景觀探索。我們提供理論分析，表明CoTo促進了分層dropout穩定性和線性模式連接性，並採用合作博弈方法來量化每個適配器的邊際貢獻。大量實驗表明，CoTo始終能提升單任務性能、增強多任務合併準確性、提高剪枝魯棒性並減少訓練開銷，同時與各種LoRA變體相容。代碼可在https://github.com/zwebzone/coto取得。", "audio": "audios/2506.05713v1.mp3", "timestamp": "2025-06-09T20:21:15.180747"}
{"query": "Diffusion Model", "id": "2506.05806v1", "url": "http://arxiv.org/abs/2506.05806v1", "title": "LLIA -- Enabling Low-Latency Interactive Avatars: Real-Time Audio-Driven Portrait Video Generation with Diffusion Models", "summary": "Diffusion-based models have gained wide adoption in the virtual human\ngeneration due to their outstanding expressiveness. However, their substantial\ncomputational requirements have constrained their deployment in real-time\ninteractive avatar applications, where stringent speed, latency, and duration\nrequirements are paramount. We present a novel audio-driven portrait video\ngeneration framework based on the diffusion model to address these challenges.\nFirstly, we propose robust variable-length video generation to reduce the\nminimum time required to generate the initial video clip or state transitions,\nwhich significantly enhances the user experience. Secondly, we propose a\nconsistency model training strategy for Audio-Image-to-Video to ensure\nreal-time performance, enabling a fast few-step generation. Model quantization\nand pipeline parallelism are further employed to accelerate the inference\nspeed. To mitigate the stability loss incurred by the diffusion process and\nmodel quantization, we introduce a new inference strategy tailored for\nlong-duration video generation. These methods ensure real-time performance and\nlow latency while maintaining high-fidelity output. Thirdly, we incorporate\nclass labels as a conditional input to seamlessly switch between speaking,\nlistening, and idle states. Lastly, we design a novel mechanism for\nfine-grained facial expression control to exploit our model's inherent\ncapacity. Extensive experiments demonstrate that our approach achieves\nlow-latency, fluid, and authentic two-way communication. On an NVIDIA RTX\n4090D, our model achieves a maximum of 78 FPS at a resolution of 384x384 and 45\nFPS at a resolution of 512x512, with an initial video generation latency of 140\nms and 215 ms, respectively.", "authors": ["Haojie Yu", "Zhaonian Wang", "Yihan Pan", "Meng Cheng", "Hao Yang", "Chao Wang", "Tao Xie", "Xiaoming Xu", "Xiaoming Wei", "Xunliang Cai"], "published_date": "2025-06-06", "title_zh": "LLIA——基於擴散模型的即時音訊驅動人像影片生成：實現低延遲互動式虛擬化身", "summary_zh": "基於擴散的模型因其卓越表現力廣泛應用於虛擬人像生成，但其龐大計算需求限制了其在實時互動頭像應用中的部署。為應對這些挑戰，本文提出一種基於擴散模型的新型音訊驅動人像影片生成框架。首先，提出穩健的可變長度影片生成，以減少生成初始影片片段或狀態轉換所需的最短時間，從而顯著提升用戶體驗。其次，提出音訊-圖像到影片的一致性模型訓練策略，確保實時效能，實現快速的少步生成。進一步採用模型量化和管線並行化來加速推理速度。為減輕擴散過程和模型量化所導致的穩定性損失，我們引入一種針對長時影片生成的新推理策略。這些方法確保了實時效能和低延遲，同時保持了高保真輸出。第三，我們納入類別標籤作為條件輸入，以無縫切換說話、聆聽和閒置狀態。最後，我們設計了一種用於精細面部表情控制的新機制，以利用我們模型的固有能力。大量實驗表明，我們的方法實現了低延遲、流暢且真實的雙向通訊。在NVIDIA RTX 4090D上，我們的模型在384x384解析度下達到最高78 FPS，在512x512解析度下達到45 FPS，初始影片生成延遲分別為140毫秒和215毫秒。", "audio": "audios/2506.05806v1.mp3", "timestamp": "2025-06-09T20:21:29.969916"}
{"query": "AI", "id": "2506.05990v1", "url": "http://arxiv.org/abs/2506.05990v1", "title": "Leveraging Generative AI for Enhancing Automated Assessment in Programming Education Contests", "summary": "Competitive programming contests play a crucial role in cultivating\ncomputational thinking and algorithmic skills among learners. However,\ngenerating comprehensive test cases to effectively assess programming solutions\nremains resource-intensive and challenging for educators. This paper introduces\nan innovative NLP-driven method leveraging generative AI (large language\nmodels) to automate the creation of high-quality test cases for competitive\nprogramming assessments. We extensively evaluated our approach on diverse\ndatasets, including 25 years of Romanian Informatics Olympiad (OJI) data for\n5th graders, recent competitions hosted on the Kilonova.ro platform, and the\nInternational Informatics Olympiad in Teams (IIOT). Our results demonstrate\nthat AI-generated test cases substantially enhanced assessments, notably\nidentifying previously undetected errors in 67% of the OJI 5th grade\nprogramming problems. These improvements underscore the complementary\neducational value of our technique in formative assessment contexts. By openly\nsharing our prompts, translated datasets, and methodologies, we offer practical\nNLP-based tools that educators and contest organizers can readily integrate to\nenhance assessment quality, reduce workload, and deepen insights into learner\nperformance.", "authors": ["Stefan Dascalescu", "Adrian Marius Dumitran", "Mihai Alexandru Vasiluta"], "published_date": "2025-06-06", "title_zh": "利用生成式人工智能強化程式設計教育競賽中的自動化評估", "summary_zh": "競賽程式設計在培養運算思維和演算法技能方面至關重要，但產生全面的測試案例以有效評估程式設計解決方案，對教育者而言仍然耗費資源且具挑戰性。本文提出一種創新的自然語言處理驅動方法，利用生成式人工智慧（大型語言模型）自動產生高品質的競賽程式設計評估測試案例。我們在包括羅馬尼亞資訊奧林匹克競賽（OJI）25年五年級數據、Kilonova.ro平台上的近期競賽以及國際團體資訊奧林匹克競賽（IIOT）等多樣數據集上，廣泛評估了該方法。結果表明，人工智慧生成的測試案例顯著增強了評估效果，尤其是在OJI五年級程式設計問題中，識別出先前未發現的67%錯誤。這些改進強調了該技術在形成性評估中互補的教育價值。通過公開分享提示詞、翻譯後的數據集和方法論，我們提供了實用的自然語言處理工具，教育者和競賽組織者可輕鬆整合以提高評估質量、減輕工作量並加深對學習者表現的洞察。", "audio": "audios/2506.05990v1.mp3", "timestamp": "2025-06-09T21:18:05.694535"}
{"query": "Foundation Model", "id": "2506.05683v1", "url": "http://arxiv.org/abs/2506.05683v1", "title": "Multi-Modal Multi-Task Federated Foundation Models for Next-Generation Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence in AR/VR/MR", "summary": "Extended reality (XR) systems, which consist of virtual reality (VR),\naugmented reality (AR), and mixed reality (XR), offer a transformative\ninterface for immersive, multi-modal, and embodied human-computer interaction.\nIn this paper, we envision that multi-modal multi-task (M3T) federated\nfoundation models (FedFMs) can offer transformative capabilities for XR systems\nthrough integrating the representational strength of M3T foundation models\n(FMs) with the privacy-preserving model training principles of federated\nlearning (FL). We present a modular architecture for FedFMs, which entails\ndifferent coordination paradigms for model training and aggregations. Central\nto our vision is the codification of XR challenges that affect the\nimplementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality\ndiversity, (2) Hardware heterogeneity and system-level constraints, (3)\nInteractivity and embodied personalization, (4) Functional/task variability,\nand (5) Temporality and environmental variability. We illustrate the\nmanifestation of these dimensions across a set of emerging and anticipated\napplications of XR systems. Finally, we propose evaluation metrics, dataset\nrequirements, and design tradeoffs necessary for the development of\nresource-aware FedFMs in XR. This perspective aims to chart the technical and\nconceptual foundations for context-aware privacy-preserving intelligence in the\nnext generation of XR systems.", "authors": ["Fardis Nadimi", "Payam Abdisarabshali", "Kasra Borazjani", "Jacob Chakareski", "Seyyedali Hosseinalipour"], "published_date": "2025-06-06", "title_zh": "用於下一代擴展實境系統的多模態多任務聯邦基礎模型：邁向AR/VR/MR中保護隱私的分布式智能", "summary_zh": "擴展實境系統(XR)透過虛擬實境(VR)、擴增實境(AR)和混合實境(MR)提供沉浸式、多模態和具體化的人機互動介面。本文提出多模態多任務聯邦基礎模型(FedFMs)可整合其強大的表徵能力與聯邦學習(FL)的隱私保護原則，為XR系統帶來變革。我們提出FedFMs的模組化架構，包含模型訓練和聚合的不同協調範例，並闡述影響FedFMs實作的XR挑戰，歸納為SHIFT五個面向：感測器和模態多樣性、硬體異質性和系統層級限制、互動性和具體化個人化、功能/任務可變性，以及時間性和環境可變性。我們藉由XR系統的新興和預期應用來說明這些面向，並提出評估指標、資料集要求和設計取捨，以開發XR中具資源意識的FedFMs。此觀點旨在為下一代XR系統中具情境感知能力的隱私保護智慧奠定技術和概念基礎。", "audio": "audios/2506.05683v1.mp3", "timestamp": "2025-06-09T21:18:18.584635"}
{"query": "Diffusion Model", "id": "2506.05762v1", "url": "http://arxiv.org/abs/2506.05762v1", "title": "BiTrajDiff: Bidirectional Trajectory Generation with Diffusion Models for Offline Reinforcement Learning", "summary": "Recent advances in offline Reinforcement Learning (RL) have proven that\neffective policy learning can benefit from imposing conservative constraints on\npre-collected datasets. However, such static datasets often exhibit\ndistribution bias, resulting in limited generalizability. To address this\nlimitation, a straightforward solution is data augmentation (DA), which\nleverages generative models to enrich data distribution. Despite the promising\nresults, current DA techniques focus solely on reconstructing future\ntrajectories from given states, while ignoring the exploration of history\ntransitions that reach them. This single-direction paradigm inevitably hinders\nthe discovery of diverse behavior patterns, especially those leading to\ncritical states that may have yielded high-reward outcomes. In this work, we\nintroduce Bidirectional Trajectory Diffusion (BiTrajDiff), a novel DA framework\nfor offline RL that models both future and history trajectories from any\nintermediate states. Specifically, we decompose the trajectory generation task\ninto two independent yet complementary diffusion processes: one generating\nforward trajectories to predict future dynamics, and the other generating\nbackward trajectories to trace essential history transitions.BiTrajDiff can\nefficiently leverage critical states as anchors to expand into potentially\nvaluable yet underexplored regions of the state space, thereby facilitating\ndataset diversity. Extensive experiments on the D4RL benchmark suite\ndemonstrate that BiTrajDiff achieves superior performance compared to other\nadvanced DA methods across various offline RL backbones.", "authors": ["Yunpeng Qing", "Shuo Chen", "Yixiao Chi", "Shunyu Liu", "Sixu Lin", "Changqing Zou"], "published_date": "2025-06-06", "title_zh": "BiTrajDiff：基於擴散模型的離線強化學習雙向軌跡生成", "summary_zh": "離線強化學習透過對預先蒐集資料集施加保守約束來提升策略學習效果。然而，靜態資料集常有分佈偏差，限制其泛化能力。資料增強 (DA) 可利用生成模型豐富資料分佈，是解決此問題的直接方法。現有DA技術僅重建未來軌跡，忽略了歷史轉移的探索，限制了多樣行為模式的發現，特別是那些可能帶來高回報的關鍵狀態。本研究提出雙向軌跡擴散 (BiTrajDiff)，一種新穎的離線強化學習DA框架，可對任何中間狀態建模未來和歷史軌跡。BiTrajDiff將軌跡生成分解為兩個獨立且互補的擴散過程：一個生成前向軌跡以預測未來動態，另一個生成後向軌跡以追溯重要歷史轉移。BiTrajDiff能有效利用關鍵狀態作為錨點，擴展到有潛在價值但未充分探索的狀態空間區域，從而促進資料集多樣性。在D4RL基準測試套件上的廣泛實驗表明，相較於其他先進的DA方法，BiTrajDiff在各種離線強化學習骨幹網絡上均表現出卓越性能。", "audio": "audios/2506.05762v1.mp3", "timestamp": "2025-06-09T21:18:28.225818"}
{"query": "AI", "id": "2506.05967v1", "url": "http://arxiv.org/abs/2506.05967v1", "title": "Preference Learning for AI Alignment: a Causal Perspective", "summary": "Reward modelling from preference data is a crucial step in aligning large\nlanguage models (LLMs) with human values, requiring robust generalisation to\nnovel prompt-response pairs. In this work, we propose to frame this problem in\na causal paradigm, providing the rich toolbox of causality to identify the\npersistent challenges, such as causal misidentification, preference\nheterogeneity, and confounding due to user-specific factors. Inheriting from\nthe literature of causal inference, we identify key assumptions necessary for\nreliable generalisation and contrast them with common data collection\npractices. We illustrate failure modes of naive reward models and demonstrate\nhow causally-inspired approaches can improve model robustness. Finally, we\noutline desiderata for future research and practices, advocating targeted\ninterventions to address inherent limitations of observational data.", "authors": ["Katarzyna Kobalczyk", "Mihaela van der Schaar"], "published_date": "2025-06-06", "title_zh": "人工智慧對齊之偏好學習：因果視角", "summary_zh": "基於偏好資料的獎勵模型建構，對於使大型語言模型與人類價值觀對齊至關重要，需要對新的提示-回應配對具備穩健的泛化能力。本文將此問題置於因果關係範式中，利用因果工具箱識別持續存在的挑戰，如因果誤判、偏好異質性及使用者特定因素造成的混淆。承襲因果推論文獻，我們辨識出可靠泛化所需的關鍵假設，並將其與常見的資料收集實踐進行對比。我們闡述了樸素獎勵模型的失效模式，並展示了受因果啟發的方法如何提升模型穩健性。最後，我們概述了未來研究和實踐的期望，提倡有針對性的干預措施，以解決觀測資料的固有局限性。", "audio": "audios/2506.05967v1.mp3", "timestamp": "2025-06-09T22:18:24.773967"}
{"query": "Foundation Model", "id": "2506.05641v1", "url": "http://arxiv.org/abs/2506.05641v1", "title": "Projectable Models: One-Shot Generation of Small Specialized Transformers from Large Ones", "summary": "Modern Foundation Models (FMs) are typically trained on corpora spanning a\nwide range of different data modalities, topics and downstream tasks. Utilizing\nthese models can be very computationally expensive and is out of reach for most\nconsumer devices. Furthermore, most of the broad FM knowledge may actually be\nirrelevant for a specific task at hand. Here we explore a technique for mapping\nparameters of a large Transformer to parameters of a smaller specialized model.\nBy making this transformation task-specific, we aim to capture a narrower scope\nof the knowledge needed for performing a specific task by a smaller model. We\nstudy our method on image modeling tasks, showing that performance of generated\nmodels exceeds that of universal conditional models.", "authors": ["Andrey Zhmoginov", "Jihwan Lee", "Mark Sandler"], "published_date": "2025-06-06", "title_zh": "可投射模型：從大型轉換器單次生成小型專業轉換器", "summary_zh": "現代基礎模型通常以涵蓋廣泛數據模態、主題和下游任務的語料庫進行訓練。使用這些模型在計算上非常昂貴，且超出大多數消費設備的承受能力。此外，大多數廣泛的基礎模型知識可能與特定任務無關。本文探討一種將大型Transformer的參數映射到較小專業模型參數的技術。透過使此轉換具有任務針對性，旨在透過較小的模型捕獲執行特定任務所需的更窄範圍知識。我們在圖像建模任務上研究此方法，表明生成模型的性能超過通用條件模型。", "audio": "audios/2506.05641v1.mp3", "timestamp": "2025-06-09T22:18:29.622425"}
{"query": "Diffusion Model", "id": "2506.05710v1", "url": "http://arxiv.org/abs/2506.05710v1", "title": "Latent Diffusion Model Based Denoising Receiver for 6G Semantic Communication: From Stochastic Differential Theory to Application", "summary": "In this paper, a novel semantic communication framework empowered by\ngenerative artificial intelligence (GAI) is proposed, specifically leveraging\nthe capabilities of diffusion models (DMs). A rigorous theoretical foundation\nis established based on stochastic differential equations (SDEs), which\nelucidates the denoising properties of DMs in mitigating additive white\nGaussian noise (AWGN) in latent semantic representations. Crucially, a\nclosed-form analytical relationship between the signal-to-noise ratio (SNR) and\nthe denoising timestep is derived, enabling the optimal selection of diffusion\nparameters for any given channel condition. To address the distribution\nmismatch between the received signal and the DM's training data, a\nmathematically principled scaling mechanism is introduced, ensuring robust\nperformance across a wide range of SNRs without requiring model fine-tuning.\nBuilt upon this theoretical insight, we develop a latent diffusion model\n(LDM)-based semantic transceiver, wherein a variational autoencoder (VAE) is\nemployed for efficient semantic compression, and a pretrained DM serves as a\nuniversal denoiser. Notably, the proposed architecture is fully training-free\nat inference time, offering high modularity and compatibility with large-scale\npretrained LDMs. This design inherently supports zero-shot generalization and\nmitigates the challenges posed by out-of-distribution inputs. Extensive\nexperimental evaluations demonstrate that the proposed framework significantly\noutperforms conventional neural-network-based semantic communication baselines,\nparticularly under low SNR conditions and distributional shifts, thereby\nestablishing a promising direction for GAI-driven robust semantic transmission\nin future 6G systems.", "authors": ["Xiucheng Wang", "Honggang Jia", "Nan Cheng", "Dusit Niyato"], "published_date": "2025-06-06", "title_zh": "基於潛在擴散模型的6G語義通信去噪接收器：從隨機微分理論到應用", "summary_zh": "本研究提出一種基於生成式人工智慧的新型語義通訊框架，利用擴散模型（DM）的能力。基於隨機微分方程（SDE）建立了嚴格的理論基礎，闡明了DM在減輕潛在語義表示中加性高斯白雜訊（AWGN）的去噪特性。推導出訊號雜訊比（SNR）與去噪時間步長之間的閉合形式解析關係，從而能夠為任何給定的通道條件選擇最佳擴散參數。為了解決接收訊號與DM訓練數據之間的分配不匹配問題，引入了一種數學原理的縮放機制，確保在各種SNR範圍內實現穩健的性能，而無需模型微調。基於此理論，我們開發了一種基於潛在擴散模型（LDM）的語義收發器，其中採用變分自編碼器（VAE）進行高效語義壓縮，而預訓練的DM則充當通用降噪器。值得注意的是，所提出的架構在推論時是完全免訓練的，具有高度的模組化和與大規模預訓練LDM的相容性。這種設計本質上支持零樣本泛化，並減輕了分佈外輸入帶來的挑戰。廣泛的實驗評估表明，所提出的框架顯著優於傳統的基於神經網路的語義通訊基準，尤其是在低SNR條件和分佈偏移下，從而為未來6G系統中GAI驅動的穩健語義傳輸奠定了有希望的方向。", "audio": "audios/2506.05710v1.mp3", "timestamp": "2025-06-09T22:18:38.551680"}
{"query": "AI", "id": "2506.05962v1", "url": "http://arxiv.org/abs/2506.05962v1", "title": "Quantum Checkers: The Development and Analysis of a Quantum Combinatorial Game", "summary": "This paper develops and analyses a novel quantum combinatorial game: quantum\ncheckers (codenamed Cheqqers). The concepts of superposition, entanglement,\nmeasurements and interference from quantum mechanics are integrated into the\ngame of checkers by adding new types of legal moves. The addition of these new\nrules is done gradually by introducing several levels of `quantumness'. Quantum\ncheckers provides a framework for interpolating between a known and solved\nclassical game and a more complex quantum game, and serves as 1) a benchmark\nfor AI players learning to play quantum games and 2) an interesting game for\nhuman players that allows them to build intuition for quantum phenomena. We\nprovide the initial analysis on the complexity of this game using random agents\nand a Monte Carlo tree search agent.", "authors": ["Marien Raat", "Luuk van den Nouweland", "Matthias Müller-Brockhausen", "Mike Preuss", "Evert van Nieuwenburg"], "published_date": "2025-06-06", "title_zh": "量子跳棋：一種量子組合遊戲的發展與分析", "summary_zh": "本文提出並分析一種新型量子組合遊戲：量子跳棋。該遊戲透過引入新的合法移動方式，將量子力學中的疊加、糾纏、測量和干涉等概念融入傳統跳棋。多個量子化層級逐步引入新規則。量子跳棋提供了一個框架，可在已知的、已解決的經典遊戲和更複雜的量子遊戲之間進行插值，並作為1）人工智慧玩家學習量子遊戲的基準，以及2）一個有趣的遊戲，讓人們建立對量子現象的直覺。我們利用隨機代理和蒙地卡羅樹搜尋代理，對此遊戲的複雜性進行初步分析。", "audio": "audios/2506.05962v1.mp3", "timestamp": "2025-06-09T23:18:41.881463"}
{"query": "Foundation Model", "id": "2506.05616v1", "url": "http://arxiv.org/abs/2506.05616v1", "title": "Toward Greater Autonomy in Materials Discovery Agents: Unifying Planning, Physics, and Scientists", "summary": "We aim at designing language agents with greater autonomy for crystal\nmaterials discovery. While most of existing studies restrict the agents to\nperform specific tasks within predefined workflows, we aim to automate workflow\nplanning given high-level goals and scientist intuition. To this end, we\npropose Materials Agent unifying Planning, Physics, and Scientists, known as\nMAPPS. MAPPS consists of a Workflow Planner, a Tool Code Generator, and a\nScientific Mediator. The Workflow Planner uses large language models (LLMs) to\ngenerate structured and multi-step workflows. The Tool Code Generator\nsynthesizes executable Python code for various tasks, including invoking a\nforce field foundation model that encodes physics. The Scientific Mediator\ncoordinates communications, facilitates scientist feedback, and ensures\nrobustness through error reflection and recovery. By unifying planning,\nphysics, and scientists, MAPPS enables flexible and reliable materials\ndiscovery with greater autonomy, achieving a five-fold improvement in\nstability, uniqueness, and novelty rates compared with prior generative models\nwhen evaluated on the MP-20 data. We provide extensive experiments across\ndiverse tasks to show that MAPPS is a promising framework for autonomous\nmaterials discovery.", "authors": ["Lianhao Zhou", "Hongyi Ling", "Keqiang Yan", "Kaiji Zhao", "Xiaoning Qian", "Raymundo Arróyave", "Xiaofeng Qian", "Shuiwang Ji"], "published_date": "2025-06-05", "title_zh": "邁向更自主化的材料發現代理：統合規劃、物理學與科學家", "summary_zh": "本研究旨在設計更具自主性的晶體材料發現語言代理。有別於現有研究將代理限制在預定義工作流程中的特定任務，本研究旨在根據高階目標與科學家直覺，自動化工作流程規劃。為此，我們提出整合規劃、物理學和科學家的材料代理MAPPS。MAPPS包含工作流程規劃器、工具代碼生成器和科學協調器。工作流程規劃器利用大型語言模型生成結構化的多步驟工作流程；工具代碼生成器合成可執行的Python代碼，以執行各種任務，包括調用編碼物理學的力場基礎模型；科學協調器協調溝通、促進科學家回饋，並透過錯誤反思和恢復確保穩健性。MAPPS透過整合規劃、物理學和科學家，實現更靈活可靠的材料發現，在MP-20資料集上的評估顯示，其穩定性、獨特性和新穎性比先前的生成模型提高五倍。廣泛的實驗表明，MAPPS是一個有前景的自主材料發現框架。", "audio": "audios/2506.05616v1.mp3", "timestamp": "2025-06-09T23:18:51.898084"}
{"query": "Diffusion Model", "id": "2506.05680v1", "url": "http://arxiv.org/abs/2506.05680v1", "title": "Learning Design-Score Manifold to Guide Diffusion Models for Offline Optimization", "summary": "Optimizing complex systems, from discovering therapeutic drugs to designing\nhigh-performance materials, remains a fundamental challenge across science and\nengineering, as the underlying rules are often unknown and costly to evaluate.\nOffline optimization aims to optimize designs for target scores using\npre-collected datasets without system interaction. However, conventional\napproaches may fail beyond training data, predicting inaccurate scores and\ngenerating inferior designs. This paper introduces ManGO, a diffusion-based\nframework that learns the design-score manifold, capturing the design-score\ninterdependencies holistically. Unlike existing methods that treat design and\nscore spaces in isolation, ManGO unifies forward prediction and backward\ngeneration, attaining generalization beyond training data. Key to this is its\nderivative-free guidance for conditional generation, coupled with adaptive\ninference-time scaling that dynamically optimizes denoising paths. Extensive\nevaluations demonstrate that ManGO outperforms 24 single- and 10\nmulti-objective optimization methods across diverse domains, including\nsynthetic tasks, robot control, material design, DNA sequence, and real-world\nengineering optimization.", "authors": ["Tailin Zhou", "Zhilin Chen", "Wenlong Lyu", "Zhitang Chen", "Danny H. K. Tsang", "Jun Zhang"], "published_date": "2025-06-06", "title_zh": "學習設計-評分流形以引導擴散模型進行離線優化", "summary_zh": "優化複雜系統，如藥物開發和高性能材料設計，是一項根本挑戰，因其底層規則未知且評估成本高昂。離線優化旨在利用預先收集的數據集優化設計，無需系統互動。然而，傳統方法可能在訓練數據之外失效，導致預測不準確和設計不佳。本文提出 ManGO，一種基於擴散的模型，學習設計-分數流形，全面捕捉設計與分數的相互依賴性。與孤立處理設計和分數空間的現有方法不同，ManGO 統一了正向預測和反向生成，實現了超越訓練數據的泛化能力。其關鍵在於無導數條件生成引導，以及自適應推理時尺度調整，動態優化降噪路徑。廣泛評估表明，ManGO 在合成任務、機器人控制、材料設計、DNA 序列和真實工程優化等不同領域，優於 24 種單目標和 10 種多目標優化方法。", "audio": "audios/2506.05680v1.mp3", "timestamp": "2025-06-09T23:19:01.429903"}
{"query": "AI", "id": "2506.05904v1", "url": "http://arxiv.org/abs/2506.05904v1", "title": "Proactive Assistant Dialogue Generation from Streaming Egocentric Videos", "summary": "Recent advances in conversational AI have been substantial, but developing\nreal-time systems for perceptual task guidance remains challenging. These\nsystems must provide interactive, proactive assistance based on streaming\nvisual inputs, yet their development is constrained by the costly and\nlabor-intensive process of data collection and system evaluation. To address\nthese limitations, we present a comprehensive framework with three key\ncontributions. First, we introduce a novel data curation pipeline that\nsynthesizes dialogues from annotated egocentric videos, resulting in \\dataset,\na large-scale synthetic dialogue dataset spanning multiple domains. Second, we\ndevelop a suite of automatic evaluation metrics, validated through extensive\nhuman studies. Third, we propose an end-to-end model that processes streaming\nvideo inputs to generate contextually appropriate responses, incorporating\nnovel techniques for handling data imbalance and long-duration videos. This\nwork lays the foundation for developing real-time, proactive AI assistants\ncapable of guiding users through diverse tasks. Project page:\nhttps://pro-assist.github.io/", "authors": ["Yichi Zhang", "Xin Luna Dong", "Zhaojiang Lin", "Andrea Madotto", "Anuj Kumar", "Babak Damavandi", "Joyce Chai", "Seungwhan Moon"], "published_date": "2025-06-06", "title_zh": "從串流自我中心視訊中生成主動式助理對話", "summary_zh": "對話式人工智慧進展顯著，但開發用於感知任務指導的即時系統仍具挑戰。此類系統需根據串流視覺輸入提供互動式、主動式協助，然其開發受限於資料收集與系統評估之高成本與勞力密集特性。為解決這些限制，我們提出一全面性框架，包含三項關鍵貢獻：一、引入一新型資料整理流程，可自標註之第一人稱視角影片合成對話，產生涵蓋多領域的大規模合成對話資料集；二、開發一系列自動評估指標，並經由廣泛的人工研究驗證；三、提出一端到端模型，處理串流影片輸入以產生符合情境之回應，並結合處理資料不平衡與長時間影片的新穎技術。本研究為開發具備指導使用者完成各種任務能力的即時、主動式人工智慧助理奠定基礎。專案頁面：https://pro-assist.github.io/", "audio": "audios/2506.05904v1.mp3", "timestamp": "2025-06-10T01:37:43.312009"}
{"query": "Foundation Model", "id": "2506.06105v2", "url": "http://arxiv.org/abs/2506.06105v2", "title": "Text-to-LoRA: Instant Transformer Adaption", "summary": "While Foundation Models provide a general tool for rapid content creation,\nthey regularly require task-specific adaptation. Traditionally, this exercise\ninvolves careful curation of datasets and repeated fine-tuning of the\nunderlying model. Fine-tuning techniques enable practitioners to adapt\nfoundation models for many new applications but require expensive and lengthy\ntraining while being notably sensitive to hyperparameter choices. To overcome\nthese limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting\nlarge language models (LLMs) on the fly solely based on a natural language\ndescription of the target task. T2L is a hypernetwork trained to construct\nLoRAs in a single inexpensive forward pass. After training T2L on a suite of 9\npre-trained LoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc\nreconstructed LoRA instances match the performance of task-specific adapters\nacross the corresponding test sets. Furthermore, T2L can compress hundreds of\nLoRA instances and zero-shot generalize to entirely unseen tasks. This approach\nprovides a significant step towards democratizing the specialization of\nfoundation models and enables language-based adaptation with minimal compute\nrequirements.\n  Our code is available at https://github.com/SakanaAI/text-to-lora", "authors": ["Rujikorn Charakorn", "Edoardo Cetin", "Yujin Tang", "Robert Tjarko Lange"], "published_date": "2025-06-06", "title_zh": "文本到LoRA：即時變換器適配", "summary_zh": "大型語言模型雖能快速生成內容，但常需針對特定任務調整。傳統方法需仔細整理資料集並反覆微調模型，過程耗時耗力且對超參數敏感。為克服這些限制，我們提出Text-to-LoRA (T2L)，此模型僅依據目標任務的自然語言描述，即可即時調整大型語言模型。T2L是一個超網絡，透過單次正向傳播構建LoRA。我們在9個預訓練的LoRA適配器(如GSM8K、Arc)上訓練T2L，結果顯示，臨時重建的LoRA實例在相應測試集上的表現與特定任務適配器相當。此外，T2L能壓縮數百個LoRA實例，並零樣本泛化至完全未見過的任務。此方法朝向普及化基礎模型專業化邁進了一大步，並能以最少的計算需求實現基於語言的適應。", "audio": "audios/2506.06105v2.mp3", "timestamp": "2025-06-10T01:37:50.312008"}
{"query": "Diffusion Model", "id": "2506.05960v2", "url": "http://arxiv.org/abs/2506.05960v2", "title": "AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models", "summary": "Significant investments have been made towards the commodification of\ndiffusion models for generation of diverse media. Their mass-market adoption is\nhowever still hobbled by the intense hardware resource requirements of\ndiffusion model inference. Model quantization strategies tailored specifically\ntowards diffusion models have been useful in easing this burden, yet have\ngenerally explored the Uniform Scalar Quantization (USQ) family of quantization\nmethods. In contrast, Vector Quantization (VQ) methods, which operate on groups\nof multiple related weights as the basic unit of compression, have seen\nsubstantial success in Large Language Model (LLM) quantization. In this work,\nwe apply codebook-based additive vector quantization to the problem of\ndiffusion model compression. Our resulting approach achieves a new Pareto\nfrontier for the extremely low-bit weight quantization on the standard\nclass-conditional benchmark of LDM-4 on ImageNet at 20 inference time steps.\nNotably, we report sFID 1.92 points lower than the full-precision model at W4A8\nand the best-reported results for FID, sFID and ISC at W2A8. We are also able\nto demonstrate FLOPs savings on arbitrary hardware via an efficient inference\nkernel, as opposed to savings resulting from small integer operations which may\nlack broad hardware support.", "authors": ["Adil Hasan", "Thomas Peyrin"], "published_date": "2025-06-06", "title_zh": "AQUATIC-Diff：極小壓縮擴散模型的加性量化", "summary_zh": "為促進多元媒體生成，擴散模型已投入大量商業化資源。然而，其推論階段對硬體資源的高度需求阻礙了大規模普及。針對擴散模型的模型量化策略雖有助於減輕此負擔，但主要集中於均勻純量量化。向量量化方法透過壓縮多個相關權重組，已在大型語言模型量化中取得顯著成功。本研究將基於碼本的加法向量量化應用於擴散模型壓縮。在ImageNet的LDM-4類別條件基準測試上，使用20個推論時間步長，我們的方案在極低位元權重量化方面達到了新的帕累托前沿。值得注意的是，在W4A8下，我們報告的sFID比全精度模型低1.92點，並在W2A8下取得了FID、sFID和ISC的最佳結果。相較於依賴可能缺乏廣泛硬體支援的小整數運算，我們亦透過高效的推論核心，在任意硬體上實現了FLOPs的節省。", "audio": "audios/2506.05960v2.mp3", "timestamp": "2025-06-10T01:38:00.807172"}
{"query": "AI", "id": "2506.05887v1", "url": "http://arxiv.org/abs/2506.05887v1", "title": "Explainability in Context: A Multilevel Framework Aligning AI Explanations with Stakeholder with LLMs", "summary": "The growing application of artificial intelligence in sensitive domains has\nintensified the demand for systems that are not only accurate but also\nexplainable and trustworthy. Although explainable AI (XAI) methods have\nproliferated, many do not consider the diverse audiences that interact with AI\nsystems: from developers and domain experts to end-users and society. This\npaper addresses how trust in AI is influenced by the design and delivery of\nexplanations and proposes a multilevel framework that aligns explanations with\nthe epistemic, contextual, and ethical expectations of different stakeholders.\nThe framework consists of three layers: algorithmic and domain-based,\nhuman-centered, and social explainability. We highlight the emerging role of\nLarge Language Models (LLMs) in enhancing the social layer by generating\naccessible, natural language explanations. Through illustrative case studies,\nwe demonstrate how this approach facilitates technical fidelity, user\nengagement, and societal accountability, reframing XAI as a dynamic,\ntrust-building process.", "authors": ["Marilyn Bello", "Rafael Bello", "Maria-Matilde García", "Ann Nowé", "Iván Sevillano-García", "Francisco Herrera"], "published_date": "2025-06-06", "title_zh": "情境脈絡下的可解釋性：一個多層次框架，利用大型語言模型將人工智慧解釋與利害關係人對齊", "summary_zh": "人工智慧於敏感領域應用日增，對準確、可解釋且可信任系統之需求亦趨迫切。儘管可解釋人工智慧（XAI）方法激增，然多數未考量與人工智慧系統互動之多元受眾：從開發者、領域專家至終端使用者與社會大眾。本文探討AI信任如何受解釋之設計與傳遞影響，並提出多層次框架，使解釋與不同利害關係人之認知、情境及倫理期望相符。此框架包含演算法及領域、以人為本、社會可解釋性三層。本文強調大型語言模型（LLMs）於增強社會層面之新興角色，藉由產生易於理解之自然語言解釋達成目標。透過案例研究，本文闡述此方法如何促進技術忠實度、使用者參與及社會責任，將XAI重新定義為動態的信任建立過程。", "audio": "audios/2506.05887v1.mp3", "timestamp": "2025-06-10T03:18:17.295345"}
{"query": "Foundation Model", "id": "2506.05850v2", "url": "http://arxiv.org/abs/2506.05850v2", "title": "Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models", "summary": "We identify \\textbf{Cross-lingual Collapse}, a systematic drift in which the\nchain-of-thought (CoT) of a multilingual language model reverts to its dominant\npre-training language even when the prompt is expressed in a different\nlanguage. Recent large language models (LLMs) with reinforcement learning with\nverifiable reward (RLVR) have achieved strong logical reasoning performances by\nexposing their intermediate reasoning traces, giving rise to large reasoning\nmodels (LRMs). However, the mechanism behind multilingual reasoning in LRMs is\nnot yet fully explored. To investigate the issue, we fine-tune multilingual\nLRMs with Group-Relative Policy Optimization (GRPO) on translated versions of\nthe GSM$8$K and SimpleRL-Zoo datasets in three different languages: Chinese,\nKorean, and Ukrainian. During training, we monitor both task accuracy and\nlanguage consistency of the reasoning chains. Our experiments reveal three key\nfindings: (i) GRPO rapidly amplifies pre-training language imbalances, leading\nto the erosion of low-resource languages within just a few hundred updates;\n(ii) language consistency reward mitigates this drift but does so at the\nexpense of an almost 5 - 10 pp drop in accuracy. and (iii) the resulting\nlanguage collapse is severely damaging and largely irreversible, as subsequent\nfine-tuning struggles to steer the model back toward its original\ntarget-language reasoning capabilities. Together, these findings point to a\nremarkable conclusion: \\textit{not all languages are trained equally for\nreasoning}. Furthermore, our paper sheds light on the roles of reward shaping,\ndata difficulty, and pre-training priors in eliciting multilingual reasoning.", "authors": ["Cheonbok Park", "Jeonghoon Kim", "Joosung Lee", "Sanghwan Bae", "Jaegul Choo", "Kang Min Yoo"], "published_date": "2025-06-06", "title_zh": "跨語言崩潰：語言中心基礎模型如何塑造大型語言模型中的推理", "summary_zh": "本研究揭示跨語言崩潰現象，即多語言模型的思維鏈(CoT)會系統性地偏離提示語言，轉向其主要的預訓練語言。儘管採用可驗證獎勵的強化學習(RLVR)的大型語言模型(LLM)通過展示中間推理過程，在邏輯推理方面表現出色，但大型推理模型(LRM)中多語言推理的機制尚未完全探明。為此，我們使用群體相對策略優化(GRPO)在GSM8K和SimpleRL-Zoo數據集的翻譯版本上，對多語言LRM進行微調，使用的語言包括中文、韓語和烏克蘭語。實驗監測任務準確度和推理鏈的語言一致性，發現：(i) GRPO迅速放大預訓練語言的不平衡，導致低資源語言在數百次更新內便衰退；(ii) 語言一致性獎勵可緩解此現象，但會導致準確度下降5-10個百分點；(iii) 語言崩潰具有嚴重破壞性且幾乎不可逆轉，後續微調難以將模型導回原始目標語言的推理能力。研究表明，並非所有語言都經過同等程度的推理訓練，並闡明了獎勵塑造、數據難度和預訓練先驗在激發多語推理方面的作用。", "audio": "audios/2506.05850v2.mp3", "timestamp": "2025-06-10T03:18:30.252524"}
{"query": "Diffusion Model", "id": "2506.05668v1", "url": "http://arxiv.org/abs/2506.05668v1", "title": "RNE: a plug-and-play framework for diffusion density estimation and inference-time control", "summary": "In this paper, we introduce the Radon-Nikodym Estimator (RNE), a flexible,\nplug-and-play framework for diffusion inference-time density estimation and\ncontrol, based on the concept of the density ratio between path distributions.\nRNE connects and unifies a variety of existing density estimation and\ninference-time control methods under a single and intuitive perspective,\nstemming from basic variational inference and probabilistic principles\ntherefore offering both theoretical clarity and practical versatility.\nExperiments demonstrate that RNE achieves promising performances in diffusion\ndensity estimation and inference-time control tasks, including annealing,\ncomposition of diffusion models, and reward-tilting.", "authors": ["Jiajun He", "José Miguel Hernández-Lobato", "Yuanqi Du", "Francisco Vargas"], "published_date": "2025-06-06", "title_zh": "RNE：用於擴散密度估計與推論時控制的即插即用框架", "summary_zh": "本研究提出Radon-Nikodym估計器(RNE)，一種彈性化的隨插即用框架，用於擴散模型的推論階段密度估計與控制，其基於路徑分佈的密度比概念。RNE連結並統整現有多種密度估計與推論階段控制方法，提供直觀的視角、理論清晰性與實用性。實驗結果表明，RNE在擴散密度估計與推論階段控制任務(包括退火、擴散模型組合與獎勵傾斜)中表現出色。", "audio": "audios/2506.05668v1.mp3", "timestamp": "2025-06-10T03:18:35.897664"}
{"query": "AI", "id": "2506.08006v1", "url": "http://arxiv.org/abs/2506.08006v1", "title": "Dreamland: Controllable World Creation with Simulator and Generative Models", "summary": "Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that encodes both pixel-level and object-level\nsemantics and geometry as an intermediate representation to bridge the\nsimulator and the generative model. This approach enhances controllability,\nminimizes adaptation cost through early alignment with real-world\ndistributions, and supports off-the-shelf use of existing and future pretrained\ngenerative models. We further construct a D3Sim dataset to facilitate the\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\nthat Dreamland outperforms existing baselines with 50.8% improved image\nquality, 17.9% stronger controllability, and has great potential to enhance\nembodied agent training. Code and data will be made available.", "authors": ["Sicheng Mo", "Ziyang Leng", "Leon Liu", "Weizhen Wang", "Honglin He", "Bolei Zhou"], "published_date": "2025-06-09", "title_zh": "夢境：基於模擬器與生成模型的可控世界創建", "summary_zh": "大規模影片生成模型雖能合成多樣且逼真的動態世界內容，但缺乏元素級別的可控性，阻礙了其在場景編輯和具身AI代理訓練中的應用。本研究提出Dreamland，一種混合世界生成框架，結合了基於物理模擬器的精細控制以及大規模預訓練生成模型的光寫實內容輸出。透過設計分層世界抽象，將像素級和物件級的語義及幾何資訊編碼為中間表示，從而連接模擬器和生成模型。此方法提升了可控性，透過與真實世界分佈的早期對齊，降低了適應成本，並支援現有及未來預訓練生成模型的即用即取。我們進一步構建了D3Sim數據集，以促進混合生成管道的訓練和評估。實驗結果表明，Dreamland在圖像品質上優於現有基準線50.8%，可控性提升17.9%，並具有增強具身代理訓練的巨大潛力。程式碼和數據將公開提供。", "audio": "audios/2506.08006v1.mp3", "timestamp": "2025-06-10T04:27:31.328053"}
{"query": "Foundation Model", "id": "2506.07940v1", "url": "http://arxiv.org/abs/2506.07940v1", "title": "Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation", "summary": "Foundation model fine-tuning faces a fundamental challenge: existing AutoML\nplatforms rely on single optimisation strategies that explore only a fraction\nof viable hyperparameter configurations. In this white paper, We introduce\nGradients, a decentralised AutoML platform that transforms hyperparameter\noptimisation into a competitive marketplace where independent miners compete to\ndiscover optimal configurations. Economic incentives align individual\nexploration with collective optimisation goals, driving systematic\ninvestigation of hyperparameter regions that centralised methods miss. We\nevaluate our approach across 180 controlled experiments spanning diverse model\narchitectures (70M to 70B parameters) and task types. Gradients achieves an\n82.8\\% win rate against HuggingFace AutoTrain and 100\\% against TogetherAI,\nDatabricks, and Google Cloud, with mean improvements of 11.8\\% and 42.1\\%\nrespectively. Complex reasoning and retrieval tasks show particularly strong\ngains of 30-40\\%, whilst diffusion models achieve 23.4\\% improvements for\nperson-specific generation. These results demonstrate that competitive,\neconomically-driven approaches can systematically discover superior\nconfigurations that centralised AutoML consistently miss.", "authors": ["Christopher Subia-Waud"], "published_date": "2025-06-09", "title_zh": "梯度：市場與微調的交匯——一種模型最佳化的分散式方法", "summary_zh": "基於大型模型微調面臨挑戰：現有自動機器學習平台僅採用單一優化策略，難以探索所有超參數配置。本研究提出Gradients，一個去中心化自動機器學習平台，將超參數優化轉化為競爭市場，礦工們競相尋找最佳配置。經濟誘因使個體探索與集體優化目標一致，系統性地探索中心化方法遺漏的超參數區域。實驗結果顯示，Gradients在多種模型架構和任務類型上優於HuggingFace AutoTrain、TogetherAI、Databricks和Google Cloud，平均提升分別為11.8%和42.1%。複雜推理和檢索任務提升顯著，達30-40%，個人化生成擴散模型提升23.4%。研究證明，具經濟驅動的競爭方法可系統性地發現優於中心化自動機器學習的配置。", "audio": "audios/2506.07940v1.mp3", "timestamp": "2025-06-10T04:27:38.593601"}
{"query": "Diffusion Model", "id": "2506.08013v1", "url": "http://arxiv.org/abs/2506.08013v1", "title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets", "summary": "Multi-task learning for dense prediction is limited by the need for extensive\nannotation for every task, though recent works have explored training with\npartial task labels. Leveraging the generalization power of diffusion models,\nwe extend the partial learning setup to a zero-shot setting, training a\nmulti-task model on multiple synthetic datasets, each labeled for only a subset\nof tasks. Our method, StableMTL, repurposes image generators for latent\nregression. Adapting a denoising framework with task encoding, per-task\nconditioning and a tailored training scheme. Instead of per-task losses\nrequiring careful balancing, a unified latent loss is adopted, enabling\nseamless scaling to more tasks. To encourage inter-task synergy, we introduce a\nmulti-stream model with a task-attention mechanism that converts N-to-N task\ninteractions into efficient 1-to-N attention, promoting effective cross-task\nsharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.", "authors": ["Anh-Quan Cao", "Ivan Lopes", "Raoul de Charette"], "published_date": "2025-06-09", "title_zh": "StableMTL：利用潛在擴散模型從部分標註合成數據集進行多任務學習", "summary_zh": "稠密預測多任務學習受限於大量任務標註，近年研究探索了部分標籤訓練。藉助擴散模型泛化能力，本文將部分學習擴展至零樣本情境，於多個合成數據集上訓練多任務模型，各數據集僅標註部分任務。所提StableMTL方法將圖像生成器重用於潛在迴歸，調整去噪框架，採用任務編碼、逐任務條件控制及客製化訓練方案。捨棄需仔細平衡的逐任務損失，改用統一潛在損失，實現任務無縫擴展。為促進任務協同，引入多流模型及任務注意力機制，將N對N任務交互轉化為高效1對N注意力，提升跨任務共享。StableMTL在7項任務及8個基準測試中均優於基準線。", "audio": "audios/2506.08013v1.mp3", "timestamp": "2025-06-10T04:27:45.658871"}
{"query": "AI", "id": "2506.07997v1", "url": "http://arxiv.org/abs/2506.07997v1", "title": "Supporting Construction Worker Well-Being with a Multi-Agent Conversational AI System", "summary": "The construction industry is characterized by both high physical and\npsychological risks, yet supports of mental health remain limited. While\nadvancements in artificial intelligence (AI), particularly large language\nmodels (LLMs), offer promising solutions, their potential in construction\nremains largely underexplored. To bridge this gap, we developed a\nconversational multi-agent system that addresses industry-specific challenges\nthrough an AI-driven approach integrated with domain knowledge. In parallel, it\nfulfills construction workers' basic psychological needs by enabling\ninteractions with multiple agents, each has a distinct persona. This approach\nensures that workers receive both practical problem-solving support and social\nengagement, ultimately contributing to their overall well-being. We evaluate\nits usability and effectiveness through a within-subjects user study with 12\nparticipants. The results show that our system significantly outperforms the\nsingle-agent baseline, achieving improvements of 18% in usability, 40% in\nself-determination, 60% in social presence, and 60% in trust. These findings\nhighlight the promise of LLM-driven AI systems in providing domain-specific\nsupport for construction workers.", "authors": ["Fan Yang", "Yuan Tian", "Jiansong Zhang"], "published_date": "2025-06-09", "title_zh": "運用多代理人會話式人工智慧系統支持建築工人的福祉", "summary_zh": "營建業具高身心風險，然心理健康支持有限。人工智慧（AI）及大型語言模型（LLM）雖具潛力，在營建業應用仍待開發。本研究開發一對話式多代理人系統，整合領域知識，以AI驅動解決營建業特定挑戰，並透過多重人格代理人互動，滿足工人基本心理需求，提供實務問題解決及社交互動，提升整體福祉。針對12名受試者進行組內使用者研究，評估系統可用性與效能，結果顯示本系統顯著優於單一代理人基準線，可用性提升18%，自主性提升40%，社交臨場感提升60%，信任度提升60%。研究結果凸顯LLM驅動之AI系統在為營建工人提供領域特定支持方面的潛力。", "audio": "audios/2506.07997v1.mp3", "timestamp": "2025-06-10T05:19:07.468226"}
{"query": "Foundation Model", "id": "2506.07886v1", "url": "http://arxiv.org/abs/2506.07886v1", "title": "EgoM2P: Egocentric Multimodal Multitask Pretraining", "summary": "Understanding multimodal signals in egocentric vision, such as RGB video,\ndepth, camera poses, and gaze, is essential for applications in augmented\nreality, robotics, and human-computer interaction. These capabilities enable\nsystems to better interpret the camera wearer's actions, intentions, and\nsurrounding environment. However, building large-scale egocentric multimodal\nand multitask models presents unique challenges. Egocentric data are inherently\nheterogeneous, with large variations in modality coverage across devices and\nsettings. Generating pseudo-labels for missing modalities, such as gaze or\nhead-mounted camera trajectories, is often infeasible, making standard\nsupervised learning approaches difficult to scale. Furthermore, dynamic camera\nmotion and the complex temporal and spatial structure of first-person video\npose additional challenges for the direct application of existing multimodal\nfoundation models.\n  To address these challenges, we introduce a set of efficient temporal\ntokenizers and propose EgoM2P, a masked modeling framework that learns from\ntemporally aware multimodal tokens to train a large, general-purpose model for\negocentric 4D understanding. This unified design supports multitasking across\ndiverse egocentric perception and synthesis tasks, including gaze prediction,\negocentric camera tracking, and monocular depth estimation from egocentric\nvideo. EgoM2P also serves as a generative model for conditional egocentric\nvideo synthesis. Across these tasks, EgoM2P matches or outperforms specialist\nmodels while being an order of magnitude faster. We will fully open-source\nEgoM2P to support the community and advance egocentric vision research. Project\npage: https://egom2p.github.io/", "authors": ["Gen Li", "Yutong Chen", "Yiqian Wu", "Kaifeng Zhao", "Marc Pollefeys", "Siyu Tang"], "published_date": "2025-06-09", "title_zh": "自我中心多模態多任務預訓練", "summary_zh": "理解以自我為中心的視覺中的多模態訊號，如RGB影片、深度、相機姿態和視線，對於擴增實境、機器人學和人機互動至關重要。建立大規模自我中心多模態和多任務模型面臨獨特挑戰，例如數據異質性以及難以生成缺失模態的偽標籤。動態相機運動和第一人稱視訊的複雜時空結構也增加了直接應用現有多模態基礎模型的難度。\n\n為了解決這些挑戰，我們引入了一組高效的時間分詞器，並提出EgoM2P，一種基於遮蔽建模框架，可從具有時間意識的多模態token中學習，以訓練用於自我中心4D理解的大型通用模型。此統一設計支援跨多種自我中心感知和合成任務的多任務處理，包括視線預測、自我中心相機追蹤和從自我中心視訊進行的單眼深度估計。EgoM2P亦可作為條件自我中心視訊合成的生成模型。在這些任務中，EgoM2P在速度上領先專用模型一個數量級，同時性能與之相當或更優。我們將完全開源EgoM2P，以支援社群並推動自我中心視覺研究。", "audio": "audios/2506.07886v1.mp3", "timestamp": "2025-06-10T05:19:15.562649"}
{"query": "Diffusion Model", "id": "2506.08009v1", "url": "http://arxiv.org/abs/2506.08009v1", "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion", "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/", "authors": ["Xun Huang", "Zhengqi Li", "Guande He", "Mingyuan Zhou", "Eli Shechtman"], "published_date": "2025-06-09", "title_zh": "自強制：彌合自迴歸視訊擴散中的訓練-測試差距", "summary_zh": "自激式訓練是一種自迴歸影片擴散模型的新穎訓練範例。它解決了暴露偏差問題，即模型在真實資料情境下訓練後，必須在推論時根據自身不完善的輸出生成序列。與先前基於真實情境幀對未來幀進行去噪的方法不同，自激式訓練通過在訓練期間執行帶有鍵值（KV）快取的自迴歸展開，根據先前自身生成的輸出調節每個幀的生成。這種策略能夠通過影片層級的整體損失進行監督，直接評估整個生成序列的品質，而非僅依賴傳統的逐幀目標。為確保訓練效率，我們採用了幾步擴散模型以及隨機梯度截斷策略，有效平衡了計算成本和效能。我們進一步引入了滾動KV快取機制，實現高效的自迴歸影片外推。大量實驗表明，我們的方法在一張GPU上實現了亞秒級延遲的即時串流影片生成，同時達到甚至超越了速度較慢且非因果擴散模型的生成品質。", "audio": "audios/2506.08009v1.mp3", "timestamp": "2025-06-10T05:19:22.512839"}
{"query": "AI", "id": "2506.07982v1", "url": "http://arxiv.org/abs/2506.07982v1", "title": "$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment", "summary": "Existing benchmarks for conversational AI agents simulate single-control\nenvironments, where only the AI agent can use tools to interact with the world,\nwhile the user remains a passive information provider. This differs from\nreal-world scenarios like technical support, where users need to actively\nparticipate in modifying the state of the (shared) world. In order to address\nthis gap, we introduce $\\tau^2$-bench, with four key contributions:\n  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both\nagent and user make use of tools to act in a shared, dynamic environment that\ntests both agent coordination and communication,\n  2) A compositional task generator that programmatically creates diverse,\nverifiable tasks from atomic components, ensuring domain coverage and\ncontrolled complexity,\n  3) A reliable user simulator tightly coupled with the environment, whose\nbehavior is constrained by tools and observable states, improving simulation\nfidelity,\n  4) Fine-grained analysis of agent performance through multiple ablations\nincluding separating errors arising from reasoning vs\ncommunication/coordination.\n  In particular, our experiments show significant performance drops when agents\nshift from no-user to dual-control, highlighting the challenges of guiding\nusers. Overall, $\\tau^2$-bench provides a controlled testbed for agents that\nmust both reason effectively and guide user actions.", "authors": ["Victor Barres", "Honghua Dong", "Soham Ray", "Xujie Si", "Karthik Narasimhan"], "published_date": "2025-06-09", "title_zh": "$τ^2$-Bench：雙重控制環境下對話型代理的評估", "summary_zh": "現有會話式AI代理的評測基準模擬單一控制環境，僅AI代理可使用工具與世界互動，使用者被動提供資訊。為解決此與技術支援等現實場景的差異，我們提出$\\tau^2$-bench，其包含：一、電信雙重控制領域，建模為Dec-POMDP，代理和使用者均利用工具在共享動態環境中行動，測試協調與溝通；二、組合式任務產生器，以程式化方式從原子組件創建多樣化、可驗證的任務，確保領域覆蓋和受控複雜性；三、可靠的使用者模擬器，與環境緊密耦合，其行為受工具和可觀察狀態約束，提升模擬真實度；四、細粒度代理效能分析，透過多種消融研究，區分推理與溝通/協調產生的錯誤。實驗顯示，代理從無使用者到雙重控制時效能顯著下降，突顯引導使用者的挑戰。$\\tau^2$-bench為有效推理並引導使用者行動的代理提供受控的測試平台。", "audio": "audios/2506.07982v1.mp3", "timestamp": "2025-06-10T08:27:47.081652"}
{"query": "Foundation Model", "id": "2506.07740v1", "url": "http://arxiv.org/abs/2506.07740v1", "title": "Flow-Anything: Learning Real-World Optical Flow Estimation from Large-Scale Single-view Images", "summary": "Optical flow estimation is a crucial subfield of computer vision, serving as\na foundation for video tasks. However, the real-world robustness is limited by\nanimated synthetic datasets for training. This introduces domain gaps when\napplied to real-world applications and limits the benefits of scaling up\ndatasets. To address these challenges, we propose \\textbf{Flow-Anything}, a\nlarge-scale data generation framework designed to learn optical flow estimation\nfrom any single-view images in the real world. We employ two effective steps to\nmake data scaling-up promising. First, we convert a single-view image into a 3D\nrepresentation using advanced monocular depth estimation networks. This allows\nus to render optical flow and novel view images under a virtual camera. Second,\nwe develop an Object-Independent Volume Rendering module and a Depth-Aware\nInpainting module to model the dynamic objects in the 3D representation. These\ntwo steps allow us to generate realistic datasets for training from large-scale\nsingle-view images, namely \\textbf{FA-Flow Dataset}. For the first time, we\ndemonstrate the benefits of generating optical flow training data from\nlarge-scale real-world images, outperforming the most advanced unsupervised\nmethods and supervised methods on synthetic datasets. Moreover, our models\nserve as a foundation model and enhance the performance of various downstream\nvideo tasks.", "authors": ["Yingping Liang", "Ying Fu", "Yutao Hu", "Wenqi Shao", "Jiaming Liu", "Debing Zhang"], "published_date": "2025-06-09", "title_zh": "流動萬物：基於大規模單視圖圖像學習真實世界光流估計", "summary_zh": "光流估計是電腦視覺的關鍵領域，但動畫合成資料集的訓練限制了其真實世界的穩健性，導致領域差距。為了解決此問題，我們提出 Flow-Anything，一種大規模資料生成框架，旨在從真實世界的單視圖圖像中學習光流估計。該框架首先利用單眼深度估計網路將單視圖圖像轉換為3D表示，進而渲染光流和新視圖圖像。其次，開發物體無關體積渲染模組和深度感知修復模組，以模擬3D表示中的動態物體。由此生成FA-Flow資料集，實現從大規模真實世界圖像中生成訓練資料。實驗證明，該方法優於最先進的無監督和合成資料集上的監督方法，並可作為基礎模型增強各種下游影片任務的效能。", "audio": "audios/2506.07740v1.mp3", "timestamp": "2025-06-10T08:27:53.078295"}
{"query": "Diffusion Model", "id": "2506.08004v1", "url": "http://arxiv.org/abs/2506.08004v1", "title": "Dynamic View Synthesis as an Inverse Problem", "summary": "In this work, we address dynamic view synthesis from monocular videos as an\ninverse problem in a training-free setting. By redesigning the noise\ninitialization phase of a pre-trained video diffusion model, we enable\nhigh-fidelity dynamic view synthesis without any weight updates or auxiliary\nmodules. We begin by identifying a fundamental obstacle to deterministic\ninversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and\nresolve it by introducing a novel noise representation, termed K-order\nRecursive Noise Representation. We derive a closed form expression for this\nrepresentation, enabling precise and efficient alignment between the\nVAE-encoded and the DDIM inverted latents. To synthesize newly visible regions\nresulting from camera motion, we introduce Stochastic Latent Modulation, which\nperforms visibility aware sampling over the latent space to complete occluded\nregions. Comprehensive experiments demonstrate that dynamic view synthesis can\nbe effectively performed through structured latent manipulation in the noise\ninitialization phase.", "authors": ["Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-06-09", "title_zh": "動態視圖合成：作為一個逆問題", "summary_zh": "本研究探討單目影片的動態視圖合成，視為免訓練的反問題。透過重新設計預訓練視訊擴散模型的雜訊初始化階段，無需權重更新或輔助模組即可實現高保真動態視圖合成。研究首先指出零終端信噪比排程對確定性反演構成阻礙，並透過一種名為K階遞迴雜訊表示的新穎雜訊表示法解決此問題。導出此表示法的閉合形式，實現VAE編碼潛在變量與DDIM反演潛在變量之間的精確高效對齊。為合成相機運動產生的新可見區域，引入隨機潛在調製，執行可見性感知的潛在空間採樣以完成遮蔽區域。實驗結果表明，透過雜訊初始化階段的結構化潛在變量操作，可有效執行動態視圖合成。", "audio": "audios/2506.08004v1.mp3", "timestamp": "2025-06-10T08:27:58.380020"}
{"query": "AI", "id": "2506.07957v1", "url": "http://arxiv.org/abs/2506.07957v1", "title": "Understanding the Error Sensitivity of Privacy-Aware Computing", "summary": "Homomorphic Encryption (HE) enables secure computation on encrypted data\nwithout decryption, allowing a great opportunity for privacy-preserving\ncomputation. In particular, domains such as healthcare, finance, and\ngovernment, where data privacy and security are of utmost importance, can\nbenefit from HE by enabling third-party computation and services on sensitive\ndata. In other words, HE constitutes the \"Holy Grail\" of cryptography: data\nremains encrypted all the time, being protected while in use.\n  HE's security guarantees rely on noise added to data to make relatively\nsimple problems computationally intractable. This error-centric intrinsic HE\nmechanism generates new challenges related to the fault tolerance and\nrobustness of HE itself: hardware- and software-induced errors during HE\noperation can easily evade traditional error detection and correction\nmechanisms, resulting in silent data corruption (SDC).\n  In this work, we motivate a thorough discussion regarding the sensitivity of\nHE applications to bit faults and provide a detailed error characterization\nstudy of CKKS (Cheon-Kim-Kim-Song). This is one of the most popular HE schemes\ndue to its fixed-point arithmetic support for AI and machine learning\napplications. We also delve into the impact of the residue number system (RNS)\nand the number theoretic transform (NTT), two widely adopted HE optimization\ntechniques, on CKKS' error sensitivity. To the best of our knowledge, this is\nthe first work that looks into the robustness and error sensitivity of\nhomomorphic encryption and, as such, it can pave the way for critical future\nwork in this area.", "authors": ["Matías Mazzanti", "Esteban Mocskos", "Augusto Vega", "Pradip Bose"], "published_date": "2025-06-09", "title_zh": "理解隱私感知計算的誤差敏感性", "summary_zh": "同態加密技術可在無需解密的情況下安全運算加密數據，為隱私保護運算提供重大機會。醫療、金融與政府等數據隱私至關重要的領域，可藉由同態加密在敏感數據上進行第三方運算與服務。同態加密的安全性依賴於添加到數據中的噪聲，使相對簡單的問題在計算上變得難以處理。這種以錯誤為中心的內在機制產生了與容錯和同態加密本身穩健性相關的新挑戰：同態加密運算期間的硬體或軟體錯誤可能輕易繞過傳統的錯誤檢測與校正機制，導致靜默數據損壞。本文旨在深入探討同態加密應用對位元錯誤的敏感性，並針對CKKS方案進行詳細的錯誤特性研究。CKKS因其對人工智慧和機器學習應用的定點算術支持而廣受歡迎。此外，本文還深入研究了餘數系統（RNS）和數論變換（NTT）這兩種廣泛採用的同態加密優化技術對CKKS錯誤敏感性的影響。據我們所知，這是首個探討同態加密穩健性和錯誤敏感性的研究，可為該領域未來的關鍵研究奠定基礎。", "audio": "audios/2506.07957v1.mp3", "timestamp": "2025-06-10T09:21:13.228482"}
{"query": "Foundation Model", "id": "2506.07647v1", "url": "http://arxiv.org/abs/2506.07647v1", "title": "Foundation Model Empowered Synesthesia of Machines (SoM): AI-native Intelligent Multi-Modal Sensing-Communication Integration", "summary": "To support future intelligent multifunctional sixth-generation (6G) wireless\ncommunication networks, Synesthesia of Machines (SoM) is proposed as a novel\nparadigm for artificial intelligence (AI)-native intelligent multi-modal\nsensing-communication integration. However, existing SoM system designs rely on\ntask-specific AI models and face challenges such as scarcity of massive\nhigh-quality datasets, constrained modeling capability, poor generalization,\nand limited universality. Recently, foundation models (FMs) have emerged as a\nnew deep learning paradigm and have been preliminarily applied to SoM-related\ntasks, but a systematic design framework is still lacking. In this paper, we\nfor the first time present a systematic categorization of FMs for SoM system\ndesign, dividing them into general-purpose FMs, specifically large language\nmodels (LLMs), and SoM domain-specific FMs, referred to as wireless foundation\nmodels. Furthermore, we derive key characteristics of FMs in addressing\nexisting challenges in SoM systems and propose two corresponding roadmaps,\ni.e., LLM-based and wireless foundation model-based design. For each roadmap,\nwe provide a framework containing key design steps as a guiding pipeline and\nseveral representative case studies of FM-empowered SoM system design.\nSpecifically, we propose LLM-based path loss generation (LLM4PG) and scatterer\ngeneration (LLM4SG) schemes, and wireless channel foundation model (WiCo) for\nSoM mechanism exploration, LLM-based wireless multi-task SoM transceiver\n(LLM4WM) and wireless foundation model (WiFo) for SoM-enhanced transceiver\ndesign, and wireless cooperative perception foundation model (WiPo) for\nSoM-enhanced cooperative perception, demonstrating the significant superiority\nof FMs over task-specific models. Finally, we summarize and highlight potential\ndirections for future research.", "authors": ["Xiang Cheng", "Boxun Liu", "Xuanyu Liu", "Ensong Liu", "Ziwei Huang"], "published_date": "2025-06-09", "title_zh": "基礎模型賦能之機器聯覺(SoM): AI原生智慧型多模態感知-通信融合", "summary_zh": "為支持未來具備多功能的第六代(6G)無線通訊網路，機器共感(SoM)被提出作為一種新穎的人工智慧原生智慧型多模感測-通訊整合範式。然而，現有SoM系統設計依賴特定任務的人工智慧模型，面臨大規模高品質數據集稀缺、建模能力受限、泛化性差和通用性有限等挑戰。近年來，基石模型(FMs)作為一種新的深度學習範式興起，並已初步應用於SoM相關任務，但仍缺乏系統性的設計框架。本文首次針對SoM系統設計提出基石模型的系統分類，將其分為通用基石模型(特別是大型語言模型LLMs)和SoM領域特定基石模型(即無線基石模型)。此外，本文推導出基石模型在解決SoM系統現有挑戰方面的關鍵特性，並提出兩個相應的路線圖，即基於LLM和基於無線基石模型的設計。針對每個路線圖，本文提供包含關鍵設計步驟的框架作為指導流程，並列舉了幾個基石模型賦能的SoM系統設計的代表性案例研究。具體而言，本文提出了基於LLM的路徑損耗生成(LLM4PG)和散射體生成(LLM4SG)方案，以及用於SoM機制探索的無線通道基石模型(WiCo)、用於SoM增強型收發器設計的基於LLM的無線多任務SoM收發器(LLM4WM)和無線基石模型(WiFo)，以及用於SoM增強型協同感知的無線協同感知基石模型(WiPo)，展示了基石模型相較於特定任務模型的顯著優勢。最後，本文總結並強調了未來研究的潛在方向。", "audio": "audios/2506.07647v1.mp3", "timestamp": "2025-06-10T09:21:24.494689"}
{"query": "Diffusion Model", "id": "2506.07999v1", "url": "http://arxiv.org/abs/2506.07999v1", "title": "MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation", "summary": "Recent progress in multimodal generation has increasingly combined\nautoregressive (AR) and diffusion-based approaches, leveraging their\ncomplementary strengths: AR models capture long-range dependencies and produce\nfluent, context-aware outputs, while diffusion models operate in continuous\nlatent spaces to refine high-fidelity visual details. However, existing hybrids\noften lack systematic guidance on how and why to allocate model capacity\nbetween these paradigms. In this work, we introduce MADFormer, a Mixed\nAutoregressive and Diffusion Transformer that serves as a testbed for analyzing\nAR-diffusion trade-offs. MADFormer partitions image generation into spatial\nblocks, using AR layers for one-pass global conditioning across blocks and\ndiffusion layers for iterative local refinement within each block. Through\ncontrolled experiments on FFHQ-1024 and ImageNet, we identify two key insights:\n(1) block-wise partitioning significantly improves performance on\nhigh-resolution images, and (2) vertically mixing AR and diffusion layers\nyields better quality-efficiency balances--improving FID by up to 75% under\nconstrained inference compute. Our findings offer practical design principles\nfor future hybrid generative models.", "authors": ["Junhao Chen", "Yulia Tsvetkov", "Xiaochuang Han"], "published_date": "2025-06-09", "title_zh": "MADFormer：混合自迴歸與擴散Transformer用於連續圖像生成", "summary_zh": "多模態生成領域的新進展日益結合自迴歸(AR)和基於擴散的方法，利用它們的互補優勢：AR模型捕捉長程依賴關係並產生流暢、具上下文意識的輸出，而擴散模型在連續潛在空間中運行以完善高保真視覺細節。然而，現有的混合模型通常缺乏關於如何在這些範例之間分配模型容量的系統性指導。本研究介紹MADFormer，一種混合自迴歸和擴散Transformer，作為分析AR-擴散權衡的測試平台。MADFormer將圖像生成劃分為空間塊，使用AR層對跨塊進行一次性全局條件調整，並使用擴散層在每個塊內進行迭代局部細化。通過對FFHQ-1024和ImageNet的受控實驗，我們確定了兩個關鍵見解：(1)分塊顯著提高了高分辨率圖像的性能，以及(2)垂直混合AR和擴散層產生了更好的質量-效率平衡，在受限的推論計算下將FID提高了高達75%。我們的研究結果為未來的混合生成模型提供了實用的設計原則。", "audio": "audios/2506.07999v1.mp3", "timestamp": "2025-06-10T09:21:31.106344"}
{"query": "AI", "id": "2506.07955v1", "url": "http://arxiv.org/abs/2506.07955v1", "title": "Implementation Considerations for Automated AI Grading of Student Work", "summary": "This study explores the classroom implementation of an AI-powered grading\nplatform in K-12 settings through a co-design pilot with 19 teachers. We\ncombine platform usage logs, surveys, and qualitative interviews to examine how\nteachers use AI-generated rubrics and grading feedback. Findings reveal that\nwhile teachers valued the AI's rapid narrative feedback for formative purposes,\nthey distrusted automated scoring and emphasized the need for human oversight.\nStudents welcomed fast, revision-oriented feedback but remained skeptical of\nAI-only grading. We discuss implications for the design of trustworthy,\nteacher-centered AI assessment tools that enhance feedback while preserving\npedagogical agency.", "authors": ["Zewei", "Tian", "Alex Liu", "Lief Esbenshade", "Shawon Sarkar", "Zachary Zhang", "Kevin He", "Min Sun"], "published_date": "2025-06-09", "title_zh": "學生作業自動AI評分的實施考量", "summary_zh": "本研究探討AI驅動的評分平台在K-12課堂中的應用，透過與19位教師的共同設計試驗，結合平台使用日誌、問卷和訪談，檢視教師如何運用AI產生的評分標準與回饋。研究發現，教師重視AI快速的敘事性回饋以促進形成性評估，但對自動評分抱持懷疑，強調人工監督的必要性。學生歡迎快速、以修改為導向的回饋，但對完全由AI評分感到質疑。研究結果對設計可信賴、以教師為中心的AI評估工具有啟示作用，這些工具應能強化回饋並維護教學自主性。", "audio": "audios/2506.07955v1.mp3", "timestamp": "2025-06-10T10:21:05.257650"}
{"query": "Foundation Model", "id": "2506.07603v1", "url": "http://arxiv.org/abs/2506.07603v1", "title": "SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis", "summary": "Surgical video understanding is pivotal for enabling automated intraoperative\ndecision-making, skill assessment, and postoperative quality improvement.\nHowever, progress in developing surgical video foundation models (FMs) remains\nhindered by the scarcity of large-scale, diverse datasets for pretraining and\nsystematic evaluation. In this paper, we introduce \\textbf{SurgBench}, a\nunified surgical video benchmarking framework comprising a pretraining dataset,\n\\textbf{SurgBench-P}, and an evaluation benchmark, \\textbf{SurgBench-E}.\nSurgBench offers extensive coverage of diverse surgical scenarios, with\nSurgBench-P encompassing 53 million frames across 22 surgical procedures and 11\nspecialties, and SurgBench-E providing robust evaluation across six categories\n(phase classification, camera motion, tool recognition, disease diagnosis,\naction classification, and organ detection) spanning 72 fine-grained tasks.\nExtensive experiments reveal that existing video FMs struggle to generalize\nacross varied surgical video analysis tasks, whereas pretraining on SurgBench-P\nyields substantial performance improvements and superior cross-domain\ngeneralization to unseen procedures and modalities. Our dataset and code are\navailable upon request.", "authors": ["Jianhui Wei", "Zikai Xiao", "Danyu Sun", "Luqi Gong", "Zongxin Yang", "Zuozhu Liu", "Jian Wu"], "published_date": "2025-06-09", "title_zh": "SurgBench：外科手術影像分析之整合型大規模基準測試", "summary_zh": "手術影片理解對於實現術中自動決策、技能評估及術後品質提升至關重要。然而，手術影片基礎模型開發的進展受限於大規模、多樣化預訓練及系統性評估資料集的匱乏。本文提出SurgBench，一個整合式手術影片基準測試框架，包含預訓練資料集SurgBench-P與評估基準SurgBench-E。SurgBench提供廣泛的手術場景覆蓋，SurgBench-P包含22種手術程序和11個專科的5300萬幀畫面，而SurgBench-E提供跨六大類別（階段分類、鏡頭運動、工具識別、疾病診斷、動作分類及器官檢測）共72個細粒度任務的穩健評估。大量實驗表明，現有影片基礎模型難以泛化至不同的手術影片分析任務，而於SurgBench-P上進行預訓練可顯著提升效能，並對未見程序及模態展現卓越的跨領域泛化能力。資料集與程式碼可依需求提供。", "audio": "audios/2506.07603v1.mp3", "timestamp": "2025-06-10T10:21:12.350108"}
{"query": "Diffusion Model", "id": "2506.07998v1", "url": "http://arxiv.org/abs/2506.07998v1", "title": "Generative Modeling of Weights: Generalization or Memorization?", "summary": "Generative models, with their success in image and video generation, have\nrecently been explored for synthesizing effective neural network weights. These\napproaches take trained neural network checkpoints as training data, and aim to\ngenerate high-performing neural network weights during inference. In this work,\nwe examine four representative methods on their ability to generate novel model\nweights, i.e., weights that are different from the checkpoints seen during\ntraining. Surprisingly, we find that these methods synthesize weights largely\nby memorization: they produce either replicas, or at best simple\ninterpolations, of the training checkpoints. Current methods fail to outperform\nsimple baselines, such as adding noise to the weights or taking a simple weight\nensemble, in obtaining different and simultaneously high-performing models. We\nfurther show that this memorization cannot be effectively mitigated by\nmodifying modeling factors commonly associated with memorization in image\ndiffusion models, or applying data augmentations. Our findings provide a\nrealistic assessment of what types of data current generative models can model,\nand highlight the need for more careful evaluation of generative models in new\ndomains. Our code is available at\nhttps://github.com/boyazeng/weight_memorization.", "authors": ["Boya Zeng", "Yida Yin", "Zhiqiu Xu", "Zhuang Liu"], "published_date": "2025-06-09", "title_zh": "權重生成模型：泛化還是記憶？", "summary_zh": "生成模型在影像和影片生成領域取得成功，近期也被用於合成有效的神經網路權重。這些方法以訓練過的神經網路檢查點作為訓練資料，旨在於推論時生成高效能的權重。本研究檢驗了四種具代表性的方法，評估其生成新穎模型權重的能力，亦即不同於訓練時所見檢查點的權重。研究發現，這些方法主要透過記憶來合成權重，產生訓練檢查點的複製品或簡單的插值。在獲得不同且高效能的模型方面，現有方法未能優於簡單的基線，例如對權重添加雜訊或採用簡單的權重集成。此外，即使修改影像擴散模型中與記憶相關的建模因素或應用資料增強，也無法有效減輕這種記憶現象。研究結果對當前生成模型可以建模的資料類型提供了實際評估，並強調需要在新領域更仔細地評估生成模型。", "audio": "audios/2506.07998v1.mp3", "timestamp": "2025-06-10T10:21:18.072200"}
{"query": "AI", "id": "2506.07949v1", "url": "http://arxiv.org/abs/2506.07949v1", "title": "Cost-Optimal Active AI Model Evaluation", "summary": "The development lifecycle of generative AI systems requires continual\nevaluation, data acquisition, and annotation, which is costly in both resources\nand time. In practice, rapid iteration often makes it necessary to rely on\nsynthetic annotation data because of the low cost, despite the potential for\nsubstantial bias. In this paper, we develop novel, cost-aware methods for\nactively balancing the use of a cheap, but often inaccurate, weak rater -- such\nas a model-based autorater that is designed to automatically assess the quality\nof generated content -- with a more expensive, but also more accurate, strong\nrater alternative such as a human. More specifically, the goal of our approach\nis to produce a low variance, unbiased estimate of the mean of the target\n\"strong\" rating, subject to some total annotation budget. Building on recent\nwork in active and prediction-powered statistical inference, we derive a family\nof cost-optimal policies for allocating a given annotation budget between weak\nand strong raters so as to maximize statistical efficiency. Using synthetic and\nreal-world data, we empirically characterize the conditions under which these\npolicies yield improvements over prior methods. We find that, especially in\ntasks where there is high variability in the difficulty of examples, our\npolicies can achieve the same estimation precision at a far lower total\nannotation budget than standard evaluation methods.", "authors": ["Anastasios N. Angelopoulos", "Jacob Eisenstein", "Jonathan Berant", "Alekh Agarwal", "Adam Fisch"], "published_date": "2025-06-09", "title_zh": "成本最佳化主動式人工智慧模型評估", "summary_zh": "生成式AI系統開發週期需持續評估、資料獲取與標註，耗費資源與時間。實務上，為求快速迭代，常因成本考量而仰賴具偏差風險的合成標註資料。本文研發成本感知方法，有效平衡低成本但常不準確的弱評估者（如基於模型的自動評估器）與高成本但較準確的強評估者（如人類）。目標是在總標註預算限制下，產出目標強評估平均值的低變異、無偏估計。基於主動與預測驅動統計推論，推導出一系列成本最佳化策略，用於分配弱評估與強評估的標註預算，以最大化統計效率。透過合成與真實世界資料，驗證這些策略在何種條件下優於既有方法。研究發現，尤其在範例難度差異大的任務中，相較於標準評估方法，我們的策略能以更低的總標註預算達成相同的估計精度。", "audio": "audios/2506.07949v1.mp3", "timestamp": "2025-06-10T11:17:04.318710"}
{"query": "Foundation Model", "id": "2506.07584v1", "url": "http://arxiv.org/abs/2506.07584v1", "title": "MIRA: Medical Time Series Foundation Model for Real-World Health Data", "summary": "A unified foundation model for medical time series -- pretrained on open\naccess and ethics board-approved medical corpora -- offers the potential to\nreduce annotation burdens, minimize model customization, and enable robust\ntransfer across clinical institutions, modalities, and tasks, particularly in\ndata-scarce or privacy-constrained environments. However, existing generalist\ntime series foundation models struggle to handle medical time series data due\nto their inherent challenges, including irregular intervals, heterogeneous\nsampling rates, and frequent missing values. To address these challenges, we\nintroduce MIRA, a unified foundation model specifically designed for medical\ntime series forecasting. MIRA incorporates a Continuous-Time Rotary Positional\nEncoding that enables fine-grained modeling of variable time intervals, a\nfrequency-specific mixture-of-experts layer that routes computation across\nlatent frequency regimes to further promote temporal specialization, and a\nContinuous Dynamics Extrapolation Block based on Neural ODE that models the\ncontinuous trajectory of latent states, enabling accurate forecasting at\narbitrary target timestamps. Pretrained on a large-scale and diverse medical\ncorpus comprising over 454 billion time points collect from publicly available\ndatasets, MIRA achieves reductions in forecasting errors by an average of 10%\nand 7% in out-of-distribution and in-distribution scenarios, respectively, when\ncompared to other zero-shot and fine-tuned baselines. We also introduce a\ncomprehensive benchmark spanning multiple downstream clinical tasks,\nestablishing a foundation for future research in medical time series modeling.", "authors": ["Hao Li", "Bowen Deng", "Chang Xu", "Zhiyuan Feng", "Viktor Schlegel", "Yu-Hao Huang", "Yizheng Sun", "Jingyuan Sun", "Kailai Yang", "Yiyao Yu", "Jiang Bian"], "published_date": "2025-06-09", "title_zh": "MIRA：用於真實世界健康數據的醫學時間序列基礎模型", "summary_zh": "針對醫療時間序列，在開放取用及倫理委員會批准的語料庫上預訓練的統一基礎模型，有潛力減少標註負擔、降低模型客製化需求，並在數據稀缺或隱私受限的環境中，實現跨臨床機構、模態和任務的穩健遷移。然而，現有的通用時間序列基礎模型難以處理醫療時間序列數據，因為其存在不規則間隔、異質採樣率和頻繁缺失值等內在挑戰。為了解決這些問題，我們提出了 MIRA，一個專為醫療時間序列預測設計的統一基礎模型。MIRA 結合了連續時間旋轉位置編碼，實現對可變時間間隔的精細建模；一個頻率特定的專家混合層，跨潛在頻率範圍引導計算，以進一步促進時間專精化；以及一個基於神經常微分方程的連續動力學外推區塊，對潛在狀態的連續軌跡進行建模，從而能夠在任意目標時間戳進行準確預測。MIRA 在包含來自公開數據集的超過 4540 億個時間點的大規模多樣化醫療語料庫上進行預訓練，與其他零樣本和微調基準模型相比，在異分布和同分布場景中，預測誤差平均降低了 10% 和 7%。我們還介紹了一個涵蓋多個下游臨床任務的綜合基準，為未來醫療時間序列建模的研究奠定了基礎。", "audio": "audios/2506.07584v1.mp3", "timestamp": "2025-06-10T11:17:13.700685"}
{"query": "Diffusion Model", "id": "2506.07986v1", "url": "http://arxiv.org/abs/2506.07986v1", "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers", "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose \\textbf{Temperature-Adjusted Cross-modal Attention\n(TACA)}, a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\n\\href{https://github.com/Vchitect/TACA}", "authors": ["Zhengyao Lv", "Tianlin Pan", "Chenyang Si", "Zhaoxi Chen", "Wangmeng Zuo", "Ziwei Liu", "Kwan-Yee K. Wong"], "published_date": "2025-06-09", "title_zh": "多模態擴散Transformer中跨模態互動之再思", "summary_zh": "多模態擴散轉換器(MM-DiT)在文本驅動的視覺生成方面表現出色，但即使如FLUX等頂尖模型，仍難以實現文本提示與生成內容間的精確對齊。研究發現MM-DiT注意力機制存在兩大問題：視覺與文本模態間的Token不平衡導致跨模態注意力被抑制，以及缺乏時間步感知的注意力權重，阻礙了對齊。為了解決這些問題，本研究提出參數高效的溫度調整跨模態注意力(TACA)，透過溫度縮放和時間步相關調整，動態地重新平衡多模態互動。結合LoRA微調，TACA在T2I-CompBench基準測試上顯著提升了文本圖像對齊效果，且計算開銷極小。在FLUX和SD3.5等模型上的測試表明，TACA能有效改善物件外觀、屬性綁定和空間關係等方面的圖像文本對齊。研究結果強調了平衡跨模態注意力在提升文本到圖像擴散模型語義保真度的重要性。", "audio": "audios/2506.07986v1.mp3", "timestamp": "2025-06-10T11:17:21.089046"}
{"query": "AI", "id": "2506.07907v1", "url": "http://arxiv.org/abs/2506.07907v1", "title": "A novel measurement of the strong-phase difference between $D^0\\to K^-π^+$ and $\\bar{D}^0\\to K^-π^+$ decays using $C$-even and $C$-odd quantum-correlated $D\\bar{D}$ pairs", "summary": "A novel measurement technique of strong-phase differences between between the\ndecay amplitudes of $D^0$ and $\\bar{D}^0$ mesons is introduced which exploits\nquantum-correlated $D\\bar{D}$ pairs produced by $e^+e^-$ collisions at energies\nabove the $\\psi(3770)$ production threshold, where $D\\bar{D}$ pairs are\nproduced in both even and odd eigenstates of the charge-conjugation symmetry.\nEmploying this technique, the first determination of a $D^0$-$\\bar{D^0}$\nrelative strong phase is reported with such data samples. The strong-phase\ndifference between $D^0\\to K^-\\pi^+$ and $\\bar{D}^0\\to K^-\\pi^+$ decays,\n$\\delta^{D}_{K\\pi}$, is measured to be $\\delta^{D}_{K\\pi}=\\left(192.8^{+11.0 +\n1.9}_{-12.4 -2.4}\\right)^\\circ$, using a dataset corresponding to an integrated\nluminosity of 7.13 $\\text{fb}^{-1}$ collected at center-of-mass energies\nbetween $4.13-4.23 \\text{ GeV}$ by the BESIII experiment.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "X. Y. Chen", "Y. B. Chen", "Y. Q. Chen", "Y. Q. Chen", "Z. Chen", "Z. J. Chen", "Z. K. Chen", "J. C. Cheng", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. Cottee-Meldrum", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De~Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "L. Feng", "Q. X. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "S. Garbolino", "I. Garzia", "L. Ge", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. K. Liu", "X. L. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. L. Ma", "Heng Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Q. A. Malik", "H. X. Mao", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "A. Marshall", "F. M. Melendi", "Y. H. Meng", "Z. X. Meng", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "C. Normand", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "X. J. Peng", "Y. Y. Peng", "K. Peters", "K. Petridis", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "J. Rademacker", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "F. Rosini", "Ch. Rosner", "M. Q. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "Zirong Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "J. J. Tang", "L. F. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. J. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "T. D. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. H. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Z. J. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "L. W. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "S. H. Yuan", "X. Q. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "Zhang", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Y. P. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "L. Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "C. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. X. Zhou", "Y. Z. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-06-09", "title_zh": "利用宇稱-正及宇稱-反量子關聯$D\\bar{D}$對測量$D^0\\to K^-π^+$與$\\bar{D}^0\\to K^-π^+$衰變間強相位差的新方法", "summary_zh": "提出一種新的強相位差測量技術，利用能量高於$\\psi(3770)$產生閾值的正負電子碰撞產生的量子關聯$D\\bar{D}$對，其中$D\\bar{D}$對產生於電荷共軛對稱性的奇偶本徵態。基於此技術，首次利用此類數據樣本測定$D^0$-$\\bar{D^0}$的相對強相位。利用BESIII實驗在質心能量4.13-4.23 GeV收集的7.13 $\\text{fb}^{-1}$積分亮度數據集，測得$D^0\\to K^-\\pi^+$和$\\bar{D}^0\\to K^-\\pi^+$衰變之間的強相位差$\\delta^{D}_{K\\pi}$為$\\delta^{D}_{K\\pi}=\\left(192.8^{+11.0 +\n1.9}_{-12.4 -2.4}\\right)^\\circ$。", "audio": "audios/2506.07907v1.mp3", "timestamp": "2025-06-10T12:39:21.629902"}
{"query": "Foundation Model", "id": "2506.07576v1", "url": "http://arxiv.org/abs/2506.07576v1", "title": "Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding", "summary": "Video understanding has been considered as one critical step towards world\nmodeling, which is an important long-term problem in AI research. Recently,\nmulti-modal foundation models have shown such potential via large-scale\npretraining. However, these models simply align encoders of different\nmodalities via contrastive learning, while lacking deeper multi-modal\ninteractions, which is critical for understanding complex target movements with\ndiversified video scenes. To fill this gap, we propose a unified Super Encoding\nNetwork (SEN) for video understanding, which builds up such distinct\ninteractions through recursive association of multi-modal encoders in the\nfoundation models. Specifically, we creatively treat those well-trained\nencoders as \"super neurons\" in our SEN. Via designing a Recursive Association\n(RA) block, we progressively fuse multi-modalities with the input video, based\non knowledge integrating, distributing, and prompting of super neurons in a\nrecursive manner. In this way, our SEN can effectively encode deeper\nmulti-modal interactions, for prompting various video understanding tasks in\ndownstream. Extensive experiments show that, our SEN can remarkably boost the\nfour most representative video tasks, including tracking, recognition,\nchatting, and editing, e.g., for pixel-level tracking, the average jaccard\nindex improves 2.7%, temporal coherence(TC) drops 8.8% compared to the popular\nCaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%,\nand frame consistency increases 4.1% compared to the popular TuneA-Video\napproach.", "authors": ["Boyu Chen", "Siran Chen", "Kunchang Li", "Qinglin Xu", "Yu Qiao", "Yali Wang"], "published_date": "2025-06-09", "title_zh": "超編碼網絡：用於視頻理解的多模態編碼器遞歸關聯", "summary_zh": "視訊理解是世界建模的關鍵一步，而世界建模是人工智慧研究中的重要長期問題。近期，多模態基礎模型透過大規模預訓練展現了潛力。然而，這些模型僅透過對比學習對齊不同模態的編碼器，缺乏更深層次的多模態互動，這對於理解具有多樣化視訊場景的複雜目標運動至關重要。為此，本文提出一種統一的超編碼網路(SEN)用於視訊理解，透過遞迴關聯基礎模型中的多模態編碼器來建立這種獨特的互動。具體而言，我們創新地將這些訓練有素的編碼器視為SEN中的超神經元。透過設計遞迴關聯(RA)區塊，我們基於知識整合、分發和提示，以遞迴方式逐步融合多模態與輸入視訊。如此一來，SEN可以有效地編碼更深層次的多模態互動，以促進下游各種視訊理解任務。大量實驗表明，SEN可以顯著提升四個最具代表性的視訊任務，包括追蹤、識別、聊天和編輯。例如，在像素級追蹤中，平均傑卡德指數提高了2.7%，時間一致性(TC)相較於流行的CaDeX++方法下降了8.8%。對於一次性視訊編輯，文本對齊相較於流行的TuneA-Video方法提高了6.4%，幀一致性提高了4.1%。", "audio": "audios/2506.07576v1.mp3", "timestamp": "2025-06-10T12:39:33.823058"}
{"query": "Diffusion Model", "id": "2506.07923v1", "url": "http://arxiv.org/abs/2506.07923v1", "title": "Efficient Seismic Data Interpolation via Sparse Attention Transformer and Diffusion Model", "summary": "Seismic data interpolation is a critical pre-processing step for improving\nseismic imaging quality and remains a focus of academic innovation. To address\nthe computational inefficiencies caused by extensive iterative resampling in\ncurrent plug-and-play diffusion interpolation methods, we propose the\ndiffusion-enhanced sparse attention transformer (Diff-spaformer), a novel deep\nlearning framework. Our model integrates transformer architectures and\ndiffusion models via a Seismic Prior Extraction Network (SPEN), which serves as\na bridge module. Full-layer sparse multi-head attention and feed-forward\npropagation capture global information distributions, while the diffusion model\nprovides robust prior guidance. To mitigate the computational burden of\nhigh-dimensional representations, self-attention is computed along the channel\nrather than the spatial dimension. We show that using negative squared\nEuclidean distance to compute sparse affinity matrices better suits seismic\ndata modeling, enabling broader contribution from amplitude feature nodes. An\nadaptive ReLU function further discards low or irrelevant self-attention\nvalues. We conduct training within a single-stage optimization framework,\nrequiring only a few reverse diffusion sampling steps during inference.\nExtensive experiments demonstrate improved interpolation fidelity and\ncomputational efficiency for both random and continuous missing data, offering\na new paradigm for high-efficiency seismic data reconstruction under complex\ngeological conditions.", "authors": ["Xiaoli Wei", "Chunxia Zhang", "Baisong Jiang", "Anxiang Di", "Deng Xiong", "Jiangshe Zhang", "Mingming Gong"], "published_date": "2025-06-09", "title_zh": "基於稀疏注意力變換器與擴散模型的地震資料高效內插", "summary_zh": "地震資料內插是提升地震成像品質的關鍵預處理步驟，學術界持續投入創新研究。為了解決現有隨插即用擴散內插法中，大量迭代重採樣導致的計算效率低下問題，我們提出了一種新型深度學習框架：擴散增強稀疏注意力轉換器(Diff-spaformer)。該模型透過地震先驗提取網絡(SPEN)整合轉換器架構與擴散模型，作為橋接模組。全層稀疏多頭注意力和前饋傳播捕捉全局信息分佈，而擴散模型提供穩健的先驗指導。為減輕高維表示的計算負擔，自注意力計算沿通道而非空間維度進行。我們證明，使用負平方歐幾里德距離計算稀疏親和力矩陣更適合地震資料建模，從而擴大了幅度特徵節點的貢獻。自適應ReLU函數進一步捨棄低或不相關的自注意力值。我們在單階段優化框架內進行訓練，僅需少量反向擴散採樣步驟即可完成推理。大量實驗表明，無論是隨機缺失資料還是連續缺失資料，該方法都能提升內插保真度和計算效率，為複雜地質條件下的高效地震資料重建提供了一種新範例。", "audio": "audios/2506.07923v1.mp3", "timestamp": "2025-06-10T12:39:42.826354"}
{"query": "AI", "id": "2506.07906v1", "url": "http://arxiv.org/abs/2506.07906v1", "title": "First observation of quantum correlations in $e^+e^-\\to XD\\bar{D}$ and $C$-even constrained $D\\bar{D}$ pairs", "summary": "The study of meson pairs produced with quantum correlations gives direct\naccess to parameters that are challenging to measure in other systems. In this\nLetter, the existence of quantum correlations due to charge-conjugation\nsymmetry $C$ are demonstrated in $D\\bar{D}$ pairs produced through the\nprocesses $e^+e^-\\to D\\bar{D}$, $e^+e^- \\to D^{*}\\bar{D}$, and $e^+e^- \\to\nD^{*} \\bar{D}^*$, where the lack of charge superscripts refers to an admixture\nof neutral-charm-meson particle and antiparticle states, using $7.13 \\text{\nfb}^{-1}$ of $e^+e^-$ collision data collected by the BESIII experiment between\ncenter-of-mass energies of $4.13-4.23 \\text{ GeV}$. Processes with either\n$C$-even or $C$-odd constraints are identified and separated. A procedure is\npresented that harnesses the entangled production process to enable\nmeasurements of $D^0$-meson hadronic parameters. This study provides the first\nconfirmation of quantum correlations in $e^+e^-\\to X D\\bar{D}$ processes and\nthe first observation of a $C$-even constrained $D\\bar{D}$ system. The\nprocedure is applied to measure $\\delta^{D}_{K\\pi}$, the strong phase between\nthe $D^0\\to K^-\\pi^+$ and $\\bar{D}^0\\to K^-\\pi^+$ decay amplitudes, which\nresults in the determination of $\\delta^{D}_{K\\pi}=\\left(192.8^{+11.0 +\n1.9}_{-12.4 -2.4}\\right)^\\circ$. The potential for measurements of other\nhadronic decay parameters and charm mixing with these and future datasets is\nalso discussed.", "authors": ["BESIII Collaboration", "M. Ablikim", "M. N. Achasov", "P. Adlarson", "X. C. Ai", "R. Aliberti", "A. Amoroso", "Q. An", "Y. Bai", "O. Bakina", "Y. Ban", "H. -R. Bao", "V. Batozskaya", "K. Begzsuren", "N. Berger", "M. Berlowski", "M. Bertani", "D. Bettoni", "F. Bianchi", "E. Bianco", "A. Bortone", "I. Boyko", "R. A. Briere", "A. Brueggemann", "H. Cai", "M. H. Cai", "X. Cai", "A. Calcaterra", "G. F. Cao", "N. Cao", "S. A. Cetin", "X. Y. Chai", "J. F. Chang", "G. R. Che", "Y. Z. Che", "C. H. Chen", "Chao Chen", "G. Chen", "H. S. Chen", "H. Y. Chen", "M. L. Chen", "S. J. Chen", "S. L. Chen", "S. M. Chen", "T. Chen", "X. R. Chen", "X. T. Chen", "X. Y. Chen", "Y. B. Chen", "Y. Q. Chen", "Y. Q. Chen", "Z. Chen", "Z. J. Chen", "Z. K. Chen", "J. C. Cheng", "S. K. Choi", "X. Chu", "G. Cibinetto", "F. Cossio", "J. Cottee-Meldrum", "J. J. Cui", "H. L. Dai", "J. P. Dai", "A. Dbeyssi", "R. E. de Boer", "D. Dedovich", "C. Q. Deng", "Z. Y. Deng", "A. Denig", "I. Denysenko", "M. Destefanis", "F. De Mori", "B. Ding", "X. X. Ding", "Y. Ding", "Y. Ding", "Y. X. Ding", "J. Dong", "L. Y. Dong", "M. Y. Dong", "X. Dong", "M. C. Du", "S. X. Du", "S. X. Du", "Y. Y. Duan", "Z. H. Duan", "P. Egorov", "G. F. Fan", "J. J. Fan", "Y. H. Fan", "J. Fang", "J. Fang", "S. S. Fang", "W. X. Fang", "Y. Q. Fang", "R. Farinelli", "L. Fava", "F. Feldbauer", "G. Felici", "C. Q. Feng", "J. H. Feng", "L. Feng", "Q. X. Feng", "Y. T. Feng", "M. Fritsch", "C. D. Fu", "J. L. Fu", "Y. W. Fu", "H. Gao", "X. B. Gao", "Y. Gao", "Y. N. Gao", "Y. N. Gao", "Y. Y. Gao", "S. Garbolino", "I. Garzia", "L. Ge", "P. T. Ge", "Z. W. Ge", "C. Geng", "E. M. Gersabeck", "A. Gilman", "K. Goetzen", "J. D. Gong", "L. Gong", "W. X. Gong", "W. Gradl", "S. Gramigna", "M. Greco", "M. H. Gu", "Y. T. Gu", "C. Y. Guan", "A. Q. Guo", "L. B. Guo", "M. J. Guo", "R. P. Guo", "Y. P. Guo", "A. Guskov", "J. Gutierrez", "K. L. Han", "T. T. Han", "F. Hanisch", "K. D. Hao", "X. Q. Hao", "F. A. Harris", "K. K. He", "K. L. He", "F. H. Heinsius", "C. H. Heinz", "Y. K. Heng", "C. Herold", "P. C. Hong", "G. Y. Hou", "X. T. Hou", "Y. R. Hou", "Z. L. Hou", "H. M. Hu", "J. F. Hu", "Q. P. Hu", "S. L. Hu", "T. Hu", "Y. Hu", "Z. M. Hu", "G. S. Huang", "K. X. Huang", "L. Q. Huang", "P. Huang", "X. T. Huang", "Y. P. Huang", "Y. S. Huang", "T. Hussain", "N. Hüsken", "N. in der Wiesche", "J. Jackson", "Q. Ji", "Q. P. Ji", "W. Ji", "X. B. Ji", "X. L. Ji", "Y. Y. Ji", "Z. K. Jia", "D. Jiang", "H. B. Jiang", "P. C. Jiang", "S. J. Jiang", "T. J. Jiang", "X. S. Jiang", "Y. Jiang", "J. B. Jiao", "J. K. Jiao", "Z. Jiao", "S. Jin", "Y. Jin", "M. Q. Jing", "X. M. Jing", "T. Johansson", "S. Kabana", "N. Kalantar-Nayestanaki", "X. L. Kang", "X. S. Kang", "M. Kavatsyuk", "B. C. Ke", "V. Khachatryan", "A. Khoukaz", "R. Kiuchi", "O. B. Kolcu", "B. Kopf", "M. Kuessner", "X. Kui", "N. Kumar", "A. Kupsc", "W. Kühn", "Q. Lan", "W. N. Lan", "T. T. Lei", "M. Lellmann", "T. Lenz", "C. Li", "C. Li", "C. H. Li", "C. K. Li", "D. M. Li", "F. Li", "G. Li", "H. B. Li", "H. J. Li", "H. N. Li", "Hui Li", "J. R. Li", "J. S. Li", "K. Li", "K. L. Li", "K. L. Li", "L. J. Li", "Lei Li", "M. H. Li", "M. R. Li", "P. L. Li", "P. R. Li", "Q. M. Li", "Q. X. Li", "R. Li", "S. X. Li", "T. Li", "T. Y. Li", "W. D. Li", "W. G. Li", "X. Li", "X. H. Li", "X. L. Li", "X. Y. Li", "X. Z. Li", "Y. Li", "Y. G. Li", "Y. P. Li", "Z. J. Li", "Z. Y. Li", "C. Liang", "H. Liang", "Y. F. Liang", "Y. T. Liang", "G. R. Liao", "L. B. Liao", "M. H. Liao", "Y. P. Liao", "J. Libby", "A. Limphirat", "C. C. Lin", "D. X. Lin", "L. Q. Lin", "T. Lin", "B. J. Liu", "B. X. Liu", "C. Liu", "C. X. Liu", "F. Liu", "F. H. Liu", "Feng Liu", "G. M. Liu", "H. Liu", "H. B. Liu", "H. H. Liu", "H. M. Liu", "Huihui Liu", "J. B. Liu", "J. J. Liu", "K. Liu", "K. Liu", "K. Y. Liu", "Ke Liu", "L. C. Liu", "Lu Liu", "M. H. Liu", "M. H. Liu", "P. L. Liu", "Q. Liu", "S. B. Liu", "T. Liu", "W. K. Liu", "W. M. Liu", "W. T. Liu", "X. Liu", "X. Liu", "X. K. Liu", "X. L. Liu", "X. Y. Liu", "Y. Liu", "Y. Liu", "Y. Liu", "Y. B. Liu", "Z. A. Liu", "Z. D. Liu", "Z. Q. Liu", "X. C. Lou", "F. X. Lu", "H. J. Lu", "J. G. Lu", "X. L. Lu", "Y. Lu", "Y. H. Lu", "Y. P. Lu", "Z. H. Lu", "C. L. Luo", "J. R. Luo", "J. S. Luo", "M. X. Luo", "T. Luo", "X. L. Luo", "Z. Y. Lv", "X. R. Lyu", "Y. F. Lyu", "Y. H. Lyu", "F. C. Ma", "H. L. Ma", "Heng Ma", "J. L. Ma", "L. L. Ma", "L. R. Ma", "Q. M. Ma", "R. Q. Ma", "R. Y. Ma", "T. Ma", "X. T. Ma", "X. Y. Ma", "Y. M. Ma", "F. E. Maas", "I. MacKay", "M. Maggiora", "S. Malde", "Q. A. Malik", "H. X. Mao", "Y. J. Mao", "Z. P. Mao", "S. Marcello", "A. Marshall", "F. M. Melendi", "Y. H. Meng", "Z. X. Meng", "G. Mezzadri", "H. Miao", "T. J. Min", "R. E. Mitchell", "X. H. Mo", "B. Moses", "N. Yu. Muchnoi", "J. Muskalla", "Y. Nefedov", "F. Nerling", "L. S. Nie", "I. B. Nikolaev", "Z. Ning", "S. Nisar", "Q. L. Niu", "W. D. Niu", "C. Normand", "S. L. Olsen", "Q. Ouyang", "S. Pacetti", "X. Pan", "Y. Pan", "A. Pathak", "Y. P. Pei", "M. Pelizaeus", "H. P. Peng", "X. J. Peng", "Y. Y. Peng", "K. Peters", "K. Petridis", "J. L. Ping", "R. G. Ping", "S. Plura", "V. Prasad", "F. Z. Qi", "H. R. Qi", "M. Qi", "S. Qian", "W. B. Qian", "C. F. Qiao", "J. H. Qiao", "J. J. Qin", "J. L. Qin", "L. Q. Qin", "L. Y. Qin", "P. B. Qin", "X. P. Qin", "X. P. Qin", "X. S. Qin", "Z. H. Qin", "J. F. Qiu", "Z. H. Qu", "J. Rademacker", "C. F. Redmer", "A. Rivetti", "M. Rolo", "G. Rong", "S. S. Rong", "F. Rosini", "Ch. Rosner", "M. Q. Ruan", "N. Salone", "A. Sarantsev", "Y. Schelhaas", "K. Schoenning", "M. Scodeggio", "K. Y. Shan", "W. Shan", "X. Y. Shan", "Z. J. Shang", "J. F. Shangguan", "L. G. Shao", "M. Shao", "C. P. Shen", "H. F. Shen", "W. H. Shen", "X. Y. Shen", "B. A. Shi", "H. Shi", "J. L. Shi", "J. Y. Shi", "S. Y. Shi", "X. Shi", "H. L. Song", "J. J. Song", "T. Z. Song", "W. M. Song", "Y. J. Song", "Y. X. Song", "Zirong Song", "S. Sosio", "S. Spataro", "F. Stieler", "S. S Su", "Y. J. Su", "G. B. Sun", "G. X. Sun", "H. Sun", "H. K. Sun", "J. F. Sun", "K. Sun", "L. Sun", "S. S. Sun", "T. Sun", "Y. C. Sun", "Y. H. Sun", "Y. J. Sun", "Y. Z. Sun", "Z. Q. Sun", "Z. T. Sun", "C. J. Tang", "G. Y. Tang", "J. Tang", "J. J. Tang", "L. F. Tang", "Y. A. Tang", "L. Y. Tao", "M. Tat", "J. X. Teng", "J. Y. Tian", "W. H. Tian", "Y. Tian", "Z. F. Tian", "I. Uman", "B. Wang", "B. Wang", "Bo Wang", "C. Wang", "C. Wang", "Cong Wang", "D. Y. Wang", "H. J. Wang", "J. J. Wang", "K. Wang", "L. L. Wang", "L. W. Wang", "M. Wang", "M. Wang", "N. Y. Wang", "S. Wang", "T. Wang", "T. J. Wang", "W. Wang", "W. Wang", "W. P. Wang", "X. Wang", "X. F. Wang", "X. J. Wang", "X. L. Wang", "X. N. Wang", "Y. Wang", "Y. D. Wang", "Y. F. Wang", "Y. H. Wang", "Y. J. Wang", "Y. L. Wang", "Y. N. Wang", "Y. Q. Wang", "Yaqian Wang", "Yi Wang", "Yuan Wang", "Z. Wang", "Z. L. Wang", "Z. L. Wang", "Z. Q. Wang", "Z. Y. Wang", "D. H. Wei", "H. R. Wei", "F. Weidner", "S. P. Wen", "Y. R. Wen", "U. Wiedner", "G. Wilkinson", "M. Wolke", "C. Wu", "J. F. Wu", "L. H. Wu", "L. J. Wu", "L. J. Wu", "Lianjie Wu", "S. G. Wu", "S. M. Wu", "X. Wu", "X. H. Wu", "Y. J. Wu", "Z. Wu", "L. Xia", "X. M. Xian", "B. H. Xiang", "D. Xiao", "G. Y. Xiao", "H. Xiao", "Y. L. Xiao", "Z. J. Xiao", "C. Xie", "K. J. Xie", "X. H. Xie", "Y. Xie", "Y. G. Xie", "Y. H. Xie", "Z. P. Xie", "T. Y. Xing", "C. F. Xu", "C. J. Xu", "G. F. Xu", "H. Y. Xu", "H. Y. Xu", "M. Xu", "Q. J. Xu", "Q. N. Xu", "T. D. Xu", "W. Xu", "W. L. Xu", "X. P. Xu", "Y. Xu", "Y. Xu", "Y. C. Xu", "Z. S. Xu", "F. Yan", "H. Y. Yan", "L. Yan", "W. B. Yan", "W. C. Yan", "W. H. Yan", "W. P. Yan", "X. Q. Yan", "H. J. Yang", "H. L. Yang", "H. X. Yang", "J. H. Yang", "R. J. Yang", "T. Yang", "Y. Yang", "Y. F. Yang", "Y. H. Yang", "Y. Q. Yang", "Y. X. Yang", "Y. Z. Yang", "M. Ye", "M. H. Ye", "Z. J. Ye", "Junhao Yin", "Z. Y. You", "B. X. Yu", "C. X. Yu", "G. Yu", "J. S. Yu", "L. W. Yu", "M. C. Yu", "T. Yu", "X. D. Yu", "Y. C. Yu", "C. Z. Yuan", "H. Yuan", "J. Yuan", "J. Yuan", "L. Yuan", "S. C. Yuan", "S. H. Yuan", "X. Q. Yuan", "Y. Yuan", "Z. Y. Yuan", "C. X. Yue", "Ying Yue", "A. A. Zafar", "S. H. Zeng", "X. Zeng", "Y. Zeng", "Y. J. Zeng", "Y. J. Zeng", "X. Y. Zhai", "Y. H. Zhan", "Zhang", "A. Q. Zhang", "B. L. Zhang", "B. X. Zhang", "D. H. Zhang", "G. Y. Zhang", "G. Y. Zhang", "H. Zhang", "H. Zhang", "H. C. Zhang", "H. H. Zhang", "H. Q. Zhang", "H. R. Zhang", "H. Y. Zhang", "J. Zhang", "J. Zhang", "J. J. Zhang", "J. L. Zhang", "J. Q. Zhang", "J. S. Zhang", "J. W. Zhang", "J. X. Zhang", "J. Y. Zhang", "J. Z. Zhang", "Jianyu Zhang", "L. M. Zhang", "Lei Zhang", "N. Zhang", "P. Zhang", "Q. Zhang", "Q. Y. Zhang", "R. Y. Zhang", "S. H. Zhang", "Shulei Zhang", "X. M. Zhang", "X. Y Zhang", "X. Y. Zhang", "Y. Zhang", "Y. Zhang", "Y. T. Zhang", "Y. H. Zhang", "Y. M. Zhang", "Y. P. Zhang", "Z. D. Zhang", "Z. H. Zhang", "Z. L. Zhang", "Z. L. Zhang", "Z. X. Zhang", "Z. Y. Zhang", "Z. Y. Zhang", "Z. Z. Zhang", "Zh. Zh. Zhang", "G. Zhao", "J. Y. Zhao", "J. Z. Zhao", "L. Zhao", "L. Zhao", "M. G. Zhao", "N. Zhao", "R. P. Zhao", "S. J. Zhao", "Y. B. Zhao", "Y. L. Zhao", "Y. X. Zhao", "Z. G. Zhao", "A. Zhemchugov", "B. Zheng", "B. M. Zheng", "J. P. Zheng", "W. J. Zheng", "X. R. Zheng", "Y. H. Zheng", "B. Zhong", "C. Zhong", "H. Zhou", "J. Q. Zhou", "J. Y. Zhou", "S. Zhou", "X. Zhou", "X. K. Zhou", "X. R. Zhou", "X. Y. Zhou", "Y. X. Zhou", "Y. Z. Zhou", "A. N. Zhu", "J. Zhu", "K. Zhu", "K. J. Zhu", "K. S. Zhu", "L. Zhu", "L. X. Zhu", "S. H. Zhu", "T. J. Zhu", "W. D. Zhu", "W. D. Zhu", "W. J. Zhu", "W. Z. Zhu", "Y. C. Zhu", "Z. A. Zhu", "X. Y. Zhuang", "J. H. Zou", "J. Zu"], "published_date": "2025-06-09", "title_zh": "$e^+e^-\\to XD\\bar{D}$ 及 C-偶約束 $D\\bar{D}$ 對中量子關聯性的首次觀測", "summary_zh": "本研究利用BESIII實驗在質心能量4.13-4.23 GeV下收集的7.13 fb⁻¹ e⁺e⁻碰撞數據，驗證了e⁺e⁻→D D⁻、e⁺e⁻→D* D⁻和e⁺e⁻→D* D⁻*過程中，因電荷共軛對稱性(C)產生的量子關聯。研究區分了C-偶數和C-奇數約束的過程，提出一種利用量子糾纏產生過程測量D⁰介子強子參數的方法。本研究首次證實了e⁺e⁻→X D D⁻過程中的量子關聯，並首次觀測到C-偶數約束的D D⁻系統。應用此方法測量D⁰→K⁻π⁺和D⁰⁻→K⁻π⁺衰變振幅之間的強相位差δD(Kπ)=(192.8(+11.0 + 1.9, -12.4 -2.4))°。同時探討了利用現有及未來數據集測量其他強子衰變參數和粲混合的可能性。", "audio": "audios/2506.07906v1.mp3", "timestamp": "2025-06-10T13:33:17.821895"}
{"query": "Foundation Model", "id": "2506.07559v1", "url": "http://arxiv.org/abs/2506.07559v1", "title": "Cross-channel Perception Learning for H&E-to-IHC Virtual Staining", "summary": "With the rapid development of digital pathology, virtual staining has become\na key technology in multimedia medical information systems, offering new\npossibilities for the analysis and diagnosis of pathological images. However,\nexisting H&E-to-IHC studies often overlook the cross-channel correlations\nbetween cell nuclei and cell membranes. To address this issue, we propose a\nnovel Cross-Channel Perception Learning (CCPL) strategy. Specifically, CCPL\nfirst decomposes HER2 immunohistochemical staining into Hematoxylin and DAB\nstaining channels, corresponding to cell nuclei and cell membranes,\nrespectively. Using the pathology foundation model Gigapath's Tile Encoder,\nCCPL extracts dual-channel features from both the generated and real images and\nmeasures cross-channel correlations between nuclei and membranes. The features\nof the generated and real stained images, obtained through the Tile Encoder,\nare also used to calculate feature distillation loss, enhancing the model's\nfeature extraction capabilities without increasing the inference burden.\nAdditionally, CCPL performs statistical analysis on the focal optical density\nmaps of both single channels to ensure consistency in staining distribution and\nintensity. Experimental results, based on quantitative metrics such as PSNR,\nSSIM, PCC, and FID, along with professional evaluations from pathologists,\ndemonstrate that CCPL effectively preserves pathological features, generates\nhigh-quality virtual stained images, and provides robust support for automated\npathological diagnosis using multimedia medical data.", "authors": ["Hao Yang", "JianYu Wu", "Run Fang", "Xuelian Zhao", "Yuan Ji", "Zhiyu Chen", "Guibin He", "Junceng Guo", "Yang Liu", "Xinhua Zeng"], "published_date": "2025-06-09", "title_zh": "用於蘇木精-伊紅染色至免疫組織化學染色虛擬染色的跨通道感知學習", "summary_zh": "數位病理快速發展，虛擬染色成為多媒體醫療資訊系統的關鍵技術，為病理圖像分析與診斷提供新途徑。現有H&E轉IHC研究常忽略細胞核與細胞膜間的跨通道關聯。為此，我們提出跨通道感知學習(CCPL)策略。CCPL首先將HER2免疫組織化學染色分解為對應細胞核的蘇木精和對應細胞膜的DAB染色通道。利用病理基礎模型Gigapath的Tile Encoder，CCPL提取生成圖像和真實圖像的雙通道特徵，衡量細胞核與細胞膜間的跨通道關聯。透過Tile Encoder獲得的生成圖像和真實染色圖像特徵，亦用於計算特徵蒸餾損失，提升模型特徵提取能力且不增加推論負擔。此外，CCPL對單通道焦點光密度圖進行統計分析，確保染色分佈與強度的一致性。實驗結果顯示，CCPL能有效保留病理特徵，生成高品質虛擬染色圖像，為基於多媒體醫療數據的自動病理診斷提供強大支援。", "audio": "audios/2506.07559v1.mp3", "timestamp": "2025-06-10T13:33:25.350106"}
{"query": "Diffusion Model", "id": "2506.07903v1", "url": "http://arxiv.org/abs/2506.07903v1", "title": "Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces", "summary": "Diffusion models have demonstrated remarkable performance in generating\nunimodal data across various tasks, including image, video, and text\ngeneration. On the contrary, the joint generation of multimodal data through\ndiffusion models is still in the early stages of exploration. Existing\napproaches heavily rely on external preprocessing protocols, such as tokenizers\nand variational autoencoders, to harmonize varied data representations into a\nunified, unimodal format. This process heavily demands the high accuracy of\nencoders and decoders, which can be problematic for applications with limited\ndata. To lift this restriction, we propose a novel framework for building\nmultimodal diffusion models on arbitrary state spaces, enabling native\ngeneration of coupled data across different modalities. By introducing an\ninnovative decoupled noise schedule for each modality, we enable both\nunconditional and modality-conditioned generation within a single model\nsimultaneously. We empirically validate our approach for text-image generation\nand mixed-type tabular data synthesis, demonstrating that it achieves\ncompetitive performance.", "authors": ["Kevin Rojas", "Yuchen Zhu", "Sichen Zhu", "Felix X. -F. Ye", "Molei Tao"], "published_date": "2025-06-09", "title_zh": "擴散萬物：任意狀態空間上的多模態擴散模型", "summary_zh": "擴散模型在單模態數據生成方面表現卓越，如圖像、影片及文字生成。然而，利用擴散模型聯合生成多模態數據仍處於探索初期。現有方法仰賴外部預處理，如分詞器與變分自編碼器，將不同數據表示轉化為統一的單模態格式。此過程高度依賴編碼器與解碼器的準確性，對數據有限的應用構成挑戰。為此，我們提出一種新框架，可在任意狀態空間構建多模態擴散模型，原生生成跨模態耦合數據。透過為每個模態引入解耦噪聲schedule，我們可在單一模型中同時實現無條件與模態條件生成。經文字圖像生成和混合型表格數據合成的實驗驗證，我們的模型展現了具競爭力的效能。", "audio": "audios/2506.07903v1.mp3", "timestamp": "2025-06-10T13:33:32.520256"}
